# 섹션 3: Microsoft Olive 최적화 도구 모음

## 목차
1. [소개](../../../Module04)
2. [Microsoft Olive란?](../../../Module04)
3. [설치](../../../Module04)
4. [빠른 시작 가이드](../../../Module04)
5. [예제: Qwen3를 ONNX INT4로 변환하기](../../../Module04)
6. [고급 사용법](../../../Module04)
7. [Olive Recipes 저장소](../../../Module04)
8. [모범 사례](../../../Module04)
9. [문제 해결](../../../Module04)
10. [추가 자료](../../../Module04)

## 소개

Microsoft Olive는 하드웨어를 고려한 강력하고 사용하기 쉬운 모델 최적화 도구로, 다양한 하드웨어 플랫폼에 머신러닝 모델을 배포하기 위한 최적화 과정을 간소화합니다. CPU, GPU, 또는 AI 가속기를 대상으로 하든, Olive는 모델 정확도를 유지하면서 최적의 성능을 달성할 수 있도록 도와줍니다.

## Microsoft Olive란?

Olive는 하드웨어를 고려한 모델 최적화 도구로, 모델 압축, 최적화, 컴파일 등 업계 최고 수준의 기술을 통합합니다. ONNX Runtime과 함께 E2E 추론 최적화 솔루션으로 작동합니다.

### 주요 기능

- **하드웨어 인식 최적화**: 대상 하드웨어에 가장 적합한 최적화 기술을 자동으로 선택
- **40개 이상의 내장 최적화 구성 요소**: 모델 압축, 양자화, 그래프 최적화 등 포함
- **간단한 CLI 인터페이스**: 일반적인 최적화 작업을 위한 간단한 명령어
- **다중 프레임워크 지원**: PyTorch, Hugging Face 모델, ONNX와 호환
- **인기 모델 지원**: Llama, Phi, Qwen, Gemma 등과 같은 인기 모델 아키텍처를 자동으로 최적화 가능

### 장점

- **개발 시간 단축**: 다양한 최적화 기술을 수동으로 실험할 필요 없음
- **성능 향상**: 최대 6배의 속도 향상 (일부 사례)
- **크로스 플랫폼 배포**: 최적화된 모델이 다양한 하드웨어 및 운영 체제에서 작동
- **정확도 유지**: 최적화가 성능을 향상시키면서 모델 품질 유지

## 설치

### 사전 요구 사항

- Python 3.8 이상
- pip 패키지 관리자
- 가상 환경 (권장)

### 기본 설치

가상 환경 생성 및 활성화:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```
  
자동 최적화 기능이 포함된 Olive 설치:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
  
### 선택적 종속성

Olive는 추가 기능을 위한 다양한 선택적 종속성을 제공합니다:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```
  
### 설치 확인

```bash
olive --help
```
  
성공적으로 설치되면 Olive CLI 도움말 메시지가 표시됩니다.

## 빠른 시작 가이드

### 첫 번째 최적화

Olive의 자동 최적화 기능을 사용하여 작은 언어 모델을 최적화해 봅시다:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
### 이 명령어의 역할

최적화 과정은 다음을 포함합니다: 로컬 캐시에서 모델 가져오기, ONNX 그래프 캡처 및 가중치를 ONNX 데이터 파일에 저장, ONNX 그래프 최적화, RTN 방법을 사용하여 모델을 int4로 양자화.

### 명령어 매개변수 설명

- `--model_name_or_path`: Hugging Face 모델 식별자 또는 로컬 경로
- `--output_path`: 최적화된 모델이 저장될 디렉토리
- `--device`: 대상 장치 (cpu, gpu)
- `--provider`: 실행 제공자 (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: 추론을 위한 ONNX Runtime Generate AI 사용
- `--precision`: 양자화 정밀도 (int4, int8, fp16)
- `--log_level`: 로깅 상세 수준 (0=최소, 1=상세)

## 예제: Qwen3를 ONNX INT4로 변환하기

[Hugging Face의 예제](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)를 기반으로 Qwen3 모델을 최적화하는 방법은 다음과 같습니다:

### 1단계: 모델 다운로드 (선택 사항)

다운로드 시간을 최소화하려면 필수 파일만 캐시하세요:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```
  
### 2단계: Qwen3 모델 최적화

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
### 3단계: 최적화된 모델 테스트

최적화된 모델을 테스트하기 위한 간단한 Python 스크립트를 작성하세요:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```
  
### 출력 구조

최적화 후 출력 디렉토리에는 다음이 포함됩니다:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```
  

## 고급 사용법

### 구성 파일

더 복잡한 최적화 워크플로를 위해 JSON 구성 파일을 사용할 수 있습니다:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```
  
구성을 사용하여 실행:

```bash
olive run --config config.json
```
  
### GPU 최적화

CUDA GPU 최적화를 위해:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
DirectML (Windows)용:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
### Olive를 사용한 미세 조정

Olive는 모델 미세 조정도 지원합니다:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```
  

## 모범 사례

### 1. 모델 선택
- 테스트를 위해 작은 모델로 시작 (예: 0.5B-7B 파라미터)
- 대상 모델 아키텍처가 Olive에서 지원되는지 확인

### 2. 하드웨어 고려 사항
- 최적화 대상을 배포 하드웨어와 일치시킴
- CUDA 호환 하드웨어가 있는 경우 GPU 최적화 사용
- Windows 머신에서 통합 그래픽을 사용하는 경우 DirectML 고려

### 3. 정밀도 선택
- **INT4**: 최대 압축, 약간의 정확도 손실
- **INT8**: 크기와 정확도의 균형
- **FP16**: 최소한의 정확도 손실, 중간 크기 감소

### 4. 테스트 및 검증
- 특정 사용 사례로 최적화된 모델 테스트
- 성능 지표 비교 (지연 시간, 처리량, 정확도)
- 평가를 위한 대표 입력 데이터 사용

### 5. 반복 최적화
- 빠른 결과를 위해 자동 최적화로 시작
- 세부 제어를 위해 구성 파일 사용
- 다양한 최적화 패스를 실험

## 문제 해결

### 일반적인 문제

#### 1. 설치 문제
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```
  
#### 2. CUDA/GPU 문제
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```
  
#### 3. 메모리 문제
- 최적화 중 작은 배치 크기 사용
- 먼저 높은 정밀도의 양자화 시도 (int4 대신 int8)
- 모델 캐싱을 위한 충분한 디스크 공간 확보

#### 4. 모델 로딩 오류
- 모델 경로 및 접근 권한 확인
- 모델이 `trust_remote_code=True`를 요구하는지 확인
- 필요한 모든 모델 파일이 다운로드되었는지 확인

### 도움 받기

- **문서**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub 이슈**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **예제**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Recipes 저장소

### Olive Recipes 소개

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) 저장소는 인기 있는 AI 모델을 위한 최적화 레시피 모음을 제공하여 Olive 도구를 보완합니다. 이 저장소는 공개 모델 최적화 및 독점 모델을 위한 최적화 워크플로 생성에 실용적인 참고 자료로 사용됩니다.

### 주요 기능

- **100개 이상의 사전 제작된 레시피**: 인기 모델을 위한 최적화 구성 제공
- **다중 아키텍처 지원**: 트랜스포머 모델, 비전 모델, 멀티모달 아키텍처 포함
- **하드웨어별 최적화**: CPU, GPU, 특수 가속기를 위한 레시피 제공
- **인기 모델 패밀리**: Phi, Llama, Qwen, Gemma, Mistral 등 포함

### 지원되는 모델 패밀리

#### 언어 모델
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 시리즈 (0.5B ~ 14B)
- **Google Gemma**: 다양한 Gemma 모델 구성
- **Mistral AI**: Mistral-7B 시리즈
- **DeepSeek**: R1-Distill 시리즈 모델

#### 비전 및 멀티모달 모델
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP 모델**: 다양한 CLIP-ViT 구성
- **ResNet**: ResNet-50 최적화
- **Vision Transformers**: ViT-base-patch16-224

#### 특수 모델
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: 기본 및 다국어 변형
- **Sentence Transformers**: all-MiniLM-L6-v2

### Olive Recipes 사용법

#### 방법 1: 특정 레시피 클론

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```
  
#### 방법 2: 템플릿으로 레시피 사용

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```
  
### 레시피 구조

각 레시피 디렉토리에는 일반적으로 다음이 포함됩니다:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```
  
### 예제: Phi-4-mini 레시피 사용

Phi-4-mini 레시피를 예로 들어보겠습니다:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```
  
구성 파일에는 일반적으로 다음이 포함됩니다:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```
  
### 레시피 사용자 정의

#### 대상 하드웨어 수정

대상 하드웨어를 변경하려면 `systems` 섹션을 업데이트하세요:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```
  
#### 최적화 매개변수 조정

다른 최적화 수준을 위해 `passes` 섹션을 수정하세요:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```
  
### 나만의 레시피 만들기

1. **유사한 모델로 시작**: 유사한 아키텍처의 모델 레시피 찾기
2. **모델 구성 업데이트**: 구성에서 모델 이름/경로 변경
3. **매개변수 조정**: 필요한 최적화 매개변수 수정
4. **테스트 및 검증**: 최적화를 실행하고 결과 검증
5. **공유 기여**: 레시피를 저장소에 기여 고려

### 레시피 사용의 장점

#### 1. **검증된 구성**
- 특정 모델에 대한 테스트된 최적화 설정
- 최적 매개변수 찾기에서 시행착오 방지

#### 2. **하드웨어별 튜닝**
- 다양한 실행 제공자에 대해 사전 최적화
- CPU, GPU, NPU 대상의 준비된 구성

#### 3. **포괄적 지원**
- 가장 인기 있는 오픈소스 모델 지원
- 새로운 모델 릴리스와 함께 정기 업데이트

#### 4. **커뮤니티 기여**
- AI 커뮤니티와의 협력 개발
- 공유된 지식과 모범 사례

### Olive Recipes에 기여하기

저장소에 포함되지 않은 모델을 최적화했다면:

1. **저장소 포크**: olive-recipes의 포크 생성
2. **레시피 디렉토리 생성**: 모델을 위한 새 디렉토리 추가
3. **구성 포함**: olive_config.json 및 지원 파일 추가
4. **사용법 문서화**: 명확한 README와 사용 지침 제공
5. **풀 리퀘스트 제출**: 커뮤니티에 기여

### 성능 벤치마크

많은 레시피에는 다음을 보여주는 성능 벤치마크가 포함됩니다:
- **지연 시간 개선**: 기본값 대비 일반적으로 2-6배 속도 향상
- **메모리 감소**: 양자화를 통해 메모리 사용량 50-75% 감소
- **정확도 유지**: 95-99% 정확도 보존

### AI 도구와의 통합

레시피는 다음과 원활하게 작동합니다:
- **VS Code AI Toolkit**: 모델 최적화를 위한 직접 통합
- **Azure Machine Learning**: 클라우드 기반 최적화 워크플로
- **ONNX Runtime**: 최적화된 추론 배포

## 추가 자료

### 공식 링크
- **GitHub 저장소**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipes 저장소**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime 문서**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face 예제**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### 커뮤니티 예제
- **Jupyter 노트북**: Olive GitHub 저장소에서 사용 가능 — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code 확장 프로그램**: VS Code용 AI Toolkit 개요 — https://learn.microsoft.com/azure/ai-toolkit/overview
- **블로그 게시물**: Microsoft 오픈소스 블로그 — https://opensource.microsoft.com/blog/

### 관련 도구
- **ONNX Runtime**: 고성능 추론 엔진 — https://onnxruntime.ai/
- **Hugging Face Transformers**: 많은 호환 모델의 출처 — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: 클라우드 기반 최적화 워크플로 — https://learn.microsoft.com/azure/machine-learning/

## ➡️ 다음 단계

- [04: OpenVINO Toolkit 최적화 도구 모음](./04.openvino.md)

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전이 권위 있는 출처로 간주되어야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.