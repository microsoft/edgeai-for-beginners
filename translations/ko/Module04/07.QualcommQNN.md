# 섹션 7 : Qualcomm QNN (Qualcomm Neural Network) 최적화 스위트

## 목차
1. [소개](../../../Module04)
2. [Qualcomm QNN이란?](../../../Module04)
3. [설치](../../../Module04)
4. [빠른 시작 가이드](../../../Module04)
5. [예제: QNN을 사용한 모델 변환 및 최적화](../../../Module04)
6. [고급 사용법](../../../Module04)
7. [모범 사례](../../../Module04)
8. [문제 해결](../../../Module04)
9. [추가 자료](../../../Module04)

## 소개

Qualcomm QNN (Qualcomm Neural Network)은 Qualcomm의 AI 하드웨어 가속기(예: Hexagon NPU, Adreno GPU, Kryo CPU)의 잠재력을 최대한 발휘할 수 있도록 설계된 포괄적인 AI 추론 프레임워크입니다. 모바일 기기, 엣지 컴퓨팅 플랫폼, 자동차 시스템을 대상으로 하든 관계없이 QNN은 Qualcomm의 특화된 AI 처리 장치를 활용하여 최대 성능과 에너지 효율을 제공하는 최적화된 추론 기능을 제공합니다.

## Qualcomm QNN이란?

Qualcomm QNN은 개발자가 Qualcomm의 이기종 컴퓨팅 아키텍처에서 AI 모델을 효율적으로 배포할 수 있도록 하는 통합 AI 추론 프레임워크입니다. Hexagon NPU(Neural Processing Unit), Adreno GPU, Kryo CPU에 액세스하기 위한 통합 프로그래밍 인터페이스를 제공하며, 모델 레이어와 작업에 따라 최적의 처리 장치를 자동으로 선택합니다.

### 주요 기능

- **이기종 컴퓨팅**: NPU, GPU, CPU에 대한 통합 액세스 및 자동 작업 분배
- **하드웨어 인식 최적화**: Qualcomm Snapdragon 플랫폼에 특화된 최적화
- **양자화 지원**: 고급 INT8, INT16 및 혼합 정밀도 양자화 기술
- **모델 변환 도구**: TensorFlow, PyTorch, ONNX, Caffe 모델 직접 지원
- **엣지 AI 최적화**: 전력 효율성을 중점으로 모바일 및 엣지 배포 시나리오에 최적화

### 혜택

- **최대 성능**: 특화된 AI 하드웨어를 활용하여 최대 15배 성능 향상
- **전력 효율성**: 모바일 및 배터리 구동 장치에 최적화된 지능형 전력 관리
- **저지연**: 실시간 애플리케이션을 위한 하드웨어 가속 추론
- **확장 가능한 배포**: Qualcomm 생태계 전반에 걸쳐 스마트폰에서 자동차 플랫폼까지
- **생산 준비 완료**: 수백만 대의 배포된 장치에서 사용된 검증된 프레임워크

## 설치

### 사전 요구 사항

- Qualcomm QNN SDK (Qualcomm 등록 필요)
- Python 3.7 이상
- 호환 가능한 Qualcomm 하드웨어 또는 시뮬레이터
- Android NDK (모바일 배포용)
- Linux 또는 Windows 개발 환경

### QNN SDK 설정

1. **등록 및 다운로드**: Qualcomm Developer Network를 방문하여 QNN SDK를 등록하고 다운로드
2. **SDK 압축 해제**: 개발 디렉토리에 QNN SDK를 압축 해제
3. **환경 변수 설정**: QNN 도구 및 라이브러리 경로 구성

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Python 환경 설정

가상 환경 생성 및 활성화:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

필요한 Python 패키지 설치:

```bash
pip install numpy tensorflow torch onnx
```

### 설치 확인

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

성공하면 각 QNN 도구에 대한 도움말 정보를 확인할 수 있습니다.

## 빠른 시작 가이드

### 첫 번째 모델 변환

간단한 PyTorch 모델을 Qualcomm 하드웨어에서 실행하도록 변환해 봅시다:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### ONNX를 QNN 형식으로 변환

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### QNN 모델 라이브러리 생성

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### 이 과정이 하는 일

최적화 워크플로는 원본 모델을 ONNX 형식으로 변환하고, ONNX를 QNN 중간 표현으로 변환하며, 하드웨어 특화 최적화를 적용하고, 배포를 위한 컴파일된 모델 라이브러리를 생성하는 과정을 포함합니다.

### 주요 매개변수 설명

- `--input_network`: 원본 ONNX 모델 파일
- `--output_path`: 생성된 C++ 소스 파일
- `--input_dim`: 최적화를 위한 입력 텐서 차원
- `--quantization_overrides`: 사용자 정의 양자화 구성
- `-t x86_64-linux-clang`: 대상 아키텍처 및 컴파일러

## 예제: QNN을 사용한 모델 변환 및 최적화

### 단계 1: 양자화를 활용한 고급 모델 변환

다음은 변환 중 사용자 정의 양자화를 적용하는 방법입니다:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

사용자 정의 양자화로 변환:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### 단계 2: 다중 백엔드 최적화

NPU, GPU, CPU 간 이기종 실행을 구성:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### 단계 3: 배포를 위한 컨텍스트 바이너리 생성

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### 단계 4: QNN 런타임을 사용한 추론

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### 출력 구조

최적화 후 배포 디렉토리에는 다음이 포함됩니다:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## 고급 사용법

### 사용자 정의 백엔드 구성

특정 백엔드 최적화를 구성:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### 동적 양자화

더 나은 정확도를 위해 런타임에서 양자화 적용:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### 성능 프로파일링

다양한 백엔드에서 성능 모니터링:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### 자동 백엔드 선택

모델 특성에 따라 지능형 백엔드 선택 구현:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## 모범 사례

### 1. 모델 아키텍처 최적화
- **레이어 융합**: Conv+BatchNorm+ReLU와 같은 작업을 결합하여 NPU 활용도 향상
- **깊이별 분리 컨볼루션**: 모바일 배포를 위해 표준 컨볼루션 대신 선호
- **양자화 친화적 설계**: ReLU 활성화를 사용하고 양자화가 잘 되지 않는 작업은 피하기

### 2. 양자화 전략
- **사후 양자화**: 빠른 배포를 위해 이 방법으로 시작
- **캘리브레이션 데이터셋**: 모든 입력 변형을 포함하는 대표 데이터 사용
- **혼합 정밀도**: 대부분의 레이어에 INT8 사용, 중요한 레이어는 높은 정밀도 유지

### 3. 백엔드 선택 지침
- **NPU (HTP)**: CNN 워크로드, 양자화된 모델, 전력 민감 애플리케이션에 최적
- **GPU**: 계산 집약적 작업, 대형 모델, FP16 정밀도에 최적
- **CPU**: 지원되지 않는 작업 및 디버깅을 위한 대체 옵션

### 4. 성능 최적화
- **배치 크기**: 실시간 애플리케이션에는 배치 크기 1 사용, 처리량에는 더 큰 배치 사용
- **입력 전처리**: 데이터 복사 및 변환 오버헤드 최소화
- **컨텍스트 재사용**: 런타임 컴파일 오버헤드를 피하기 위해 컨텍스트를 미리 컴파일

### 5. 메모리 관리
- **텐서 할당**: 런타임 오버헤드를 피하기 위해 가능한 경우 정적 할당 사용
- **메모리 풀**: 자주 할당되는 텐서를 위한 사용자 정의 메모리 풀 구현
- **버퍼 재사용**: 추론 호출 간 입력/출력 버퍼 재사용

### 6. 전력 최적화
- **성능 모드**: 열 제한에 따라 적절한 성능 모드 사용
- **동적 주파수 스케일링**: 작업량에 따라 시스템이 주파수를 조정하도록 허용
- **유휴 상태 관리**: 사용하지 않을 때 리소스를 적절히 해제

## 문제 해결

### 일반적인 문제

#### 1. SDK 설치 문제
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. 모델 변환 오류
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. 양자화 문제
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. 성능 문제
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. 메모리 문제
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. 백엔드 호환성
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### 성능 디버깅

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### 도움 받기

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN 문서**: SDK 패키지에서 제공
- **커뮤니티 포럼**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **기술 지원**: Qualcomm 개발자 포털을 통해 제공

## 추가 자료

### 공식 링크
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon 플랫폼**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **개발자 포털**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI 엔진**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### 학습 자료
- **시작 가이드**: QNN SDK 문서에서 제공
- **모델 저장소**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **최적화 가이드**: SDK 문서에 포괄적인 최적화 지침 포함
- **비디오 튜토리얼**: [Qualcomm Developer YouTube 채널](https://www.youtube.com/c/QualcommDeveloperNetwork)

### 통합 도구
- **SNPE (레거시)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Qualcomm 하드웨어에 최적화된 모델
- **Android Neural Networks API**: Android NNAPI와의 통합
- **TensorFlow Lite Delegate**: TFLite용 Qualcomm 대리자

### 성능 벤치마크
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI 연구**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### 커뮤니티 예제
- **샘플 애플리케이션**: QNN SDK 예제 디렉토리에서 제공
- **GitHub 저장소**: 커뮤니티 기여 예제 및 도구
- **기술 블로그**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### 관련 도구
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - 고급 양자화 및 압축 기술
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - 비교 및 대체 배포용
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - 크로스 플랫폼 추론 엔진

### 하드웨어 사양
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon 플랫폼**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ 다음 단계

[모듈 5: SLMOps 및 생산 배포](../Module05/README.md)를 탐색하여 소형 언어 모델 라이프사이클 관리의 운영 측면에 대해 알아보세요.

---

**면책 조항**:  
이 문서는 AI 번역 서비스 [Co-op Translator](https://github.com/Azure/co-op-translator)를 사용하여 번역되었습니다. 정확성을 위해 최선을 다하고 있지만, 자동 번역에는 오류나 부정확성이 포함될 수 있습니다. 원본 문서의 원어 버전을 권위 있는 출처로 간주해야 합니다. 중요한 정보의 경우, 전문적인 인간 번역을 권장합니다. 이 번역 사용으로 인해 발생하는 오해나 잘못된 해석에 대해 책임을 지지 않습니다.