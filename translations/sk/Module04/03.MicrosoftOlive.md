# Sekcia 3: Microsoft Olive Optimization Suite

## Obsah
1. [Úvod](../../../Module04)
2. [Čo je Microsoft Olive?](../../../Module04)
3. [Inštalácia](../../../Module04)
4. [Rýchly sprievodca](../../../Module04)
5. [Príklad: Konverzia Qwen3 na ONNX INT4](../../../Module04)
6. [Pokročilé použitie](../../../Module04)
7. [Repozitár Olive Recipes](../../../Module04)
8. [Najlepšie postupy](../../../Module04)
9. [Riešenie problémov](../../../Module04)
10. [Ďalšie zdroje](../../../Module04)

## Úvod

Microsoft Olive je výkonný a jednoduchý nástroj na optimalizáciu modelov, ktorý je prispôsobený hardvéru. Uľahčuje proces optimalizácie modelov strojového učenia pre nasadenie na rôznych hardvérových platformách. Či už cielite na CPU, GPU alebo špecializované AI akcelerátory, Olive vám pomôže dosiahnuť optimálny výkon pri zachovaní presnosti modelu.

## Čo je Microsoft Olive?

Olive je jednoduchý nástroj na optimalizáciu modelov prispôsobený hardvéru, ktorý kombinuje špičkové techniky v oblasti kompresie, optimalizácie a kompilácie modelov. Funguje s ONNX Runtime ako komplexné riešenie pre optimalizáciu inferencie.

### Kľúčové vlastnosti

- **Optimalizácia prispôsobená hardvéru**: Automaticky vyberá najlepšie techniky optimalizácie pre váš cieľový hardvér
- **Viac ako 40 zabudovaných komponentov na optimalizáciu**: Zahŕňa kompresiu modelov, kvantizáciu, optimalizáciu grafov a ďalšie
- **Jednoduché CLI rozhranie**: Jednoduché príkazy pre bežné úlohy optimalizácie
- **Podpora viacerých frameworkov**: Funguje s PyTorch, Hugging Face modelmi a ONNX
- **Podpora populárnych modelov**: Olive dokáže automaticky optimalizovať populárne architektúry modelov ako Llama, Phi, Qwen, Gemma a ďalšie

### Výhody

- **Zníženie času vývoja**: Nie je potrebné manuálne experimentovať s rôznymi technikami optimalizácie
- **Zlepšenie výkonu**: Výrazné zrýchlenie (až 6x v niektorých prípadoch)
- **Nasadenie na rôznych platformách**: Optimalizované modely fungujú na rôznych hardvéroch a operačných systémoch
- **Zachovanie presnosti**: Optimalizácie zachovávajú kvalitu modelu pri zlepšení výkonu

## Inštalácia

### Predpoklady

- Python 3.8 alebo vyšší
- Správca balíkov pip
- Virtuálne prostredie (odporúčané)

### Základná inštalácia

Vytvorte a aktivujte virtuálne prostredie:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Nainštalujte Olive s funkciami automatickej optimalizácie:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Voliteľné závislosti

Olive ponúka rôzne voliteľné závislosti pre ďalšie funkcie:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Overenie inštalácie

```bash
olive --help
```

Ak je inštalácia úspešná, mali by ste vidieť pomocnú správu Olive CLI.

## Rýchly sprievodca

### Vaša prvá optimalizácia

Optimalizujme malý jazykový model pomocou funkcie automatickej optimalizácie Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Čo tento príkaz robí

Proces optimalizácie zahŕňa: získanie modelu z lokálnej cache, zachytenie ONNX grafu a uloženie váh do ONNX dátového súboru, optimalizáciu ONNX grafu a kvantizáciu modelu na int4 pomocou metódy RTN.

### Vysvetlenie parametrov príkazu

- `--model_name_or_path`: Identifikátor modelu Hugging Face alebo lokálna cesta
- `--output_path`: Adresár, kde bude uložený optimalizovaný model
- `--device`: Cieľové zariadenie (cpu, gpu)
- `--provider`: Poskytovateľ vykonávania (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Použitie ONNX Runtime Generate AI pre inferenciu
- `--precision`: Presnosť kvantizácie (int4, int8, fp16)
- `--log_level`: Úroveň podrobnosti logovania (0=minimálna, 1=podrobná)

## Príklad: Konverzia Qwen3 na ONNX INT4

Na základe poskytnutého príkladu Hugging Face na [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), tu je postup optimalizácie modelu Qwen3:

### Krok 1: Stiahnutie modelu (voliteľné)

Na minimalizáciu času sťahovania cacheujte iba potrebné súbory:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Krok 2: Optimalizácia modelu Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Krok 3: Testovanie optimalizovaného modelu

Vytvorte jednoduchý Python skript na testovanie vášho optimalizovaného modelu:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Štruktúra výstupu

Po optimalizácii bude váš výstupný adresár obsahovať:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Pokročilé použitie

### Konfiguračné súbory

Pre zložitejšie pracovné postupy optimalizácie môžete použiť JSON konfiguračné súbory:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Spustenie s konfiguráciou:

```bash
olive run --config config.json
```

### Optimalizácia GPU

Pre optimalizáciu CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Pre DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Jemné doladenie s Olive

Olive podporuje aj jemné doladenie modelov:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Najlepšie postupy

### 1. Výber modelu
- Začnite s menšími modelmi na testovanie (napr. 0.5B-7B parametrov)
- Uistite sa, že cieľová architektúra modelu je podporovaná Olive

### 2. Hardvérové úvahy
- Prispôsobte cieľ optimalizácie vášmu hardvéru na nasadenie
- Použite optimalizáciu GPU, ak máte hardvér kompatibilný s CUDA
- Zvážte DirectML pre Windows zariadenia s integrovanou grafikou

### 3. Výber presnosti
- **INT4**: Maximálna kompresia, mierna strata presnosti
- **INT8**: Dobrá rovnováha medzi veľkosťou a presnosťou
- **FP16**: Minimálna strata presnosti, stredné zníženie veľkosti

### 4. Testovanie a validácia
- Vždy testujte optimalizované modely s vašimi konkrétnymi prípadmi použitia
- Porovnajte výkonnostné metriky (latencia, priepustnosť, presnosť)
- Použite reprezentatívne vstupné údaje na hodnotenie

### 5. Iteratívna optimalizácia
- Začnite s automatickou optimalizáciou pre rýchle výsledky
- Použite konfiguračné súbory na jemné doladenie
- Experimentujte s rôznymi optimalizačnými krokmi

## Riešenie problémov

### Bežné problémy

#### 1. Problémy s inštaláciou
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problémy s CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problémy s pamäťou
- Použite menšie veľkosti batchov počas optimalizácie
- Skúste kvantizáciu s vyššou presnosťou najskôr (int8 namiesto int4)
- Uistite sa, že máte dostatok miesta na disku pre cache modelu

#### 4. Chyby pri načítaní modelu
- Overte cestu k modelu a prístupové povolenia
- Skontrolujte, či model vyžaduje `trust_remote_code=True`
- Uistite sa, že všetky potrebné súbory modelu sú stiahnuté

### Získanie pomoci

- **Dokumentácia**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Príklady**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Repozitár Olive Recipes

### Úvod do Olive Recipes

Repozitár [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) dopĺňa hlavný nástroj Olive tým, že poskytuje komplexnú zbierku pripravených optimalizačných receptov pre populárne AI modely. Tento repozitár slúži ako praktický referenčný zdroj pre optimalizáciu verejne dostupných modelov aj pre vytváranie pracovných postupov optimalizácie pre vlastné modely.

### Kľúčové vlastnosti

- **Viac ako 100 predpripravených receptov**: Pripravené konfigurácie optimalizácie pre populárne modely
- **Podpora viacerých architektúr**: Zahŕňa transformer modely, vizuálne modely a multimodálne architektúry
- **Optimalizácie prispôsobené hardvéru**: Recepty prispôsobené pre CPU, GPU a špecializované akcelerátory
- **Populárne rodiny modelov**: Zahŕňa Phi, Llama, Qwen, Gemma, Mistral a mnoho ďalších

### Podporované rodiny modelov

Repozitár obsahuje optimalizačné recepty pre:

#### Jazykové modely
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 séria (0.5B až 14B)
- **Google Gemma**: Rôzne konfigurácie modelov Gemma
- **Mistral AI**: Séria Mistral-7B
- **DeepSeek**: Modely série R1-Distill

#### Vizuálne a multimodálne modely
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP modely**: Rôzne konfigurácie CLIP-ViT
- **ResNet**: Optimalizácie ResNet-50
- **Vision Transformers**: ViT-base-patch16-224

#### Špecializované modely
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Základné a viacjazyčné varianty
- **Sentence Transformers**: all-MiniLM-L6-v2

### Používanie Olive Recipes

#### Metóda 1: Klonovanie konkrétneho receptu

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Metóda 2: Použitie receptu ako šablóny

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Štruktúra receptu

Každý adresár receptu zvyčajne obsahuje:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Príklad: Použitie receptu Phi-4-mini

Použime recept Phi-4-mini ako príklad:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Konfiguračný súbor zvyčajne obsahuje:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Prispôsobenie receptov

#### Úprava cieľového hardvéru

Na zmenu cieľového hardvéru aktualizujte sekciu `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Úprava parametrov optimalizácie

Upravte sekciu `passes` pre rôzne úrovne optimalizácie:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Vytvorenie vlastného receptu

1. **Začnite s podobným modelom**: Nájdite recept pre model s podobnou architektúrou
2. **Aktualizujte konfiguráciu modelu**: Zmeňte názov/cestu modelu v konfigurácii
3. **Upravte parametre**: Prispôsobte parametre optimalizácie podľa potreby
4. **Testujte a validujte**: Spustite optimalizáciu a validujte výsledky
5. **Prispievajte späť**: Zvážte prispievanie vášho receptu do repozitára

### Výhody používania receptov

#### 1. **Overené konfigurácie**
- Testované nastavenia optimalizácie pre konkrétne modely
- Vyhýba sa pokusom a omylom pri hľadaní optimálnych parametrov

#### 2. **Ladenie prispôsobené hardvéru**
- Predoptimalizované pre rôznych poskytovateľov vykonávania
- Pripravené konfigurácie pre CPU, GPU a NPU ciele

#### 3. **Komplexné pokrytie**
- Podpora najpopulárnejších open-source modelov
- Pravidelné aktualizácie s novými vydaniami modelov

#### 4. **Príspevky komunity**
- Spoločný vývoj s AI komunitou
- Zdieľané znalosti a najlepšie postupy

### Prispievanie do Olive Recipes

Ak ste optimalizovali model, ktorý nie je pokrytý v repozitári:

1. **Forknite repozitár**: Vytvorte si vlastný fork olive-recipes
2. **Vytvorte adresár receptu**: Pridajte nový adresár pre váš model
3. **Zahrňte konfiguráciu**: Pridajte olive_config.json a podporné súbory
4. **Zdokumentujte použitie**: Poskytnite jasný README s pokynmi
5. **Odošlite Pull Request**: Prispievajte späť do komunity

### Výkonnostné porovnania

Mnohé recepty obsahujú výkonnostné porovnania, ktoré ukazujú:
- **Zlepšenie latencie**: Typické zrýchlenie 2-6x oproti základnej verzii
- **Zníženie pamäte**: Zníženie pamäťového využitia o 50-75% pomocou kvantizácie
- **Zachovanie presnosti**: Zachovanie presnosti na úrovni 95-99%

### Integrácia s AI nástrojmi

Recepty fungujú bezproblémovo s:
- **VS Code AI Toolkit**: Priama integrácia pre optimalizáciu modelov
- **Azure Machine Learning**: Optimalizačné pracovné postupy v cloude
- **ONNX Runtime**: Optimalizované nasadenie inferencie

## Ďalšie zdroje

### Oficiálne odkazy
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Repozitár Olive Recipes**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime Dokumentácia**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Príklad**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Príklady komunity
- **Jupyter Notebooks**: Dostupné v Olive GitHub repozitári — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Rozšírenie**: AI Toolkit pre VS Code prehľad — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogové príspevky**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Súvisiace nástroje
- **ONNX Runtime**: Vysoko výkonný inferenčný engine — https://onnxruntime.ai/
- **Hugging Face Transformers**: Zdroj mnohých kompatibilných modelov — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Optimalizačné pracovné postupy v cloude — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Čo ďalej

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keď sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho rodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nenesieme zodpovednosť za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.