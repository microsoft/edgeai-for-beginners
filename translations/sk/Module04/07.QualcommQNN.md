# Sekcia 7: Qualcomm QNN (Qualcomm Neural Network) Optimalizačný balík

## Obsah
1. [Úvod](../../../Module04)
2. [Čo je Qualcomm QNN?](../../../Module04)
3. [Inštalácia](../../../Module04)
4. [Rýchly sprievodca](../../../Module04)
5. [Príklad: Konverzia a optimalizácia modelov pomocou QNN](../../../Module04)
6. [Pokročilé použitie](../../../Module04)
7. [Najlepšie postupy](../../../Module04)
8. [Riešenie problémov](../../../Module04)
9. [Ďalšie zdroje](../../../Module04)

## Úvod

Qualcomm QNN (Qualcomm Neural Network) je komplexný rámec pre AI inferenciu navrhnutý na využitie plného potenciálu AI hardvérových akcelerátorov od Qualcommu, vrátane Hexagon NPU, Adreno GPU a Kryo CPU. Či už sa zameriavate na mobilné zariadenia, platformy pre edge computing alebo automobilové systémy, QNN poskytuje optimalizované inferenčné schopnosti, ktoré využívajú špecializované AI procesory Qualcommu na maximálny výkon a energetickú efektívnosť.

## Čo je Qualcomm QNN?

Qualcomm QNN je jednotný rámec pre AI inferenciu, ktorý umožňuje vývojárom efektívne nasadzovať AI modely naprieč heterogénnou výpočtovou architektúrou Qualcommu. Poskytuje jednotné programovacie rozhranie na prístup k Hexagon NPU (Neural Processing Unit), Adreno GPU a Kryo CPU, pričom automaticky vyberá optimálnu výpočtovú jednotku pre rôzne vrstvy modelu a operácie.

### Kľúčové vlastnosti

- **Heterogénne výpočty**: Jednotný prístup k NPU, GPU a CPU s automatickým rozdelením záťaže
- **Optimalizácia podľa hardvéru**: Špecializované optimalizácie pre platformy Qualcomm Snapdragon
- **Podpora kvantizácie**: Pokročilé techniky kvantizácie INT8, INT16 a zmiešanej presnosti
- **Nástroje na konverziu modelov**: Priama podpora modelov TensorFlow, PyTorch, ONNX a Caffe
- **Optimalizácia pre Edge AI**: Navrhnuté špeciálne pre mobilné a edge nasadenia s dôrazom na energetickú efektívnosť

### Výhody

- **Maximálny výkon**: Využitie špecializovaného AI hardvéru na zlepšenie výkonu až 15x
- **Energetická efektívnosť**: Optimalizované pre mobilné zariadenia a zariadenia na batérie s inteligentným riadením energie
- **Nízka latencia**: Hardvérom akcelerovaná inferencia s minimálnou režijnou záťažou pre aplikácie v reálnom čase
- **Škálovateľné nasadenie**: Od smartfónov po automobilové platformy naprieč ekosystémom Qualcommu
- **Pripravené na produkciu**: Overený rámec používaný v miliónoch nasadených zariadení

## Inštalácia

### Predpoklady

- Qualcomm QNN SDK (vyžaduje registráciu u Qualcommu)
- Python 3.7 alebo vyšší
- Kompatibilný hardvér Qualcomm alebo simulátor
- Android NDK (pre mobilné nasadenie)
- Vývojové prostredie Linux alebo Windows

### Nastavenie QNN SDK

1. **Registrácia a stiahnutie**: Navštívte Qualcomm Developer Network na registráciu a stiahnutie QNN SDK
2. **Rozbalenie SDK**: Rozbaľte QNN SDK do vášho vývojového adresára
3. **Nastavenie premenných prostredia**: Konfigurujte cesty pre nástroje a knižnice QNN

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Nastavenie Python prostredia

Vytvorte a aktivujte virtuálne prostredie:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Nainštalujte požadované Python balíčky:

```bash
pip install numpy tensorflow torch onnx
```

### Overenie inštalácie

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Ak je inštalácia úspešná, mali by ste vidieť informácie o pomocníkovi pre každý nástroj QNN.

## Rýchly sprievodca

### Vaša prvá konverzia modelu

Konvertujme jednoduchý PyTorch model na spustenie na hardvéri Qualcomm:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Konverzia ONNX do formátu QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Generovanie knižnice modelu QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Čo tento proces robí

Optimalizačný pracovný postup zahŕňa: konverziu pôvodného modelu do formátu ONNX, preklad ONNX do medzireprezentácie QNN, aplikáciu optimalizácií špecifických pre hardvér a generovanie skompilovanej knižnice modelu na nasadenie.

### Vysvetlenie kľúčových parametrov

- `--input_network`: Zdrojový ONNX súbor modelu
- `--output_path`: Generovaný C++ zdrojový súbor
- `--input_dim`: Rozmery vstupného tenzora pre optimalizáciu
- `--quantization_overrides`: Vlastná konfigurácia kvantizácie
- `-t x86_64-linux-clang`: Cieľová architektúra a kompilátor

## Príklad: Konverzia a optimalizácia modelov pomocou QNN

### Krok 1: Pokročilá konverzia modelu s kvantizáciou

Tu je postup, ako aplikovať vlastnú kvantizáciu počas konverzie:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Konverzia s vlastnou kvantizáciou:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Krok 2: Optimalizácia pre viac backendov

Konfigurácia pre heterogénne vykonávanie naprieč NPU, GPU a CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Krok 3: Vytvorenie binárneho kontextu pre nasadenie

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Krok 4: Inferencia s QNN Runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Štruktúra výstupu

Po optimalizácii bude váš adresár nasadenia obsahovať:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Pokročilé použitie

### Konfigurácia vlastného backendu

Konfigurácia špecifických optimalizácií backendu:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dynamická kvantizácia

Aplikácia kvantizácie počas behu pre lepšiu presnosť:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Profilovanie výkonu

Monitorovanie výkonu naprieč rôznymi backendmi:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Automatický výber backendu

Implementácia inteligentného výberu backendu na základe charakteristík modelu:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Najlepšie postupy

### 1. Optimalizácia architektúry modelu
- **Fúzia vrstiev**: Kombinujte operácie ako Conv+BatchNorm+ReLU pre lepšie využitie NPU
- **Hĺbkovo separovateľné konvolúcie**: Uprednostnite ich pred štandardnými konvolúciami pre mobilné nasadenie
- **Dizajny priateľské ku kvantizácii**: Používajte ReLU aktivácie a vyhýbajte sa operáciám, ktoré sa ťažko kvantizujú

### 2. Kvantizačná stratégia
- **Kvantizácia po tréningu**: Začnite s týmto pre rýchle nasadenie
- **Kalibračná dátová sada**: Používajte reprezentatívne dáta pokrývajúce všetky variácie vstupov
- **Zmiešaná presnosť**: Používajte INT8 pre väčšinu vrstiev, kritické vrstvy ponechajte vo vyššej presnosti

### 3. Usmernenia pre výber backendu
- **NPU (HTP)**: Najlepšie pre CNN pracovné záťaže, kvantizované modely a aplikácie citlivé na energiu
- **GPU**: Optimálne pre výpočtovo náročné operácie, väčšie modely a FP16 presnosť
- **CPU**: Záložné riešenie pre nepodporované operácie a ladenie

### 4. Optimalizácia výkonu
- **Veľkosť dávky**: Používajte veľkosť dávky 1 pre aplikácie v reálnom čase, väčšie dávky pre priepustnosť
- **Predspracovanie vstupu**: Minimalizujte kopírovanie dát a režijné náklady na konverziu
- **Opätovné použitie kontextu**: Predkompilujte kontexty, aby ste sa vyhli režijným nákladom na kompiláciu počas behu

### 5. Správa pamäte
- **Alokácia tenzorov**: Používajte statickú alokáciu, ak je to možné, aby ste sa vyhli režijným nákladom počas behu
- **Pamäťové pooly**: Implementujte vlastné pamäťové pooly pre často alokované tenzory
- **Opätovné použitie bufferov**: Opätovne používajte vstupné/výstupné buffery naprieč inferenčnými volaniami

### 6. Optimalizácia energie
- **Režimy výkonu**: Používajte vhodné režimy výkonu na základe tepelných obmedzení
- **Dynamické škálovanie frekvencie**: Umožnite systému škálovať frekvenciu na základe záťaže
- **Správa nečinného stavu**: Správne uvoľnite zdroje, keď nie sú používané

## Riešenie problémov

### Bežné problémy

#### 1. Problémy s inštaláciou SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Chyby pri konverzii modelu
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Problémy s kvantizáciou
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Problémy s výkonom
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Problémy s pamäťou
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Kompatibilita backendu
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Ladenie výkonu

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Získanie pomoci

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN Dokumentácia**: Dostupná v balíku SDK
- **Fóra komunity**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Technická podpora**: Prostredníctvom portálu Qualcomm developer

## Ďalšie zdroje

### Oficiálne odkazy
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon Platformy**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Developer Portal**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Zdroje na učenie
- **Sprievodca začiatkom**: Dostupný v dokumentácii QNN SDK
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Optimalizačný sprievodca**: Dokumentácia SDK obsahuje komplexné pokyny na optimalizáciu
- **Video tutoriály**: [Qualcomm Developer YouTube Channel](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Nástroje na integráciu
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Predoptimalizované modely pre hardvér Qualcomm
- **Android Neural Networks API**: Integrácia s Android NNAPI
- **TensorFlow Lite Delegate**: Qualcomm delegát pre TFLite

### Výkonnostné benchmarky
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Príklady z komunity
- **Ukážkové aplikácie**: Dostupné v adresári príkladov QNN SDK
- **GitHub Repozitáre**: Príklady a nástroje prispievané komunitou
- **Technické blogy**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Súvisiace nástroje
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Pokročilé techniky kvantizácie a kompresie
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Na porovnanie a záložné nasadenie
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Multiplatformový inferenčný engine

### Špecifikácie hardvéru
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon Platformy**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Čo ďalej

Pokračujte vo vašej ceste Edge AI preskúmaním [Modulu 5: SLMOps a nasadenie do produkcie](../Module05/README.md), kde sa dozviete o operačných aspektoch správy životného cyklu malých jazykových modelov.

---

**Zrieknutie sa zodpovednosti**:  
Tento dokument bol preložený pomocou služby AI prekladu [Co-op Translator](https://github.com/Azure/co-op-translator). Aj keď sa snažíme o presnosť, prosím, berte na vedomie, že automatizované preklady môžu obsahovať chyby alebo nepresnosti. Pôvodný dokument v jeho rodnom jazyku by mal byť považovaný za autoritatívny zdroj. Pre kritické informácie sa odporúča profesionálny ľudský preklad. Nenesieme zodpovednosť za akékoľvek nedorozumenia alebo nesprávne interpretácie vyplývajúce z použitia tohto prekladu.