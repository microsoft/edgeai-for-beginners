# AI-agendid ja vÃ¤ikemudelid: pÃµhjalik juhend

## Sissejuhatus

Selles juhendis uurime AI-agente ja vÃ¤ikemudeleid (SLM) ning nende tÃ¤iustatud rakendusstrateegiaid serva-arvutuskeskkondades. KÃ¤sitleme agentliku AI pÃµhikontseptsioone, SLM optimeerimistehnikaid, praktilisi juurutusstrateegiaid ressursipiirangutega seadmetele ja Microsoft Agent Frameworki tootmisvalmis agentide sÃ¼steemide loomiseks.

Tehisintellekti maastik kogeb 2025. aastal paradigmatilist muutust. Kui 2023. aasta oli vestlusrobotite ja 2024. aasta kaaspilootide buumi aasta, siis 2025 kuulub AI-agentidele â€“ intelligentsetele sÃ¼steemidele, mis mÃµtlevad, arutlevad, planeerivad, kasutavad tÃ¶Ã¶riistu ja tÃ¤idavad Ã¼lesandeid minimaalse inimsekkumisega, tuginedes Ã¼ha enam tÃµhusatele vÃ¤ikemudelitele. Microsoft Agent Framework tÃµuseb juhtivaks lahenduseks nende intelligentsete sÃ¼steemide loomiseks, pakkudes offline servapÃµhiseid vÃµimalusi.

## Ã•pieesmÃ¤rgid

Selle juhendi lÃµpuks oskate:

- ğŸ¤– MÃµista AI-agentide ja agentlike sÃ¼steemide pÃµhikontseptsioone
- ğŸ”¬ Tuvastada vÃ¤ikemudelite eelised suurte mudelite ees agentlikes rakendustes
- ğŸš€ Ã•ppida tÃ¤iustatud SLM juurutusstrateegiaid serva-arvutuskeskkondades
- ğŸ“± Rakendada praktilisi SLM-pÃµhiseid agente reaalses maailmas
- ğŸ—ï¸ Luua tootmisvalmis agente Microsoft Agent Frameworki abil
- ğŸŒ Juurutada offline servapÃµhiseid agente kohaliku LLM-i ja SLM-i integreerimisega
- ğŸ”§ Integreerida Microsoft Agent Framework Foundry Localiga serva juurutamiseks

## AI-agentide mÃµistmine: alused ja klassifikatsioon

### Definitsioon ja pÃµhikontseptsioonid

Tehisintellekti (AI) agent viitab sÃ¼steemile vÃµi programmile, mis suudab autonoomselt tÃ¤ita Ã¼lesandeid kasutaja vÃµi teise sÃ¼steemi nimel, kujundades oma tÃ¶Ã¶voogu ja kasutades olemasolevaid tÃ¶Ã¶riistu. Erinevalt traditsioonilisest AI-st, mis lihtsalt vastab teie kÃ¼simustele, suudab agent iseseisvalt tegutseda eesmÃ¤rkide saavutamiseks.

### Agentide klassifikatsiooniraamistik

Agentide piiride mÃµistmine aitab valida sobivaid agentide tÃ¼Ã¼pe erinevate arvutuskeskkondade jaoks:

- **ğŸ”¬ Lihtsad refleksagendid**: ReeglipÃµhised sÃ¼steemid, mis reageerivad koheselt tajudele (termostaadid, lihtne automatiseerimine)
- **ğŸ“± MudelipÃµhised agendid**: SÃ¼steemid, mis sÃ¤ilitavad sisemist olekut ja mÃ¤lu (robot-tolmuimejad, navigatsioonisÃ¼steemid)
- **âš–ï¸ EesmÃ¤rgipÃµhised agendid**: SÃ¼steemid, mis planeerivad ja tÃ¤idavad jÃ¤rjestusi eesmÃ¤rkide saavutamiseks (marsruudi planeerijad, Ã¼lesannete ajastajad)
- **ğŸ§  Ã•ppivad agendid**: Kohanduvad sÃ¼steemid, mis aja jooksul parandavad oma jÃµudlust (soovitussÃ¼steemid, isikupÃ¤rastatud assistendid)

### AI-agentide peamised eelised

AI-agendid pakuvad mitmeid pÃµhilisi eeliseid, mis muudavad nad ideaalseks serva-arvutuskeskkondade rakendustes:

**Operatiivne autonoomia**: Agendid vÃµimaldavad iseseisvat Ã¼lesannete tÃ¤itmist ilma pideva inimjÃ¤relevalveta, muutes nad ideaalseks reaalajas rakendustes. Nad vajavad minimaalset jÃ¤relevalvet, sÃ¤ilitades samas kohanemisvÃµime, vÃµimaldades juurutamist ressursipiirangutega seadmetel vÃ¤hendatud operatiivkuludega.

**Juurutamise paindlikkus**: Need sÃ¼steemid vÃµimaldavad seadmesisesi AI-vÃµimekust ilma internetiÃ¼henduse vajaduseta, parandavad privaatsust ja turvalisust kohaliku tÃ¶Ã¶tlemise kaudu, neid saab kohandada valdkonnaspetsiifiliste rakenduste jaoks ja need sobivad erinevatesse serva-arvutuskeskkondadesse.

**KulutÃµhusus**: Agentide sÃ¼steemid pakuvad kulutÃµhusat juurutamist vÃµrreldes pilvepÃµhiste lahendustega, vÃ¤hendades operatiivkulusid ja madalamaid ribalaiuse nÃµudeid servarakenduste jaoks.

## TÃ¤iustatud vÃ¤ikemudelite strateegiad

### SLM (vÃ¤ikemudeli) alused

VÃ¤ikemudel (SLM) on keelemudel, mis mahub tavalisele tarbijaseadmele ja suudab teha jÃ¤reldusi piisavalt madala latentsusega, et olla praktiline Ã¼he kasutaja agentlike pÃ¤ringute teenindamisel. Praktilises mÃµttes on SLM-id tavaliselt mudelid, millel on vÃ¤hem kui 10 miljardit parameetrit.

**Formaatide avastamise funktsioonid**: SLM-id pakuvad tÃ¤iustatud tuge erinevatele kvantiseerimistasemetele, platvormidevahelist Ã¼hilduvust, reaalajas jÃµudluse optimeerimist ja serva juurutamise vÃµimalusi. Kasutajad saavad juurdepÃ¤Ã¤su tÃ¤iustatud privaatsusele kohaliku tÃ¶Ã¶tlemise kaudu ja WebGPU toele brauseripÃµhise juurutamise jaoks.

**Kvantiseerimistasemete kogud**: Populaarsed SLM-formaadid hÃµlmavad Q4_K_M tasakaalustatud kompressiooni jaoks mobiilirakendustes, Q5_K_S seeria kvaliteedile keskendunud serva juurutamiseks, Q8_0 peaaegu originaalse tÃ¤psuse jaoks vÃµimsatel servaseadmetel ja eksperimentaalsed formaadid nagu Q2_K ultra-madalate ressursside stsenaariumide jaoks.

### GGUF (General GGML Universal Format) SLM-i juurutamiseks

GGUF toimib peamise formaadina kvantiseeritud SLM-ide juurutamiseks CPU ja servaseadmetel, olles spetsiaalselt optimeeritud agentlike rakenduste jaoks:

**Agentide optimeeritud funktsioonid**: Formaat pakub ulatuslikke ressursse SLM-i konverteerimiseks ja juurutamiseks, pakkudes tÃ¤iustatud tuge tÃ¶Ã¶riistade kasutamiseks, struktureeritud vÃ¤ljundite genereerimiseks ja mitme pÃ¶Ã¶rdega vestlusteks. Platvormidevaheline Ã¼hilduvus tagab agentide jÃ¤rjepideva kÃ¤itumise erinevatel servaseadmetel.

**JÃµudluse optimeerimine**: GGUF vÃµimaldab tÃµhusat mÃ¤lukasutust agentide tÃ¶Ã¶voogude jaoks, toetab dÃ¼naamilist mudelite laadimist mitme agendi sÃ¼steemide jaoks ja pakub optimeeritud jÃ¤reldusi reaalajas agentide interaktsioonide jaoks.

### Serva optimeeritud SLM-raamistikud

#### Llama.cpp optimeerimine agentide jaoks

Llama.cpp pakub tipptasemel kvantiseerimistehnikaid, mis on spetsiaalselt optimeeritud agentlike SLM-i juurutamiseks:

**Agentide spetsiifiline kvantiseerimine**: Raamistik toetab Q4_0 (optimaalne mobiilsete agentide juurutamiseks 75% suuruse vÃ¤hendamisega), Q5_1 (tasakaalustatud kvaliteedi-kompressiooni jaoks serva jÃ¤reldusagentidele) ja Q8_0 (peaaegu originaalse kvaliteedi jaoks tootmisagentide sÃ¼steemides). TÃ¤iustatud formaadid vÃµimaldavad ultra-kompressitud agente Ã¤Ã¤rmuslike serva stsenaariumide jaoks.

**Rakenduse eelised**: CPU-optimeeritud jÃ¤reldus SIMD kiirendusega pakub mÃ¤lutÃµhusat agentide tÃ¤itmist. Platvormidevaheline Ã¼hilduvus x86, ARM ja Apple Silicon arhitektuuride vahel vÃµimaldab universaalseid agentide juurutamise vÃµimalusi.

#### Apple MLX raamistik SLM-agentide jaoks

Apple MLX pakub natiivset optimeerimist, mis on spetsiaalselt loodud SLM-pÃµhiste agentide jaoks Apple Silicon seadmetel:

**Apple Silicon agentide optimeerimine**: Raamistik kasutab Ã¼htset mÃ¤lustruktuuri Metal Performance Shaders integratsiooniga, automaatset segatud tÃ¤psust agentide jÃ¤reldamiseks ja optimeeritud mÃ¤luriba mitme agendi sÃ¼steemide jaoks. SLM-agendid nÃ¤itavad erakordset jÃµudlust M-seeria kiipidel.

**Arenduse funktsioonid**: Python ja Swift API tugi agentide spetsiifiliste optimeerimistega, automaatne diferentseerimine agentide Ãµppimiseks ja sujuv integreerimine Apple'i arendustÃ¶Ã¶riistadega pakuvad terviklikke agentide arenduskeskkondi.

#### ONNX Runtime platvormidevaheliste SLM-agentide jaoks

ONNX Runtime pakub universaalset jÃ¤reldusmootorit, mis vÃµimaldab SLM-agentidel tÃ¶Ã¶tada jÃ¤rjepidevalt erinevatel riistvaraplatvormidel ja operatsioonisÃ¼steemidel:

**Universaalne juurutamine**: ONNX Runtime tagab SLM-agentide jÃ¤rjepideva kÃ¤itumise Windowsi, Linuxi, macOS-i, iOS-i ja Androidi platvormidel. See platvormidevaheline Ã¼hilduvus vÃµimaldab arendajatel kirjutada Ã¼ks kord ja juurutada kÃµikjal, vÃ¤hendades oluliselt arenduse ja hoolduse koormust mitme platvormi rakenduste jaoks.

**Riistvara kiirenduse valikud**: Raamistik pakub optimeeritud tÃ¤itmisvÃµimalusi erinevate riistvarakonfiguratsioonide jaoks, sealhulgas CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) ja spetsialiseeritud kiirendid (Intel VPU, Qualcomm NPU). SLM-agendid saavad automaatselt kasutada parimat saadaolevat riistvara ilma koodimuudatusteta.

**Tootmisvalmis funktsioonid**: ONNX Runtime pakub ettevÃµtte tasemel funktsioone, mis on olulised tootmisagentide juurutamiseks, sealhulgas graafiku optimeerimine kiiremaks jÃ¤reldamiseks, mÃ¤luhaldus ressursipiirangutega keskkondade jaoks ja pÃµhjalikud profiilimisvahendid jÃµudluse analÃ¼Ã¼siks. Raamistik toetab nii Python kui C++ API-sid paindlikuks integreerimiseks.

## SLM vs LLM agentlikes sÃ¼steemides: tÃ¤iustatud vÃµrdlus

### SLM-i eelised agentide rakendustes

**Operatiivne tÃµhusus**: SLM-id pakuvad 10-30Ã— kulude vÃ¤hendamist vÃµrreldes LLM-idega agentide Ã¼lesannete jaoks, vÃµimaldades reaalajas agentlike vastuseid suurel skaalal. Nad pakuvad kiiremaid jÃ¤reldusaegu tÃ¤nu vÃ¤hendatud arvutuslikule keerukusele, muutes nad ideaalseks interaktiivsete agentide rakendustes.

**Serva juurutamise vÃµimalused**: SLM-id vÃµimaldavad seadmesisesi agentide tÃ¤itmist ilma interneti sÃµltuvuseta, parandatud privaatsust kohaliku agentide tÃ¶Ã¶tlemise kaudu ja kohandamist valdkonnaspetsiifiliste agentide rakenduste jaoks, mis sobivad erinevatesse serva-arvutuskeskkondadesse.

**Agentide spetsiifiline optimeerimine**: SLM-id paistavad silma tÃ¶Ã¶riistade kasutamises, struktureeritud vÃ¤ljundite genereerimises ja rutiinsete otsustusprotsesside tÃ¶Ã¶voogudes, mis moodustavad 70-80% tÃ¼Ã¼pilistest agentide Ã¼lesannetest.

### Millal kasutada SLM-i vs LLM-i agentide sÃ¼steemides

**Ideaalne SLM-idele**:
- **Korduvad agentide Ã¼lesanded**: Andmesisestus, vormide tÃ¤itmine, rutiinsed API-kutsed
- **TÃ¶Ã¶riistade integreerimine**: Andmebaasi pÃ¤ringud, failitoimingud, sÃ¼steemi interaktsioonid
- **Struktureeritud tÃ¶Ã¶vood**: Eelnevalt mÃ¤Ã¤ratletud agentide protsesside jÃ¤rgimine
- **Valdkonnaspetsiifilised agendid**: Klienditeenindus, ajastamine, pÃµhianalÃ¼Ã¼s
- **Kohalik tÃ¶Ã¶tlemine**: Privaatsustundlikud agentide operatsioonid

**Parem LLM-idele**:
- **Kompleksne arutlemine**: Uute probleemide lahendamine, strateegiline planeerimine
- **Avatud vestlused**: Ãœldine vestlus, loovad arutelud
- **Lai teadmiste ulatus**: Uuringud, mis nÃµuavad ulatuslikke Ã¼ldteadmisi
- **Uued olukorrad**: TÃ¤iesti uute agentide stsenaariumide kÃ¤sitlemine

### HÃ¼briidne agentide arhitektuur

Optimaalne lÃ¤henemine Ã¼hendab SLM-id ja LLM-id heterogeensetes agentlikes sÃ¼steemides:

**Nutikas agentide orkestreerimine**:
1. **SLM esmaseks**: KÃ¤sitle 70-80% rutiinsetest agentide Ã¼lesannetest kohapeal
2. **LLM vajadusel**: Suuna keerulised pÃ¤ringud pilvepÃµhistele suurematele mudelitele
3. **Spetsialiseeritud SLM-id**: Erinevad vÃ¤ikemudelid erinevate agentide valdkondade jaoks
4. **Kulude optimeerimine**: Minimeeri kallid LLM-kutsed intelligentse suunamise kaudu

## Tootmisvalmis SLM-agentide juurutusstrateegiad

### Foundry Local: ettevÃµtte tasemel serva AI runtime

Foundry Local (https://github.com/microsoft/foundry-local) on Microsofti lipulaev lahendus vÃ¤ikemudelite juurutamiseks tootmise servakeskkondades. See pakub tÃ¤ielikku runtime-keskkonda, mis on spetsiaalselt loodud SLM-pÃµhiste agentide jaoks, pakkudes ettevÃµtte tasemel funktsioone ja sujuvaid integreerimisvÃµimalusi.

**PÃµhiarhitektuur ja funktsioonid**:
- **OpenAI-Ã¼hilduv API**: TÃ¤ielik Ã¼hilduvus OpenAI SDK ja Agent Frameworki integreerimistega
- **Automaatne riistvara optimeerimine**: Mudelivariantide intelligentne valik vastavalt saadaolevale riistvarale (CUDA GPU, Qualcomm NPU, CPU)
- **Mudelihaldus**: Automaatne mudelite allalaadimine, vahemÃ¤llu salvestamine ja elutsÃ¼kli haldamine
- **Teenuse avastamine**: Nullkonfiguratsiooniga teenuse tuvastamine agentide raamistikeks
- **Ressursside optimeerimine**: Intelligentne mÃ¤luhaldus ja energiatÃµhusus serva juurutamiseks

#### Paigaldamine ja seadistamine

**Platvormidevaheline paigaldamine**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Kiire algus agentide arendamiseks**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Agent Frameworki integreerimine

**Foundry Local SDK integreerimine**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Automaatne mudeli valik ja riistvara optimeerimine**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Tootmise juurutusmustrid

**Ãœhe agendi tootmise seadistus**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Mitme agendi tootmise orkestreerimine**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### EttevÃµtte funktsioonid ja jÃ¤lgimine

**Tervise jÃ¤lgimine ja nÃ¤htavus**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Ressursside haldamine ja automaatne skaleerimine**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### TÃ¤iustatud konfiguratsioon ja optimeerimine

**Kohandatud mudeli konfiguratsioon**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Tootmise juurutamise kontrollnimekiri**:

âœ… **Teenuse konfiguratsioon**:
- Konfigureeri sobivad mudeli aliased kasutusjuhtumite jaoks
- Sea ressursipiirangud ja jÃ¤lgimislÃ¤ved
- Luba tervisekontrollid ja metrikate kogumine
- Konfigureeri automaatne taaskÃ¤ivitamine ja tÃµrkeotsing

âœ… **Turvalisuse seadistamine**:
- Luba ainult kohalik API-juurdepÃ¤Ã¤s (vÃ¤lise juurdepÃ¤Ã¤su puudumine)
- Konfigureeri sobiv API-vÃµtmete haldamine
- Sea Ã¼les agentide interaktsioonide auditeerimislogid
- Rakenda tootmise kasutuse jaoks kiiruse piiramine

âœ… **JÃµudluse optimeerimine**:
- Testi mudeli jÃµudlust eeldatava koormuse all
- Konfigureeri sobivad kvantiseerimistasemed
- Sea Ã¼les mudeli vahemÃ¤llu salvestamise ja soojendamise strateegiad
- JÃ¤lgi mÃ¤lu ja CPU kasutuse mustreid

âœ… **Integreerimise testimine**:
- Testi agentide raamistikku integreerimist
- Kinnita offline tÃ¶Ã¶vÃµimekused
- Testi tÃµrkeotsingu ja taastamise stsenaariume
- Kinnita otsast lÃµpuni agentide tÃ¶Ã¶vood

### Ollama: lihtsustatud SLM-agentide juurutamine

### Ollama: kogukonnale keskendunud SLM-agentide juurutamine

Ollama pakub kogukonna juhitud lÃ¤henemist SLM-agentide juurut
- Testi Microsoft Agent Frameworki integreerimist  
- Kontrolli vÃµrguÃ¼henduseta tÃ¶Ã¶vÃµimekust  
- Testi tÃµrkesiirde stsenaariume ja veakÃ¤sitlust  
- Kinnita agentide tÃ¶Ã¶voogude terviklikkus  

**VÃµrdlus Foundry Localiga**:  

| Funktsioon | Foundry Local | Ollama |  
|------------|---------------|--------|  
| **Sihtkasutus** | EttevÃµtte tootmine | Arendus ja kogukond |  
| **MudeliekosÃ¼steem** | Microsofti kureeritud | Ulatuslik kogukond |  
| **Riistvara optimeerimine** | Automaatne (CUDA/NPU/CPU) | KÃ¤sitsi seadistamine |  
| **EttevÃµtte funktsioonid** | Sisseehitatud jÃ¤lgimine, turvalisus | Kogukonna tÃ¶Ã¶riistad |  
| **Paigaldamise keerukus** | Lihtne (winget install) | Lihtne (curl install) |  
| **API Ã¼hilduvus** | OpenAI + laiendused | OpenAI standard |  
| **Tugi** | Microsofti ametlik | Kogukonna juhitud |  
| **Parim kasutus** | Tootmisagendid | PrototÃ¼Ã¼pimine, uurimistÃ¶Ã¶ |  

**Millal valida Ollama**:  
- **Arendus ja prototÃ¼Ã¼pimine**: Kiire katsetamine erinevate mudelitega  
- **Kogukonna mudelid**: JuurdepÃ¤Ã¤s uusimatele kogukonna loodud mudelitele  
- **Hariduslik kasutus**: AI agentide arendamise Ãµppimine ja Ãµpetamine  
- **Uurimisprojektid**: Akadeemilised uuringud, mis vajavad mitmekesist mudelivalikut  
- **Kohandatud mudelid**: Kohandatud mudelite loomine ja testimine  

### VLLM: KÃµrge jÃµudlusega SLM agentide jÃ¤reldamine  

VLLM (Very Large Language Model inference) pakub suure lÃ¤bilaskevÃµimega ja mÃ¤lusÃ¤Ã¤stlikku jÃ¤reldusmootorit, mis on spetsiaalselt optimeeritud tootmise SLM-i juurutamiseks suurel skaalal. Kui Foundry Local keskendub kasutusmugavusele ja Ollama kogukonna mudelitele, siis VLLM paistab silma kÃµrge jÃµudlusega stsenaariumides, mis nÃµuavad maksimaalset lÃ¤bilaskevÃµimet ja tÃµhusat ressursside kasutamist.  

**PÃµhiarhitektuur ja funktsioonid**:  
- **PagedAttention**: Revolutsiooniline mÃ¤luhaldus tÃµhusaks tÃ¤helepanu arvutamiseks  
- **DÃ¼naamiline rÃ¼hmitamine**: Nutikas pÃ¤ringute rÃ¼hmitamine optimaalse lÃ¤bilaskevÃµime saavutamiseks  
- **GPU optimeerimine**: TÃ¤iustatud CUDA tuumad ja tensorite paralleelsuse tugi  
- **OpenAI Ã¼hilduvus**: TÃ¤ielik API Ã¼hilduvus sujuvaks integreerimiseks  
- **Spekulatiivne dekodeerimine**: TÃ¤iustatud jÃ¤relduse kiirendamise tehnikad  
- **Kvantiseerimise tugi**: INT4, INT8 ja FP16 kvantiseerimine mÃ¤lusÃ¤Ã¤stlikkuseks  

#### Paigaldamine ja seadistamine  

**PaigaldamisvÃµimalused**:  
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```
  
**Kiire algus agentide arendamiseks**:  
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```
  
#### Agent Frameworki integreerimine  

**VLLM koos Microsoft Agent Frameworkiga**:  
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```
  
**KÃµrge lÃ¤bilaskevÃµimega multi-agent sÃ¼steem**:  
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```
  
#### Tootmise juurutamise mustrid  

**EttevÃµtte VLLM tootmisteenus**:  
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```
  
#### EttevÃµtte funktsioonid ja jÃ¤lgimine  

**TÃ¤iustatud VLLM jÃµudluse jÃ¤lgimine**:  
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```
  
#### TÃ¤iustatud konfiguratsioon ja optimeerimine  

**Tootmise VLLM konfiguratsiooni mallid**:  
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```
  
**Tootmise juurutamise kontrollnimekiri VLLM jaoks**:  

âœ… **Riistvara optimeerimine**:  
- Seadista tensorite paralleelsus mitme GPU jaoks  
- Luba kvantiseerimine (AWQ/GPTQ) mÃ¤lusÃ¤Ã¤stlikkuseks  
- MÃ¤Ã¤ra optimaalne GPU mÃ¤lu kasutus (85-95%)  
- Seadista sobivad rÃ¼hma suurused lÃ¤bilaskevÃµime jaoks  

âœ… **JÃµudluse hÃ¤Ã¤lestamine**:  
- Luba eelfikseerimine korduvate pÃ¤ringute jaoks  
- Seadista tÃ¼keldatud eeltÃ¤itmine pikkade jÃ¤rjestuste jaoks  
- Luba spekulatiivne dekodeerimine kiiremaks jÃ¤reldamiseks  
- Optimeeri max_num_seqs vastavalt riistvarale  

âœ… **Tootmise funktsioonid**:  
- Seadista tervise jÃ¤lgimine ja mÃµÃµdikute kogumine  
- Konfigureeri automaatne taaskÃ¤ivitamine ja tÃµrkesiire  
- Rakenda pÃ¤ringute jÃ¤rjekorda ja koormuse tasakaalustamist  
- Seadista pÃµhjalik logimine ja hoiatused  

âœ… **Turvalisus ja tÃ¶Ã¶kindlus**:  
- Konfigureeri tulemÃ¼Ã¼ri reeglid ja juurdepÃ¤Ã¤sukontrollid  
- Seadista API kiiruse piiramine ja autentimine  
- Rakenda sujuv sulgemine ja puhastamine  
- Konfigureeri varundamine ja katastroofide taastamine  

âœ… **Integreerimise testimine**:  
- Testi Microsoft Agent Frameworki integreerimist  
- Kinnita kÃµrge lÃ¤bilaskevÃµimega stsenaariumid  
- Testi tÃµrkesiiret ja taastumisprotseduure  
- VÃµrdle jÃµudlust koormuse all  

**VÃµrdlus teiste lahendustega**:  

| Funktsioon | VLLM | Foundry Local | Ollama |  
|------------|------|---------------|--------|  
| **Sihtkasutus** | KÃµrge lÃ¤bilaskevÃµimega tootmine | EttevÃµtte kasutusmugavus | Arendus ja kogukond |  
| **JÃµudlus** | Maksimaalne lÃ¤bilaskevÃµime | Tasakaalustatud | Hea |  
| **MÃ¤lusÃ¤Ã¤stlikkus** | PagedAttention optimeerimine | Automaatne optimeerimine | Standardne |  
| **Seadistamise keerukus** | KÃµrge (palju parameetreid) | Madal (automaatne) | Madal (lihtne) |  
| **Mastaapsus** | SuurepÃ¤rane (tensor/pipeline paralleelsus) | Hea | Piiratud |  
| **Kvantiseerimine** | TÃ¤iustatud (AWQ, GPTQ, FP8) | Automaatne | Standard GGUF |  
| **EttevÃµtte funktsioonid** | Vajab kohandatud rakendust | Sisseehitatud | Kogukonna tÃ¶Ã¶riistad |  
| **Parim kasutus** | Suuremahulised tootmisagendid | EttevÃµtte tootmine | Arendus |  

**Millal valida VLLM**:  
- **KÃµrge lÃ¤bilaskevÃµime nÃµuded**: Sajad pÃ¤ringud sekundis  
- **Suuremahulised juurutused**: Mitme GPU ja mitme sÃµlmega juurutused  
- **JÃµudluskriitiline**: Alla sekundi vastuseajad suurel skaalal  
- **TÃ¤iustatud optimeerimine**: Vajadus kohandatud kvantiseerimise ja rÃ¼hmitamise jÃ¤rele  
- **RessursisÃ¤Ã¤stlikkus**: Maksimaalne kallite GPU riistvara kasutus  

## Reaalsed SLM agentide rakendused  

### Klienditeeninduse SLM agendid  
- **SLM vÃµimekus**: Kontootsingud, paroolide lÃ¤htestamine, tellimuste oleku kontroll  
- **Kuluefektiivsus**: 10-kordne jÃ¤relduskulude vÃ¤hendamine vÃµrreldes LLM agentidega  
- **JÃµudlus**: Kiiremad vastuseajad ja jÃ¤rjepidev kvaliteet rutiinsete pÃ¤ringute jaoks  

### Ã„rijuhtimise SLM agendid  
- **Arve tÃ¶Ã¶tlemise agendid**: Andmete vÃ¤ljavÃµtmine, teabe valideerimine, suunamine kinnitamiseks  
- **E-posti haldamise agendid**: Kategooriate mÃ¤Ã¤ramine, prioriteetide seadmine, automaatsete vastuste koostamine  
- **Ajastamise agendid**: Kohtumiste koordineerimine, kalendrite haldamine, meeldetuletuste saatmine  

### Isiklikud SLM digitaalsed assistendid  
- **Ãœlesannete haldamise agendid**: TÃµhusalt loovad, uuendavad ja korraldavad Ã¼lesannete nimekirju  
- **Teabe kogumise agendid**: Uurivad teemasid, koostavad kokkuvÃµtteid kohapeal  
- **Suhtlusagendid**: Koostavad e-kirju, sÃµnumeid, sotsiaalmeedia postitusi privaatselt  

### Kauplemise ja finantsvaldkonna SLM agendid  
- **TurujÃ¤lgimise agendid**: JÃ¤lgivad hindu, tuvastavad trende reaalajas  
- **Aruannete koostamise agendid**: Loovad automaatselt igapÃ¤evaseid/nÃ¤dalaseid kokkuvÃµtteid  
- **Riskihindamise agendid**: Hindavad portfelli positsioone kohalike andmete abil  

### Tervishoiu tugiteenuste SLM agendid  
- **Patsiendi ajastamise agendid**: Koordineerivad kohtumisi, saadavad automaatseid meeldetuletusi  
- **Dokumentatsiooni agendid**: Loovad meditsiinilisi kokkuvÃµtteid ja aruandeid kohapeal  
- **Retsepti haldamise agendid**: JÃ¤lgivad tÃ¤iendusi, kontrollivad koostoimeid privaatselt  

## Microsoft Agent Framework: Tootmisvalmis agentide arendus  

### Ãœlevaade ja arhitektuur  

Microsoft Agent Framework pakub terviklikku, ettevÃµtte tasemel platvormi AI agentide loomiseks, juurutamiseks ja haldamiseks, mis suudavad tÃ¶Ã¶tada nii pilves kui ka vÃµrguÃ¼henduseta servakeskkondades. Raamistik on spetsiaalselt loodud tÃ¶Ã¶tama sujuvalt vÃ¤ikeste keelemudelite ja servaarvutuse stsenaariumidega, muutes selle ideaalseks privaatsustundlike ja ressursipiirangutega juurutuste jaoks.  

**PÃµhikomponendid**:  
- **Agendi kÃ¤ituskeskkond**: Kergekaaluline tÃ¤itmiskeskkond, optimeeritud servaseadmete jaoks  
- **TÃ¶Ã¶riistade integreerimissÃ¼steem**: Laiendatav pistikprogrammide arhitektuur vÃ¤liste teenuste ja API-de Ã¼hendamiseks  
- **Oleku haldamine**: PÃ¼siv agendi mÃ¤lu ja konteksti kÃ¤sitlus seansside vahel  
- **Turvalisuskiht**: Sisseehitatud turvakontrollid ettevÃµtte juurutamiseks  
- **Orkestreerimismootor**: Mitme agendi koordineerimine ja tÃ¶Ã¶voogude haldamine  

### Servajuurutuse vÃµtmefunktsioonid  

**VÃµrguÃ¼henduseta esmane arhitektuur**: Microsoft Agent Framework on loodud vÃµrguÃ¼henduseta esmaste pÃµhimÃµtete jÃ¤rgi, vÃµimaldades agentidel tÃµhusalt tÃ¶Ã¶tada ilma pideva internetiÃ¼henduseta. See hÃµlmab kohalikku mudelite jÃ¤reldamist, vahemÃ¤llu salvestatud teadmiste baase, vÃµrguÃ¼henduseta tÃ¶Ã¶riistade tÃ¤itmist ja sujuvat degradeerumist, kui pilveteenused pole saadaval.  

**Ressursside optimeerimine**: Raamistik pakub nutikat ressursside haldamist automaatse mÃ¤lusÃ¤Ã¤stlikkusega SLM-ide jaoks, CPU/GPU koormuse tasakaalustamist servaseadmete jaoks, kohanduvat mudelivalikut vastavalt saadavatele ressurssidele ja energiasÃ¤Ã¤stlikke jÃ¤reldusmustreid mobiilseks juurutamiseks.  

**Turvalisus ja privaatsus**: EttevÃµtte tasemel turvafunktsioonid hÃµlmavad kohalikku andmetÃ¶Ã¶tlust privaatsuse sÃ¤ilitamiseks, krÃ¼pteeritud agendi suhtluskanaleid, rollipÃµhiseid juurdepÃ¤Ã¤sukontrolle agendi vÃµimekuste jaoks ja auditeerimislogisid vastavusnÃµuete tÃ¤itmiseks.  

### Integreerimine Foundry Localiga  

Microsoft Agent Framework integreerub sujuvalt Foundry Localiga, pakkudes tÃ¤ielikku serva AI lahendust:  

**Automaatne mudelite avastamine**: Raamistik tuvastab ja Ã¼hendub automaatselt Foundry Locali instantsidega, avastab saadaval olevad SLM mudelid ja valib optimaalsed mudelid vastavalt agendi nÃµuetele ja riistvara vÃµimekusele.  

**DÃ¼naamiline mudelite laadimine**: Agendid saavad dÃ¼naamiliselt laadida erinevaid SLM-e konkreetsete Ã¼lesannete jaoks, vÃµimaldades mitme mudeliga agendisÃ¼steeme, kus erinevad mudelid kÃ¤sitlevad erinevat tÃ¼Ã¼pi pÃ¤ringuid, ja automaatset tÃµrkesiiret mudelite vahel vastavalt saadavusele ja jÃµudlusele.  

**JÃµudluse optimeerimine**: Integreeritud vahemÃ¤lu mehhanismid vÃ¤hendavad mudelite laadimisaega, Ã¼henduste jagamine optimeerib API-kÃµnesid Foundry Localile ja nutikas rÃ¼hmitamine parandab lÃ¤bilaskevÃµimet mitme agendi pÃ¤ringute jaoks.  

### Agentide loomine Microsoft Agent Frameworkiga  

#### Agendi mÃ¤Ã¤ratlemine ja konfiguratsioon  

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```
  
#### TÃ¶Ã¶riistade integreerimine servastsenaariumide jaoks  

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```
  
#### Mitme agendi orkestreerimine  

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```
  
### TÃ¤iustatud servajuurutuse mustrid  

#### Hierarhiline agendi arhitektuur  

**Kohalikud agendiklastrid**: Juurutage mitu spetsialiseeritud SLM agenti servaseadmetele, igaÃ¼ks optimeeritud konkreetsete Ã¼lesannete jaoks. Kasutage kergeid mudeleid nagu Qwen2.5-0.5B lihtsate suunamis- ja ajastamisÃ¼lesannete jaoks, keskmise suurusega mudeleid nagu Phi-4-Mini klienditeeninduse ja dokumentatsiooni jaoks ning suuremaid mudeleid keerukate arutluste jaoks, kui ressursid seda vÃµimaldavad.  

**Serva-pilve koordineerimine**: Rakendage nutikaid eskalatsioonimustreid, kus kohalikud agendid kÃ¤sitlevad rutiinseid Ã¼lesandeid, pilveagendid pakuvad keerulist arutlust, kui Ã¼henduvus seda vÃµimaldab, ja sujuv Ã¼leminek serva ja pilve tÃ¶Ã¶tlemise vahel sÃ¤ilitab jÃ¤rjepidevuse.  

#### Juurutuse konfiguratsioonid  

**Ãœhe seadme juurutus**:  
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```
  
**Jaotatud servajuurutus**:  
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```
  
### JÃµudluse optimeerimine servaagentide jaoks  

#### Mudelivaliku strateegiad  

**ÃœlesandepÃµhine mudeli mÃ¤Ã¤ramine**: Microsoft Agent Framework vÃµimaldab nutikat mudelivalikut vastavalt Ã¼lesande keerukusele ja nÃµuetele:  

- **Lihtsad Ã¼lesanded** (kÃ¼simused ja vastused, suunamine): Qwen2.5-0.5B (500MB, <100ms vastus)  
- **Keskmised Ã¼lesanded** (klienditeenindus, ajastamine): Phi-4-Mini (2.4GB, 200-500ms vastus)  
- **Keerukad Ã¼lesanded** (tehniline analÃ¼Ã¼s, planeerimine): Phi-4 (7GB, 1-3s vastus, kui ressursid seda vÃµimaldavad)  

**DÃ¼naamiline mudelite vahetamine**: Agendid saavad vahetada mudeleid vastavalt sÃ¼steemi koormusele, Ã¼lesande keerukuse hindamisele, kasutaja prioriteetidele ja saadavatele riistvararesurssidele.  

#### MÃ¤lu ja ressursside haldamine  

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```
  
### EttevÃµtte integreerimise mustrid  

#### Turvalisus ja vastavus  

**Kohalik andmetÃ¶Ã¶tlus**: KÃµik agendi tÃ¶Ã¶tlemine toimub kohapeal, tagades tundlike andmete servaseadmest lahkumise vÃ¤ltimise. See hÃµlmab kliendiandmete kaitset, HIPAA vastavust tervishoiuagentide jaoks, finantsandmete turvalisust pangandusagentide jaoks ja GDPR vastavust Euroopa juurutuste jaoks.  

**JuurdepÃ¤Ã¤sukontroll**: RollipÃµhised Ãµigused kontrollivad, milliseid tÃ¶Ã¶riistu agendid saavad kasutada, kasutaja autentimine agendi interaktsioonide jaoks ja auditeerimisjÃ¤ljed kÃµigi agendi toimingute ja otsuste jaoks.  

#### JÃ¤lgimine ja jÃ¤lgitavus  

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```
  
### Reaalsed rakenduse nÃ¤ited  

#### JaemÃ¼Ã¼gi servaagendi sÃ¼steem  

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```
  
#### Tervishoiu tugiteenuste agent  

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```
  
### Parimad tavad Microsoft Agent Frameworki jaoks  

#### Arenduse
**Raamistiku valik agentide juurutamiseks**: Valige optimeerimisraamistikud vastavalt sihtseadmetele ja agentide nÃµuetele. Kasutage Llama.cpp CPU optimeeritud agentide juurutamiseks, Apple MLX Apple Siliconi agentide rakenduste jaoks ja ONNX platvormidevahelise Ã¼hilduvuse tagamiseks.

## Praktiline SLM agentide konverteerimine ja kasutusjuhtumid

### Agentide juurutamise stsenaariumid pÃ¤riselus

**Mobiilsed agentide rakendused**: Q4_K formaadid sobivad suurepÃ¤raselt nutitelefonide agentide rakendusteks, kuna neil on minimaalne mÃ¤lukasutus, samas kui Q8_0 pakub tasakaalustatud jÃµudlust tahvelarvutite agentide sÃ¼steemide jaoks. Q5_K formaadid tagavad kÃµrge kvaliteedi mobiilsete produktiivsusagentide jaoks.

**Lauaarvutite ja servaagentide arvutamine**: Q5_K tagab optimaalse jÃµudluse lauaarvutite agentide rakendustes, Q8_0 pakub kÃµrgekvaliteedilist jÃ¤reldust tÃ¶Ã¶jaamade agentide keskkondades ja Q4_K vÃµimaldab tÃµhusat tÃ¶Ã¶tlemist servaagentide seadmetes.

**Teadus- ja eksperimentaalsed agendid**: TÃ¤iustatud kvantiseerimisformaadid vÃµimaldavad uurida ultra-madala tÃ¤psusega agentide jÃ¤reldusi akadeemilisteks uuringuteks ja kontseptsioonide tÃµestamiseks, mis vajavad Ã¤Ã¤rmiselt piiratud ressursse.

### SLM agentide jÃµudluse vÃµrdlused

**Agentide jÃ¤reldamise kiirus**: Q4_K saavutab mobiilsetel protsessoritel kÃµige kiiremad agentide vastamisajad, Q5_K pakub tasakaalustatud kiiruse ja kvaliteedi suhet Ã¼ldiste agentide rakenduste jaoks, Q8_0 tagab keerukate agentide Ã¼lesannete jaoks Ã¼limalt kÃµrge kvaliteedi ja eksperimentaalsed formaadid maksimaalse lÃ¤bilaskevÃµime spetsialiseeritud agentide riistvarale.

**Agentide mÃ¤lunÃµuded**: Agentide kvantiseerimistasemed ulatuvad Q2_K-st (alla 500 MB vÃ¤ikeste agentide mudelite jaoks) kuni Q8_0-ni (umbes 50% algsest suurusest), kusjuures eksperimentaalsed konfiguratsioonid saavutavad maksimaalse tihenduse ressursside piiratud agentide keskkondades.

## VÃ¤ljakutsed ja kaalutlused SLM agentide jaoks

### JÃµudluse kompromissid agentide sÃ¼steemides

SLM agentide juurutamine nÃµuab hoolikat kaalumist mudeli suuruse, agentide vastamiskiiruse ja vÃ¤ljundi kvaliteedi vahel. Kuigi Q4_K pakub erakordset kiirust ja tÃµhusust mobiilsete agentide jaoks, tagab Q8_0 keerukate agentide Ã¼lesannete jaoks Ã¼limalt kÃµrge kvaliteedi. Q5_K on tasakaalustatud valik, mis sobib enamiku Ã¼ldiste agentide rakenduste jaoks.

### Riistvara Ã¼hilduvus SLM agentide jaoks

Erinevatel servaseadmetel on erinevad vÃµimed SLM agentide juurutamiseks. Q4_K tÃ¶Ã¶tab tÃµhusalt lihtsamatel protsessoritel lihtsate agentide jaoks, Q5_K vajab mÃµÃµdukaid arvutusressursse tasakaalustatud agentide jÃµudluse jaoks ja Q8_0 kasutab Ã¤ra kÃµrgema klassi riistvara arenenud agentide vÃµimekuse jaoks.

### Turvalisus ja privaatsus SLM agentide sÃ¼steemides

Kuigi SLM agendid vÃµimaldavad kohalikku tÃ¶Ã¶tlemist privaatsuse parandamiseks, tuleb rakendada asjakohaseid turvameetmeid, et kaitsta agentide mudeleid ja andmeid servakeskkondades. See on eriti oluline, kui juurutatakse kÃµrge tÃ¤psusega agentide formaate ettevÃµtete keskkondades vÃµi tihendatud agentide formaate rakendustes, mis kÃ¤sitlevad tundlikke andmeid.

## Tulevikutrendid SLM agentide arenduses

SLM agentide maastik areneb pidevalt koos edusammudega tihendustehnikates, optimeerimismeetodites ja serva juurutamise strateegiates. Tulevased arengud hÃµlmavad tÃµhusamaid kvantiseerimisalgoritme agentide mudelite jaoks, paremaid tihendusmeetodeid agentide tÃ¶Ã¶voogude jaoks ja paremat integreerimist serva riistvara kiirenditega agentide tÃ¶Ã¶tlemiseks.

**SLM agentide turu prognoosid**: Hiljutiste uuringute kohaselt vÃµib agentide juhitud automatiseerimine kÃµrvaldada 40â€“60% korduvatest kognitiivsetest Ã¼lesannetest ettevÃµtete tÃ¶Ã¶voogudes aastaks 2027, kusjuures SLM-id juhivad seda transformatsiooni tÃ¤nu nende kulutÃµhususele ja juurutamise paindlikkusele.

**Tehnoloogilised suundumused SLM agentides**:
- **Spetsialiseeritud SLM agendid**: Valdkonnaspetsiifilised mudelid, mis on koolitatud konkreetsete agentide Ã¼lesannete ja tÃ¶Ã¶stusharude jaoks
- **Servaagentide arvutamine**: TÃ¤iustatud seadmesisesed agentide vÃµimekused parema privaatsuse ja vÃ¤hendatud latentsusega
- **Agentide orkestreerimine**: Parem koordineerimine mitme SLM agendi vahel dÃ¼naamilise suunamise ja koormuse tasakaalustamisega
- **Demokratiseerimine**: SLM-i paindlikkus vÃµimaldab laiemat osalust agentide arendamisel erinevates organisatsioonides

## SLM agentidega alustamine

### Samm 1: Microsoft Agent Frameworki keskkonna seadistamine

**Paigalda sÃµltuvused**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inicialiseeri Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Samm 2: Valige oma SLM agentide rakenduste jaoks
Populaarsed valikud Microsoft Agent Frameworki jaoks:
- **Microsoft Phi-4 Mini (3.8B)**: SuurepÃ¤rane Ã¼ldiste agentide Ã¼lesannete jaoks tasakaalustatud jÃµudlusega
- **Qwen2.5-0.5B (0.5B)**: ÃœlitÃ¤pne lihtsate suunamis- ja klassifitseerimisagentide jaoks
- **Qwen2.5-Coder-0.5B (0.5B)**: Spetsialiseerunud koodiga seotud agentide Ã¼lesannete jaoks
- **Phi-4 (7B)**: TÃ¤iustatud pÃµhjendamine keerukate serva stsenaariumide jaoks, kui ressursid seda vÃµimaldavad

### Samm 3: Looge oma esimene agent Microsoft Agent Frameworkiga

**PÃµhiline agendi seadistamine**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Samm 4: MÃ¤Ã¤ratlege agendi ulatus ja nÃµuded
Alustage keskendunud, hÃ¤sti mÃ¤Ã¤ratletud agentide rakendustega, kasutades Microsoft Agent Frameworki:
- **Ãœhe valdkonna agendid**: Klienditeenindus VÃ•I ajakava koostamine VÃ•I uurimistÃ¶Ã¶
- **Selged agendi eesmÃ¤rgid**: Konkreetsed, mÃµÃµdetavad eesmÃ¤rgid agendi jÃµudluse jaoks
- **Piiratud tÃ¶Ã¶riistade integreerimine**: Maksimaalselt 3â€“5 tÃ¶Ã¶riista esialgse agendi juurutamise jaoks
- **MÃ¤Ã¤ratletud agendi piirid**: Selged eskaleerimisteed keerukate stsenaariumide jaoks
- **Serva-esimene disain**: Eelistage vÃµrguÃ¼henduseta funktsionaalsust ja kohalikku tÃ¶Ã¶tlemist

### Samm 5: Rakendage serva juurutamine Microsoft Agent Frameworkiga

**Ressursside konfiguratsioon**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Rakendage turvameetmed servaagentide jaoks**:
- **Kohalik sisendi valideerimine**: Kontrollige pÃ¤ringuid ilma pilve sÃµltuvuseta
- **VÃµrguÃ¼henduseta vÃ¤ljundi filtreerimine**: Veenduge, et vastused vastavad kvaliteedistandarditele kohapeal
- **Serva turvakontrollid**: Rakendage turvameetmeid ilma internetiÃ¼henduse vajaduseta
- **Kohalik jÃ¤lgimine**: JÃ¤lgige jÃµudlust ja tuvastage probleeme serva telemeetria abil

### Samm 6: MÃµÃµtke ja optimeerige servaagentide jÃµudlust
- **Agendi Ã¼lesannete tÃ¤itmise mÃ¤Ã¤rad**: JÃ¤lgige edukuse mÃ¤Ã¤rasid vÃµrguÃ¼henduseta stsenaariumides
- **Agendi vastamisajad**: Tagage serva juurutamise jaoks alla sekundi vastamisajad
- **Ressursside kasutamine**: JÃ¤lgige servaseadmete mÃ¤lu, CPU ja aku kasutust
- **KulutÃµhusus**: VÃµrrelge serva juurutamise kulusid pilvepÃµhiste alternatiividega
- **VÃµrguÃ¼henduseta tÃ¶Ã¶kindlus**: MÃµÃµtke agendi jÃµudlust vÃµrguÃ¼henduse katkestuste ajal

## Olulised punktid SLM agentide rakendamiseks

1. **SLM-id on agentide jaoks piisavad**: Enamiku agentide Ã¼lesannete jaoks toimivad vÃ¤ikesed mudelid sama hÃ¤sti kui suured, pakkudes samal ajal mÃ¤rkimisvÃ¤Ã¤rseid eeliseid
2. **Agentide kulutÃµhusus**: SLM agentide kÃ¤itamine on 10â€“30 korda odavam, muutes need majanduslikult elujÃµuliseks laialdaseks juurutamiseks
3. **Spetsialiseerumine tÃ¶Ã¶tab agentide jaoks**: TÃ¤pselt hÃ¤Ã¤lestatud SLM-id Ã¼letavad sageli Ã¼ldotstarbelisi LLM-e konkreetsetes agentide rakendustes
4. **HÃ¼briidne agendi arhitektuur**: Kasutage SLM-e rutiinsete agentide Ã¼lesannete jaoks, LLM-e keerukate pÃµhjenduste jaoks, kui vaja
5. **Microsoft Agent Framework vÃµimaldab tootmisjuurutamist**: Pakub ettevÃµtte tasemel tÃ¶Ã¶riistu servaagentide loomiseks, juurutamiseks ja haldamiseks
6. **Serva-esimese disaini pÃµhimÃµtted**: VÃµrguÃ¼henduseta vÃµimekusega agendid, millel on kohalik tÃ¶Ã¶tlemine, tagavad privaatsuse ja tÃ¶Ã¶kindluse
7. **Foundry Local integratsioon**: Sujuv Ã¼hendus Microsoft Agent Frameworki ja kohaliku mudeli jÃ¤reldamise vahel
8. **Tulevik kuulub SLM agentidele**: VÃ¤ikesed keelemudelid koos tootmisraamistikega on agentse AI tulevik, vÃµimaldades demokratiseeritud ja tÃµhusat agentide juurutamist

## Viited ja lisalugemine

### PÃµhiuuringud ja publikatsioonid

#### AI agendid ja agentide sÃ¼steemid
- **"Language Agents as Optimizable Graphs"** (2024) - PÃµhiuuringud agentide arhitektuuri ja optimeerimise strateegiate kohta
  - Autorid: Wenyue Hua, Lishan Yang jt.
  - Link: https://arxiv.org/abs/2402.16823
  - Olulised tÃ¤helepanekud: GraafikupÃµhine agentide disain ja optimeerimisstrateegiad

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Autorid: Zhiheng Xi, Wenxiang Chen jt.
  - Link: https://arxiv.org/abs/2309.07864
  - Olulised tÃ¤helepanekud: LLM-pÃµhiste agentide vÃµimekuse ja rakenduste pÃµhjalik Ã¼levaade

- **"Cognitive Architectures for Language Agents"** (2024)
  - Autorid: Theodore Sumers, Shunyu Yao jt.
  - Link: https://arxiv.org/abs/2309.02427
  - Olulised tÃ¤helepanekud: Kognitiivsed raamistikud intelligentsete agentide disainimiseks

#### VÃ¤ikesed keelemudelid ja optimeerimine
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Autorid: Microsoft Research Team
  - Link: https://arxiv.org/abs/2404.14219
  - Olulised tÃ¤helepanekud: SLM-i disainipÃµhimÃµtted ja mobiilne juurutamine

- **"Qwen2.5 Technical Report"** (2024)
  - Autorid: Alibaba Cloud Team
  - Link: https://arxiv.org/abs/2407.10671
  - Olulised tÃ¤helepanekud: TÃ¤iustatud SLM-i treenimistehnikad ja jÃµudluse optimeerimine

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Autorid: Peiyuan Zhang, Guangtao Zeng jt.
  - Link: https://arxiv.org/abs/2401.02385
  - Olulised tÃ¤helepanekud: ÃœlitÃ¤psete mudelite disain ja treenimise efektiivsus

### Ametlik dokumentatsioon ja raamistikud

#### Microsoft Agent Framework
- **Ametlik dokumentatsioon**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHubi repositoorium**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Peamine repositoorium**: https://github.com/microsoft/foundry-local
- **Dokumentatsioon**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Peamine repositoorium**: https://github.com/vllm-project/vllm
- **Dokumentatsioon**: https://docs.vllm.ai/


#### Ollama
- **Ametlik veebisait**: https://ollama.ai/
- **GitHubi repositoorium**: https://github.com/ollama/ollama

### Mudelite optimeerimise raamistikud

#### Llama.cpp
- **Repositoorium**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentatsioon**: https://microsoft.github.io/Olive/
- **GitHubi repositoorium**: https://github.com/microsoft/Olive

#### OpenVINO
- **Ametlik veebisait**: https://docs.openvino.ai/

#### Apple MLX
- **Repositoorium**: https://github.com/ml-explore/mlx

### TÃ¶Ã¶stusaruanded ja turuanalÃ¼Ã¼s

#### AI agentide turu-uuring
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Link: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - Olulised tÃ¤helepanekud: Turusuundumused ja ettevÃµtete kasutuselevÃµtu mustrid

#### Tehnilised vÃµrdlused

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Link: https://mlcommons.org/en/inference-edge/
  - Olulised tÃ¤helepanekud: Standardiseeritud jÃµudlusmÃµÃµdikud serva juurutamiseks

### Standardid ja spetsifikatsioonid

#### Mudelite formaadid ja standardid
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Platvormidevaheline mudeli formaat Ã¼hilduvuse tagamiseks
- **GGUF spetsifikatsioon**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Kvantiseeritud mudeli formaat CPU jÃ¤reldamiseks
- **OpenAI API spetsifikatsioon**: https://platform.openai.com/docs/api-reference
  - Standardne API formaat keelemudeli integreerimiseks

#### Turvalisus ja vastavus
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI sÃ¼steemid**: Raamistik AI sÃ¼steemide ja ohutuse jaoks
- **IEEE standardid AI jaoks**: https://standards.ieee.org/industry-connections/ai/

Ãœleminek SLM-pÃµhistele agentidele esindab fundamentaalset muutust AI juurutamise lÃ¤henemises. Microsoft Agent Framework koos kohalike platvormide ja tÃµhusate vÃ¤ikeste keelemudelitega pakub tÃ¤ielikku lahendust tootmisvalmis agentide loomiseks, mis tÃ¶Ã¶tavad tÃµhusalt servakeskkondades. Keskendudes tÃµhususele, spetsialiseerumisele ja praktilisele kasutatavusele, muudab see tehnoloogiline komplekt AI agendid kÃ¤ttesaadavamaks, taskukohasemaks ja tÃµhusamaks reaalse maailma rakendustes igas tÃ¶Ã¶stusharus ja serva arvutuskeskkonnas.

Edasi liikudes aastasse 2025 avab Ã¼ha vÃµimekamate vÃ¤ikeste mudelite, keerukate agentide raamistikute nagu Microsoft Agent Framework ja tugevate serva juurutamise platvormide kombinatsioon uusi vÃµimalusi autonoomsetele sÃ¼steemidele, mis suudavad tÃµhusalt tÃ¶Ã¶tada servaseadmetel, sÃ¤ilitades samal ajal privaatsuse, vÃ¤hendades kulusid ja pakkudes erakordseid kasutajakogemusi.

**JÃ¤rgmised sammud rakendamiseks**:
1. **Uurige funktsioonide kutsumist**: Ã•ppige, kuidas SLM-id integreerivad tÃ¶Ã¶riistu ja struktureeritud vÃ¤ljundeid
2. **Valdage Model Context Protocol (MCP)**: MÃµistke arenenud agentide suhtlusmustreid

---

**LahtiÃ¼tlus**:  
See dokument on tÃµlgitud AI tÃµlketeenuse [Co-op Translator](https://github.com/Azure/co-op-translator) abil. Kuigi pÃ¼Ã¼ame tagada tÃ¤psust, palume arvestada, et automaatsed tÃµlked vÃµivad sisaldada vigu vÃµi ebatÃ¤psusi. Algne dokument selle algses keeles tuleks pidada autoriteetseks allikaks. Olulise teabe puhul soovitame kasutada professionaalset inimtÃµlget. Me ei vastuta arusaamatuste vÃµi valesti tÃµlgenduste eest, mis vÃµivad tekkida selle tÃµlke kasutamise tÃµttu.