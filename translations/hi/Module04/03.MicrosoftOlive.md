# अनुभाग 3 : Microsoft Olive ऑप्टिमाइज़ेशन सूट

## सामग्री सूची
1. [परिचय](../../../Module04)
2. [Microsoft Olive क्या है?](../../../Module04)
3. [इंस्टॉलेशन](../../../Module04)
4. [त्वरित प्रारंभ गाइड](../../../Module04)
5. [उदाहरण: Qwen3 को ONNX INT4 में बदलना](../../../Module04)
6. [उन्नत उपयोग](../../../Module04)
7. [Olive Recipes रिपॉजिटरी](../../../Module04)
8. [सर्वोत्तम प्रथाएँ](../../../Module04)
9. [समस्या निवारण](../../../Module04)
10. [अतिरिक्त संसाधन](../../../Module04)

## परिचय

Microsoft Olive एक शक्तिशाली, उपयोग में आसान हार्डवेयर-अवेयर मॉडल ऑप्टिमाइज़ेशन टूलकिट है जो विभिन्न हार्डवेयर प्लेटफॉर्म पर मशीन लर्निंग मॉडल को तैनात करने की प्रक्रिया को सरल बनाता है। चाहे आप CPUs, GPUs, या विशेष AI एक्सेलेरेटर को लक्षित कर रहे हों, Olive आपको मॉडल की सटीकता बनाए रखते हुए इष्टतम प्रदर्शन प्राप्त करने में मदद करता है।

## Microsoft Olive क्या है?

Olive एक उपयोग में आसान हार्डवेयर-अवेयर मॉडल ऑप्टिमाइज़ेशन टूल है जो मॉडल कंप्रेशन, ऑप्टिमाइज़ेशन और कंपाइलेशन में उद्योग-अग्रणी तकनीकों को जोड़ता है। यह ONNX Runtime के साथ एक E2E इंफेरेंस ऑप्टिमाइज़ेशन समाधान के रूप में काम करता है।

### मुख्य विशेषताएँ

- **हार्डवेयर-अवेयर ऑप्टिमाइज़ेशन**: आपके लक्षित हार्डवेयर के लिए सर्वोत्तम ऑप्टिमाइज़ेशन तकनीकों का स्वचालित चयन
- **40+ बिल्ट-इन ऑप्टिमाइज़ेशन घटक**: मॉडल कंप्रेशन, क्वांटाइज़ेशन, ग्राफ ऑप्टिमाइज़ेशन और अधिक को कवर करता है
- **आसान CLI इंटरफ़ेस**: सामान्य ऑप्टिमाइज़ेशन कार्यों के लिए सरल कमांड
- **मल्टी-फ्रेमवर्क सपोर्ट**: PyTorch, Hugging Face मॉडल और ONNX के साथ काम करता है
- **लोकप्रिय मॉडल सपोर्ट**: Olive स्वचालित रूप से Llama, Phi, Qwen, Gemma आदि जैसे लोकप्रिय मॉडल आर्किटेक्चर को आउट-ऑफ-द-बॉक्स ऑप्टिमाइज़ कर सकता है

### लाभ

- **विकास समय में कमी**: विभिन्न ऑप्टिमाइज़ेशन तकनीकों के साथ मैन्युअल रूप से प्रयोग करने की आवश्यकता नहीं
- **प्रदर्शन में सुधार**: महत्वपूर्ण गति सुधार (कुछ मामलों में 6x तक)
- **क्रॉस-प्लेटफॉर्म तैनाती**: ऑप्टिमाइज़ किए गए मॉडल विभिन्न हार्डवेयर और ऑपरेटिंग सिस्टम पर काम करते हैं
- **सटीकता बनाए रखना**: ऑप्टिमाइज़ेशन प्रदर्शन में सुधार करते हुए मॉडल की गुणवत्ता बनाए रखते हैं

## इंस्टॉलेशन

### आवश्यकताएँ

- Python 3.8 या उच्चतर
- pip पैकेज मैनेजर
- वर्चुअल एनवायरनमेंट (अनुशंसित)

### बेसिक इंस्टॉलेशन

वर्चुअल एनवायरनमेंट बनाएं और सक्रिय करें:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

ऑटो-ऑप्टिमाइज़ेशन सुविधाओं के साथ Olive इंस्टॉल करें:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### वैकल्पिक निर्भरताएँ

Olive अतिरिक्त सुविधाओं के लिए विभिन्न वैकल्पिक निर्भरताएँ प्रदान करता है:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### इंस्टॉलेशन सत्यापित करें

```bash
olive --help
```

यदि सफल हो, तो आपको Olive CLI हेल्प संदेश दिखाई देना चाहिए।

## त्वरित प्रारंभ गाइड

### आपका पहला ऑप्टिमाइज़ेशन

आइए Olive के ऑटो-ऑप्टिमाइज़ेशन फीचर का उपयोग करके एक छोटा भाषा मॉडल ऑप्टिमाइज़ करें:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### यह कमांड क्या करता है

ऑप्टिमाइज़ेशन प्रक्रिया में शामिल हैं: स्थानीय कैश से मॉडल प्राप्त करना, ONNX ग्राफ को कैप्चर करना और वेट्स को ONNX डेटा फ़ाइल में स्टोर करना, ONNX ग्राफ को ऑप्टिमाइज़ करना, और RTN विधि का उपयोग करके मॉडल को int4 में क्वांटाइज़ करना।

### कमांड पैरामीटर की व्याख्या

- `--model_name_or_path`: Hugging Face मॉडल पहचानकर्ता या स्थानीय पथ
- `--output_path`: वह डायरेक्टरी जहाँ ऑप्टिमाइज़ किया गया मॉडल सेव होगा
- `--device`: लक्षित डिवाइस (cpu, gpu)
- `--provider`: निष्पादन प्रदाता (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: इंफेरेंस के लिए ONNX Runtime Generate AI का उपयोग करें
- `--precision`: क्वांटाइज़ेशन सटीकता (int4, int8, fp16)
- `--log_level`: लॉगिंग की स्पष्टता (0=न्यूनतम, 1=विस्तृत)

## उदाहरण: Qwen3 को ONNX INT4 में बदलना

Hugging Face पर दिए गए उदाहरण [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) के आधार पर, यहाँ Qwen3 मॉडल को ऑप्टिमाइज़ करने का तरीका है:

### चरण 1: मॉडल डाउनलोड करें (वैकल्पिक)

डाउनलोड समय को कम करने के लिए केवल आवश्यक फ़ाइलें कैश करें:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### चरण 2: Qwen3 मॉडल ऑप्टिमाइज़ करें

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### चरण 3: ऑप्टिमाइज़ किए गए मॉडल का परीक्षण करें

अपने ऑप्टिमाइज़ किए गए मॉडल का परीक्षण करने के लिए एक सरल Python स्क्रिप्ट बनाएं:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### आउटपुट संरचना

ऑप्टिमाइज़ेशन के बाद, आपका आउटपुट डायरेक्टरी निम्नलिखित को शामिल करेगा:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## उन्नत उपयोग

### कॉन्फ़िगरेशन फ़ाइलें

अधिक जटिल ऑप्टिमाइज़ेशन वर्कफ़्लो के लिए, आप JSON कॉन्फ़िगरेशन फ़ाइलों का उपयोग कर सकते हैं:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

कॉन्फ़िगरेशन के साथ चलाएँ:

```bash
olive run --config config.json
```

### GPU ऑप्टिमाइज़ेशन

CUDA GPU ऑप्टिमाइज़ेशन के लिए:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) के लिए:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Olive के साथ फाइन-ट्यूनिंग

Olive मॉडल को फाइन-ट्यूनिंग का भी समर्थन करता है:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## सर्वोत्तम प्रथाएँ

### 1. मॉडल चयन
- परीक्षण के लिए छोटे मॉडल से शुरू करें (जैसे, 0.5B-7B पैरामीटर)
- सुनिश्चित करें कि आपका लक्षित मॉडल आर्किटेक्चर Olive द्वारा समर्थित है

### 2. हार्डवेयर विचार
- अपने ऑप्टिमाइज़ेशन लक्ष्य को अपने तैनाती हार्डवेयर से मिलाएँ
- यदि आपके पास CUDA-संगत हार्डवेयर है तो GPU ऑप्टिमाइज़ेशन का उपयोग करें
- Windows मशीनों के लिए DirectML पर विचार करें जिनमें इंटीग्रेटेड ग्राफिक्स हैं

### 3. सटीकता चयन
- **INT4**: अधिकतम कंप्रेशन, थोड़ी सटीकता हानि
- **INT8**: आकार और सटीकता का अच्छा संतुलन
- **FP16**: न्यूनतम सटीकता हानि, मध्यम आकार में कमी

### 4. परीक्षण और मान्यता
- हमेशा अपने विशिष्ट उपयोग मामलों के साथ ऑप्टिमाइज़ किए गए मॉडल का परीक्षण करें
- प्रदर्शन मेट्रिक्स (लेटेंसी, थ्रूपुट, सटीकता) की तुलना करें
- मूल्यांकन के लिए प्रतिनिधि इनपुट डेटा का उपयोग करें

### 5. पुनरावृत्त ऑप्टिमाइज़ेशन
- त्वरित परिणामों के लिए ऑटो-ऑप्टिमाइज़ेशन से शुरू करें
- फाइन-ग्रेन कंट्रोल के लिए कॉन्फ़िगरेशन फ़ाइलों का उपयोग करें
- विभिन्न ऑप्टिमाइज़ेशन पास के साथ प्रयोग करें

## समस्या निवारण

### सामान्य समस्याएँ

#### 1. इंस्टॉलेशन समस्याएँ
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU समस्याएँ
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. मेमोरी समस्याएँ
- ऑप्टिमाइज़ेशन के दौरान छोटे बैच आकार का उपयोग करें
- पहले उच्च सटीकता के साथ क्वांटाइज़ेशन का प्रयास करें (int8 के बजाय int4)
- मॉडल कैशिंग के लिए पर्याप्त डिस्क स्थान सुनिश्चित करें

#### 4. मॉडल लोडिंग त्रुटियाँ
- मॉडल पथ और एक्सेस अनुमतियों को सत्यापित करें
- जाँचें कि क्या मॉडल को `trust_remote_code=True` की आवश्यकता है
- सुनिश्चित करें कि सभी आवश्यक मॉडल फ़ाइलें डाउनलोड की गई हैं

### सहायता प्राप्त करना

- **डॉक्यूमेंटेशन**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **उदाहरण**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Recipes रिपॉजिटरी

### Olive Recipes का परिचय

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) रिपॉजिटरी मुख्य Olive टूलकिट को पूरक करती है और लोकप्रिय AI मॉडल के लिए उपयोग के लिए तैयार ऑप्टिमाइज़ेशन रेसिपी का व्यापक संग्रह प्रदान करती है। यह रिपॉजिटरी सार्वजनिक रूप से उपलब्ध मॉडल को ऑप्टिमाइज़ करने और मालिकाना मॉडल के लिए ऑप्टिमाइज़ेशन वर्कफ़्लो बनाने के लिए एक व्यावहारिक संदर्भ के रूप में कार्य करती है।

### मुख्य विशेषताएँ

- **100+ प्री-बिल्ट रेसिपी**: लोकप्रिय मॉडलों के लिए उपयोग के लिए तैयार ऑप्टिमाइज़ेशन कॉन्फ़िगरेशन
- **मल्टी-आर्किटेक्चर सपोर्ट**: ट्रांसफॉर्मर मॉडल, विज़न मॉडल और मल्टीमॉडल आर्किटेक्चर को कवर करता है
- **हार्डवेयर-विशिष्ट ऑप्टिमाइज़ेशन**: CPU, GPU और विशेष एक्सेलेरेटर के लिए तैयार रेसिपी
- **लोकप्रिय मॉडल परिवार**: Phi, Llama, Qwen, Gemma, Mistral और कई अन्य शामिल हैं

### समर्थित मॉडल परिवार

रिपॉजिटरी में निम्नलिखित के लिए ऑप्टिमाइज़ेशन रेसिपी शामिल हैं:

#### भाषा मॉडल
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 श्रृंखला (0.5B से 14B)
- **Google Gemma**: विभिन्न Gemma मॉडल कॉन्फ़िगरेशन
- **Mistral AI**: Mistral-7B श्रृंखला
- **DeepSeek**: R1-Distill श्रृंखला मॉडल

#### विज़न और मल्टीमॉडल मॉडल
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP मॉडल**: विभिन्न CLIP-ViT कॉन्फ़िगरेशन
- **ResNet**: ResNet-50 ऑप्टिमाइज़ेशन
- **विज़न ट्रांसफॉर्मर्स**: ViT-base-patch16-224

#### विशेष मॉडल
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: बेस और मल्टीलिंगुअल वेरिएंट
- **सेंटेंस ट्रांसफॉर्मर्स**: all-MiniLM-L6-v2

### Olive Recipes का उपयोग करना

#### विधि 1: विशिष्ट रेसिपी क्लोन करें

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### विधि 2: रेसिपी को टेम्पलेट के रूप में उपयोग करें

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### रेसिपी संरचना

प्रत्येक रेसिपी डायरेक्टरी में आमतौर पर निम्नलिखित शामिल होते हैं:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### उदाहरण: Phi-4-mini रेसिपी का उपयोग करना

आइए Phi-4-mini रेसिपी का उपयोग एक उदाहरण के रूप में करें:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

कॉन्फ़िगरेशन फ़ाइल आमतौर पर निम्नलिखित को शामिल करती है:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### रेसिपी को अनुकूलित करना

#### लक्षित हार्डवेयर को संशोधित करना

लक्ष्य हार्डवेयर बदलने के लिए, `systems` अनुभाग को अपडेट करें:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### ऑप्टिमाइज़ेशन पैरामीटर को समायोजित करना

विभिन्न ऑप्टिमाइज़ेशन स्तरों के लिए `passes` अनुभाग को संशोधित करें:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### अपनी खुद की रेसिपी बनाना

1. **समान मॉडल से शुरू करें**: समान आर्किटेक्चर वाले मॉडल के लिए एक रेसिपी खोजें
2. **मॉडल कॉन्फ़िगरेशन अपडेट करें**: कॉन्फ़िगरेशन में मॉडल नाम/पथ बदलें
3. **पैरामीटर समायोजित करें**: आवश्यकतानुसार ऑप्टिमाइज़ेशन पैरामीटर संशोधित करें
4. **परीक्षण और मान्यता**: ऑप्टिमाइज़ेशन चलाएँ और परिणामों को मान्य करें
5. **वापस योगदान करें**: अपनी रेसिपी को रिपॉजिटरी में योगदान करने पर विचार करें

### रेसिपी का उपयोग करने के लाभ

#### 1. **सिद्ध कॉन्फ़िगरेशन**
- विशिष्ट मॉडलों के लिए परीक्षण किए गए ऑप्टिमाइज़ेशन सेटिंग्स
- इष्टतम पैरामीटर खोजने में ट्रायल-एंड-एरर से बचता है

#### 2. **हार्डवेयर-विशिष्ट ट्यूनिंग**
- विभिन्न निष्पादन प्रदाताओं के लिए पूर्व-ऑप्टिमाइज़ किया गया
- CPU, GPU और NPU लक्ष्यों के लिए उपयोग के लिए तैयार कॉन्फ़िगरेशन

#### 3. **व्यापक कवरेज**
- सबसे लोकप्रिय ओपन-सोर्स मॉडलों का समर्थन करता है
- नए मॉडल रिलीज़ के साथ नियमित अपडेट

#### 4. **समुदाय योगदान**
- AI समुदाय के साथ सहयोगात्मक विकास
- साझा ज्ञान और सर्वोत्तम प्रथाएँ

### Olive Recipes में योगदान करना

यदि आपने किसी मॉडल को ऑप्टिमाइज़ किया है जो रिपॉजिटरी में शामिल नहीं है:

1. **रिपॉजिटरी को फोर्क करें**: olive-recipes की अपनी खुद की फोर्क बनाएं
2. **रेसिपी डायरेक्टरी बनाएं**: अपने मॉडल के लिए एक नई डायरेक्टरी जोड़ें
3. **कॉन्फ़िगरेशन शामिल करें**: olive_config.json और सहायक फ़ाइलें जोड़ें
4. **उपयोग का दस्तावेज़ीकरण करें**: स्पष्ट README के साथ निर्देश प्रदान करें
5. **पुल रिक्वेस्ट सबमिट करें**: समुदाय में योगदान करें

### प्रदर्शन बेंचमार्क

कई रेसिपी में प्रदर्शन बेंचमार्क शामिल होते हैं जो दिखाते हैं:
- **लेटेंसी सुधार**: बेसलाइन पर सामान्यतः 2-6x गति वृद्धि
- **मेमोरी में कमी**: क्वांटाइज़ेशन के साथ 50-75% मेमोरी उपयोग में कमी
- **सटीकता बनाए रखना**: 95-99% सटीकता संरक्षण

### AI टूलकिट के साथ एकीकरण

रेसिपी सहजता से काम करती हैं:
- **VS Code AI Toolkit**: मॉडल ऑप्टिमाइज़ेशन के लिए सीधा एकीकरण
- **Azure Machine Learning**: क्लाउड-आधारित ऑप्टिमाइज़ेशन वर्कफ़्लो
- **ONNX Runtime**: ऑप्टिमाइज़्ड इंफेरेंस तैनाती

## अतिरिक्त संसाधन

### आधिकारिक लिंक
- **GitHub रिपॉजिटरी**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipes रिपॉजिटरी**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime डॉक्यूमेंटेशन**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face उदाहरण**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### समुदाय उदाहरण
- **Jupyter नोटबुक्स**: Olive GitHub रिपॉजिटरी में उपलब्ध — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code एक्सटेंशन**: VS Code के लिए AI Toolkit का अवलोकन — https://learn.microsoft.com/azure/ai-toolkit/overview
- **ब्लॉग पोस्ट्स**: Microsoft ओपन सोर्स ब्लॉग — https://opensource.microsoft.com/blog/

### संबंधित उपकरण
- **ONNX Runtime**: उच्च-प्रदर्शन इंफेरेंस इंजन — https://onnxruntime.ai/
- **Hugging Face Transformers**: कई संगत मॉडलों का स्रोत — https://huggingface.co/docs/transformers/index

---

**अस्वीकरण**:  
यह दस्तावेज़ AI अनुवाद सेवा [Co-op Translator](https://github.com/Azure/co-op-translator) का उपयोग करके अनुवादित किया गया है। जबकि हम सटीकता के लिए प्रयास करते हैं, कृपया ध्यान दें कि स्वचालित अनुवाद में त्रुटियां या अशुद्धियां हो सकती हैं। मूल भाषा में दस्तावेज़ को आधिकारिक स्रोत माना जाना चाहिए। महत्वपूर्ण जानकारी के लिए, पेशेवर मानव अनुवाद की सिफारिश की जाती है। इस अनुवाद के उपयोग से उत्पन्न किसी भी गलतफहमी या गलत व्याख्या के लिए हम उत्तरदायी नहीं हैं।