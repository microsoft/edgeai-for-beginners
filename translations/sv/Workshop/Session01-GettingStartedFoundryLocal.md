# Session 1: Komma ig√•ng med Foundry Local

## Sammanfattning

L√§r dig att installera, konfigurera och k√∂ra dina f√∂rsta AI-modeller med Microsoft Foundry Local. Denna praktiska session ger en steg-f√∂r-steg introduktion till lokal inferens, fr√•n installation till att bygga din f√∂rsta chattapplikation med modeller som Phi-4, Qwen och DeepSeek.

## L√§randem√•l

Efter denna session kommer du att kunna:

- **Installera och konfigurera**: St√§lla in Foundry Local med korrekt installationsverifiering
- **Beh√§rska CLI-operationer**: Anv√§nda Foundry Local CLI f√∂r modellhantering och distribution
- **K√∂ra din f√∂rsta modell**: Framg√•ngsrikt distribuera och interagera med en lokal AI-modell
- **Bygga en chattapplikation**: Skapa en grundl√§ggande chattapplikation med Foundry Local Python SDK
- **F√∂rst√• lokal AI**: F√• en grundl√§ggande f√∂rst√•else f√∂r lokal inferens och modellhantering

## F√∂rkunskapskrav

### Systemkrav

- **Windows**: Windows 11 (22H2 eller senare) ELLER **macOS**: macOS 11+ (begr√§nsat st√∂d)
- **RAM**: Minst 8GB, rekommenderat 16GB+
- **Lagring**: Minst 10GB ledigt utrymme f√∂r modeller
- **Python**: Version 3.10 eller senare installerad
- **Admin√•tkomst**: Administrat√∂rsbeh√∂righet f√∂r installation

### Utvecklingsmilj√∂

- Visual Studio Code med Python-till√§gg (rekommenderas)
- Kommandorads√•tkomst (PowerShell p√• Windows, Terminal p√• macOS)
- Git f√∂r att klona repositories (valfritt)

## Workshopfl√∂de (30 minuter)

### Steg 1: Installera Foundry Local (5 minuter)

#### Installation p√• Windows

Installera Foundry Local med Windows paketmanager:

```powershell
# Install via winget (recommended)
winget install Microsoft.FoundryLocal
```

Alternativ: Ladda ner direkt fr√•n [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)

#### Installation p√• macOS (begr√§nsat st√∂d)

> [!NOTE] 
> macOS-st√∂d √§r f√∂r n√§rvarande i f√∂rhandsgranskning. Kontrollera officiell dokumentation f√∂r senaste tillg√§nglighet.

Om tillg√§ngligt, installera med Homebrew:

```bash
# If Homebrew formula is available
brew update
brew install foundry-local

# Or manual download (check official docs for latest)
curl -L -o foundry-local.tar.gz "https://download.microsoft.com/foundry-local/latest/macos/foundry-local.tar.gz"
tar -xzf foundry-local.tar.gz
sudo ./install.sh
```

**Alternativ f√∂r macOS-anv√§ndare:**
- Anv√§nd en Windows 11 VM (Parallels/UTM) och f√∂lj Windows-stegen
- K√∂r via container om tillg√§ngligt och konfigurera `FOUNDRY_LOCAL_ENDPOINT`

### Steg 2: Verifiera installationen (3 minuter)

Efter installationen, starta om din terminal och verifiera att Foundry Local fungerar:

```powershell
# Check if Foundry Local is installed correctly
foundry --version

# View available commands
foundry --help
```

F√∂rv√§ntad output b√∂r visa versionsinformation och tillg√§ngliga kommandon.

### Steg 3: St√§ll in Python-milj√∂ (5 minuter)

Skapa en dedikerad Python-milj√∂ f√∂r denna workshop:

**Windows:**
```powershell
# Create virtual environment
py -m venv .venv

# Activate environment
.\.venv\Scripts\Activate.ps1

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

**macOS/Linux:**
```bash
# Create virtual environment
python3 -m venv .venv

# Activate environment
source .venv/bin/activate

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

### Steg 4: K√∂r din f√∂rsta modell (7 minuter)

Nu ska vi k√∂ra v√•r f√∂rsta AI-modell lokalt!

#### B√∂rja med Phi-4 Mini (Rekommenderad f√∂rsta modell)

```powershell
# Download and start phi-4-mini (lightweight, fast)
foundry model run phi-4-mini

# Test the model with a simple prompt
foundry model run phi-4-mini --prompt "Hello, introduce yourself in one sentence"
```

> [!TIP]
> Detta kommando laddar ner modellen (f√∂rsta g√•ngen) och startar automatiskt Foundry Local-tj√§nsten.

#### Kontrollera vad som k√∂rs

```powershell
# List available models (shows downloaded models)
foundry model list

# Check service status
foundry service status

# See what models are cached locally
foundry cache list
```

#### Testa olika modeller

N√§r phi-4-mini fungerar, experimentera med andra modeller:

```powershell
# Larger model with better capabilities
foundry model run gpt-oss-20b --prompt "Explain edge AI in simple terms"

# Fast, efficient model
foundry model run qwen2.5-0.5b --prompt "What are the benefits of local AI inference?"
```

### Steg 5: Bygg din f√∂rsta chattapplikation (10 minuter)

Nu ska vi skapa en Python-applikation som anv√§nder de modeller vi just startade.

#### Skapa chattskriptet

Skapa en ny fil som heter `my_first_chat.py` (eller anv√§nd det medf√∂ljande exemplet):

```python
#!/usr/bin/env python3
"""
My First Foundry Local Chat Application
Using FoundryLocalManager for automatic service management
"""

import os
from foundry_local import FoundryLocalManager
from openai import OpenAI

def main():
    # Get model alias from environment or use default
    alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-4-mini")
    
    try:
        # Initialize Foundry Local Manager (auto-starts service, downloads model)
        manager = FoundryLocalManager(alias)
        
        # Create OpenAI client pointing to local endpoint
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-needed"
        )
        
        # Get the actual model ID for this alias
        model_id = manager.get_model_info(alias).id
        
        print("ü§ñ Welcome to your first local AI chat!")
        print(f"ÔøΩ Using model: {alias} -> {model_id}")
        print(f"üåê Endpoint: {manager.endpoint}")
        print("ÔøΩüí° Type 'quit' to exit\n")
        
    except Exception as e:
        print(f"‚ùå Failed to initialize Foundry Local: {e}")
        print("üí° Make sure Foundry Local is installed: foundry --version")
        return
    
    while True:
        # Get user input
        user_message = input("You: ").strip()
        
        if user_message.lower() in ['quit', 'exit', 'bye']:
            print("üëã Goodbye!")
            break
            
        if not user_message:
            continue
            
        try:
            # Send message to local AI model
            response = client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": "You are a helpful AI assistant running locally."},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=200,
                temperature=0.7
            )
            
            # Display the response
            ai_response = response.choices[0].message.content
            print(f"ü§ñ AI: {ai_response}\n")
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            print("üí° Check service status: foundry service status\n")

if __name__ == "__main__":
    main()
```

> [!TIP]
> **Relaterade exempel**: F√∂r mer avancerad anv√§ndning, se:
>
> - **Python-exempel**: `Workshop/samples/session01/chat_bootstrap.py` - Inkluderar str√∂mmande svar och felhantering
> - **Jupyter Notebook**: `Workshop/notebooks/session01_chat_bootstrap.ipynb` - Interaktiv version med detaljerade f√∂rklaringar

#### Testa din chattapplikation

```powershell
# No need to manually start models - FoundryLocalManager handles this!
# Just run your chat application
python my_first_chat.py
```

Alternativ: Anv√§nd de medf√∂ljande exemplen direkt

```powershell
# Try the complete sample with streaming support
cd Workshop/samples
python -m session01.chat_bootstrap "Your question here"
```

Eller utforska den interaktiva notebooken
√ñppna Workshop/notebooks/session01_chat_bootstrap.ipynb i VS Code

Testa dessa exempelkonversationer:

- "Vad √§r Microsoft Foundry Local?"
- "Lista 3 f√∂rdelar med att k√∂ra AI-modeller lokalt"
- "Hj√§lp mig f√∂rst√• edge AI"

## Vad du har √•stadkommit

Grattis! Du har framg√•ngsrikt:

1. ‚úÖ **Installerat Foundry Local** och verifierat att det fungerar
2. ‚úÖ **Startat din f√∂rsta AI-modell** (phi-4-mini) lokalt
3. ‚úÖ **Testat olika modeller** via kommandoraden
4. ‚úÖ **Byggt en chattapplikation** som ansluter till din lokala AI
5. ‚úÖ **Upplevt lokal AI-inferens** utan molnberoenden

## F√∂rst√• vad som h√§nde

### Lokal AI-inferens

- Dina AI-modeller k√∂rs helt p√• din dator
- Ingen data skickas till molnet
- Svar genereras lokalt med din CPU/GPU
- Integritet och s√§kerhet uppr√§tth√•lls

### Modellhantering

- `foundry model run` laddar ner och startar modeller
- **FoundryLocalManager SDK** hanterar automatiskt tj√§nststart och modellinl√§sning
- Modeller cachas lokalt f√∂r framtida anv√§ndning
- Flera modeller kan laddas ner, men vanligtvis k√∂rs en √•t g√•ngen
- Tj√§nsten hanterar automatiskt modellens livscykel

### SDK vs CLI-metoder

- **CLI-metod**: Manuell modellhantering med `foundry model run <model>`
- **SDK-metod**: Automatisk tj√§nst + modellhantering med `FoundryLocalManager(alias)`
- **Rekommendation**: Anv√§nd SDK f√∂r applikationer, CLI f√∂r testning och utforskning

## Referens f√∂r vanliga kommandon

### Viktiga CLI-kommandon

```powershell
# Installation & Setup
foundry --version              # Check installation
foundry --help                 # View all commands

# Model Management
foundry model list             # List available models
foundry model run <model>      # Download and start a model
foundry model run <model> --prompt "text"  # One-shot prompt
foundry cache list             # Show downloaded models

# Service Management
foundry service status         # Check if service is running
foundry service start          # Start the service manually
foundry service stop           # Stop the service
```

### Modellrekommendationer

- **phi-4-mini**: B√§sta startmodellen - snabb, l√§ttviktig, bra kvalitet
- **qwen2.5-0.5b**: Snabbaste inferensen, minimal minnesanv√§ndning
- **gpt-oss-20b**: H√∂gre kvalitet p√• svar, kr√§ver mer resurser
- **deepseek-coder-1.3b**: Optimerad f√∂r programmering och koduppgifter

## Fels√∂kning

### "Foundry command not found"

**L√∂sning:**

```powershell
# Restart your terminal after installation
# Or manually add to PATH (Windows)
$env:PATH += ";C:\Program Files\Microsoft\FoundryLocal"
```

### "Model failed to load"

**L√∂sning:**

```powershell
# Check available system memory
foundry service status

# Try a smaller model first
foundry model run phi-4-mini

# Check disk space for model downloads
# Models are stored in: %USERPROFILE%\.foundry\models (Windows)
```

### "Connection refused on localhost"

**L√∂sning:**

```powershell
# Check if service is running
foundry service status

# Start service if needed
foundry service start

# Verify the port (default is 5273)
# Check for port conflicts with: netstat -an | findstr 5273
```

## N√§sta steg

### Omedelbara n√§sta √•tg√§rder

1. **Experimentera** med olika modeller och fr√•gor
2. **Modifiera** din chattapplikation f√∂r att testa olika modeller
3. **Skapa** egna fr√•gor och testa svar
4. **Utforska** Session 2: Bygga RAG-applikationer

### Avancerad l√§rv√§g

1. **Session 2**: Bygg AI-l√∂sningar med RAG (Retrieval-Augmented Generation)
2. **Session 3**: J√§mf√∂r olika open-source-modeller
3. **Session 4**: Arbeta med de senaste modellerna
4. **Session 5**: Bygg multi-agent AI-system

## Milj√∂variabler (valfritt)

F√∂r mer avancerad anv√§ndning kan du st√§lla in dessa milj√∂variabler:

| Variabel | Syfte | Exempel |
|----------|-------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Standardmodell att anv√§nda | `phi-4-mini` |
| `FOUNDRY_LOCAL_ENDPOINT` | √Ösidos√§tt URL f√∂r endpoint | `http://localhost:5273/v1` |

Skapa en `.env`-fil i din projektmapp:
```
FOUNDRY_LOCAL_ALIAS=phi-4-mini
FOUNDRY_LOCAL_ENDPOINT=auto
```

## Ytterligare resurser

### Dokumentation

- [Foundry Local Python SDK Referens](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk?pivots=programming-language-python)
- [Foundry Local Installationsguide](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)
- [Modellkatalog](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/models)

### Exempelkod

- **Session01 Python-exempel**: `Workshop/samples/session01/chat_bootstrap.py` - Komplett chattapp med str√∂mning
- **Session01 Notebook**: `Workshop/notebooks/session01_chat_bootstrap.ipynb` - Interaktiv handledning  
- [Module08 Sample 01](../Module08/samples/01/README.md) - REST Chat Snabbstart
- [Module08 Sample 02](../Module08/samples/02/README.md) - OpenAI SDK Integration
- [Module08 Sample 03](../Module08/samples/03/README.md) - Modelluppt√§ckt & Benchmarking

### Community

- [Foundry Local GitHub Diskussioner](https://github.com/microsoft/Foundry-Local/discussions)
- [Azure AI Community](https://techcommunity.microsoft.com/category/artificialintelligence)

---

**Sessionens l√§ngd**: 30 minuter praktiskt arbete + 15 minuter Q&A  
**Sv√•righetsniv√•**: Nyb√∂rjare  
**F√∂rkunskapskrav**: Windows 11/macOS 11+, Python 3.10+, Admin√•tkomst

## Workshop Exempelscenario

### Verklig kontext

**Scenario**: Ett f√∂retags IT-team beh√∂ver utv√§rdera AI-inferens p√• enheten f√∂r att bearbeta k√§nslig feedback fr√•n anst√§llda utan att skicka data till externa tj√§nster.

**Ditt m√•l**: Visa att lokala AI-modeller kan ge kvalitativa svar med sub-sekund latens samtidigt som fullst√§ndig dataintegritet uppr√§tth√•lls.

### Testfr√•gor

Anv√§nd dessa fr√•gor f√∂r att validera din installation:

```json
[
    "List two benefits of local inference.",
    "Summarize why keeping data on device improves privacy.",
    "Give one trade-off when choosing a small model over a large model."
]
```

### Framg√•ngskriterier

- ‚úÖ Alla fr√•gor f√•r svar inom 2 sekunder
- ‚úÖ Ingen data l√§mnar din lokala maskin
- ‚úÖ Svaren √§r relevanta och hj√§lpsamma
- ‚úÖ Din chattapplikation fungerar smidigt

Denna validering s√§kerst√§ller att din Foundry Local-installation √§r redo f√∂r de avancerade workshops i Sessionerna 2-6.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Ansvarsfriskrivning**:  
Detta dokument har √∂versatts med hj√§lp av AI-√∂vers√§ttningstj√§nsten [Co-op Translator](https://github.com/Azure/co-op-translator). √Ñven om vi str√§var efter noggrannhet, b√∂r det noteras att automatiserade √∂vers√§ttningar kan inneh√•lla fel eller felaktigheter. Det ursprungliga dokumentet p√• dess ursprungliga spr√•k b√∂r betraktas som den auktoritativa k√§llan. F√∂r kritisk information rekommenderas professionell m√§nsklig √∂vers√§ttning. Vi ansvarar inte f√∂r eventuella missf√∂rst√•nd eller feltolkningar som uppst√•r vid anv√§ndning av denna √∂vers√§ttning.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->