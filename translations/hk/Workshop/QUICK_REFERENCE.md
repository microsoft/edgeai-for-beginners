<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "93615ab69c8773b52c4437d537f6acea",
  "translation_date": "2025-10-28T20:45:23+00:00",
  "source_file": "Workshop/QUICK_REFERENCE.md",
  "language_code": "hk"
}
-->
# å·¥ä½œåŠç¯„ä¾‹ - å¿«é€Ÿåƒè€ƒå¡

**æœ€å¾Œæ›´æ–°**ï¼š2025å¹´10æœˆ8æ—¥

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

```bash
# 1. Ensure Foundry Local is running
foundry service status
foundry model run phi-4-mini

# 2. Install dependencies
pip install -r Workshop/requirements.txt

# 3. Run a sample
cd Workshop/samples
python -m session01.chat_bootstrap "What is edge AI?"
```

---

## ğŸ“‚ ç¯„ä¾‹æ¦‚è¦½

| èª²ç¨‹ | ç¯„ä¾‹ | ç›®çš„ | æ™‚é–“ |
|------|------|------|------|
| 01 | `chat_bootstrap.py` | åŸºæœ¬èŠå¤© + ä¸²æµ | ~30ç§’ |
| 02 | `rag_pipeline.py` | RAGèˆ‡åµŒå…¥ | ~45ç§’ |
| 02 | `rag_eval_ragas.py` | RAGè©•ä¼° | ~60ç§’ |
| 03 | `benchmark_oss_models.py` | æ¨¡å‹åŸºæº–æ¸¬è©¦ | ~2åˆ†é˜ |
| 04 | `model_compare.py` | SLM vs LLM | ~45ç§’ |
| 05 | `agents_orchestrator.py` | å¤šä»£ç†ç³»çµ± | ~60ç§’ |
| 06 | `models_router.py` | æ„åœ–è·¯ç”± | ~45ç§’ |
| 06 | `models_pipeline.py` | å¤šæ­¥é©Ÿç®¡é“ | ~60ç§’ |

---

## ğŸ› ï¸ ç’°å¢ƒè®Šæ•¸

### å¿…éœ€
```bash
# Choose model
set FOUNDRY_LOCAL_ALIAS=phi-4-mini

# Override endpoint (optional)
set FOUNDRY_LOCAL_ENDPOINT=http://localhost:8000

# Show token usage
set SHOW_USAGE=1
```

### ç‰¹å®šèª²ç¨‹
```bash
# Session 02: RAG
set RAG_QUESTION="What is local inference?"
set EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Session 03: Benchmarking
set BENCH_MODELS=phi-4-mini,qwen2.5-0.5b
set BENCH_ROUNDS=3
set BENCH_STREAM=1

# Session 04: Comparison
set SLM_ALIAS=phi-4-mini
set LLM_ALIAS=qwen2.5-7b

# Session 05: Agents
set AGENT_MODEL_PRIMARY=phi-4-mini
set AGENT_QUESTION="Why use edge AI?"

# Session 06: Pipeline
set PIPELINE_TASK="Your task here"
```

---

## âœ… é©—è­‰èˆ‡æ¸¬è©¦

```bash
# Validate syntax and imports
python scripts/validate_samples.py

# Validate specific session
python scripts/validate_samples.py --session 01

# Run smoke tests
python scripts/test_samples.py --quick

# Verbose testing
python scripts/test_samples.py --verbose
```

---

## ğŸ› ç–‘é›£æ’è§£

### é€£æ¥éŒ¯èª¤
```bash
# Check Foundry Local
foundry service status

# Start if needed
foundry service start
foundry model run phi-4-mini
```

### åŒ¯å…¥éŒ¯èª¤
```bash
# Install missing dependencies
pip install sentence-transformers ragas datasets

# Or install all
pip install -r Workshop/requirements.txt
```

### æ¨¡å‹æœªæ‰¾åˆ°
```bash
# List available models
foundry model ls

# Download model
foundry model download phi-4-mini
```

### æ€§èƒ½ç·©æ…¢
```bash
# Use smaller model
set FOUNDRY_LOCAL_ALIAS=qwen2.5-0.5b

# Reduce benchmark rounds
set BENCH_ROUNDS=1
```

---

## ğŸ“– å¸¸è¦‹æ¨¡å¼

### åŸºæœ¬èŠå¤©
```python
from workshop_utils import chat_once

text, usage = chat_once(
    'phi-4-mini',
    messages=[{"role": "user", "content": "Hello"}],
    max_tokens=100,
    temperature=0.7
)
```

### ç²å–å®¢æˆ¶ç«¯
```python
from workshop_utils import get_client

manager, client, model_id = get_client(
    alias='phi-4-mini',
    endpoint=None  # Auto-detect
)
```

### éŒ¯èª¤è™•ç†
```python
try:
    manager, client, model_id = get_client(alias)
except Exception as e:
    print(f"[ERROR] Failed: {e}")
    print("[INFO] Check: foundry service status")
    sys.exit(1)
```

### ä¸²æµ
```python
stream = client.chat.completions.create(
    model=model_id,
    messages=messages,
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

---

## ğŸ“Š æ¨¡å‹é¸æ“‡

| æ¨¡å‹ | å¤§å° | æœ€é©ç”¨é€” | é€Ÿåº¦ |
|------|------|----------|------|
| `qwen2.5-0.5b` | 0.5B | å¿«é€Ÿåˆ†é¡ | âš¡âš¡âš¡ |
| `qwen2.5-coder-0.5b` | 0.5B | å¿«é€Ÿä»£ç¢¼ç”Ÿæˆ | âš¡âš¡âš¡ |
| `gemma-2-2b` | 2B | å‰µæ„å¯«ä½œ | âš¡âš¡ |
| `phi-3.5-mini` | 3.5B | ä»£ç¢¼ã€é‡æ§‹ | âš¡âš¡ |
| `phi-4-mini` | 4B | é€šç”¨ã€æ‘˜è¦ | âš¡âš¡ |
| `qwen2.5-7b` | 7B | è¤‡é›œæ¨ç† | âš¡ |

---

## ğŸ”— è³‡æº

- **SDKæ–‡ä»¶**ï¼šhttps://github.com/microsoft/Foundry-Local/tree/main/sdk/python
- **å¿«é€Ÿåƒè€ƒ**ï¼š`Workshop/FOUNDRY_SDK_QUICKREF.md`
- **æ›´æ–°æ‘˜è¦**ï¼š`Workshop/SAMPLES_UPDATE_SUMMARY.md`
- **é·ç§»èªªæ˜**ï¼š`Workshop/SDK_MIGRATION_NOTES.md`

---

## ğŸ’¡ æç¤º

1. **ç·©å­˜å®¢æˆ¶ç«¯**ï¼š`workshop_utils` ç‚ºæ‚¨ç·©å­˜
2. **ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹**ï¼šæ¸¬è©¦æ™‚å…ˆä½¿ç”¨ `qwen2.5-0.5b`
3. **å•Ÿç”¨ä½¿ç”¨çµ±è¨ˆ**ï¼šè¨­ç½® `SHOW_USAGE=1` ä»¥è¿½è¹¤ token ä½¿ç”¨æƒ…æ³
4. **æ‰¹é‡è™•ç†**ï¼šä¾æ¬¡è™•ç†å¤šå€‹æç¤º
5. **é™ä½ max_tokens**ï¼šæ¸›å°‘å»¶é²ä»¥ç²å¾—å¿«é€Ÿå›æ‡‰

---

## ğŸ¯ ç¯„ä¾‹å·¥ä½œæµç¨‹

### æ¸¬è©¦æ‰€æœ‰å…§å®¹
```bash
python scripts/validate_samples.py
python scripts/test_samples.py --quick
```

### åŸºæº–æ¸¬è©¦æ¨¡å‹
```bash
cd samples
set BENCH_MODELS=phi-4-mini,qwen2.5-0.5b
set BENCH_ROUNDS=3
python -m session03.benchmark_oss_models
```

### RAGç®¡é“
```bash
cd samples
set RAG_QUESTION="What is RAG?"
python -m session02.rag_pipeline
```

### å¤šä»£ç†ç³»çµ±
```bash
cd samples
set AGENT_QUESTION="Why edge AI for healthcare?"
python -m session05.agents_orchestrator
```

---

**å¿«é€Ÿå¹«åŠ©**ï¼šåœ¨ `samples` ç›®éŒ„ä¸­ä½¿ç”¨ `--help` é‹è¡Œä»»ä½•ç¯„ä¾‹æˆ–æŸ¥çœ‹æ–‡æª”å­—ç¬¦ä¸²ï¼š
```bash
python -c "import session01.chat_bootstrap; help(session01.chat_bootstrap)"
```

---

**æ‰€æœ‰ç¯„ä¾‹å·²æ–¼2025å¹´10æœˆæ›´æ–°ï¼Œç¬¦åˆFoundry Local SDKæœ€ä½³å¯¦è¸** âœ¨

---

**å…è²¬è²æ˜**ï¼š  
æ­¤æ–‡ä»¶å·²ä½¿ç”¨äººå·¥æ™ºèƒ½ç¿»è­¯æœå‹™ [Co-op Translator](https://github.com/Azure/co-op-translator) é€²è¡Œç¿»è­¯ã€‚å„˜ç®¡æˆ‘å€‘è‡´åŠ›æ–¼æä¾›æº–ç¢ºçš„ç¿»è­¯ï¼Œä½†è«‹æ³¨æ„ï¼Œè‡ªå‹•ç¿»è­¯å¯èƒ½åŒ…å«éŒ¯èª¤æˆ–ä¸æº–ç¢ºä¹‹è™•ã€‚åŸå§‹æ–‡ä»¶çš„æ¯èªç‰ˆæœ¬æ‡‰è¢«è¦–ç‚ºæ¬Šå¨ä¾†æºã€‚å°æ–¼é‡è¦ä¿¡æ¯ï¼Œå»ºè­°ä½¿ç”¨å°ˆæ¥­äººå·¥ç¿»è­¯ã€‚æˆ‘å€‘ä¸å°å› ä½¿ç”¨æ­¤ç¿»è­¯è€Œå¼•èµ·çš„ä»»ä½•èª¤è§£æˆ–èª¤é‡‹æ‰¿æ“”è²¬ä»»ã€‚