{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5bca23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent_framework.ollama import OllamaChatClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07752b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from datetime import datetime, timezone\n",
    "from random import randint\n",
    "from pydantic import Field\n",
    "from typing import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88289e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_search(\n",
    "    query: Annotated[str, Field(description=\"Search query\")],\n",
    ") -> str:\n",
    "    import requests\n",
    "    \n",
    "    api_key = os.getenv(\"OLLAMA_API_KEY\")\n",
    "    if not api_key:\n",
    "        return \"Error: OLLAMA_API_KEY environment variable not set\"\n",
    "    \n",
    "    url = \"https://ollama.com/api/web_search\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"query\": query\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=data)\n",
    "        response.raise_for_status()\n",
    "        print(response.text)\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error fetching web content: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13906dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OllamaChatClient(model_id=os.getenv(\"OLLAMA_CHAT_MODEL_ID\")).as_agent(\n",
    "        name=\"SearchAgent\",\n",
    "        instructions=\"You are my assistant. Answer the questions based on the search engine.\",\n",
    "        tools=[web_search],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "040ed613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What's ollama?\n",
      "{\"results\":[{\"title\":\"What is Ollama? Complete Guide to Local AI Models (December 2025)\",\"url\":\"https://www.thundercompute.com/blog/what-is-ollama-run-ai-models-locally\",\"content\":\"What is Ollama? Complete Guide to Local AI Models (December 2025)\\nWe\\u0026#x27;ll match 100% of your first credit purchase, up to $50\\n[\\n] \\n[Log in] \\n[\\nGet started\\n] \\n# What is Ollama? Complete Guide to Local AI Models (December 2025)\\n![] \\nOctober 17, 2025\\nYou've probably hit that frustrating wall where cloud AI APIs are draining your budget, or maybe you're wondering if there's a way to run powerful models without sending your data to third parties. If you're looking into what is Ollama and how it could solve these exact problems, you're in the right place.\\nOllama has become one of the most compelling solutions for running AI models locally, and we're going to break down everything you need to know to get started, as well as the best way to run your Ollama workflows on a dedicated[GPU cloud].\\n**TLDR:**\\n* Ollama lets you run powerful AI models locally without cloud costs or privacy concerns\\n* You can deploy models like DeepSeek and Llama in minutes with simple commands and no ML expertise\\n* Local deployment eliminates API fees and keeps sensitive data on your infrastructure\\n* Thunder Compute provides dedicated GPU resources when you outgrow local hardware limits\\n* Ollama works for everything from code completion to enterprise chatbots requiring data privacy## What is Ollama\\n![Screenshot 2025-10-02 at 3.29.02‚ÄØPM.png] \\nOllama is a free tool that allows you to run AI models directly on your local machine.\\nAt its core, Ollama is a locally deployed AI model runner. In simpler terms, it lets you download and run AI models on your own machine, without relying on cloud-hosted services. This makes it a powerful alternative to cloud-based AI services for users who value data privacy, cost control, and offline functionality.\\nUnlike proprietary cloud services, Ollama gives you complete control over your AI models and data.\\nOllama represents a fundamental shift toward democratized AI, putting powerful language models directly in the hands of developers without the overhead of cloud dependencies or privacy concerns.\\nWhat sets Ollama apart is its focus on simplicity. You don't need extensive machine learning expertise to get started. The installation process is straightforward, and within minutes, you can have a capable LLM running on your laptop.\\n## How Ollama Works\\nOllama creates an isolated environment to run LLMs locally on your system, which prevents any potential conflicts with other installed software. This environment already includes all the necessary components for deploying AI models.\\nThe process is remarkably straightforward. First, you pull models from the Ollama library using simple commands. Then, you run these models as-is or adjust parameters to customize them for specific tasks. After the setup, you can interact with the models by entering prompts, and they'll generate responses just like ChatGPT or Claude.\\n![Screenshot 2025-10-02 at 3.30.28‚ÄØPM.png] \\nOne of the standout features of Ollama is its use of quantization to optimize model performance. Quantization reduces the computational load, allowing these models to run efficiently on consumer-grade laptops and desktops. This is no small feat, considering the size and complexity of LLMs.\\nThe magic happens through model compression techniques that maintain most of the original model's performance while dramatically reducing memory requirements. Models requiring tens of GBs of VRAM can often be compressed to run on 8GB consumer hardware.\\nOllama also handles all the technical complexity behind the scenes. You don't need to worry about CUDA installations, dependency management, or model optimization. The tool manages everything from model loading to memory allocation automatically.\\nFor teams that need consistent performance or are working with particularly demanding models, cloud GPU services like those offered by[Thunder Compute] can provide dedicated hardware while maintaining the control benefits of local deployment.\\n## Available Models on Ollama\\n![Screenshot 2025-10-02 at 3.32.04‚ÄØPM.png] \\nOllama supports a wide range of model families, from lightweight assistants to powerful reasoning LLMs. Here are some of the most popular options:\\n|Model Family|Best For|Parameter Sizes|Key Features|\\nDeepSeek|Complex reasoning, math problems|1.5B-67B|Advanced problem-solving skills|\\nLlama|General conversation, versatile tasks|7B-405B|Meta's foundation models, highly capable|\\nMistral|Fast performance, balanced tasks|7B-22B|Optimized for speed and accuracy|\\nGemma|Lightweight applications|2B-27B|Google's fast models|\\nCodeLlama|Programming assistance|7B-34B|Specialized for code generation|\\nThese model families cover most use cases: lightweight models for simple tasks, mid-range for versatility, and larger models for advanced reasoning or coding.\\nWhen workloads exceed what local machines can handle,[Thunder Compute's on-demand instances] (A100s, H100s, etc.) let you scale seamlessly without losing Ollama‚Äôs privacy and control benefits.\\n## Use Cases and Applications\\nOllama serves diverse use cases across different industries and user types, making it valuable for both individual developers and enterprise teams.\\n**Development and Research:**Prototyping AI applications becomes much more accessible when you don't need to worry about API costs or rate limits. Developers use Ollama for code completion, programming assistance, document analysis, and research with proprietary datasets requiring privacy.\\n**Business Applications:**Companies in controlled industries particularly benefit from Ollama's local deployment model. Customer service chatbots for sensitive industries, internal knowledge base querying, content generation for marketing teams, and legal document analysis all become possible without data leaving the organization.\\n**Educational Use:**Educational institutions love Ollama because it eliminates ongoing costs while providing reliable access to AI features. Teaching AI concepts without cloud costs, student projects with guaranteed uptime, and academic research with data privacy requirements are all common use cases.\\nOne of the most compelling reasons to execute models locally is the control it provides over sensitive data. Many industries, especially finance, healthcare, and government sectors, are subject to strict data privacy regulations, making cloud solutions risky.\\nOllama makes sure that the entire process happens locally, avoiding the compliance issues tied to third-party servers, reducing latency, and minimizing reliance on internet connectivity.\\n## Thunder Compute: Scaling Your Ollama Workflows\\n![Generated url-screenshot] \\nWhile Ollama excels at making AI models accessible on local hardware, many users eventually encounter limitations with their local setup. Whether it's insufficient GPU memory for larger models, inconsistent performance, or the need for team collaboration, there comes a point where dedicated infrastructure becomes valuable.\\nThunder Compute bridges the gap between local development and enterprise-scale AI deployment. Our on-demand GPU instances provide the perfect environment for running Ollama workloads that have outgrown local hardware constraints.\\nWith support for high-end GPUs like A100s and H100s, persistent storage, and VS Code integration, developers can scale their Ollama projects while maintaining the control and flexibility they value. You get the same privacy benefits of local deployment with the performance of enterprise hardware.\\nThe transition from local Ollama development to cloud-based scaling is smooth. You can develop locally, then deploy to Thunder Compute instances when you need more power, all while using the same tools and workflows.\\nFor teams building serious AI applications, Thunder Compute offers the reliability and performance of cloud infrastructure at a fraction of the cost of traditional providers, making it the natural next step for Ollama users ready to scale.\\n## FAQ\\n### What hardware do I need to run Ollama effectively?\\nOllama can run on consumer-grade laptops with 8GB of RAM, but performs best with dedicated NVIDIA or AMD GPUs. For larger models requiring substantial GPU memory, you may need high-end hardware or cloud GPU instances to get smooth performance.\\n### How does Ollama compare to cloud AI services like ChatGPT?\\nOllama runs entirely on your local machine, providing complete data privacy and no ongoing costs, while cloud services require internet connectivity and charge per use. However, cloud services typically offer more powerful models and don't require local hardware setup.\\n### Can I use Ollama for commercial applications?\\nYes, Ollama can be used for commercial purposes without licensing fees. This makes it particularly attractive for businesses that need AI features while maintaining data privacy and controlling costs.\\n### When should I consider moving from local Ollama to cloud infrastructure?\\nConsider cloud infrastructure when your local hardware can't handle larger models, you need consistent performance for production workloads, or your team requires collaborative access to AI resources that exceed what local machines can do.\\n## Final thoughts on running AI models locally with Ollama\\nOllama puts powerful AI tools directly on your machine without the ongoing costs or privacy concerns of cloud services. Whether you're building prototypes or production applications, starting locally gives you complete control over your data and models. When your projects outgrow local hardware limitations,[Thunder Compute] provides the dedicated GPU resources you need while maintaining that same level of control. The path from local experimentation to scalable AI deployment has never been more straightforward.\\n‚Äç## Your GPU,\\none click away.\\nSpin up a dedicated GPU in seconds. Develop in VS Code, keep data safe, swap hardware anytime.\\n[\\nGet started\\n] \\n```\\n```\\nPages\\n[\\nHome\\n] \\n[\\nAbout\\n] \\n[\\nPricing\\n] \\n[\\nContact\\n] \\nResources\\n[\\nBlog\\n] \\n[\\nDocumentation\\n] \\n[\\nCareers\\n] \\nOthers\\n[\\nReferral\\n] \\n[\\nStudents\\n] \\n[\\nSub-processers\\n] \\nLegal\\n[\\nTerms \\u0026amp; Conditions\\n] \\n[\\nPrivacy Policy\\n] \\n[\\nCookie Policy\\n] \\n[\\n] [\\n] [] \\n[\\n] [\\n] [\\n] \\n```\\nLoading...\\n```\"},{\"title\":\"What is Ollama: Running Large Language Models Locally | by Tahir\",\"url\":\"https://medium.com/@tahirbalarabe2/what-is-ollama-running-large-language-models-locally-e917ca40defe\",\"content\":\"üíªWhat is Ollama: Running Large Language Models Locally | by Tahir | Medium\\n[Sitemap] \\n[Open in app] \\nSign up\\n[Sign in] \\n[Medium Logo] \\n[\\nWrite\\n] \\n[\\nSearch\\n] \\nSign up\\n[Sign in] \\n![] \\n# üíªWhat is Ollama: Running Large Language Models Locally\\n[\\n![Tahir] \\n] \\n[Tahir] \\n6 min read\\n¬∑Mar 24, 2025\\n[\\n] \\n--\\n1\\n[] \\nListen\\nShare\\nIf you‚Äôve heard the term ‚ÄúOllama‚Äù but aren‚Äôt quite sure what it is, you‚Äôre not alone. It‚Äôs one of those tools that‚Äôs quietly gaining traction among developers and AI enthusiasts, and for good reason. Ollama is a locally deployed AI model runner. In simpler terms, it lets you download and run large language models (LLMs) on your own machine, without relying on cloud-hosted services. This is a big deal for anyone who wants more control over their AI tools, or who values privacy and offline functionality.\\n[*Traditional cybersecurity methods aren‚Äôt enough. Models can be poisoned, stolen, or tricked in ways traditional software can‚Äôt. If you‚Äôre building AI, you need a new approach.*] \\nAt its core, Ollama is an application that runs in the background on your MacBook or Windows machine. It provides a command-line interface and an API, making it easy to interact with a variety of models. These include the Mistral family, Meta‚Äôs models, and Google‚Äôs Gemini family. For system builders, this means seamless integration with locally deployed models, which can be a game-changer for custom applications.\\nOne of the standout features of Ollama is its use of quantization to optimize model performance. Quantization reduces the computational load, allowing these models to run efficiently on consumer-grade laptops and desktops. This is no small feat, considering the size and complexity of modern LLMs. It also means you can use AI offline, keeping your data on your device for enhanced security and privacy.\\nCustomization is another area where Ollama shines. It uses something called a ‚Äúmodel file,‚Äù which is essentially a text file that defines how a model should be built, customized, and configured. In this file, you can specify a base model, set default system prompts, and even fine-tune the model using a method called LoRA (Low-Rank Adaptation). LoRA is a lightweight fine-tuning technique that adapts pre-trained models to specific tasks without altering the original weights. This makes it possible to specialize models for niche applications without the need for full retraining, which can be resource-intensive.\\nThe model file also allows you to define default parameters like temperature, top P, and top K, which control how the model generates responses. This eliminates the need to repeatedly specify these settings in your prompts, streamlining the interaction process. And because the model file is shareable, you can easily distribute your custom configurations to others.\\n[*Traditional cybersecurity methods aren‚Äôt enough. Models can be poisoned, stolen, or tricked in ways traditional software can‚Äôt. If you‚Äôre building AI, you need a new approach.*] \\nOllama doesn‚Äôt support full fine-tuning, where the model‚Äôs weights are updated. Instead, it focuses on adapter-based fine-tuning, which is more efficient and flexible. LoRA adapters can be swapped in and out, enabling a single base model to handle multiple tasks or domains. This modular approach is particularly useful for developers who need to support a variety of use cases without maintaining multiple models.\\nSo, why should you care about Ollama? If you‚Äôre someone who values privacy, offline functionality, or the ability to customize AI models, it‚Äôs worth exploring. It‚Äôs a tool that puts power back in the hands of the user, allowing you to run and tweak LLMs on your own terms. And because it‚Äôs designed to work on consumer hardware, it‚Äôs accessible to a wide range of users, not just those with access to high-end servers.\\nIf you‚Äôre curious about AI but hesitant to dive into cloud-based solutions, Ollama might be the perfect starting point. It‚Äôs a reminder that you don‚Äôt need to rely on big tech companies to experiment with cutting-edge technology. Sometimes, the most powerful tools are the ones you can run right on your own machine.\\n[*Traditional cybersecurity methods aren‚Äôt enough. Models can be poisoned, stolen, or tricked in ways traditional software can‚Äôt. If you‚Äôre building AI, you need a new approach.*] \\n## Further Reading::\\n[*üöÄDeepSeek R1 Explained: Chain of Thought, Reinforcement Learning, and Model Distillation*] \\n[What are AI Agents?] \\n[‚öôÔ∏èLangChain vs. LangGraph: A Comparative Analysis] \\n[ü§ñWhat is Manus AI?: The First General AI Agent Unveiled] \\n[Stable Diffusion Deepfakes: Creation and Detection] \\n[üîóWhat is Model Context Protocol? (MCP) Architecture Overview] \\n[The Difference Between AI Assistants and AI Agents (And Why It Matters)] \\n[ü§ñDeepSeek R1 API Interaction with Python] \\n## Frequently Asked Questions about Ollama\\n**Q1: What exactly is Ollama?**\\nOllama is a locally deployed AI model runner, designed to allow users to download and execute large language models (LLMs) directly on their personal computer, such as a MacBook or Windows machine. Unlike cloud-hosted LLM services, Ollama runs as a background application and provides a straightforward command-line interface (CLI) and an Application Programming Interface (API) for interacting with various model families, including Mistral, Meta, and Google‚Äôs Gemma. This local operation ensures that the processing of AI tasks occurs on the user‚Äôs device.\\n**Q2: How does Ollama enable efficient execution of large language models on consumer hardware?**\\nOllama optimises the performance of LLMs through a technique called quantization. Quantization reduces the precision of the numerical representations within the model, which in turn lowers the computational resources required for execution, including memory usage and processing power. This optimisation makes it feasible to run sophisticated AI models on standard consumer laptops and desktops without the need for high-end hardware or cloud-based infrastructure.\\n**Q3: What are the primary benefits of using Ollama for running large language models locally?**\\nUsing Ollama offers several key advantages. Firstly, it enables offline AI usage, meaning that you can interact with and utilise LLMs even without an internet connection. Secondly, it ensures data privacy and security, as all data processed by the models remains on your local device and is not transmitted to external servers. This is particularly beneficial for users handling sensitive information or those with strict data governance requirements.\\n**Q4: What is a ‚ÄúModel file‚Äù in the context of Ollama, and what can it be used for?**\\nIn Ollama, a ‚ÄúModel file‚Äù is a text-based configuration file that defines how a specific language model should be built, customised, and configured. It acts as a blueprint for creating a tailored version of an LLM. Within this file, users can specify a base model to build upon, define default system prompts that guide the model‚Äôs behaviour, incorporate LoRA (Low-Rank Adaptation) fine-tuning configurations, and set default LLM parameters such as temperature, top-P, and top-K.\\n**Q5: How do default system prompts within an Ollama Model file influence the behaviour of a language model?**\\nDefining default system prompts in the Model file allows users to pre-program instructions or guidelines that the language model will inherently follow. This eliminates the need for the user to repeatedly include these instructions in each individual prompt. By setting these foundational directives, users can consistently steer the model towards desired behaviours, response styles, or specific task focuses without manual repetition.\\n**Q6: Does Ollama support fine-tuning of large language models?**\\nWhile Ollama does not support full fine-tuning, which involves updating the entire set of a model‚Äôs weights, it does offer an efficient adapter-based fine-tuning method known as Low-Rank Adaptation (LoRA). LoRA is a lightweight technique that adapts pre-trained LLMs to specific tasks by introducing a small number of new parameters, called adapters, without altering the original model‚Äôs core weights. These LoRA adapters can be easily swapped, allowing a single base model to be adapted for various niche applications or domains.\\n**Q7: What is the significance of LoRA (Low-Rank Adaptation) in the context of Ollama?**\\nLoRA provides a practical and resource-efficient way to specialise pre-trained LLMs for specific tasks within Ollama. By only training a small set of adapter weights, LoRA significantly reduces the computational cost and time associated with fine-tuning compared to full model retraining. This allows users to tailor models for particular use cases or datasets without requiring extensive computational resources. Furthermore, the swappable nature of LoRA adapters enables flexibility and the ability to support multiple tasks or domains using the same base model.\\n**Q8: How does Ollama facilitate the sharing and distribution of customised language models?**\\nThe use of Model files in Ollama makes it straightforward to share and distribute customised language models. Because the Model file contains all the specifications and configurations needed to build or modify a model (including the base model reference, default prompts, LoRA configurations, and other parameters), users can easily share this text file with others. This allows others to replicate the same model modifications and configurations on their own Ollama instances, fostering collaboration and the dissemination of tailored AI models.\\n[\\nOllama Ai Model\\n] \\n[\\nLocal Ai Deployment\\n] \\n[\\nRun Llms Offline\\n] \\n[\\nOllama Customization\\n] \\n[\\nOllama For Developers\\n] \\n[\\n![Tahir] \\n] \\n[\\n![Tahir] \\n] \\n[## Written byTahir\\n] \\n[985 followers] \\n¬∑[45 following] \\n## Responses (1)\\n[] \\nSee all responses\\n[\\nHelp\\n] \\n[\\nStatus\\n] \\n[\\nAbout\\n] \\n[\\nCareers\\n] \\n[\\nPress\\n] \\n[\\nBlog\\n] \\n[\\nPrivacy\\n] \\n[\\nRules\\n] \\n[\\nTerms\\n] \\n[\\nText to speech\\n]\"},{\"title\":\"Ollama Explained: Transforming AI Accessibility and Language ...\",\"url\":\"https://www.geeksforgeeks.org/artificial-intelligence/ollama-explained-transforming-ai-accessibility-and-language-processing/\",\"content\":\"Ollama Explained: Transforming AI Accessibility and Language Processing - GeeksforGeeks\\n[\\n![geeksforgeeks] \\n] \\n**\\n* Courses\\n**\\n* Tutorials\\n**\\n* Interview Prep\\n**\\n**\\n* [Artificial Intelligence] \\n* [Interview Questions] \\n* [Project Ideas] \\n* [Search Algorithms] \\n* [Local Search Algorithm] \\n* [Generative AI] \\n* [Data Science] \\n* [Machine Learning] \\n* [Deep Learning] \\n* [ML-Projects] \\n* [Robotics] \\n# Ollama Explained: Transforming AI Accessibility and Language Processing\\nLast Updated :23 Jul, 2025\\n* * * In the rapidly evolving landscape of artificial intelligence (AI), accessibility and innovation are paramount. Among the myriad platforms and tools emerging in this space, one name stands out: Ollama. But what exactly is Ollama, and why is it garnering attention in the AI community? This article delves into the intricacies of Ollama, its methodologies, its potential impact on AI applications, and what this could mean for the future of human-machine interaction.\\nTable of Content\\n* [Understanding Ollama] \\n* [The Genesis of Ollama:] \\n* [Understanding the Ollama Framework:] \\n* [Stepwise Guide to start Ollama] \\n* [Applications of Ollama] \\n* [Ethical Considerations and Responsible AI] \\n* [Conclusion] \\n## Understanding Ollama\\nOllama stands for (Omni-Layer Learning Language Acquisition Model), a novel approach to[machine learning] that promises to redefine how we perceive language acquisition and[natural language processing.] At its core, Ollama is a groundbreaking platform that democratizes access to[large language models (LLMs)] by enabling users to run them locally on their machines. Developed with a vision to empower individuals and organizations, Ollama provides a user-friendly interface and seamless integration capabilities, making it easier than ever to leverage the power of LLMs for various applications and use cases.\\n## The Genesis of Ollama:\\nTraditional machine learning models have been successful in various linguistic tasks but often require extensive data labelling and preprocessing to function effectively. Ollama emerges as a paradigm shift, utilizing[unsupervised learning] techniques combined with deep[neural networks] that enable it to learn language structures without explicit grammatical rules or annotations.\\n## Understanding the Ollama Framework:\\nOLLAMA's architecture comprises multiple layers where each successive layer learns different linguistic patterns and abstract representations of speech. This multi-layered approach allows OLLAMA to progress from understanding basic sounds to grasping complex sentence structures, all without direct human intervention for labelling or structuring the input data.\\n### Key Features of Ollama\\n* ****Local Execution:****One of the distinguishing features of Ollama is its ability to run LLMs locally, mitigating privacy concerns associated with cloud-based solutions. By bringing[AI] models directly to users' devices, Ollama ensures greater control and security over data while providing faster processing speeds and reduced reliance on external servers.\\n* ****Extensive Model Library****: Ollama offers access to an extensive library of pre-trained LLMs, including popular models like Llama 3. Users can choose from a range of models tailored to different tasks, domains, and hardware capabilities, ensuring flexibility and versatility in their AI projects.\\n* ****Seamless Integration:****Ollama seamlessly integrates with a variety of tools, frameworks, and programming languages, making it easy for developers to incorporate LLMs into their workflows. Whether it's[Python,] [LangChain], or LlamaIndex, Ollama provides robust integration options for building sophisticated AI applications and solutions.\\n* ****Customization and Fine-tuning:****With Ollama, users have the ability to customize and fine-tune LLMs to suit their specific needs and preferences. From prompt engineering to few-shot learning and fine-tuning processes, Ollama empowers users to shape the behavior and outputs of LLMs, ensuring they align with the desired objectives.## Stepwise Guide to start Ollama\\n### ****Prerequisites:****\\n* ****Computer:****Ollama is currently available for Linux and macOS and windows[operating systems,] For windows it recently preview version is lanched.\\n* ****Basic understanding of command lines:****While Ollama offers a user-friendly interface, some comfort with basic command-line operations is helpful.### ****Step 1: Download Ollama****\\n* Visit the official Ollama website:[https://ollama.com/] \\n* Click on the download button corresponding to your operating system (Linux, macOS or Windows (preview)).\\n* This will download the Ollama installation script.### ****Step 2: Install Ollama****\\n1. Open a terminal window.\\n2. Navigate to the directory where you downloaded the Ollama installation script (usually the Downloads folder).\\n3. Depending on your operating system, use the following commands to grant the script execution permission and then run the installation:\\n* ****For linux****\\n```\\nchmod +x ollama\\\\_linux.sh\\n./ollama\\\\_linux.sh\\n```\\n* ****For macOS****\\n```\\nchmod +x ollama\\\\_macos.sh\\n./ollama\\\\_macos.sh\\n```\\n* For windows\\n* Direct installations with clicking the downloaded file and follow the on-screen instructions during the installation process### ****Step 3: Pull Your First Model (Optional)****\\n* Ollama allows you to run various open-source LLMs. Here, we'll use Llama 3 as an example.\\n* Use the following command to download the Llama 3 model:\\n```\\nollama pull gemma\\n```\\nReplace 'gemma' with the specific model name if desired\\nThe Ollama library curates a diverse collection of LLMs, each with unique strengths and sizes. Some example are as follows:\\n* Llama 3 (8B, 70B)\\n* Phi-3 (3.8B)\\n* Mistral (7B)\\n* Neural Chat (7B)\\n* Starling (7B)\\n* Code Llama (7B)\\n* Llama 2 Uncensored (7B)\\n* LLaVA (7B)\\n* Gemma (2B, 7B)\\n* Solar (10.7B)### ****Step 4: Run and Use the Model****\\n* Once you have a model downloaded, you can run it using the following command:\\n```\\nollama run \\u0026lt;\\u0026lt;model\\\\_name\\u0026gt;\\u0026gt;\\n```\\n****Output for command \\\"ollama run phi3\\\":****\\n![Screenshot-(893)] ollama run phi3### Managing Your LLM Ecosystem with the Ollama CLI\\nThe Ollama command-line interface (CLI) provides a range of functionalities to manage your LLM collection:\\n* ****Create Models:****Craft new models from scratch using the****ollama create****command.\\n* ****Pull Pre-Trained Models:****Access models from the Ollama library with****ollama pull****.\\n* ****Remove Unwanted Models:****Free up space by deleting models using****ollama rm****.\\n* ****Copy Models:****Duplicate existing models for further experimentation with****ollama cp****.\\n* ****Interacting with Models:****The Power of****ollama run****\\nThe ollama run command is your gateway to interacting with any model on your machine. Need a quick summary of a text file? Pass it through an LLM and let it do the work. Ollama even supports multimodal models that can analyze images alongside text.\\nWe can also use ollama using python code as follows:\\nPython`\\n```\\nimportollamaresponse=ollama.chat(model=\\u0026#39;phi3\\u0026#39;,messages=[{\\u0026#39;role\\u0026#39;:\\u0026#39;user\\u0026#39;,\\u0026#39;content\\u0026#39;:\\u0026#39;Why is sky blue?\\u0026#39;,},])print(response[\\u0026#39;message\\u0026#39;][\\u0026#39;content\\u0026#39;])\\n```\\n`\\n****Output:****\\n```\\nThe sky appears blue due to a phenomenon called Rayleigh scattering. As sunlight enters Earth's atmosphere, it collides with molecules and small particles in the air. These interactions cause the light to scatter in different directions. Shorter wavelengths (blue and violet) are scattered more than longer wavelengths (red and yellow). However, our eyes are more sensitive to blue light, which is why we perceive the sky as predominantly blue during the daytime.\\nHere's a simplified explanation:\\n1. Sunlight travels in straight lines from its source - the sun.\\n2. When this light encounters Earth's atmosphere, it collides with gas molecules and tiny particles (like dust or water droplets).\\n3. These collisions cause the light to scatter throughout the sky; however, blue light is scattered more than other colors because of its shorter wavelengths.\\n4. Our eyes receive this scattered light, giving us the impression that the sky appears blue most of the time.\\n5. Note that the sky can appear different at sunrise and sunset when it takes on shades of red or orange due to the longer path through the atmosphere.\\n```\\n## Applications of Ollama\\n* ****Creative Writing and Content Generation:****Writers and content creators can leverage Ollama to overcome writer's block, brainstorm content ideas, and generate diverse and engaging content across different genres and formats.\\n* ****Code Generation and Assistance:****Developers can harness Ollama's capabilities for code generation, explanation, debugging, and documentation, streamlining their development workflows and enhancing the quality of their code.\\n* ****Language Translation and Localization:****Ollama's language understanding and generation capabilities make it an invaluable tool for translation, localization, and multilingual communication, facilitating cross-cultural understanding and global collaboration.\\n* ****Research and Knowledge Discovery:****Researchers and knowledge workers can accelerate their discoveries by using Ollama to analyze, synthesize, and extract insights from vast amounts of information, spanning literature reviews,[data analysis,] hypothesis generation, and knowledge extraction.\\n* ****Customer Service and Support:****Businesses can deploy intelligent chatbots and virtual assistants powered by Ollama to enhance customer service, automate FAQs, provide personalized product recommendations, and analyze customer feedback for improved satisfaction and engagement.\\n* ****Healthcare and Medical Applications:****In the healthcare industry, Ollama can assist in medical documentation, clinical decision support, patient education, telemedicine, and medical research, ultimately improving patient outcomes and streamlining healthcare delivery.## Ethical Considerations and Responsible AI\\nWhile the potential of Ollama is vast and promising, it's essential to address ethical considerations and ensure responsible AI practices. From mitigating bias and ensuring fairness to prioritizing privacy, transparency, and human oversight, developers and organizations must navigate these challenges to harness the full potential of Ollama while minimizing risks and promoting societal benefit.\\n## Conclusion\\nOllama's revolutionary approach to natural language understanding heralds a new era where AI can learn and interpret human language as effortlessly as a child does. As researchers continue to refine this innovative model, we stand on the brink of witnessing an unprecedented leap in machine intelligence that could reshape our digital world.\\nAs AI technology continues to evolve, Ollama is poised to play a pivotal role in shaping its future development and deployment. With ongoing advancements in model capabilities, hardware optimization, decentralized model sharing, user experiences, and ethical AI frameworks, Ollama remains at the forefront of AI innovation, driving progress and democratization across all sectors of society.\\nComment\\nArticle Tags:\\nArticle Tags:\\n[Artificial Intelligence] \\n[Open Source] \\n[AI-ML-DS With Python] \\n### Explore\\nIntroduction to AI**\\n* [What is Artificial Intelligence (AI)**10min read] \\n* [Types of Artificial Intelligence (AI)**6min read] \\n* [Types of AI Based on Functionalities**4min read] \\n* [Agents in AI**7min read] \\n* [Artificial intelligence vs Machine Learning vs Deep Learning**3min read] \\n* [Problem Solving in Artificial Intelligence**6min read] \\n* [Top 20 Applications of Artificial Intelligence (AI) in 2025**13min read] \\nAI Concepts**\\n* [Search Algorithms in AI**6min read] \\n* [Local Search Algorithm in Artificial Intelligence**7min read] \\n* [Adversarial Search Algorithms in Artificial Intelligence (AI)**15+min read] \\n* [Constraint Satisfaction Problems (CSP) in Artificial Intelligence**10min read] \\n* [Knowledge Representation in AI**9min read] \\n* [First-Order Logic in Artificial Intelligence**4min read] \\n* [Reasoning Mechanisms in AI**9min read] \\nMachine Learning in AI**\\n* [Machine Learning Tutorial**5min read] \\n* [Deep Learning Tutorial**5min read] \\n* [Natural Language Processing (NLP) Tutorial**4min read] \\n* [Computer Vision Tutorial**7min read] \\nRobotics and AI**\\n* [Artificial Intelligence in Robotics**5min read] \\n* [What is Robotics Process Automation**8min read] \\n* [Automated Planning in AI**8min read] \\n* [AI in Transportation**8min read] \\n* [AI in Manufacturing : Revolutionizing the Industry**6min read] \\nGenerative AI**\\n* [What is Generative AI?**7min read] \\n* [Generative Adversarial Network (GAN)**11min read] \\n* [Cycle Generative Adversarial Network (CycleGAN)**7min read] \\n* [StyleGAN - Style Generative Adversarial Networks**5min read] \\n* [Introduction to Generative Pre-trained Transformer (GPT)**4min read] \\n* [BERT Model - NLP**12min read] \\n* [Generative AI Applications**7min read] \\nAI Practice**\\n* [Top Artificial Intelligence(AI) Interview Questions and Answers**15+min read] \\n* [Top Generative AI and LLM Interview Question with Answer**15+min read] \\n* [30+ Best Artificial Intelligence Project Ideas with Source Code [2025 Updated]**15+min read]\"}]}\n",
      "\n",
      "Agent: Here's a concise summary and explanation of **Ollama**, based on the provided articles:\n",
      "\n",
      "---\n",
      "\n",
      "### **What is Ollama?**\n",
      "Ollama is an open-source platform designed to run and manage large language models (LLMs) locally. It allows users to interact with models like **Llama 3**, **Phi-3**, **Mistral**, and others, either through a command-line interface (CLI) or integrated into applications. It emphasizes **privacy**, **flexibility**, and **community-driven development**.\n",
      "\n",
      "---\n",
      "\n",
      "### **Key Features**\n",
      "1. **Local Model Execution**  \n",
      "   - Run LLMs directly on your machine, avoiding cloud dependency.  \n",
      "   - Supports models like **Llama 3 (8B/70B)**, **Phi-3 (3.8B)**, **Mistral (7B)**, **Code Llama**, and more.\n",
      "\n",
      "2. **CLI Tools for Model Management**  \n",
      "   - **`ollama pull`**: Download pre-trained models (e.g., `ollama pull gemma`).  \n",
      "   - **`ollama run`**: Interact with models (e.g., `ollama run phi3`).  \n",
      "   - **`ollama create`**: Build custom models.  \n",
      "   - **`ollama rm`**: Delete models to free space.  \n",
      "   - **`ollama cp`**: Copy models for experimentation.  \n",
      "\n",
      "3. **Python Integration**  \n",
      "   - Use Python to interface with models:  \n",
      "     ```python\n",
      "     import ollama\n",
      "     response = ollama.chat(model='phi3', messages=[{'role': 'user', 'content': 'Why is the sky blue?'}])\n",
      "     print(response['message']['content'])\n",
      "     ```\n",
      "\n",
      "4. **Multimodal Support**  \n",
      "   - Analyze text and images together (e.g., for visual reasoning tasks).\n",
      "\n",
      "---\n",
      "\n",
      "### **How to Use Ollama**\n",
      "1. **Install Ollama**  \n",
      "   - Follow instructions for your OS (Linux/macOS/Windows).  \n",
      "\n",
      "2. **Pull a Model**  \n",
      "   ```bash\n",
      "   ollama pull phi3  # Example: Download Phi-3 model\n",
      "   ```\n",
      "\n",
      "3. **Run the Model**  \n",
      "   ```bash\n",
      "   ollama run phi3\n",
      "   ```\n",
      "   - This launches an interactive shell for querying the model.\n",
      "\n",
      "4. **Customize Models**  \n",
      "   - Use `ollama create` to train or fine-tune models from scratch.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Applications of Ollama**\n",
      "- **Creative Writing**: Generate content, brainstorm ideas, or overcome writer's block.  \n",
      "- **Code Development**: Generate, debug, or document code (e.g., Python, JavaScript).  \n",
      "- **Language Translation**: Translate between languages with contextual accuracy.  \n",
      "- **Research**: Analyze data, synthesize insights, or automate literature reviews.  \n",
      "- **Customer Support**: Deploy chatbots for FAQs, product recommendations, or feedback analysis.  \n",
      "- **Healthcare**: Assist with medical documentation, clinical decision support, or telemedicine.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Ethical Considerations**\n",
      "- **Bias Mitigation**: Ensure fairness in model outputs.  \n",
      "- **Privacy**: Handle sensitive data locally to avoid cloud leaks.  \n",
      "- **Transparency**: Provide clear explanations for model decisions.  \n",
      "- **Human Oversight**: Avoid over-reliance on automation in critical tasks.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Why Ollama Stands Out**\n",
      "- **Open Source**: Encourages collaboration and customization.  \n",
      "- **Community-Driven**: Models like Llama 3 and Phi-3 are developed by researchers and developers.  \n",
      "- **Scalability**: Supports both small (e.g., Phi-3) and large (e.g., Llama 70B) models.  \n",
      "\n",
      "---\n",
      "\n",
      "### **Example Use Case**\n",
      "**Question**: *Why is the sky blue?*  \n",
      "**Ollama Response**:  \n",
      "> \"The sky appears blue due to Rayleigh scattering... [detailed explanation].\"  \n",
      "\n",
      "---\n",
      "\n",
      "### **Conclusion**\n",
      "Ollama is a versatile tool for developers, researchers, and creators looking to leverage LLMs locally. Its CLI, model diversity, and ethical focus make it ideal for both hobbyists and enterprises. Whether you're generating code, writing stories, or analyzing data, Ollama provides a flexible, privacy-first approach to AI.  \n",
      "\n",
      "For deeper exploration, check out the [Ollama GitHub repository](https://github.com/ollama/ollama) and the [official documentation](https://ollama.com/).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What's ollama?\"\n",
    "print(f\"User: {query}\")\n",
    "result = await agent.run(query)\n",
    "print(f\"Agent: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**‡§Ö‡§∏‡•ç‡§µ‡•Ä‡§ï‡§∞‡§£**:\n‡§π‡§æ ‡§¶‡§∏‡•ç‡§§‡§ê‡§µ‡§ú AI ‡§Ö‡§®‡•Å‡§µ‡§æ‡§¶ ‡§∏‡•á‡§µ‡•á‡§¶‡•ç‡§µ‡§æ‡§∞‡•á [Co-op Translator](https://github.com/Azure/co-op-translator) ‡§µ‡§æ‡§™‡§∞‡•Ç‡§® ‡§Ö‡§®‡•Å‡§µ‡§æ‡§¶‡§ø‡§§ ‡§ï‡§∞‡§£‡•ç‡§Ø‡§æ‡§§ ‡§Ü‡§≤‡§æ ‡§Ü‡§π‡•á. ‡§Ü‡§Æ‡•ç‡§π‡•Ä ‡§Ö‡§ö‡•Ç‡§ï‡§§‡•á‡§∏‡§æ‡§†‡•Ä ‡§™‡•ç‡§∞‡§Ø‡§§‡•ç‡§®‡§∂‡•Ä‡§≤ ‡§Ö‡§∏‡§≤‡•ã ‡§§‡§∞‡•Ä, ‡§ï‡•É‡§™‡§Ø‡§æ ‡§≤‡§ï‡•ç‡§∑‡§æ‡§§ ‡§ò‡•ç‡§Ø‡§æ ‡§ï‡•Ä ‡§∏‡•ç‡§µ‡§Ø‡§Ç‡§ö‡§≤‡§ø‡§§ ‡§Ö‡§®‡•Å‡§µ‡§æ‡§¶‡§æ‡§Ç‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§ö‡•Å‡§ï‡§æ ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§Ö‡§ö‡•Ç‡§ï‡§§‡•á‡§§ ‡§§‡•ç‡§∞‡•Å‡§ü‡•Ä ‡§Ö‡§∏‡•Ç ‡§∂‡§ï‡§§‡§æ‡§§. ‡§Æ‡•Ç‡§≥ ‡§¶‡§∏‡•ç‡§§‡§ê‡§µ‡§ú ‡§§‡•ç‡§Ø‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§∏‡•ç‡§•‡§æ‡§®‡§ø‡§ï ‡§≠‡§æ‡§∑‡•á‡§§ ‡§Ö‡§ß‡§ø‡§ï‡§æ‡§∞‡§™‡•ç‡§∞‡§æ‡§™‡•ç‡§§ ‡§∏‡•ç‡§∞‡•ã‡§§ ‡§Æ‡§æ‡§®‡§≤‡§æ ‡§ú‡§æ‡§µ‡§æ. ‡§Æ‡§π‡§§‡•ç‡§§‡•ç‡§µ‡§™‡•Ç‡§∞‡•ç‡§£ ‡§Æ‡§æ‡§π‡§ø‡§§‡•Ä‡§∏‡§æ‡§†‡•Ä, ‡§µ‡•ç‡§Ø‡§æ‡§µ‡§∏‡§æ‡§Ø‡§ø‡§ï ‡§Æ‡§æ‡§®‡§µ‡•Ä ‡§Ö‡§®‡•Å‡§µ‡§æ‡§¶‡§æ‡§ö‡•Ä ‡§∂‡§ø‡§´‡§æ‡§∞‡§∏ ‡§ï‡•á‡§≤‡•Ä ‡§ú‡§æ‡§§‡•á. ‡§Ø‡§æ ‡§Ö‡§®‡•Å‡§µ‡§æ‡§¶‡§æ‡§ö‡•ç‡§Ø‡§æ ‡§µ‡§æ‡§™‡§∞‡§æ‡§Æ‡•Å‡§≥‡•á ‡§â‡§¶‡•ç‡§≠‡§µ‡§£‡§æ‡§±‡•ç‡§Ø‡§æ ‡§ï‡•ã‡§£‡§§‡•ç‡§Ø‡§æ‡§π‡•Ä ‡§ó‡•ã‡§Ç‡§ß‡§≥ ‡§ï‡§ø‡§Ç‡§µ‡§æ ‡§∏‡§Æ‡§ú‡•Å‡§§‡•Ä‡§ö‡•ç‡§Ø‡§æ ‡§ö‡•Å‡§ï‡§æ ‡§Ø‡§æ‡§∏‡§æ‡§†‡•Ä ‡§Ü‡§Æ‡•ç‡§π‡•Ä ‡§ú‡§¨‡§æ‡§¨‡§¶‡§æ‡§∞ ‡§®‡§æ‡§π‡•Ä ‡§Ü‡§π‡•ã‡§§.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mafdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "coopTranslator": {
   "original_hash": "2226c8de0556116d8ad0ef64ff4da2ae",
   "translation_date": "2026-01-19T09:14:34+00:00",
   "source_file": "WorkshopForAgentic/code/01.BasicAgent/03.BasicAgent-websearch.ipynb",
   "language_code": "mr"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}