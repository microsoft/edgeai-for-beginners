# Bölüm 7: Qualcomm QNN (Qualcomm Neural Network) Optimizasyon Paketi

## İçindekiler
1. [Giriş](../../../Module04)
2. [Qualcomm QNN Nedir?](../../../Module04)
3. [Kurulum](../../../Module04)
4. [Hızlı Başlangıç Kılavuzu](../../../Module04)
5. [Örnek: Modelleri QNN ile Dönüştürme ve Optimize Etme](../../../Module04)
6. [İleri Düzey Kullanım](../../../Module04)
7. [En İyi Uygulamalar](../../../Module04)
8. [Sorun Giderme](../../../Module04)
9. [Ek Kaynaklar](../../../Module04)

## Giriş

Qualcomm QNN (Qualcomm Neural Network), Qualcomm'un AI donanım hızlandırıcılarının tam potansiyelini ortaya çıkarmak için tasarlanmış kapsamlı bir AI çıkarım çerçevesidir. Hexagon NPU, Adreno GPU ve Kryo CPU gibi birimlerle çalışır. Mobil cihazlar, uç bilgi işlem platformları veya otomotiv sistemlerini hedefliyor olsanız da, QNN Qualcomm'un özel AI işleme birimlerini kullanarak maksimum performans ve enerji verimliliği sağlayan optimize edilmiş çıkarım yetenekleri sunar.

## Qualcomm QNN Nedir?

Qualcomm QNN, geliştiricilerin AI modellerini Qualcomm'un heterojen bilgi işlem mimarisi üzerinde verimli bir şekilde dağıtmasını sağlayan birleşik bir AI çıkarım çerçevesidir. Hexagon NPU (Sinir İşleme Birimi), Adreno GPU ve Kryo CPU'ya erişim için birleşik bir programlama arayüzü sunar ve farklı model katmanları ve işlemleri için en uygun işleme birimini otomatik olarak seçer.

### Temel Özellikler

- **Heterojen Bilgi İşlem**: NPU, GPU ve CPU'ya birleşik erişim ve otomatik iş yükü dağıtımı
- **Donanım Farkındalığıyla Optimizasyon**: Qualcomm Snapdragon platformları için özel optimizasyonlar
- **Kuantizasyon Desteği**: Gelişmiş INT8, INT16 ve karma hassasiyet kuantizasyon teknikleri
- **Model Dönüştürme Araçları**: TensorFlow, PyTorch, ONNX ve Caffe modelleri için doğrudan destek
- **Uç AI Optimizasyonu**: Mobil ve uç dağıtım senaryoları için tasarlanmış, güç verimliliği odaklı

### Faydalar

- **Maksimum Performans**: Özel AI donanımını kullanarak 15 kata kadar performans artışı
- **Güç Verimliliği**: Mobil ve batarya ile çalışan cihazlar için optimize edilmiş akıllı güç yönetimi
- **Düşük Gecikme Süresi**: Gerçek zamanlı uygulamalar için minimum ek yük ile donanım hızlandırmalı çıkarım
- **Ölçeklenebilir Dağıtım**: Qualcomm ekosisteminde akıllı telefonlardan otomotiv platformlarına kadar
- **Üretime Hazır**: Milyonlarca dağıtılmış cihazda kullanılan test edilmiş çerçeve

## Kurulum

### Ön Koşullar

- Qualcomm QNN SDK (Qualcomm ile kayıt gerektirir)
- Python 3.7 veya üzeri
- Uyumlu Qualcomm donanımı veya simülatör
- Android NDK (mobil dağıtım için)
- Linux veya Windows geliştirme ortamı

### QNN SDK Kurulumu

1. **Kayıt ve İndirme**: Qualcomm Developer Network'ü ziyaret ederek QNN SDK'yı kaydedin ve indirin
2. **SDK'yı Çıkarın**: QNN SDK'yı geliştirme dizininize çıkarın
3. **Ortam Değişkenlerini Ayarlayın**: QNN araçları ve kütüphaneleri için yolları yapılandırın

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Python Ortamı Kurulumu

Sanal bir ortam oluşturun ve etkinleştirin:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Gerekli Python paketlerini yükleyin:

```bash
pip install numpy tensorflow torch onnx
```

### Kurulumu Doğrulama

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Başarılı olursa, her QNN aracı için yardım bilgilerini görmelisiniz.

## Hızlı Başlangıç Kılavuzu

### İlk Model Dönüştürmeniz

Basit bir PyTorch modelini Qualcomm donanımında çalıştırmak için dönüştürelim:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### ONNX'i QNN Formatına Dönüştürme

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### QNN Model Kütüphanesi Oluşturma

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Bu Süreç Ne Yapar?

Optimizasyon iş akışı şunları içerir: orijinal modeli ONNX formatına dönüştürmek, ONNX'i QNN ara temsiline çevirmek, donanım özelinde optimizasyonlar uygulamak ve dağıtım için derlenmiş bir model kütüphanesi oluşturmak.

### Temel Parametreler Açıklaması

- `--input_network`: Kaynak ONNX model dosyası
- `--output_path`: Oluşturulan C++ kaynak dosyası
- `--input_dim`: Optimizasyon için giriş tensör boyutları
- `--quantization_overrides`: Özel kuantizasyon yapılandırması
- `-t x86_64-linux-clang`: Hedef mimari ve derleyici

## Örnek: Modelleri QNN ile Dönüştürme ve Optimize Etme

### Adım 1: Kuantizasyon ile Gelişmiş Model Dönüşümü

Dönüştürme sırasında özel kuantizasyon uygulama:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Özel kuantizasyon ile dönüştürme:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Adım 2: Çoklu Arka Uç Optimizasyonu

NPU, GPU ve CPU arasında heterojen yürütme için yapılandırma:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Adım 3: Dağıtım için Bağlam İkili Dosyası Oluşturma

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Adım 4: QNN Çalışma Zamanı ile Çıkarım

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Çıktı Yapısı

Optimizasyondan sonra, dağıtım dizininiz şunları içerecektir:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## İleri Düzey Kullanım

### Özel Arka Uç Yapılandırması

Belirli arka uç optimizasyonlarını yapılandırın:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dinamik Kuantizasyon

Daha iyi doğruluk için çalışma zamanında kuantizasyon uygulayın:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Performans Profili

Farklı arka uçlar arasında performansı izleyin:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Otomatik Arka Uç Seçimi

Model özelliklerine göre akıllı arka uç seçimi uygulayın:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## En İyi Uygulamalar

### 1. Model Mimari Optimizasyonu
- **Katman Birleştirme**: Conv+BatchNorm+ReLU gibi işlemleri birleştirerek NPU kullanımını artırın
- **Derinlik Ayrılabilir Konvolüsyonlar**: Mobil dağıtım için standart konvolüsyonlar yerine tercih edin
- **Kuantizasyona Uygun Tasarımlar**: ReLU aktivasyonları kullanın ve kuantize edilemeyen işlemlerden kaçının

### 2. Kuantizasyon Stratejisi
- **Eğitim Sonrası Kuantizasyon**: Hızlı dağıtım için bununla başlayın
- **Kalibrasyon Veri Seti**: Tüm giriş varyasyonlarını kapsayan temsilci veri kullanın
- **Karma Hassasiyet**: Çoğu katman için INT8 kullanın, kritik katmanları daha yüksek hassasiyette tutun

### 3. Arka Uç Seçim Kılavuzları
- **NPU (HTP)**: CNN iş yükleri, kuantize edilmiş modeller ve güç hassas uygulamalar için en iyisi
- **GPU**: Hesaplama yoğun işlemler, daha büyük modeller ve FP16 hassasiyet için ideal
- **CPU**: Desteklenmeyen işlemler ve hata ayıklama için yedek

### 4. Performans Optimizasyonu
- **Batch Boyutu**: Gerçek zamanlı uygulamalar için batch boyutu 1, verimlilik için daha büyük batch'ler kullanın
- **Giriş Ön İşleme**: Veri kopyalama ve dönüşüm yükünü en aza indirin
- **Bağlam Yeniden Kullanımı**: Çalışma zamanında derleme yükünü önlemek için bağlamları önceden derleyin

### 5. Bellek Yönetimi
- **Tensor Tahsisi**: Çalışma zamanı yükünü önlemek için mümkün olduğunda statik tahsis kullanın
- **Bellek Havuzları**: Sık tahsis edilen tensörler için özel bellek havuzları uygulayın
- **Tampon Yeniden Kullanımı**: Çıkarım çağrıları arasında giriş/çıkış tamponlarını yeniden kullanın

### 6. Güç Optimizasyonu
- **Performans Modları**: Termal kısıtlamalara göre uygun performans modlarını kullanın
- **Dinamik Frekans Ölçeklendirme**: Sistem iş yüküne göre frekansı ölçeklendirmesine izin verin
- **Boş Durum Yönetimi**: Kullanılmadığında kaynakları düzgün bir şekilde serbest bırakın

## Sorun Giderme

### Yaygın Sorunlar

#### 1. SDK Kurulum Problemleri
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Model Dönüştürme Hataları
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Kuantizasyon Sorunları
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Performans Sorunları
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Bellek Sorunları
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Arka Uç Uyumluluğu
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Performans Hata Ayıklama

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Yardım Alma

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN Belgeleri**: SDK paketinde mevcut
- **Topluluk Forumları**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Teknik Destek**: Qualcomm geliştirici portalı üzerinden

## Ek Kaynaklar

### Resmi Bağlantılar
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon Platformları**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Geliştirici Portalı**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Motoru**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Öğrenme Kaynakları
- **Başlangıç Kılavuzu**: QNN SDK belgelerinde mevcut
- **Model Havuzu**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Optimizasyon Kılavuzu**: SDK belgeleri kapsamlı optimizasyon yönergeleri içerir
- **Video Eğitimleri**: [Qualcomm Developer YouTube Kanalı](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Entegrasyon Araçları
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Qualcomm donanımı için önceden optimize edilmiş modeller
- **Android Neural Networks API**: Android NNAPI ile entegrasyon
- **TensorFlow Lite Delegate**: TFLite için Qualcomm delegesi

### Performans Karşılaştırmaları
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Araştırması**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Topluluk Örnekleri
- **Örnek Uygulamalar**: QNN SDK örnekler dizininde mevcut
- **GitHub Depoları**: Topluluk tarafından katkıda bulunulan örnekler ve araçlar
- **Teknik Bloglar**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### İlgili Araçlar
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Gelişmiş kuantizasyon ve sıkıştırma teknikleri
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Karşılaştırma ve yedek dağıtım için
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Çapraz platform çıkarım motoru

### Donanım Özellikleri
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon Platformları**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Sıradaki Adım

Küçük Dil Modeli yaşam döngüsü yönetiminin operasyonel yönlerini öğrenmek için [Modül 5: SLMOps ve Üretim Dağıtımı](../Module05/README.md) bölümünü keşfederek Uç AI yolculuğunuza devam edin.

---

**Feragatname**:  
Bu belge, AI çeviri hizmeti [Co-op Translator](https://github.com/Azure/co-op-translator) kullanılarak çevrilmiştir. Doğruluk için çaba göstersek de, otomatik çeviriler hata veya yanlışlıklar içerebilir. Belgenin orijinal dili, yetkili kaynak olarak kabul edilmelidir. Kritik bilgiler için profesyonel insan çevirisi önerilir. Bu çevirinin kullanımından kaynaklanan yanlış anlamalar veya yanlış yorumlamalar için sorumluluk kabul etmiyoruz.