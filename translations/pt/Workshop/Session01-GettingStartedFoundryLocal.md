<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8c30436578b1bd604c48233ecdd39701",
  "translation_date": "2025-11-11T22:41:25+00:00",
  "source_file": "Workshop/Session01-GettingStartedFoundryLocal.md",
  "language_code": "pt"
}
-->
# Sess√£o 1: Introdu√ß√£o ao Foundry Local

## Resumo

Aprenda a instalar, configurar e executar os seus primeiros modelos de IA utilizando o Microsoft Foundry Local. Esta sess√£o pr√°tica oferece uma introdu√ß√£o passo a passo √† infer√™ncia local, desde a instala√ß√£o at√© √† cria√ß√£o da sua primeira aplica√ß√£o de chat utilizando modelos como Phi-4, Qwen e DeepSeek.

## Objetivos de Aprendizagem

Ao final desta sess√£o, voc√™ ser√° capaz de:

- **Instalar e Configurar**: Configurar o Foundry Local com verifica√ß√£o adequada da instala√ß√£o
- **Dominar Opera√ß√µes CLI**: Utilizar o CLI do Foundry Local para gest√£o e implementa√ß√£o de modelos
- **Executar o Seu Primeiro Modelo**: Implementar e interagir com um modelo de IA local com sucesso
- **Criar uma Aplica√ß√£o de Chat**: Desenvolver uma aplica√ß√£o de chat b√°sica utilizando o Foundry Local Python SDK
- **Compreender IA Local**: Entender os fundamentos da infer√™ncia local e gest√£o de modelos

## Pr√©-requisitos

### Requisitos do Sistema

- **Windows**: Windows 11 (22H2 ou posterior) OU **macOS**: macOS 11+ (suporte limitado)
- **RAM**: M√≠nimo de 8GB, recomendado 16GB+
- **Armazenamento**: 10GB+ de espa√ßo livre para modelos
- **Python**: Vers√£o 3.10 ou posterior instalada
- **Acesso de Administrador**: Privil√©gios de administrador para instala√ß√£o

### Ambiente de Desenvolvimento

- Visual Studio Code com extens√£o Python (recomendado)
- Acesso √† linha de comando (PowerShell no Windows, Terminal no macOS)
- Git para clonar reposit√≥rios (opcional)

## Fluxo do Workshop (30 minutos)

### Passo 1: Instalar o Foundry Local (5 minutos)

#### Instala√ß√£o no Windows

Instale o Foundry Local utilizando o gerenciador de pacotes do Windows:

```powershell
# Install via winget (recommended)
winget install Microsoft.FoundryLocal
```

Alternativa: Fa√ßa o download diretamente de [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)

#### Instala√ß√£o no macOS (Suporte Limitado)

> [!NOTE] 
> O suporte ao macOS est√° atualmente em pr√©via. Consulte a documenta√ß√£o oficial para obter as informa√ß√µes mais recentes.

Se dispon√≠vel, instale utilizando o Homebrew:

```bash
# If Homebrew formula is available
brew update
brew install foundry-local

# Or manual download (check official docs for latest)
curl -L -o foundry-local.tar.gz "https://download.microsoft.com/foundry-local/latest/macos/foundry-local.tar.gz"
tar -xzf foundry-local.tar.gz
sudo ./install.sh
```

**Alternativa para utilizadores de macOS:**
- Utilize uma VM com Windows 11 (Parallels/UTM) e siga os passos para Windows
- Execute via container, se dispon√≠vel, e configure `FOUNDRY_LOCAL_ENDPOINT`

### Passo 2: Verificar a Instala√ß√£o (3 minutos)

Ap√≥s a instala√ß√£o, reinicie o terminal e verifique se o Foundry Local est√° funcionando:

```powershell
# Check if Foundry Local is installed correctly
foundry --version

# View available commands
foundry --help
```

A sa√≠da esperada deve mostrar informa√ß√µes de vers√£o e comandos dispon√≠veis.

### Passo 3: Configurar o Ambiente Python (5 minutos)

Crie um ambiente Python dedicado para este workshop:

**Windows:**
```powershell
# Create virtual environment
py -m venv .venv

# Activate environment
.\.venv\Scripts\Activate.ps1

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

**macOS/Linux:**
```bash
# Create virtual environment
python3 -m venv .venv

# Activate environment
source .venv/bin/activate

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

### Passo 4: Executar o Seu Primeiro Modelo (7 minutos)

Agora vamos executar o nosso primeiro modelo de IA localmente!

#### Comece com Phi-4 Mini (Modelo Recomendado)

```powershell
# Download and start phi-4-mini (lightweight, fast)
foundry model run phi-4-mini

# Test the model with a simple prompt
foundry model run phi-4-mini --prompt "Hello, introduce yourself in one sentence"
```

> [!TIP]
> Este comando faz o download do modelo (na primeira vez) e inicia automaticamente o servi√ßo Foundry Local.

#### Verifique o que est√° em execu√ß√£o

```powershell
# List available models (shows downloaded models)
foundry model list

# Check service status
foundry service status

# See what models are cached locally
foundry cache list
```

#### Experimente Outros Modelos

Depois de o phi-4-mini estar funcionando, experimente outros modelos:

```powershell
# Larger model with better capabilities
foundry model run gpt-oss-20b --prompt "Explain edge AI in simple terms"

# Fast, efficient model
foundry model run qwen2.5-0.5b --prompt "What are the benefits of local AI inference?"
```

### Passo 5: Criar a Sua Primeira Aplica√ß√£o de Chat (10 minutos)

Agora vamos criar uma aplica√ß√£o Python que utiliza os modelos que acab√°mos de iniciar.

#### Criar o Script de Chat

Crie um novo ficheiro chamado `my_first_chat.py` (ou utilize o exemplo fornecido):

```python
#!/usr/bin/env python3
"""
My First Foundry Local Chat Application
Using FoundryLocalManager for automatic service management
"""

import os
from foundry_local import FoundryLocalManager
from openai import OpenAI

def main():
    # Get model alias from environment or use default
    alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-4-mini")
    
    try:
        # Initialize Foundry Local Manager (auto-starts service, downloads model)
        manager = FoundryLocalManager(alias)
        
        # Create OpenAI client pointing to local endpoint
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-needed"
        )
        
        # Get the actual model ID for this alias
        model_id = manager.get_model_info(alias).id
        
        print("ü§ñ Welcome to your first local AI chat!")
        print(f"ÔøΩ Using model: {alias} -> {model_id}")
        print(f"üåê Endpoint: {manager.endpoint}")
        print("ÔøΩüí° Type 'quit' to exit\n")
        
    except Exception as e:
        print(f"‚ùå Failed to initialize Foundry Local: {e}")
        print("üí° Make sure Foundry Local is installed: foundry --version")
        return
    
    while True:
        # Get user input
        user_message = input("You: ").strip()
        
        if user_message.lower() in ['quit', 'exit', 'bye']:
            print("üëã Goodbye!")
            break
            
        if not user_message:
            continue
            
        try:
            # Send message to local AI model
            response = client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": "You are a helpful AI assistant running locally."},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=200,
                temperature=0.7
            )
            
            # Display the response
            ai_response = response.choices[0].message.content
            print(f"ü§ñ AI: {ai_response}\n")
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            print("üí° Check service status: foundry service status\n")

if __name__ == "__main__":
    main()
```

> [!TIP]
> **Exemplos Relacionados**: Para uso mais avan√ßado, veja:
>
> - **Exemplo em Python**: `Workshop/samples/session01/chat_bootstrap.py` - Inclui respostas em streaming e tratamento de erros
> - **Jupyter Notebook**: `Workshop/notebooks/session01_chat_bootstrap.ipynb` - Vers√£o interativa com explica√ß√µes detalhadas

#### Testar a Sua Aplica√ß√£o de Chat

```powershell
# No need to manually start models - FoundryLocalManager handles this!
# Just run your chat application
python my_first_chat.py
```

Alternativa: Utilize diretamente os exemplos fornecidos

```powershell
# Try the complete sample with streaming support
cd Workshop/samples
python -m session01.chat_bootstrap "Your question here"
```

Ou explore o notebook interativo  
Abra Workshop/notebooks/session01_chat_bootstrap.ipynb no VS Code

Experimente estas conversas de exemplo:

- "O que √© o Microsoft Foundry Local?"
- "Liste 3 benef√≠cios de executar modelos de IA localmente"
- "Ajude-me a entender IA de borda"

## O Que Voc√™ Conquistou

Parab√©ns! Voc√™ conseguiu:

1. ‚úÖ **Instalar o Foundry Local** e verificar que est√° funcionando
2. ‚úÖ **Iniciar o seu primeiro modelo de IA** (phi-4-mini) localmente
3. ‚úÖ **Testar diferentes modelos** via linha de comando
4. ‚úÖ **Criar uma aplica√ß√£o de chat** que se conecta √† sua IA local
5. ‚úÖ **Experimentar infer√™ncia de IA local** sem depend√™ncias na nuvem

## Compreendendo o Que Aconteceu

### Infer√™ncia de IA Local

- Os seus modelos de IA s√£o executados inteiramente no seu computador
- Nenhum dado √© enviado para a nuvem
- As respostas s√£o geradas localmente utilizando o seu CPU/GPU
- Privacidade e seguran√ßa s√£o mantidas

### Gest√£o de Modelos

- `foundry model run` faz o download e inicia os modelos
- **FoundryLocalManager SDK** lida automaticamente com o in√≠cio do servi√ßo e carregamento de modelos
- Os modelos s√£o armazenados em cache localmente para uso futuro
- V√°rios modelos podem ser baixados, mas geralmente apenas um √© executado por vez
- O servi√ßo gere automaticamente o ciclo de vida dos modelos

### Abordagens SDK vs CLI

- **Abordagem CLI**: Gest√£o manual de modelos com `foundry model run <model>`
- **Abordagem SDK**: Gest√£o autom√°tica de servi√ßos e modelos com `FoundryLocalManager(alias)`
- **Recomenda√ß√£o**: Utilize o SDK para aplica√ß√µes, CLI para testes e explora√ß√£o

## Refer√™ncia de Comandos Comuns

### Comandos Essenciais do CLI

```powershell
# Installation & Setup
foundry --version              # Check installation
foundry --help                 # View all commands

# Model Management
foundry model list             # List available models
foundry model run <model>      # Download and start a model
foundry model run <model> --prompt "text"  # One-shot prompt
foundry cache list             # Show downloaded models

# Service Management
foundry service status         # Check if service is running
foundry service start          # Start the service manually
foundry service stop           # Stop the service
```

### Recomenda√ß√µes de Modelos

- **phi-4-mini**: Melhor modelo inicial - r√°pido, leve, boa qualidade
- **qwen2.5-0.5b**: Infer√™ncia mais r√°pida, uso m√≠nimo de mem√≥ria
- **gpt-oss-20b**: Respostas de maior qualidade, requer mais recursos
- **deepseek-coder-1.3b**: Otimizado para tarefas de programa√ß√£o e c√≥digo

## Resolu√ß√£o de Problemas

### "Comando Foundry n√£o encontrado"

**Solu√ß√£o:**

```powershell
# Restart your terminal after installation
# Or manually add to PATH (Windows)
$env:PATH += ";C:\Program Files\Microsoft\FoundryLocal"
```

### "Falha ao carregar o modelo"

**Solu√ß√£o:**

```powershell
# Check available system memory
foundry service status

# Try a smaller model first
foundry model run phi-4-mini

# Check disk space for model downloads
# Models are stored in: %USERPROFILE%\.foundry\models (Windows)
```

### "Conex√£o recusada no localhost"

**Solu√ß√£o:**

```powershell
# Check if service is running
foundry service status

# Start service if needed
foundry service start

# Verify the port (default is 5273)
# Check for port conflicts with: netstat -an | findstr 5273
```

## Pr√≥ximos Passos

### A√ß√µes Imediatas

1. **Experimente** diferentes modelos e prompts
2. **Modifique** a sua aplica√ß√£o de chat para testar diferentes modelos
3. **Crie** os seus pr√≥prios prompts e teste as respostas
4. **Explore** a Sess√£o 2: Constru√ß√£o de aplica√ß√µes RAG

### Caminho de Aprendizagem Avan√ßado

1. **Sess√£o 2**: Desenvolver solu√ß√µes de IA com RAG (Gera√ß√£o Aumentada por Recupera√ß√£o)
2. **Sess√£o 3**: Comparar diferentes modelos de c√≥digo aberto
3. **Sess√£o 4**: Trabalhar com modelos de ponta
4. **Sess√£o 5**: Construir sistemas de IA multiagente

## Vari√°veis de Ambiente (Opcional)

Para uso mais avan√ßado, pode configurar estas vari√°veis de ambiente:

| Vari√°vel | Finalidade | Exemplo |
|----------|------------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Modelo padr√£o a utilizar | `phi-4-mini` |
| `FOUNDRY_LOCAL_ENDPOINT` | Substituir URL do endpoint | `http://localhost:5273/v1` |

Crie um ficheiro `.env` no diret√≥rio do seu projeto:
```
FOUNDRY_LOCAL_ALIAS=phi-4-mini
FOUNDRY_LOCAL_ENDPOINT=auto
```

## Recursos Adicionais

### Documenta√ß√£o

- [Refer√™ncia do Foundry Local Python SDK](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk?pivots=programming-language-python)
- [Guia de Instala√ß√£o do Foundry Local](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)
- [Cat√°logo de Modelos](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/models)

### C√≥digo de Exemplo

- **Exemplo em Python da Sess√£o01**: `Workshop/samples/session01/chat_bootstrap.py` - Aplica√ß√£o de chat completa com streaming
- **Notebook da Sess√£o01**: `Workshop/notebooks/session01_chat_bootstrap.ipynb` - Tutorial interativo  
- [Exemplo do M√≥dulo08 01](../Module08/samples/01/README.md) - Introdu√ß√£o ao Chat REST
- [Exemplo do M√≥dulo08 02](../Module08/samples/02/README.md) - Integra√ß√£o com OpenAI SDK
- [Exemplo do M√≥dulo08 03](../Module08/samples/03/README.md) - Descoberta e Benchmarking de Modelos

### Comunidade

- [Discuss√µes no GitHub do Foundry Local](https://github.com/microsoft/Foundry-Local/discussions)
- [Comunidade Azure AI](https://techcommunity.microsoft.com/category/artificialintelligence)

---

**Dura√ß√£o da Sess√£o**: 30 minutos pr√°ticos + 15 minutos de perguntas e respostas  
**N√≠vel de Dificuldade**: Iniciante  
**Pr√©-requisitos**: Windows 11/macOS 11+, Python 3.10+, Acesso de administrador

## Cen√°rio de Exemplo do Workshop

### Contexto Real

**Cen√°rio**: Uma equipa de TI empresarial precisa avaliar a infer√™ncia de IA no dispositivo para processar feedback sens√≠vel de funcion√°rios sem enviar dados para servi√ßos externos.

**Seu Objetivo**: Demonstrar que modelos de IA locais podem fornecer respostas de qualidade com lat√™ncia inferior a um segundo, mantendo total privacidade dos dados.

### Prompts de Teste

Utilize estes prompts para validar a sua configura√ß√£o:

```json
[
    "List two benefits of local inference.",
    "Summarize why keeping data on device improves privacy.",
    "Give one trade-off when choosing a small model over a large model."
]
```

### Crit√©rios de Sucesso

- ‚úÖ Todos os prompts recebem respostas em menos de 2 segundos
- ‚úÖ Nenhum dado sai do seu computador local
- ‚úÖ As respostas s√£o relevantes e √∫teis
- ‚úÖ A sua aplica√ß√£o de chat funciona sem problemas

Esta valida√ß√£o garante que a sua configura√ß√£o do Foundry Local est√° pronta para os workshops avan√ßados das Sess√µes 2-6.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Aviso**:  
Este documento foi traduzido utilizando o servi√ßo de tradu√ß√£o por IA [Co-op Translator](https://github.com/Azure/co-op-translator). Embora nos esforcemos pela precis√£o, esteja ciente de que tradu√ß√µes autom√°ticas podem conter erros ou imprecis√µes. O documento original na sua l√≠ngua nativa deve ser considerado a fonte autorit√°ria. Para informa√ß√µes cr√≠ticas, recomenda-se uma tradu√ß√£o profissional realizada por humanos. N√£o nos responsabilizamos por quaisquer mal-entendidos ou interpreta√ß√µes incorretas decorrentes do uso desta tradu√ß√£o.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->