# Секција 3: Microsoft Olive Optimization Suite

## Садржај
1. [Увод](../../../Module04)
2. [Шта је Microsoft Olive?](../../../Module04)
3. [Инсталација](../../../Module04)
4. [Водич за брзи почетак](../../../Module04)
5. [Пример: Конвертовање Qwen3 у ONNX INT4](../../../Module04)
6. [Напредна употреба](../../../Module04)
7. [Репозиторијум Olive рецепата](../../../Module04)
8. [Најбоље праксе](../../../Module04)
9. [Решавање проблема](../../../Module04)
10. [Додатни ресурси](../../../Module04)

## Увод

Microsoft Olive је моћан, једноставан за употребу алат за оптимизацију модела који је свестан хардвера и поједностављује процес оптимизације модела машинског учења за примену на различитим хардверским платформама. Без обзира да ли циљате CPU, GPU или специјализоване AI акцелераторе, Olive вам помаже да постигнете оптималне перформансе уз очување тачности модела.

## Шта је Microsoft Olive?

Olive је једноставан алат за оптимизацију модела који је свестан хардвера и комбинује водеће технике у индустрији за компресију модела, оптимизацију и компилацију. Ради са ONNX Runtime као E2E решењем за оптимизацију инференције.

### Кључне карактеристике

- **Оптимизација свесна хардвера**: Аутоматски бира најбоље технике оптимизације за ваш циљани хардвер
- **40+ уграђених компоненти за оптимизацију**: Обухвата компресију модела, квантизацију, оптимизацију графа и још много тога
- **Једноставан CLI интерфејс**: Лаки команди за уобичајене задатке оптимизације
- **Подршка за више оквира**: Ради са PyTorch, Hugging Face моделима и ONNX
- **Подршка за популарне моделе**: Olive може аутоматски оптимизовати популарне архитектуре модела као што су Llama, Phi, Qwen, Gemma, итд.

### Предности

- **Смањено време развоја**: Нема потребе за ручним експериментисањем са различитим техникама оптимизације
- **Побољшање перформанси**: Значајна побољшања брзине (до 6x у неким случајевима)
- **Крос-платформска примена**: Оптимизовани модели раде на различитим хардверима и оперативним системима
- **Очувана тачност**: Оптимизације чувају квалитет модела уз побољшање перформанси

## Инсталација

### Предуслови

- Python 3.8 или новији
- pip менаџер пакета
- Виртуелно окружење (препоручено)

### Основна инсталација

Креирајте и активирајте виртуелно окружење:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```
  
Инсталирајте Olive са функцијама за аутоматску оптимизацију:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
  
### Опциони зависности

Olive нуди различите опционе зависности за додатне функције:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```
  
### Провера инсталације

```bash
olive --help
```
  
Ако је успешно, требало би да видите Olive CLI поруку помоћи.

## Водич за брзи почетак

### Ваша прва оптимизација

Оптимизујмо мали језички модел користећи Olive-ову функцију аутоматске оптимизације:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
### Шта ова команда ради

Процес оптимизације укључује: преузимање модела из локалне кеш меморије, хватање ONNX графа и чување тежина у ONNX датотеци, оптимизацију ONNX графа и квантизацију модела на int4 користећи RTN метод.

### Објашњење параметара команде

- `--model_name_or_path`: Hugging Face идентификатор модела или локална путања
- `--output_path`: Директоријум где ће оптимизовани модел бити сачуван
- `--device`: Циљани уређај (cpu, gpu)
- `--provider`: Провајдер извршења (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Користи ONNX Runtime Generate AI за инференцију
- `--precision`: Прецизност квантизације (int4, int8, fp16)
- `--log_level`: Вербалност логовања (0=минимално, 1=детаљно)

## Пример: Конвертовање Qwen3 у ONNX INT4

На основу датог Hugging Face примера на [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), ево како да оптимизујете Qwen3 модел:

### Корак 1: Преузимање модела (опционо)

Да бисте минимизирали време преузимања, кеширајте само основне датотеке:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```
  
### Корак 2: Оптимизација Qwen3 модела

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
### Корак 3: Тестирање оптимизованог модела

Креирајте једноставан Python скрипт за тестирање вашег оптимизованог модела:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```
  
### Структура излазних података

Након оптимизације, ваш излазни директоријум ће садржати:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```
  
## Напредна употреба

### Конфигурационе датотеке

За сложеније радне токове оптимизације, можете користити JSON конфигурационе датотеке:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```
  
Покрените са конфигурацијом:

```bash
olive run --config config.json
```
  
### Оптимизација за GPU

За оптимизацију CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
За DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
### Фино подешавање са Olive

Olive такође подржава фино подешавање модела:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```
  
## Најбоље праксе

### 1. Избор модела
- Почните са мањим моделима за тестирање (нпр. 0.5B-7B параметара)
- Уверите се да је архитектура вашег циљаног модела подржана од стране Olive-а

### 2. Хардверске разматрања
- Ускладите циљ оптимизације са хардвером за примену
- Користите GPU оптимизацију ако имате хардвер компатибилан са CUDA
- Размотрите DirectML за Windows машине са интегрисаном графиком

### 3. Избор прецизности
- **INT4**: Максимална компресија, благо губљење тачности
- **INT8**: Добар баланс величине и тачности
- **FP16**: Минималан губитак тачности, умерено смањење величине

### 4. Тестирање и валидација
- Увек тестирајте оптимизоване моделе са вашим специфичним случајевима употребе
- Упоредите метрике перформанси (латенција, пропусност, тачност)
- Користите репрезентативне улазне податке за евалуацију

### 5. Итеративна оптимизација
- Почните са аутоматском оптимизацијом за брзе резултате
- Користите конфигурационе датотеке за прецизну контролу
- Експериментишите са различитим пролазима оптимизације

## Решавање проблема

### Уобичајени проблеми

#### 1. Проблеми са инсталацијом
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```
  
#### 2. Проблеми са CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```
  
#### 3. Проблеми са меморијом
- Користите мање величине серија током оптимизације
- Прво пробајте квантизацију са већом прецизношћу (int8 уместо int4)
- Уверите се да имате довољно простора на диску за кеширање модела

#### 4. Грешке при учитавању модела
- Проверите путању модела и дозволе приступа
- Проверите да ли модел захтева `trust_remote_code=True`
- Уверите се да су све потребне датотеке модела преузете

### Добијање помоћи

- **Документација**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Примери**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Репозиторијум Olive рецепата

### Увод у Olive рецепте

Репозиторијум [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) допуњује главни Olive алат пружајући свеобухватну колекцију готових рецепата за оптимизацију популарних AI модела. Овај репозиторијум служи као практична референца за оптимизацију јавно доступних модела и креирање радних токова оптимизације за власничке моделе.

### Кључне карактеристике

- **100+ готових рецепата**: Конфигурације за оптимизацију популарних модела
- **Подршка за више архитектура**: Обухвата трансформер моделе, моделе за визију и мултимодалне архитектуре
- **Оптимизације специфичне за хардвер**: Рецепти прилагођени за CPU, GPU и специјализоване акцелераторе
- **Популарне породице модела**: Укључује Phi, Llama, Qwen, Gemma, Mistral и многе друге

### Подржане породице модела

Репозиторијум укључује рецепте за оптимизацију:

#### Језички модели
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 серија (0.5B до 14B)
- **Google Gemma**: Различите конфигурације Gemma модела
- **Mistral AI**: Mistral-7B серија
- **DeepSeek**: R1-Distill серија модела

#### Модели за визију и мултимодалне моделе
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP модели**: Различите CLIP-ViT конфигурације
- **ResNet**: Оптимизације за ResNet-50
- **Vision Transformers**: ViT-base-patch16-224

#### Специјализовани модели
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Основне и вишејезичне варијанте
- **Sentence Transformers**: all-MiniLM-L6-v2

### Коришћење Olive рецепата

#### Метод 1: Клонирање специфичног рецепта

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```
  
#### Метод 2: Коришћење рецепта као шаблона

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```
  
### Структура рецепта

Сваки директоријум рецепта обично садржи:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```
  
### Пример: Коришћење Phi-4-mini рецепта

Користимо Phi-4-mini рецепт као пример:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```
  
Конфигурациона датотека обично укључује:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```
  
### Прилагођавање рецепата

#### Модификација циљаног хардвера

Да бисте променили циљани хардвер, ажурирајте секцију `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```
  
#### Подешавање параметара оптимизације

Измените секцију `passes` за различите нивое оптимизације:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```
  
### Креирање сопственог рецепта

1. **Почните са сличним моделом**: Пронађите рецепт за модел са сличном архитектуром
2. **Ажурирајте конфигурацију модела**: Промените име/путању модела у конфигурацији
3. **Подесите параметре**: Измените параметре оптимизације по потреби
4. **Тестирајте и валидирајте**: Покрените оптимизацију и валидирајте резултате
5. **Доприносите назад**: Размотрите допринос вашег рецепта у репозиторијум

### Предности коришћења рецепата

#### 1. **Проверене конфигурације**
- Тестирана подешавања оптимизације за специфичне моделе
- Избегавање метода покушаја и грешке у проналажењу оптималних параметара

#### 2. **Подешавање специфично за хардвер**
- Пре-оптимизовано за различите провајдере извршења
- Готове конфигурације за CPU, GPU и NPU циљеве

#### 3. **Свеобухватна покривеност**
- Подржава најпопуларније моделе отвореног кода
- Редовна ажурирања са новим издањима модела

#### 4. **Доприноси заједнице**
- Сараднички развој са AI заједницом
- Дељено знање и најбоље праксе

### Допринос Olive рецептима

Ако сте оптимизовали модел који није покривен у репозиторијуму:

1. **Fork репозиторијум**: Креирајте сопствени fork olive-recipes
2. **Креирајте директоријум рецепта**: Додајте нови директоријум за ваш модел
3. **Укључите конфигурацију**: Додајте olive_config.json и пратеће датотеке
4. **Документујте употребу**: Обезбедите јасан README са упутствима
5. **Пошаљите Pull Request**: Допринесите назад заједници

### Бенчмарци перформанси

Многи рецепти укључују бенчмарке перформанси који показују:
- **Побољшање латенције**: Типично 2-6x убрзање у односу на основну вредност
- **Смањење меморије**: 50-75% смањење употребе меморије са квантизацијом
- **Очување тачности**: 95-99% очувања тачности

### Интеграција са AI алатима

Рецепти се беспрекорно интегришу са:
- **VS Code AI Toolkit**: Директна интеграција за оптимизацију модела
- **Azure Machine Learning**: Радни токови оптимизације у облаку
- **ONNX Runtime**: Оптимизована примена инференције

## Додатни ресурси

### Званични линкови
- **GitHub репозиторијум**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Репозиторијум Olive рецепата**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime документација**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face пример**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Примери заједнице
- **Jupyter бележнице**: Доступне у Olive GitHub репозиторијуму — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code екстензија**: Преглед AI Toolkit за VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Блог постови**: Microsoft

---

**Одрицање од одговорности**:  
Овај документ је преведен помоћу услуге за превођење вештачке интелигенције [Co-op Translator](https://github.com/Azure/co-op-translator). Иако се трудимо да обезбедимо тачност, молимо вас да имате у виду да аутоматски преводи могу садржати грешке или нетачности. Оригинални документ на његовом изворном језику треба сматрати ауторитативним извором. За критичне информације препоручује се професионални превод од стране људи. Не преузимамо одговорност за било каква погрешна тумачења или неспоразуме који могу настати услед коришћења овог превода.