{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8376a3",
   "metadata": {},
   "source": [
    "# സെഷൻ 5 – മൾട്ടി-ഏജന്റ് ഓർക്കസ്ട്രേറ്റർ\n",
    "\n",
    "Foundry Local ഉപയോഗിച്ച് ഒരു ലളിതമായ രണ്ട്-ഏജന്റ് പൈപ്പ്‌ലൈൻ (റിസർചർ -> എഡിറ്റർ) പ്രദർശിപ്പിക്കുന്നു.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bef206",
   "metadata": {},
   "source": [
    "### വിശദീകരണം: ആശ്രിത ഇൻസ്റ്റലേഷൻ\n",
    "പ്രാദേശിക മോഡൽ ആക്‌സസ്‌ക്കും ചാറ്റ് പൂർത്തീകരണങ്ങൾക്കും ആവശ്യമായ `foundry-local-sdk`യും `openai`യും ഇൻസ്റ്റാൾ ചെയ്യുന്നു. ഐഡംപോട്ടന്റ്.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32871a",
   "metadata": {},
   "source": [
    "# സീനാരിയോ\n",
    "കുറഞ്ഞത് രണ്ട് ഏജന്റുകൾ ഉൾക്കൊള്ളുന്ന ഓർക്കസ്ട്രേറ്റർ പാറ്റേൺ നടപ്പിലാക്കുന്നു:\n",
    "- **റിസർചർ ഏജന്റ്** സംക്ഷിപ്തമായ വാസ്തവ ബുള്ളറ്റുകൾ ശേഖരിക്കുന്നു\n",
    "- **എഡിറ്റർ ഏജന്റ്** എക്സിക്യൂട്ടീവ് വ്യക്തതയ്ക്കായി പുനഃരചിക്കുന്നു\n",
    "\n",
    "ഏജന്റുകൾക്ക് പങ്കുവെക്കുന്ന മെമ്മറി, ഇടനിലവിലുള്ള ഔട്ട്പുട്ട് പരമ്പരാഗതമായി കൈമാറൽ, ഒരു ലളിതമായ പൈപ്പ്‌ലൈൻ ഫംഗ്ഷൻ എന്നിവ പ്രദർശിപ്പിക്കുന്നു. കൂടുതൽ റോളുകൾക്ക് (ഉദാ: ക്രിട്ടിക്, വെരിഫയർ) അല്ലെങ്കിൽ സമാന്തര ശാഖകൾക്ക് വിപുലീകരിക്കാവുന്നതാണ്.\n",
    "\n",
    "**പരിസ്ഥിതി വ്യത്യാസങ്ങൾ:**\n",
    "- `FOUNDRY_LOCAL_ALIAS` - ഉപയോഗിക്കാനുള്ള ഡിഫോൾട്ട് മോഡൽ (ഡിഫോൾട്ട്: phi-4-mini)\n",
    "- `AGENT_MODEL_PRIMARY` - പ്രാഥമിക ഏജന്റ് മോഡൽ (ALIAS-നെ മറികടക്കും)\n",
    "- `AGENT_MODEL_EDITOR` - എഡിറ്റർ ഏജന്റ് മോഡൽ (പ്രാഥമിക മോഡലിന് ഡിഫോൾട്ട്)\n",
    "\n",
    "**SDK റഫറൻസ്:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n",
    "\n",
    "**എങ്ങനെ പ്രവർത്തിക്കുന്നു:**\n",
    "1. **FoundryLocalManager** Foundry Local സർവീസ് സ്വയം ആരംഭിക്കുന്നു\n",
    "2. നിർദ്ദിഷ്ട മോഡൽ ഡൗൺലോഡ് ചെയ്ത് ലോഡ് ചെയ്യുന്നു (അല്ലെങ്കിൽ കാഷ് ചെയ്ത പതിപ്പ് ഉപയോഗിക്കുന്നു)\n",
    "3. ഇടപഴകലിനായി OpenAI-സമാനമായ എന്റ്പോയിന്റ് നൽകുന്നു\n",
    "4. ഓരോ ഏജന്റും പ്രത്യേക ജോലികൾക്കായി വ്യത്യസ്ത മോഡൽ ഉപയോഗിക്കാം\n",
    "5. ബിൽറ്റ്-ഇൻ റിട്രൈ ലജിക് താൽക്കാലിക പരാജയങ്ങൾ സുഖപ്രദമായി കൈകാര്യം ചെയ്യുന്നു\n",
    "\n",
    "**പ്രധാന സവിശേഷതകൾ:**\n",
    "- ✅ സ്വയം സർവീസ് കണ്ടെത്തൽ, ആരംഭിക്കൽ\n",
    "- ✅ മോഡൽ ലൈഫ്‌സൈക്കിൾ മാനേജ്മെന്റ് (ഡൗൺലോഡ്, കാഷ്, ലോഡ്)\n",
    "- ✅ പരിചിതമായ API-ക്കായി OpenAI SDK അനുയോജ്യത\n",
    "- ✅ ഏജന്റ് സ്പെഷ്യലൈസേഷനായി മൾട്ടി-മോഡൽ പിന്തുണ\n",
    "- ✅ റിട്രൈ ലജിക് ഉപയോഗിച്ച് ശക്തമായ പിശക് കൈകാര്യം\n",
    "- ✅ ലോക്കൽ ഇൻഫറൻസ് (ക്ലൗഡ് API ആവശ്യമില്ല)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d91c342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q foundry-local-sdk openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db22f6",
   "metadata": {},
   "source": [
    "### Explanation: Core Imports & Typing\n",
    "ഏജന്റ് സന്ദേശ സംഭരണത്തിനും വ്യക്തതയ്ക്കുമായി ടൈപ്പിംഗ് സൂചനകൾക്കും ഡാറ്റാക്ലാസുകൾ പരിചയപ്പെടുത്തുന്നു. തുടര്‍ന്നുള്ള ഏജന്റ് പ്രവർത്തനങ്ങൾക്ക് Foundry Local മാനേജറും OpenAI ക്ലയന്റും ഇറക്കുമതി ചെയ്യുന്നു.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72d53845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803e00f",
   "metadata": {},
   "source": [
    "### വിശദീകരണം: മോഡൽ ഇൻഷിയലൈസേഷൻ (SDK പാറ്റേൺ)\n",
    "ദൃഢമായ മോഡൽ മാനേജ്മെന്റിനായി Foundry Local Python SDK ഉപയോഗിക്കുന്നു:\n",
    "- **FoundryLocalManager(alias)** - സർവീസ് സ്വയം ആരംഭിച്ച് ആലിയാസിലൂടെ മോഡൽ ലോഡ് ചെയ്യുന്നു\n",
    "- **get_model_info(alias)** - ആലിയാസിനെ കൺക്രീറ്റ് മോഡൽ ഐഡിയായി പരിഹരിക്കുന്നു\n",
    "- **manager.endpoint** - OpenAI ക്ലയന്റിനുള്ള സർവീസ് എൻഡ്‌പോയിന്റ് നൽകുന്നു\n",
    "- **manager.api_key** - API കീ നൽകുന്നു (ലോകൽ ഉപയോഗത്തിന് ഐച്ഛികം)\n",
    "- വ്യത്യസ്ത ഏജന്റുകൾക്കായി വേർതിരിച്ച മോഡലുകൾ പിന്തുണയ്ക്കുന്നു (പ്രൈമറി vs എഡിറ്റർ)\n",
    "- പ്രതിരോധശേഷിയുള്ള എക്സ്പൊണൻഷ്യൽ ബാക്ക്ഓഫ് ഉപയോഗിച്ച് ബിൽറ്റ്-ഇൻ റിട്രൈ ലജിക്\n",
    "- സർവീസ് തയ്യാറാണെന്ന് ഉറപ്പാക്കാൻ കണക്ഷൻ പരിശോധന\n",
    "\n",
    "**പ്രധാന SDK പാറ്റേൺ:**\n",
    "```python\n",
    "manager = FoundryLocalManager(alias)\n",
    "model_info = manager.get_model_info(alias)\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key)\n",
    "```\n",
    "\n",
    "**ലൈഫ്‌സൈക്കിൾ മാനേജ്മെന്റ്:**\n",
    "- മാനേജർമാർ ശരിയായ ക്ലീനപ്പിനായി ഗ്ലോബലായി സൂക്ഷിക്കുന്നു\n",
    "- ഓരോ ഏജന്റും പ്രത്യേകതയ്ക്കായി വ്യത്യസ്ത മോഡൽ ഉപയോഗിക്കാം\n",
    "- സ്വയം സർവീസ് കണ്ടെത്തലും കണക്ഷൻ കൈകാര്യം ചെയ്യലും\n",
    "- പരാജയങ്ങളിൽ എക്സ്പൊണൻഷ്യൽ ബാക്ക്ഓഫ് ഉപയോഗിച്ച് സുന്ദരമായ റിട്രൈ\n",
    "\n",
    "ഏജന്റ് ഓർക്കസ്ട്രേഷൻ ആരംഭിക്കുന്നതിന് മുമ്പ് ശരിയായ ഇൻഷിയലൈസേഷൻ ഉറപ്പാക്കുന്നു.\n",
    "\n",
    "**റഫറൻസ്:** https://github.com/microsoft/Foundry-Local/tree/main/sdk/python/foundry_local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6552004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Initializing Primary Model: phi-4-mini\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'phi-4-mini' (attempt 1/3)...\n",
      "[OK] Initialized 'phi-4-mini' -> Phi-4-mini-instruct-cuda-gpu:4 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "Initializing Editor Model: gpt-oss-20b\n",
      "================================================================================\n",
      "[Init] Starting Foundry Local for 'gpt-oss-20b' (attempt 1/3)...\n",
      "[OK] Initialized 'gpt-oss-20b' -> gpt-oss-20b-cuda-gpu:1 at http://127.0.0.1:59959/v1\n",
      "\n",
      "================================================================================\n",
      "[Configuration Summary]\n",
      "================================================================================\n",
      "  Primary Agent:\n",
      "    - Alias: phi-4-mini\n",
      "    - Model: Phi-4-mini-instruct-cuda-gpu:4\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "\n",
      "  Editor Agent:\n",
      "    - Alias: gpt-oss-20b\n",
      "    - Model: gpt-oss-20b-cuda-gpu:1\n",
      "    - Endpoint: http://127.0.0.1:59959/v1\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Environment configuration\n",
    "PRIMARY_ALIAS = os.getenv('AGENT_MODEL_PRIMARY', os.getenv('FOUNDRY_LOCAL_ALIAS', 'phi-4-mini'))\n",
    "EDITOR_ALIAS = os.getenv('AGENT_MODEL_EDITOR', PRIMARY_ALIAS)\n",
    "\n",
    "# Store managers globally for proper lifecycle management\n",
    "primary_manager = None\n",
    "editor_manager = None\n",
    "\n",
    "def init_model(alias: str, max_retries: int = 3):\n",
    "    \"\"\"Initialize Foundry Local manager with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        alias: Model alias to initialize\n",
    "        max_retries: Number of retry attempts with exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (manager, client, model_id, endpoint)\n",
    "    \"\"\"\n",
    "    delay = 2.0\n",
    "    last_err = None\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            print(f\"[Init] Starting Foundry Local for '{alias}' (attempt {attempt}/{max_retries})...\")\n",
    "            \n",
    "            # Initialize manager - this starts the service and loads the model\n",
    "            manager = FoundryLocalManager(alias)\n",
    "            \n",
    "            # Get model info to retrieve the actual model ID\n",
    "            model_info = manager.get_model_info(alias)\n",
    "            model_id = model_info.id\n",
    "            \n",
    "            # Create OpenAI client with manager's endpoint\n",
    "            client = OpenAI(\n",
    "                base_url=manager.endpoint,\n",
    "                api_key=manager.api_key or 'not-needed'\n",
    "            )\n",
    "            \n",
    "            # Verify the connection with a simple test\n",
    "            models = client.models.list()\n",
    "            print(f\"[OK] Initialized '{alias}' -> {model_id} at {manager.endpoint}\")\n",
    "            \n",
    "            return manager, client, model_id, manager.endpoint\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            if attempt < max_retries:\n",
    "                print(f\"[Retry {attempt}/{max_retries}] Failed to init '{alias}': {e}\")\n",
    "                print(f\"[Retry] Waiting {delay:.1f}s before retry...\")\n",
    "                time.sleep(delay)\n",
    "                delay *= 2\n",
    "            else:\n",
    "                print(f\"[ERROR] Failed to initialize '{alias}' after {max_retries} attempts\")\n",
    "    \n",
    "    raise RuntimeError(f\"Failed to initialize '{alias}' after {max_retries} attempts: {last_err}\")\n",
    "\n",
    "# Initialize primary model (for researcher)\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Initializing Primary Model: {PRIMARY_ALIAS}\")\n",
    "print('='*80)\n",
    "primary_manager, primary_client, PRIMARY_MODEL_ID, primary_endpoint = init_model(PRIMARY_ALIAS)\n",
    "\n",
    "# Initialize editor model (may be same as primary)\n",
    "if EDITOR_ALIAS != PRIMARY_ALIAS:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Initializing Editor Model: {EDITOR_ALIAS}\")\n",
    "    print('='*80)\n",
    "    editor_manager, editor_client, EDITOR_MODEL_ID, editor_endpoint = init_model(EDITOR_ALIAS)\n",
    "else:\n",
    "    print(f\"\\n[Info] Editor using same model as primary\")\n",
    "    editor_manager = primary_manager\n",
    "    editor_client, EDITOR_MODEL_ID = primary_client, PRIMARY_MODEL_ID\n",
    "    editor_endpoint = primary_endpoint\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"[Configuration Summary]\")\n",
    "print('='*80)\n",
    "print(f\"  Primary Agent:\")\n",
    "print(f\"    - Alias: {PRIMARY_ALIAS}\")\n",
    "print(f\"    - Model: {PRIMARY_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {primary_endpoint}\")\n",
    "print(f\"\\n  Editor Agent:\")\n",
    "print(f\"    - Alias: {EDITOR_ALIAS}\")\n",
    "print(f\"    - Model: {EDITOR_MODEL_ID}\")\n",
    "print(f\"    - Endpoint: {editor_endpoint}\")\n",
    "print('='*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817abb14",
   "metadata": {},
   "source": [
    "### Explanation: Agent & Memory Classes\n",
    "Defines lightweight `AgentMsg` for memory entries and `Agent` encapsulating:\n",
    "- **System role** - ഏജന്റിന്റെ വ്യക്തിത്വവും നിർദ്ദേശങ്ങളും\n",
    "- **Message history** - സംഭാഷണത്തിന്റെ പശ്ചാത്തലം നിലനിർത്തുന്നു\n",
    "- **act() method** - ശരിയായ പിശക് കൈകാര്യം ചെയ്യലോടെ പ്രവർത്തനങ്ങൾ നടപ്പിലാക്കുന്നു\n",
    "\n",
    "ഏജന്റ് വ്യത്യസ്ത മോഡലുകൾ (പ്രാഥമികവും എഡിറ്ററുമായ) ഉപയോഗിക്കാനും ഓരോ ഏജന്റിനും വേർതിരിച്ച പശ്ചാത്തലം നിലനിർത്താനും കഴിയും. ഈ മാതൃക സാധ്യമാക്കുന്നു:\n",
    "- പ്രവർത്തനങ്ങൾക്കിടയിൽ മെമ്മറി നിലനിർത്തൽ\n",
    "- ഓരോ ഏജന്റിനും സൗകര്യപ്രദമായ മോഡൽ നിയോഗം\n",
    "- പിശക് വേർതിരിക്കൽയും പുനരുദ്ധാരണം\n",
    "- എളുപ്പത്തിലുള്ള ചൈനിംഗ്‌വും ഓർക്കസ്ട്രേഷനും\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e535cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Agent classes initialized with Foundry SDK support\n",
      "[INFO] Using OpenAI SDK version: openai\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class AgentMsg:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "@dataclass\n",
    "class Agent:\n",
    "    name: str\n",
    "    system: str\n",
    "    client: OpenAI = None  # Allow per-agent client assignment\n",
    "    model_id: str = None   # Allow per-agent model\n",
    "    memory: List[AgentMsg] = field(default_factory=list)\n",
    "\n",
    "    def _history(self):\n",
    "        \"\"\"Return chat history in OpenAI messages format including system + memory.\"\"\"\n",
    "        msgs = [{'role': 'system', 'content': self.system}]\n",
    "        for m in self.memory[-6:]:  # Keep last 6 messages to avoid context overflow\n",
    "            msgs.append({'role': m.role, 'content': m.content})\n",
    "        return msgs\n",
    "\n",
    "    def act(self, prompt: str, temperature: float = 0.4, max_tokens: int = 300):\n",
    "        \"\"\"Send a prompt, store user + assistant messages in memory, and return assistant text.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User input/task for the agent\n",
    "            temperature: Sampling temperature (0.0-1.0)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "        \n",
    "        Returns:\n",
    "            Assistant response text\n",
    "        \"\"\"\n",
    "        # Use agent-specific client/model or fall back to primary\n",
    "        client_to_use = self.client or primary_client\n",
    "        model_to_use = self.model_id or PRIMARY_MODEL_ID\n",
    "        \n",
    "        self.memory.append(AgentMsg('user', prompt))\n",
    "        \n",
    "        try:\n",
    "            # Build messages including system prompt and history\n",
    "            messages = self._history() + [{'role': 'user', 'content': prompt}]\n",
    "            \n",
    "            resp = client_to_use.chat.completions.create(\n",
    "                model=model_to_use,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            \n",
    "            # Validate response\n",
    "            if not resp.choices:\n",
    "                raise RuntimeError(\"No completion choices returned\")\n",
    "            \n",
    "            out = resp.choices[0].message.content or \"\"\n",
    "            \n",
    "            if not out:\n",
    "                raise RuntimeError(\"Empty response content\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            out = f\"[ERROR:{self.name}] {type(e).__name__}: {str(e)}\"\n",
    "            print(f\"[Agent Error] {self.name}: {type(e).__name__}: {str(e)}\")\n",
    "        \n",
    "        self.memory.append(AgentMsg('assistant', out))\n",
    "        return out\n",
    "\n",
    "print(\"[INFO] Agent classes initialized with Foundry SDK support\")\n",
    "print(f\"[INFO] Using OpenAI SDK version: {OpenAI.__module__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1f4bd",
   "metadata": {},
   "source": [
    "### വിശദീകരണം: ഓർക്കസ്ട്രേറ്റഡ് പൈപ്പ്‌ലൈൻ  \n",
    "രണ്ട് പ്രത്യേക ഏജന്റുകൾ സൃഷ്ടിക്കുന്നു:  \n",
    "- **റിസർചർ**: പ്രാഥമിക മോഡൽ ഉപയോഗിച്ച് വാസ്തവ വിവരങ്ങൾ ശേഖരിക്കുന്നു  \n",
    "- **എഡിറ്റർ**: വേറെ മോഡൽ ഉപയോഗിക്കാം (കൺഫിഗർ ചെയ്താൽ), മെച്ചപ്പെടുത്തുകയും പുനഃരചയിക്കുകയും ചെയ്യുന്നു  \n",
    "\n",
    "`pipeline` ഫംഗ്ഷൻ:  \n",
    "1. റിസർചർ കച്ചവട വിവരങ്ങൾ ശേഖരിക്കുന്നു  \n",
    "2. എഡിറ്റർ എക്സിക്യൂട്ടീവ്-സജ്ജമായ ഔട്ട്പുട്ടായി മെച്ചപ്പെടുത്തുന്നു  \n",
    "3. ഇടനിലയും അന്തിമ ഫലവും തിരികെ നൽകുന്നു  \n",
    "\n",
    "ഈ മാതൃക സാധ്യമാക്കുന്നു:  \n",
    "- മോഡൽ പ്രത്യേകത (വിവിധ റോളുകൾക്കായി വ്യത്യസ്ത മോഡലുകൾ)  \n",
    "- ബഹുസ്ഥാപന പ്രോസസ്സിംഗിലൂടെ ഗുണനിലവാരം മെച്ചപ്പെടുത്തൽ  \n",
    "- വിവര പരിവർത്തനത്തിന്റെ ട്രേസബിലിറ്റി  \n",
    "- കൂടുതൽ ഏജന്റുകൾക്ക് അല്ലെങ്കിൽ സമാന്തര പ്രോസസ്സിംഗിന് എളുപ്പം വിപുലീകരണം\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60bb753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[Pipeline] Question: Explain why edge AI matters for compliance and latency.\n",
      "\n",
      "[Stage 1: Research]\n",
      "Output: - **Data Sovereignty**: Edge AI allows data to be processed locally, which can help organizations comply with regional data protection regulations by keeping sensitive information within the borders o...\n",
      "\n",
      "[Stage 2: Editorial Refinement]\n"
     ]
    }
   ],
   "source": [
    "# Create specialized agents with optional model assignment\n",
    "researcher = Agent(\n",
    "    name='Researcher',\n",
    "    system='You collect concise factual bullet points.',\n",
    "    client=primary_client,\n",
    "    model_id=PRIMARY_MODEL_ID\n",
    ")\n",
    "\n",
    "editor = Agent(\n",
    "    name='Editor',\n",
    "    system='You rewrite content for clarity and an executive, action-focused tone.',\n",
    "    client=editor_client,\n",
    "    model_id=EDITOR_MODEL_ID\n",
    ")\n",
    "\n",
    "def pipeline(q: str, verbose: bool = True):\n",
    "    \"\"\"Execute multi-agent pipeline: Researcher -> Editor.\n",
    "    \n",
    "    Args:\n",
    "        q: User question/task\n",
    "        verbose: Print intermediate outputs\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with research, final outputs, and metadata\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"[Pipeline] Question: {q}\\n\")\n",
    "    \n",
    "    # Stage 1: Research\n",
    "    if verbose:\n",
    "        print(\"[Stage 1: Research]\")\n",
    "    research = researcher.act(q)\n",
    "    if verbose:\n",
    "        print(f\"Output: {research[:200]}...\\n\")\n",
    "    \n",
    "    # Stage 2: Editorial refinement\n",
    "    if verbose:\n",
    "        print(\"[Stage 2: Editorial Refinement]\")\n",
    "    rewrite = editor.act(\n",
    "        f\"Rewrite professionally with a 1-sentence executive summary first. \"\n",
    "        f\"Improve clarity, keep bullet structure if present. Source:\\n{research}\"\n",
    "    )\n",
    "    if verbose:\n",
    "        print(f\"Output: {rewrite[:200]}...\\n\")\n",
    "    \n",
    "    return {\n",
    "        'question': q,\n",
    "        'research': research,\n",
    "        'final': rewrite,\n",
    "        'models': {\n",
    "            'researcher': PRIMARY_MODEL_ID,\n",
    "            'editor': EDITOR_MODEL_ID\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Execute sample pipeline\n",
    "print(\"=\"*80)\n",
    "result = pipeline('Explain why edge AI matters for compliance and latency.')\n",
    "print(\"=\"*80)\n",
    "print(\"\\n[FINAL OUTPUT]\")\n",
    "print(result['final'])\n",
    "print(\"\\n[METADATA]\")\n",
    "print(f\"Models used: {result['models']}\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7038f2d",
   "metadata": {},
   "source": [
    "### വിശദീകരണം: പൈപ്പ്‌ലൈൻ നിർവഹണവും ഫലങ്ങളും\n",
    "കമ്പ്ലയൻസ് + ലാറ്റൻസി വിഷയത്തിലുള്ള ചോദ്യത്തിൽ മൾട്ടി-ഏജന്റ് പൈപ്പ്‌ലൈൻ പ്രവർത്തിപ്പിച്ച് താഴെ കാണിക്കുന്നവ പ്രദർശിപ്പിക്കുന്നു:\n",
    "- മൾട്ടി-സ്റ്റേജ് വിവര പരിവർത്തനം\n",
    "- ഏജന്റ് വിദഗ്ധതയും സഹകരണവും\n",
    "- മെച്ചപ്പെടുത്തലിലൂടെ ഔട്ട്പുട്ട് ഗുണമേന്മ വർദ്ധിപ്പിക്കൽ\n",
    "- ട്രേസബിലിറ്റി (ഇടത്തരം ഫലങ്ങളും അന്തിമ ഫലങ്ങളും സംരക്ഷിച്ചിരിക്കുന്നു)\n",
    "\n",
    "**ഫലം ഘടന:**\n",
    "- `question` - യഥാർത്ഥ ഉപയോക്തൃ ചോദ്യം\n",
    "- `research` - കച്ചവട ഗവേഷണ ഫലം (വാസ്തവ ബുള്ളറ്റുകൾ)\n",
    "- `final` - ശുദ്ധീകരിച്ച എക്സിക്യൂട്ടീവ് സംഗ്രഹം\n",
    "- `models` - ഓരോ ഘട്ടത്തിനും ഉപയോഗിച്ച മോഡലുകൾ\n",
    "\n",
    "**വിസ്താര ആശയങ്ങൾ:**\n",
    "1. ഗുണനിലവാര പരിശോധനയ്ക്കായി ക്രിട്ടിക് ഏജന്റ് ചേർക്കുക\n",
    "2. വ്യത്യസ്ത വശങ്ങൾക്കായി സമാന്തര ഗവേഷണ ഏജന്റുകൾ നടപ്പിലാക്കുക\n",
    "3. വാസ്തവ പരിശോധനയ്ക്കായി വെരിഫയർ ഏജന്റ് ചേർക്കുക\n",
    "4. വ്യത്യസ്ത സങ്കീർണ്ണതാ നിലകൾക്കായി വ്യത്യസ്ത മോഡലുകൾ ഉപയോഗിക്കുക\n",
    "5. ആവർത്തന മെച്ചപ്പെടുത്തലിനായി ഫീഡ്ബാക്ക് ലൂപ്പുകൾ നടപ്പിലാക്കുക\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7391fc",
   "metadata": {},
   "source": [
    "### Advanced: കസ്റ്റം ഏജന്റ് കോൺഫിഗറേഷൻ\n",
    "\n",
    "ആരംഭ സെൽ പ്രവർത്തിപ്പിക്കുന്നതിന് മുമ്പ് പരിസ്ഥിതി വ്യത്യാസങ്ങൾ മാറ്റി ഏജന്റിന്റെ പെരുമാറ്റം ഇഷ്ടാനുസൃതമാക്കാൻ ശ്രമിക്കുക:\n",
    "\n",
    "**ലഭ്യമായ മോഡലുകൾ:**\n",
    "- എല്ലാ ലഭ്യമായ മോഡലുകളും കാണാൻ ടെർമിനലിൽ `foundry model ls` ഉപയോഗിക്കുക\n",
    "- ഉദാഹരണങ്ങൾ: phi-4-mini, phi-3.5-mini, qwen2.5-7b, llama-3.2-3b, തുടങ്ങിയവ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing pipeline with multiple questions:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Question 1: What are 3 key benefits of using small language models?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>analysis<|message|>The user wants a rewrite of the entire block of text. The rewrite should be professional, include a one-sentence executive summary first, improve clarity, keep bullet structure if present. The user has provided a large amount of text. The user wants a rewrite of that te...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 2: How does RAG improve AI accuracy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**RAG (Retrieval‑Augmented Generation) empowers AI to produce highly accurate, contextually relevant responses by combining a retrieval system with a large language model (LLM).**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n",
      "\n",
      "================================================================================\n",
      "Question 3: Why is local inference important for privacy?\n",
      "================================================================================\n",
      "\n",
      "[FINAL]: <|channel|>final<|message|>**Local inference—processing data directly on the device—offers a privacy‑first, security‑enhancing approach that meets regulatory requirements and builds user trust.**<|return|>...\n",
      "[Models]: Researcher=Phi-4-mini-instruct-cuda-gpu:4, Editor=gpt-oss-20b-cuda-gpu:1\n"
     ]
    }
   ],
   "source": [
    "# Example: Use different models for different agents\n",
    "# Uncomment and modify as needed:\n",
    "\n",
    "# import os\n",
    "# os.environ['AGENT_MODEL_PRIMARY'] = 'phi-4-mini'      # Fast, good for research\n",
    "# os.environ['AGENT_MODEL_EDITOR'] = 'qwen2.5-7b'       # Higher quality for editing\n",
    "\n",
    "# Then restart the kernel and re-run all cells\n",
    "\n",
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"What are 3 key benefits of using small language models?\",\n",
    "    \"How does RAG improve AI accuracy?\",\n",
    "    \"Why is local inference important for privacy?\"\n",
    "]\n",
    "\n",
    "print(\"Testing pipeline with multiple questions:\\n\")\n",
    "for i, q in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {q}\")\n",
    "    print('='*80)\n",
    "    r = pipeline(q, verbose=False)\n",
    "    print(f\"\\n[FINAL]: {r['final'][:300]}...\")\n",
    "    print(f\"[Models]: Researcher={r['models']['researcher']}, Editor={r['models']['editor']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**അസൂയാപത്രം**:  \nഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖ അധികാരപരമായ ഉറവിടമായി കണക്കാക്കപ്പെടണം. നിർണായക വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനം ഉപയോഗിക്കുന്നതിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ തെറ്റായ വ്യാഖ്യാനങ്ങൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "cdf372e5c916086a8d278c87e189f4a3",
   "translation_date": "2025-12-16T01:29:36+00:00",
   "source_file": "Workshop/notebooks/session05_agents_orchestrator.ipynb",
   "language_code": "ml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}