# ‡¥∏‡µÜ‡¥∑‡µª 2: OpenAI SDK ‡¥á‡¥®‡µç‡¥±‡¥ó‡µç‡¥∞‡µá‡¥∑‡µª Azure AI Foundry-‡¥Ø‡µÅ‡¥Æ‡¥æ‡¥Ø‡¥ø

## ‡¥Ö‡¥µ‡¥≤‡µã‡¥ï‡¥®‡¥Ç

‡¥®‡¥ø‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥ü‡µÜ Foundry Local ‡¥Ö‡¥ü‡¥ø‡¥∏‡µç‡¥•‡¥æ‡¥®‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥®‡¥ø‡µº‡¥Æ‡µç‡¥Æ‡¥ø‡¥ö‡µç‡¥ö‡µç, ‡¥à ‡¥∏‡µÜ‡¥∑‡µª Microsoft Foundry Local-‡¥®‡µÅ‡¥Ç Azure OpenAI-‡¥®‡µÅ‡¥Ç ‡¥í‡¥∞‡µÅ‡¥™‡µã‡¥≤‡µÜ ‡¥™‡¥ø‡¥®‡µç‡¥§‡µÅ‡¥£ ‡¥®‡µΩ‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥Ü‡¥ß‡µÅ‡¥®‡¥ø‡¥ï OpenAI SDK ‡¥á‡¥®‡µç‡¥±‡¥ó‡µç‡¥∞‡µá‡¥∑‡µª ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡¥≥‡µÜ ‡¥ï‡µá‡¥®‡µç‡¥¶‡µç‡¥∞‡µÄ‡¥ï‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ. ‡¥∏‡µç‡¥µ‡¥ï‡¥æ‡¥∞‡µç‡¥Ø‡¥§‡¥ï‡µç‡¥ï‡µÅ‡¥Ç ‡¥µ‡¥ø‡¥ï‡¥∏‡¥®‡¥§‡µç‡¥§‡¥ø‡¥®‡µÅ‡¥Ç ‡¥µ‡µá‡¥£‡µç‡¥ü‡¥ø ‡¥≤‡µã‡¥ï‡µç‡¥ï‡¥≤‡¥æ‡¥Ø‡¥ø ‡¥™‡µç‡¥∞‡¥µ‡µº‡¥§‡µç‡¥§‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡¥µ‡µÅ‡¥®‡µç‡¥®, ‡¥Ü‡¥µ‡¥∂‡µç‡¥Ø‡¥Æ‡¥æ‡¥Ø‡¥™‡µç‡¥™‡µã‡µæ Azure OpenAI ‡¥µ‡¥¥‡¥ø ‡¥ï‡µç‡¥≤‡µó‡¥°‡µç ‡¥∏‡µç‡¥ï‡µÜ‡¥Ø‡¥ø‡¥≤‡¥¨‡¥ø‡¥≤‡¥ø‡¥±‡µç‡¥±‡¥ø ‡¥®‡µΩ‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥´‡µç‡¥≤‡µÜ‡¥ï‡µç‡¥∏‡¥ø‡¥¨‡¥ø‡µæ AI ‡¥Ü‡¥™‡µç‡¥≤‡¥ø‡¥ï‡µç‡¥ï‡µá‡¥∑‡¥®‡µÅ‡¥ï‡µæ ‡¥®‡¥ø‡µº‡¥Æ‡µç‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥ï‡¥≤‡¥Ø‡¥ø‡µΩ ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ ‡¥™‡µç‡¥∞‡¥æ‡¥µ‡µÄ‡¥£‡µç‡¥Ø‡¥Ç ‡¥®‡µá‡¥ü‡µÅ‡¥Ç.

## ‡¥™‡¥†‡¥® ‡¥≤‡¥ï‡µç‡¥∑‡µç‡¥Ø‡¥ô‡µç‡¥ô‡µæ

‡¥à ‡¥∏‡µÜ‡¥∑‡µª ‡¥Ö‡¥µ‡¥∏‡¥æ‡¥®‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥Æ‡µç‡¥™‡µã‡µæ, ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥ï‡¥¥‡¥ø‡¥Ø‡µÅ‡¥Ç:
- Foundry Local-‡¥®‡µÅ‡¥Ç Azure OpenAI-‡¥®‡µÅ‡¥Ç ‡¥í‡¥∞‡µÅ‡¥™‡µã‡¥≤‡µÜ ‡¥Ü‡¥ß‡µÅ‡¥®‡¥ø‡¥ï OpenAI SDK ‡¥á‡¥®‡µç‡¥±‡¥ó‡µç‡¥∞‡µá‡¥∑‡µª ‡¥ï‡µà‡¥ï‡¥æ‡¥∞‡µç‡¥Ø‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï
- ‡¥Æ‡µÜ‡¥ö‡µç‡¥ö‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü ‡¥â‡¥™‡¥Ø‡µã‡¥ï‡µç‡¥§‡µÉ ‡¥Ö‡¥®‡µÅ‡¥≠‡¥µ‡¥§‡µç‡¥§‡¥ø‡¥®‡¥æ‡¥Ø‡¥ø ‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÄ‡¥Æ‡¥ø‡¥Ç‡¥ó‡µç ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£‡¥ô‡µç‡¥ô‡µæ ‡¥®‡¥ü‡¥™‡µç‡¥™‡¥ø‡¥≤‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï
- ‡¥Æ‡µæ‡¥ü‡µç‡¥ü‡¥ø-‡¥™‡µç‡¥∞‡µä‡¥µ‡µà‡¥°‡µº ‡¥™‡¥ø‡¥®‡µç‡¥§‡µÅ‡¥£‡¥Ø‡µç‡¥ï‡µç‡¥ï‡¥æ‡¥Ø‡¥ø ‡¥∂‡¥ï‡µç‡¥§‡¥Æ‡¥æ‡¥Ø ‡¥ï‡µç‡¥≤‡¥Ø‡¥®‡µç‡¥±‡µç ‡¥´‡¥æ‡¥ï‡µç‡¥ü‡¥±‡¥ø ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡µæ ‡¥∏‡µÉ‡¥∑‡µç‡¥ü‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
- ‡¥ï‡µã‡µ∫‡¥ü‡µÜ‡¥ï‡µç‡¥∏‡µç‡¥±‡µç‡¥±‡µç ‡¥∏‡¥Ç‡¥∞‡¥ï‡µç‡¥∑‡¥£‡¥§‡µç‡¥§‡µã‡¥ü‡µÜ ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£ ‡¥Æ‡¥æ‡¥®‡µá‡¥ú‡µç‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µç ‡¥∏‡¥ø‡¥∏‡µç‡¥±‡µç‡¥±‡¥ô‡µç‡¥ô‡µæ ‡¥®‡¥ø‡µº‡¥Æ‡µç‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
- ‡¥™‡µç‡¥∞‡¥ï‡¥ü‡¥® ‡¥¨‡¥û‡µç‡¥ö‡µç‡¥Æ‡¥æ‡µº‡¥ï‡µç‡¥ï‡¥ø‡¥Ç‡¥ó‡µç, ‡¥π‡µÜ‡µΩ‡¥§‡µç‡¥§‡µç ‡¥Æ‡µã‡¥£‡¥ø‡¥±‡µç‡¥±‡¥±‡¥ø‡¥Ç‡¥ó‡µç ‡¥∏‡µç‡¥•‡¥æ‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
- ‡¥∂‡¥∞‡¥ø‡¥Ø‡¥æ‡¥Ø ‡¥™‡¥ø‡¥∂‡¥ï‡µç ‡¥ï‡µà‡¥ï‡¥æ‡¥∞‡µç‡¥Ø‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡¥≤‡µã‡¥ü‡µÜ ‡¥™‡µç‡¥∞‡µä‡¥°‡¥ï‡µç‡¥∑‡µª-‡¥∏‡¥ú‡µç‡¥ú ‡¥Ü‡¥™‡µç‡¥≤‡¥ø‡¥ï‡µç‡¥ï‡µá‡¥∑‡¥®‡µÅ‡¥ï‡µæ ‡¥µ‡¥ø‡¥®‡µç‡¥Ø‡¥∏‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï

## ‡¥Æ‡µÅ‡µª‡¥ï‡µÇ‡¥ü‡µç‡¥ü‡¥ø ‡¥Ü‡¥µ‡¥∂‡µç‡¥Ø‡¥ô‡µç‡¥ô‡µæ

- ‡¥∏‡µÜ‡¥∑‡µª 1: Foundry Local ‡¥Ü‡¥∞‡¥Ç‡¥≠‡¥ø‡¥ï‡µç‡¥ï‡µΩ ‡¥™‡µÇ‡µº‡¥§‡µç‡¥§‡¥ø‡¥Ø‡¥æ‡¥ï‡µç‡¥ï‡¥ø‡¥Ø‡¥ø‡¥ü‡µç‡¥ü‡µÅ‡¥£‡µç‡¥ü‡µç
- ‡¥™‡µç‡¥∞‡¥µ‡µº‡¥§‡µç‡¥§‡¥®‡¥ï‡µç‡¥∑‡¥Æ‡¥Æ‡¥æ‡¥Ø ‡¥Æ‡µã‡¥°‡¥≤‡µÅ‡¥ï‡¥≥‡µã‡¥ü‡µÅ‡¥ï‡µÇ‡¥ü‡¥ø‡¥Ø Foundry Local ‡¥á‡µª‡¥∏‡µç‡¥±‡µç‡¥±‡¥≤‡µá‡¥∑‡µª
- Python 3.8 ‡¥Ö‡¥≤‡µç‡¥≤‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ ‡¥Ö‡¥§‡¥ø‡¥®‡µÅ‡¥∂‡µá‡¥∑‡¥Æ‡µÅ‡¥≥‡µç‡¥≥ ‡¥™‡¥§‡¥ø‡¥™‡µç‡¥™‡µç, virtual environment ‡¥∏‡¥ú‡µç‡¥ú‡µÄ‡¥ï‡¥∞‡¥£ ‡¥∂‡µá‡¥∑‡¥ø‡¥Ø‡µã‡¥ü‡µÜ
- OpenAI Python SDK ‡¥á‡µª‡¥∏‡µç‡¥±‡µç‡¥±‡¥æ‡µæ ‡¥ö‡µÜ‡¥Ø‡µç‡¥§‡¥ø‡¥ü‡µç‡¥ü‡µÅ‡¥£‡µç‡¥ü‡µç (`pip install openai foundry-local-sdk`)
- OpenAI ‡¥∏‡µá‡¥µ‡¥®‡¥Æ‡µÅ‡¥≥‡µç‡¥≥ Azure ‡¥Ö‡¥ï‡µç‡¥ï‡µó‡¥£‡µç‡¥ü‡µç (‡¥ê‡¥ö‡µç‡¥õ‡¥ø‡¥ï‡¥Ç, ‡¥ï‡µç‡¥≤‡µó‡¥°‡µç ‡¥∏‡¥æ‡¥π‡¥ö‡¥∞‡µç‡¥Ø‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç)
- Python async/await ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡¥≥‡µÅ‡¥ü‡µÜ ‡¥Ö‡¥ü‡¥ø‡¥∏‡µç‡¥•‡¥æ‡¥® ‡¥Æ‡¥®‡¥∏‡µç‡¥∏‡¥ø‡¥≤‡¥æ‡¥ï‡µç‡¥ï‡µΩ

## ‡¥≠‡¥æ‡¥ó‡¥Ç 1: OpenAI SDK ‡¥ï‡µç‡¥≤‡¥Ø‡¥®‡µç‡¥±‡µç ‡¥´‡¥æ‡¥ï‡µç‡¥ü‡¥±‡¥ø ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡µ∫

### ‡¥Æ‡µæ‡¥ü‡µç‡¥ü‡¥ø-‡¥™‡µç‡¥∞‡µä‡¥µ‡µà‡¥°‡µº ‡¥Ü‡µº‡¥ï‡µç‡¥ï‡¥ø‡¥ü‡µÜ‡¥ï‡µç‡¥ö‡µº ‡¥Æ‡¥®‡¥∏‡µç‡¥∏‡¥ø‡¥≤‡¥æ‡¥ï‡µç‡¥ï‡µΩ

Foundry Local-‡¥®‡µÅ‡¥Ç Azure OpenAI-‡¥®‡µÅ‡¥Ç ‡¥í‡¥∞‡µÅ‡¥™‡µã‡¥≤‡µÜ ‡¥™‡µç‡¥∞‡¥µ‡µº‡¥§‡µç‡¥§‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥Ü‡¥™‡µç‡¥≤‡¥ø‡¥ï‡µç‡¥ï‡µá‡¥∑‡¥®‡µÅ‡¥ï‡µæ ‡¥®‡¥ø‡µº‡¥Æ‡µç‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥´‡µç‡¥≤‡µÜ‡¥ï‡µç‡¥∏‡¥ø‡¥¨‡¥ø‡µæ ‡¥ï‡µç‡¥≤‡¥Ø‡¥®‡µç‡¥±‡µç ‡¥∏‡µÉ‡¥∑‡µç‡¥ü‡¥ø ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡µ∫ ‡¥Ü‡¥µ‡¥∂‡µç‡¥Ø‡¥Æ‡¥æ‡¥£‡µç:

```python
# sdk_integration.py - ‡¥∏‡¥æ‡¥Æ‡µç‡¥™‡¥ø‡µæ 02 ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡µ∫
import os
from openai import OpenAI
from typing import Tuple

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False


def create_azure_client() -> Tuple[OpenAI, str]:
    """Create Azure OpenAI client."""
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    azure_api_version = os.environ.get("AZURE_OPENAI_API_VERSION", "2024-08-01-preview")
    
    if not azure_endpoint or not azure_api_key:
        raise ValueError("Azure OpenAI endpoint and API key are required")
    
    model = os.environ.get("MODEL", "your-deployment-name")
    client = OpenAI(
        base_url=f"{azure_endpoint}/openai",
        api_key=azure_api_key,
        default_query={"api-version": azure_api_version},
    )
    
    print(f"üåê Azure OpenAI client created with model: {model}")
    return client, model


def create_foundry_client() -> Tuple[OpenAI, str]:
    """Create Foundry Local client with SDK management."""
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # ‡¥∂‡¥∞‡¥ø‡¥Ø‡¥æ‡¥Ø ‡¥∏‡µá‡¥µ‡¥® ‡¥Æ‡¥æ‡¥®‡µá‡¥ú‡µç‡¥Æ‡µÜ‡¥®‡µç‡¥±‡¥ø‡¥®‡¥æ‡¥Ø‡¥ø FoundryLocalManager ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            # ‡¥≤‡µã‡¥ï‡µç‡¥ï‡µΩ Foundry ‡¥∏‡µá‡¥µ‡¥®‡¥Ç ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡µª OpenAI ‡¥ï‡µç‡¥≤‡¥Ø‡¥®‡µç‡¥±‡µç ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡µº ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            
            print(f"üè† Foundry Local SDK initialized with model: {model_info.id}")
            return client, model_info.id
        except Exception as e:
            print(f"‚ö†Ô∏è Could not use Foundry SDK ({e}), falling back to manual configuration")
    
    # ‡¥Æ‡¥æ‡¥®‡µÅ‡¥µ‡µΩ ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡¥®‡¥ø‡¥≤‡µá‡¥ï‡µç‡¥ï‡µçFallback
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    
    print(f"üîß Manual configuration with model: {alias}")
    return client, alias


def initialize_client() -> Tuple[OpenAI, str, str]:
    """Initialize the appropriate OpenAI client."""
    # Azure OpenAI ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡µª ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        try:
            client, model = create_azure_client()
            return client, model, "azure"
        except Exception as e:
            print(f"‚ùå Azure OpenAI initialization failed: {e}")
            print("üîÑ Falling back to Foundry Local...")
    
    # Foundry Local ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
    client, model = create_foundry_client()
    return client, model, "foundry"
```

## ‡¥≠‡¥æ‡¥ó‡¥Ç 2: ‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÄ‡¥Æ‡¥ø‡¥Ç‡¥ó‡µç ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥Ç ‡¥±‡¥ø‡¥Ø‡µΩ-‡¥ü‡µà‡¥Ç ‡¥á‡¥ü‡¥™‡µÜ‡¥ü‡¥≤‡µÅ‡¥Ç

### ‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÄ‡¥Æ‡¥ø‡¥Ç‡¥ó‡µç ‡¥ö‡¥æ‡¥±‡µç‡¥±‡µç ‡¥™‡µÇ‡µº‡¥§‡µç‡¥§‡µÄ‡¥ï‡¥∞‡¥£‡¥ô‡µç‡¥ô‡µæ ‡¥®‡¥ü‡¥™‡µç‡¥™‡¥ø‡¥≤‡¥æ‡¥ï‡µç‡¥ï‡µΩ

‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÄ‡¥Æ‡¥ø‡¥Ç‡¥ó‡µç, ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£‡¥ô‡µç‡¥ô‡µæ ‡¥∏‡µÉ‡¥∑‡µç‡¥ü‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥Æ‡µç‡¥™‡µã‡µæ ‡¥§‡¥®‡µç‡¥®‡µÜ ‡¥ï‡¥æ‡¥£‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡¥ø‡¥®‡¥æ‡µΩ ‡¥Æ‡µÜ‡¥ö‡µç‡¥ö‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü ‡¥â‡¥™‡¥Ø‡µã‡¥ï‡µç‡¥§‡µÉ ‡¥Ö‡¥®‡µÅ‡¥≠‡¥µ‡¥Ç ‡¥®‡µΩ‡¥ï‡µÅ‡¥®‡µç‡¥®‡µÅ:

```python
# streaming_chat.py - ‡¥∏‡¥æ‡¥Æ‡µç‡¥™‡¥ø‡µæ 02 ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡µæ ‡¥™‡¥ø‡¥®‡µç‡¥§‡µÅ‡¥ü‡¥∞‡µÅ‡¥®‡µç‡¥®‡µÅ
def streaming_chat_completion(client: OpenAI, model: str, prompt: str, max_tokens: int = 300):
    """Demonstrate streaming responses for better UX."""
    try:
        print("ü§ñ Assistant (streaming):")
        
        # ‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÄ‡¥Æ‡¥ø‡¥Ç‡¥ó‡µç ‡¥™‡µÇ‡µº‡¥§‡µç‡¥§‡µÄ‡¥ï‡¥∞‡¥£‡¥Ç ‡¥∏‡µÉ‡¥∑‡µç‡¥ü‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
        stream = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            stream=True
        )
        
        full_response = ""
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_response += content
        
        print("\n")  # ‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÄ‡¥Æ‡¥ø‡¥Ç‡¥ó‡¥ø‡¥®‡µç ‡¥∂‡µá‡¥∑‡¥Ç ‡¥™‡µÅ‡¥§‡¥ø‡¥Ø ‡¥µ‡¥∞‡¥ø
        return full_response
    except Exception as e:
        error_msg = f"Error: {e}"
        print(error_msg)
        return error_msg


# ‡¥â‡¥™‡¥Ø‡µã‡¥ó ‡¥â‡¥¶‡¥æ‡¥π‡¥∞‡¥£‡¥Ç
client, model, provider = initialize_client()
prompt = "Explain the key benefits of using Microsoft Foundry Local for AI development."
response = streaming_chat_completion(client, model, prompt)
```

### ‡¥Æ‡µæ‡¥ü‡µç‡¥ü‡¥ø-‡¥ü‡µá‡µ∫ ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£ ‡¥Æ‡¥æ‡¥®‡µá‡¥ú‡µç‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µç

```python
# conversation_manager.py
class ConversationManager:
    """Manages multi-turn conversations with context preservation."""
    
    def __init__(self, client: OpenAI, model: str, system_prompt: str = None):
        self.client = client
        self.model = model
        self.messages = []
        
        if system_prompt:
            self.messages.append({"role": "system", "content": system_prompt})
    
    def send_message(self, user_message: str, max_tokens: int = 200, stream: bool = False):
        """Send a message and get response while maintaining context."""
        # ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£‡¥§‡µç‡¥§‡¥ø‡¥≤‡µá‡¥ï‡µç‡¥ï‡µç ‡¥â‡¥™‡¥Ø‡µã‡¥ï‡µç‡¥§‡µÉ ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥Ç ‡¥ö‡µá‡µº‡¥ï‡µç‡¥ï‡µÅ‡¥ï
        self.messages.append({"role": "user", "content": user_message})
        
        try:
            if stream:
                return self._stream_response(max_tokens)
            else:
                return self._regular_response(max_tokens)
        except Exception as e:
            return f"Error: {e}"
    
    def _regular_response(self, max_tokens: int):
        """Get regular (non-streaming) response."""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            max_tokens=max_tokens
        )
        
        assistant_message = response.choices[0].message.content
        self.messages.append({"role": "assistant", "content": assistant_message})
        return assistant_message
    
    def _stream_response(self, max_tokens: int):
        """Get streaming response."""
        stream = self.client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            max_tokens=max_tokens,
            stream=True
        )
        
        full_response = ""
        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_response += content
        
        print()  # ‡¥™‡µÅ‡¥§‡¥ø‡¥Ø ‡¥µ‡¥∞‡¥ø
        self.messages.append({"role": "assistant", "content": full_response})
        return full_response
    
    def get_conversation_length(self) -> int:
        """Get the number of messages in the conversation."""
        return len(self.messages)
    
    def clear_conversation(self, keep_system: bool = True):
        """Clear conversation history."""
        if keep_system and self.messages and self.messages[0]["role"] == "system":
            self.messages = [self.messages[0]]
        else:
            self.messages = []


# ‡¥â‡¥¶‡¥æ‡¥π‡¥∞‡¥£ ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥Ç
client, model, provider = initialize_client()
system_prompt = "You are a helpful AI assistant specialized in explaining AI and machine learning concepts."
conversation = ConversationManager(client, model, system_prompt)

# ‡¥¨‡¥π‡µÅ-‡¥§‡¥µ‡¥£ ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£‡¥Ç
conversation_turns = [
    "What is the difference between AI inference on-device vs in the cloud?",
    "Which approach is better for privacy?",
    "What about performance and latency considerations?"
]

for i, turn in enumerate(conversation_turns, 1):
    print(f"\nTurn {i}: {turn}")
    response = conversation.send_message(turn, stream=True)
```

## ‡¥≠‡¥æ‡¥ó‡¥Ç 3: ‡¥™‡µç‡¥∞‡¥ï‡¥ü‡¥® ‡¥¨‡¥û‡µç‡¥ö‡µç‡¥Æ‡¥æ‡µº‡¥ï‡µç‡¥ï‡¥ø‡¥Ç‡¥ó‡µç, ‡¥µ‡¥ø‡¥∂‡¥ï‡¥≤‡¥®‡¥Ç

### ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£ ‡¥∏‡¥Æ‡¥Ø‡¥Ç ‡¥Ö‡¥≥‡¥ï‡µç‡¥ï‡µΩ

‡¥µ‡¥ø‡¥µ‡¥ø‡¥ß ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡¥®‡µÅ‡¥ï‡µæ‡¥ï‡µç‡¥ï‡¥ø‡¥ü‡¥Ø‡¥ø‡µΩ ‡¥™‡µç‡¥∞‡¥ï‡¥ü‡¥®‡¥Ç ‡¥Ö‡¥≥‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡¥Ø‡µÅ‡¥Ç ‡¥§‡¥æ‡¥∞‡¥§‡¥Æ‡µç‡¥Ø‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï‡¥Ø‡µÅ‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï:

```python
# performance_benchmark.py - ‡¥∏‡¥æ‡¥Æ‡µç‡¥™‡¥ø‡µæ 02 ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡µæ
import time
from typing import Dict, List
from openai import OpenAI

def benchmark_response_time(client: OpenAI, model: str, prompt: str, iterations: int = 3) -> Dict:
    """Benchmark response time for a given prompt."""
    times = []
    responses = []
    
    for i in range(iterations):
        start_time = time.time()
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=50  # ‡¥∏‡¥Æ‡¥Ø‡¥Æ‡µÜ‡¥ü‡µÅ‡¥ï‡µç‡¥ï‡¥≤‡¥ø‡¥®‡¥æ‡¥Ø‡¥ø ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£‡¥ô‡µç‡¥ô‡µæ ‡¥ö‡µÅ‡¥∞‡µÅ‡¥ï‡µç‡¥ï‡¥Ç ‡¥µ‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µÅ‡¥ï
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            times.append(response_time)
            responses.append(response.choices[0].message.content)
            
        except Exception as e:
            print(f"Error in iteration {i+1}: {e}")
    
    if times:
        return {
            "average_time": sum(times) / len(times),
            "min_time": min(times),
            "max_time": max(times),
            "all_times": times,
            "sample_response": responses[0] if responses else None,
            "success_rate": len(times) / iterations * 100
        }
    
    return {"error": "No successful responses"}


def compare_providers(foundry_client: OpenAI, foundry_model: str, 
                     azure_client: OpenAI, azure_model: str, test_prompts: List[str]):
    """Compare performance between Foundry Local and Azure OpenAI."""
    results = {
        "foundry_local": [],
        "azure_openai": []
    }
    
    for prompt in test_prompts:
        print(f"\nTesting prompt: '{prompt}'")
        
        # ‡¥´‡µó‡¥£‡µç‡¥ü‡µç‡¥∞‡¥ø ‡¥≤‡µã‡¥ï‡µç‡¥ï‡µΩ ‡¥™‡¥∞‡µÄ‡¥ï‡µç‡¥∑‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
        foundry_result = benchmark_response_time(foundry_client, foundry_model, prompt)
        results["foundry_local"].append({
            "prompt": prompt,
            "benchmark": foundry_result
        })
        
        # ‡¥Ö‡¥∏‡µç‡¥Ø‡µÇ‡µº ‡¥ì‡¥™‡µç‡¥™‡µ∫‡¥é‡¥ê ‡¥™‡¥∞‡µÄ‡¥ï‡µç‡¥∑‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
        azure_result = benchmark_response_time(azure_client, azure_model, prompt)
        results["azure_openai"].append({
            "prompt": prompt,
            "benchmark": azure_result
        })
        
        # ‡¥´‡¥≤‡¥ô‡µç‡¥ô‡µæ ‡¥§‡¥æ‡¥∞‡¥§‡¥Æ‡µç‡¥Ø‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï
        if "error" not in foundry_result and "error" not in azure_result:
            foundry_time = foundry_result["average_time"]
            azure_time = azure_result["average_time"]
            
            print(f"  Foundry Local: {foundry_time:.2f}s")
            print(f"  Azure OpenAI: {azure_time:.2f}s")
            print(f"  Winner: {'Foundry Local' if foundry_time < azure_time else 'Azure OpenAI'}")
    
    return results


# ‡¥â‡¥¶‡¥æ‡¥π‡¥∞‡¥£ ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥Ç
benchmark_prompts = [
    "What is AI?",
    "Explain machine learning in simple terms.",
    "List 3 benefits of edge computing."
]

# ‡¥ï‡µç‡¥≤‡¥Ø‡¥®‡µç‡¥±‡µÅ‡¥ï‡µæ ‡¥Ü‡¥∞‡¥Ç‡¥≠‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
foundry_client, foundry_model, _ = initialize_client()
# azure_client, azure_model = create_azure_client()  # ‡¥Ö‡¥∏‡µç‡¥Ø‡µÇ‡µº ‡¥ï‡µç‡¥∞‡¥Æ‡µÄ‡¥ï‡¥∞‡¥ø‡¥ö‡µç‡¥ö‡¥ø‡¥ü‡µç‡¥ü‡µÅ‡¥£‡µç‡¥ü‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ ‡¥Ö‡µ∫‡¥ï‡¥Æ‡µç‡¥Æ‡¥®‡µç‡¥±‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï

for prompt in benchmark_prompts:
    print(f"\nüìù Benchmarking: '{prompt}'")
    result = benchmark_response_time(foundry_client, foundry_model, prompt)
    
    if "error" not in result:
        print(f"   ‚è∞ Average time: {result['average_time']:.2f}s")
        print(f"   ‚ö° Fastest: {result['min_time']:.2f}s")
        print(f"   üêå Slowest: {result['max_time']:.2f}s")
        print(f"   ‚úÖ Success rate: {result['success_rate']:.1f}%")
```

### ‡¥§‡¥æ‡¥™‡¥®‡¥ø‡¥≤‡¥Ø‡µÅ‡¥Ç ‡¥™‡¥æ‡¥∞‡¥æ‡¥Æ‡µÄ‡¥±‡µç‡¥±‡µº ‡¥ü‡µÜ‡¥∏‡µç‡¥±‡µç‡¥±‡¥ø‡¥Ç‡¥ó‡µÅ‡¥Ç

```python
# parameter_testing.py
def test_temperature_effects(client: OpenAI, model: str, prompt: str):
    """Test how different temperature values affect responses."""
    temperatures = [0.1, 0.5, 0.9]
    
    print(f"Testing prompt: '{prompt}'")
    print("=" * 60)
    
    for temp in temperatures:
        print(f"\nüå°Ô∏è Temperature: {temp}")
        print("-" * 30)
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=100,
                temperature=temp
            )
            
            print(f"Response: {response.choices[0].message.content[:150]}...")
            
        except Exception as e:
            print(f"Error with temperature {temp}: {e}")


# ‡¥∏‡µÉ‡¥∑‡µç‡¥ü‡¥ø‡¥™‡¥∞‡¥µ‡µÅ‡¥Ç ‡¥µ‡¥ø‡¥∂‡¥ï‡¥≤‡¥®‡¥™‡¥∞‡¥µ‡µÅ‡¥Æ‡¥æ‡¥Ø ‡¥™‡µç‡¥∞‡µã‡¥Ç‡¥™‡µç‡¥±‡µç‡¥±‡µÅ‡¥ï‡µæ ‡¥™‡¥∞‡µÄ‡¥ï‡µç‡¥∑‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
creative_prompt = "Write a creative short story about AI."
analytical_prompt = "Explain the technical differences between GPT and BERT models."

test_temperature_effects(foundry_client, foundry_model, creative_prompt)
test_temperature_effects(foundry_client, foundry_model, analytical_prompt)
```

## ‡¥≠‡¥æ‡¥ó‡¥Ç 4: ‡¥∏‡µá‡¥µ‡¥® ‡¥Ü‡¥∞‡µã‡¥ó‡µç‡¥Ø ‡¥®‡¥ø‡¥∞‡µÄ‡¥ï‡µç‡¥∑‡¥£‡¥µ‡µÅ‡¥Ç ‡¥°‡¥Ø‡¥ó‡µç‡¥®‡µã‡¥∏‡µç‡¥±‡µç‡¥±‡¥ø‡¥ï‡µç‡¥∏‡µÅ‡¥Ç

### ‡¥∏‡¥Æ‡¥ó‡µç‡¥∞‡¥Æ‡¥æ‡¥Ø ‡¥π‡µÜ‡µΩ‡¥§‡µç‡¥§‡µç ‡¥ö‡µÜ‡¥ï‡µç‡¥ï‡µç ‡¥∏‡¥ø‡¥∏‡µç‡¥±‡µç‡¥±‡¥Ç

```python
# health_monitoring.py - ‡¥∏‡¥æ‡¥Æ‡µç‡¥™‡¥ø‡µæ 02 ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡µæ
def comprehensive_health_check(client: OpenAI, model: str, provider: str) -> Dict:
    """Perform comprehensive health check of the AI service."""
    print("üè• Comprehensive Health Check")
    print("=" * 50)
    
    health_results = {
        "provider": provider,
        "model": model,
        "timestamp": time.time(),
        "tests": {}
    }
    
    # ‡¥ü‡µÜ‡¥∏‡µç‡¥±‡µç‡¥±‡µç 1: ‡¥Æ‡µã‡¥°‡µΩ ‡¥≤‡¥ø‡¥∏‡µç‡¥±‡µç‡¥±‡¥ø‡¥Ç‡¥ó‡µç
    try:
        models_response = client.models.list()
        available_models = [m.id for m in models_response.data]
        health_results["tests"]["model_listing"] = {
            "status": "success",
            "available_models": available_models,
            "current_model_available": model in available_models
        }
        print(f"‚úÖ Model listing: SUCCESS ({len(available_models)} models)")
    except Exception as e:
        health_results["tests"]["model_listing"] = {
            "status": "failed",
            "error": str(e)
        }
        print(f"‚ùå Model listing: FAILED - {e}")
    
    # ‡¥ü‡µÜ‡¥∏‡µç‡¥±‡µç‡¥±‡µç 2: ‡¥Ö‡¥ü‡¥ø‡¥∏‡µç‡¥•‡¥æ‡¥® ‡¥™‡µÇ‡µº‡¥§‡µç‡¥§‡µÄ‡¥ï‡¥∞‡¥£‡¥Ç
    try:
        start_time = time.time()
        test_response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": "Say 'Health check successful'"}],
            max_tokens=10
        )
        response_time = time.time() - start_time
        
        health_results["tests"]["basic_completion"] = {
            "status": "success",
            "response_time": response_time,
            "response": test_response.choices[0].message.content
        }
        print(f"‚úÖ Basic completion: SUCCESS ({response_time:.2f}s)")
    except Exception as e:
        health_results["tests"]["basic_completion"] = {
            "status": "failed",
            "error": str(e)
        }
        print(f"‚ùå Basic completion: FAILED - {e}")
    
    # ‡¥ü‡µÜ‡¥∏‡µç‡¥±‡µç‡¥±‡µç 3: ‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÄ‡¥Æ‡¥ø‡¥Ç‡¥ó‡µç
    try:
        start_time = time.time()
        stream = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": "Count to 3"}],
            max_tokens=20,
            stream=True
        )
        
        stream_content = ""
        chunk_count = 0
        for chunk in stream:
            if chunk.choices[0].delta.content:
                stream_content += chunk.choices[0].delta.content
                chunk_count += 1
        
        streaming_time = time.time() - start_time
        
        health_results["tests"]["streaming"] = {
            "status": "success",
            "response_time": streaming_time,
            "chunks_received": chunk_count,
            "content": stream_content.strip()
        }
        print(f"‚úÖ Streaming: SUCCESS ({streaming_time:.2f}s, {chunk_count} chunks)")
    except Exception as e:
        health_results["tests"]["streaming"] = {
            "status": "failed",
            "error": str(e)
        }
        print(f"‚ùå Streaming: FAILED - {e}")
    
    # ‡¥Æ‡µä‡¥§‡µç‡¥§‡¥Ç ‡¥Ü‡¥∞‡µã‡¥ó‡µç‡¥Ø ‡¥∏‡µç‡¥ï‡µã‡µº
    successful_tests = sum(1 for test in health_results["tests"].values() if test["status"] == "success")
    total_tests = len(health_results["tests"])
    health_score = (successful_tests / total_tests) * 100
    
    health_results["overall_health"] = {
        "score": health_score,
        "successful_tests": successful_tests,
        "total_tests": total_tests,
        "status": "healthy" if health_score >= 70 else "degraded" if health_score >= 30 else "unhealthy"
    }
    
    print(f"\nüìä Overall Health: {health_score:.1f}% ({health_results['overall_health']['status'].upper()})")
    
    return health_results


# ‡¥â‡¥™‡¥Ø‡µã‡¥ó ‡¥â‡¥¶‡¥æ‡¥π‡¥∞‡¥£‡¥Ç
client, model, provider = initialize_client()
health_status = comprehensive_health_check(client, model, provider)
```

### ‡¥™‡¥∞‡¥ø‡¥∏‡µç‡¥•‡¥ø‡¥§‡¥ø ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡µª ‡¥µ‡¥æ‡¥≤‡¥ø‡¥°‡µá‡¥±‡µç‡¥±‡µº

```python
# config_validator.py
def validate_environment_configuration() -> Dict:
    """Validate environment configuration for both providers."""
    validation_results = {
        "foundry_local": {},
        "azure_openai": {},
        "recommendations": []
    }
    
    # ‡¥´‡µó‡¥£‡µç‡¥ü‡µç‡¥∞‡¥ø ‡¥≤‡µã‡¥ï‡µç‡¥ï‡µΩ ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡µª ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
    foundry_sdk_available = FOUNDRY_SDK_AVAILABLE
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    
    validation_results["foundry_local"] = {
        "sdk_available": foundry_sdk_available,
        "base_url": base_url,
        "model": os.environ.get("MODEL", "phi-4-mini"),
        "api_key": bool(os.environ.get("API_KEY"))
    }
    
    if not foundry_sdk_available:
        validation_results["recommendations"].append(
            "Install Foundry Local SDK: pip install foundry-local-sdk"
        )
    
    # ‡¥Ö‡¥∏‡µç‡¥Ø‡µÇ‡µº ‡¥ì‡¥™‡µç‡¥™‡µ∫‡¥é‡¥ê ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡µª ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    azure_api_version = os.environ.get("AZURE_OPENAI_API_VERSION")
    
    validation_results["azure_openai"] = {
        "endpoint_configured": bool(azure_endpoint),
        "api_key_configured": bool(azure_api_key),
        "api_version": azure_api_version or "2024-08-01-preview",
        "model": os.environ.get("MODEL", "your-deployment-name")
    }
    
    if azure_endpoint and not azure_api_key:
        validation_results["recommendations"].append(
            "Azure endpoint is set but API key is missing"
        )
    
    # ‡¥Æ‡µä‡¥§‡µç‡¥§‡¥Ç ‡¥µ‡¥ø‡¥≤‡¥Ø‡¥ø‡¥∞‡µÅ‡¥§‡µç‡¥§‡µΩ
    can_use_foundry = foundry_sdk_available or base_url
    can_use_azure = azure_endpoint and azure_api_key
    
    if not can_use_foundry and not can_use_azure:
        validation_results["recommendations"].append(
            "No valid configuration found. Set up either Foundry Local or Azure OpenAI."
        )
    
    validation_results["summary"] = {
        "foundry_ready": can_use_foundry,
        "azure_ready": can_use_azure,
        "total_options": sum([can_use_foundry, can_use_azure])
    }
    
    return validation_results


# ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡µª ‡¥®‡¥ø‡¥≤ ‡¥™‡µç‡¥∞‡¥¶‡µº‡¥∂‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
config_status = validate_environment_configuration()
print("‚öôÔ∏è Environment Configuration Status")
print("=" * 40)
print(f"üè† Foundry Local Ready: {'‚úÖ' if config_status['summary']['foundry_ready'] else '‚ùå'}")
print(f"üåê Azure OpenAI Ready: {'‚úÖ' if config_status['summary']['azure_ready'] else '‚ùå'}")
print(f"üìã Available Options: {config_status['summary']['total_options']}")

if config_status["recommendations"]:
    print("\nüí° Recommendations:")
    for rec in config_status["recommendations"]:
        print(f"   ‚Ä¢ {rec}")
```

## ‡¥≠‡¥æ‡¥ó‡¥Ç 5: ‡¥™‡¥∞‡¥ø‡¥∏‡µç‡¥•‡¥ø‡¥§‡¥ø ‡¥µ‡µá‡¥∞‡¥ø‡¥Ø‡¥¨‡¥ø‡¥≥‡µÅ‡¥ï‡¥≥‡µÅ‡¥Ç ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡µª ‡¥Æ‡¥æ‡¥®‡µá‡¥ú‡µç‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µÅ‡¥Ç

### ‡¥™‡¥∞‡¥ø‡¥∏‡µç‡¥•‡¥ø‡¥§‡¥ø ‡¥µ‡µá‡¥∞‡¥ø‡¥Ø‡¥¨‡¥ø‡µæ ‡¥±‡¥´‡¥±‡µª‡¥∏‡µç

‡¥∞‡¥£‡µç‡¥ü‡µÅ ‡¥™‡µç‡¥∞‡µä‡¥µ‡µà‡¥°‡µº‡¥Æ‡¥æ‡¥∞‡µÅ‡¥ü‡µÜ‡¥Ø‡µÅ‡¥Ç ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡¥®‡µÅ‡¥ï‡µæ‡¥ï‡µç‡¥ï‡µÅ‡¥≥‡µç‡¥≥ ‡¥™‡µÇ‡µº‡¥£‡µç‡¥£ ‡¥±‡¥´‡¥±‡µª‡¥∏‡µç:

```python
# config_reference.py - ‡¥∏‡¥æ‡¥Æ‡µç‡¥™‡¥ø‡µæ 02 ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡µæ
import os
from typing import Dict, Optional

class ConfigurationManager:
    """Manages environment configuration for multi-provider setup."""
    
    @staticmethod
    def get_foundry_config() -> Dict[str, Optional[str]]:
        """Get Foundry Local configuration from environment."""
        return {
            "MODEL": os.environ.get("MODEL", "phi-4-mini"),
            "BASE_URL": os.environ.get("BASE_URL", "http://localhost:8000"),
            "API_KEY": os.environ.get("API_KEY", ""),
        }
    
    @staticmethod
    def get_azure_config() -> Dict[str, Optional[str]]:
        """Get Azure OpenAI configuration from environment."""
        return {
            "AZURE_OPENAI_ENDPOINT": os.environ.get("AZURE_OPENAI_ENDPOINT"),
            "AZURE_OPENAI_API_KEY": os.environ.get("AZURE_OPENAI_API_KEY"),
            "AZURE_OPENAI_API_VERSION": os.environ.get("AZURE_OPENAI_API_VERSION", "2024-08-01-preview"),
            "MODEL": os.environ.get("MODEL", "your-deployment-name"),
        }
    
    @staticmethod
    def display_current_config():
        """Display current configuration status."""
        print("‚öôÔ∏è Current Configuration")
        print("=" * 40)
        
        foundry_config = ConfigurationManager.get_foundry_config()
        azure_config = ConfigurationManager.get_azure_config()
        
        print("üè† Foundry Local:")
        for key, value in foundry_config.items():
            display_value = value if value else "(not set)"
            if key == "API_KEY" and value:
                display_value = "***" + value[-4:] if len(value) > 4 else "***"
            print(f"   {key}: {display_value}")
        
        print("\nüåê Azure OpenAI:")
        for key, value in azure_config.items():
            display_value = value if value else "(not set)"
            if "KEY" in key and value:
                display_value = "***" + value[-4:] if len(value) > 4 else "***"
            print(f"   {key}: {display_value}")
        
        # ‡¥∏‡¥ú‡µÄ‡¥µ ‡¥™‡µç‡¥∞‡µä‡¥µ‡µà‡¥°‡µº ‡¥®‡¥ø‡µº‡¥£‡µç‡¥£‡¥Ø‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
        azure_ready = azure_config["AZURE_OPENAI_ENDPOINT"] and azure_config["AZURE_OPENAI_API_KEY"]
        foundry_ready = True  # ‡¥´‡µó‡¥£‡µç‡¥ü‡¥±‡¥ø ‡¥é‡¥™‡µç‡¥™‡µã‡¥¥‡µÅ‡¥Ç ‡¥°‡¥ø‡¥´‡µã‡µæ‡¥ü‡µç‡¥ü‡µÅ‡¥ï‡¥≥‡¥ø‡¥≤‡µá‡¥ï‡µç‡¥ï‡µç ‡¥Æ‡¥ü‡¥ô‡µç‡¥ô‡¥æ‡¥Ç
        
        print(f"\nüìä Provider Status:")
        print(f"   Azure OpenAI: {'‚úÖ Ready' if azure_ready else '‚ùå Not configured'}")
        print(f"   Foundry Local: {'‚úÖ Ready' if foundry_ready else '‚ùå Not available'}")
        print(f"   Active: {'Azure OpenAI' if azure_ready else 'Foundry Local'}")


# ‡¥®‡¥ø‡¥≤‡¥µ‡¥ø‡¥≤‡µÜ ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡µª ‡¥™‡µç‡¥∞‡¥¶‡µº‡¥∂‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
config_manager = ConfigurationManager()
config_manager.display_current_config()
```

### ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡µª ‡¥â‡¥¶‡¥æ‡¥π‡¥∞‡¥£‡¥ô‡µç‡¥ô‡µæ

**Windows ‡¥ï‡¥Æ‡¥æ‡µª‡¥°‡µç ‡¥™‡µç‡¥∞‡µã‡¥Ç‡¥™‡µç‡¥±‡µç‡¥±‡µç ‡¥∏‡¥ú‡µç‡¥ú‡µÄ‡¥ï‡¥∞‡¥£‡¥Ç:**

```cmd
REM Foundry Local configuration
set MODEL=phi-4-mini
set BASE_URL=http://localhost:8000
set API_KEY=

REM Azure OpenAI configuration (alternative)
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key-here
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name

REM Run the sample
python samples\02\sdk_quickstart.py
```

**PowerShell ‡¥∏‡¥ú‡µç‡¥ú‡µÄ‡¥ï‡¥∞‡¥£‡¥Ç:**

```powershell
# ‡¥´‡µó‡¥£‡µç‡¥ü‡µç‡¥∞‡¥ø ‡¥≤‡µã‡¥ï‡µç‡¥ï‡µΩ ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡µª
$env:MODEL = "phi-4-mini"
$env:BASE_URL = "http://localhost:8000"
$env:API_KEY = ""

# ‡¥Ö‡¥∏‡µç‡¥Ø‡µÇ‡µº ‡¥ì‡¥™‡µç‡¥™‡µ∫‡¥é‡¥ê ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡µª (‡¥µ‡µà‡¥ï‡µΩ‡¥™‡µç‡¥™‡¥ø‡¥ï‡¥Ç)
$env:AZURE_OPENAI_ENDPOINT = "https://your-resource.openai.azure.com"
$env:AZURE_OPENAI_API_KEY = "your-api-key-here"
$env:AZURE_OPENAI_API_VERSION = "2024-08-01-preview"
$env:MODEL = "your-deployment-name"

# ‡¥∏‡¥æ‡¥Æ‡µç‡¥™‡¥ø‡µæ ‡¥™‡µç‡¥∞‡¥µ‡µº‡¥§‡µç‡¥§‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
python samples/02/sdk_quickstart.py
```

## ‡¥≠‡¥æ‡¥ó‡¥Ç 6: ‡¥π‡¥æ‡µª‡¥°‡µç‚Äå‡¥∏‡µç-‡¥ì‡µ∫ ‡¥µ‡µç‡¥Ø‡¥æ‡¥Ø‡¥æ‡¥Æ‡¥ô‡µç‡¥ô‡µæ

### ‡¥µ‡µç‡¥Ø‡¥æ‡¥Ø‡¥æ‡¥Æ‡¥Ç 1: ‡¥Æ‡µæ‡¥ü‡µç‡¥ü‡¥ø-‡¥™‡µç‡¥∞‡µä‡¥µ‡µà‡¥°‡µº SDK ‡¥á‡¥®‡µç‡¥±‡¥ó‡µç‡¥∞‡µá‡¥∑‡µª

‡¥™‡µç‡¥∞‡µä‡¥µ‡µà‡¥°‡µº‡¥Æ‡¥æ‡¥∞‡¥ø‡µΩ ‡¥∏‡µÅ‡¥§‡¥æ‡¥∞‡µç‡¥Ø‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥Æ‡¥æ‡¥±‡µÅ‡¥®‡µç‡¥® ‡¥™‡µÇ‡µº‡¥£‡µç‡¥£ ‡¥Ü‡¥™‡µç‡¥≤‡¥ø‡¥ï‡µç‡¥ï‡µá‡¥∑‡µª ‡¥®‡¥ø‡µº‡¥Æ‡µç‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï:

```python
# exercise_1_multi_provider.py
from openai import OpenAI
from typing import Tuple, Dict, Any
import time

class MultiProviderSDKDemo:
    """Demonstrates seamless switching between Foundry Local and Azure OpenAI."""
    
    def __init__(self):
        self.clients = {}
        self.models = {}
        self.setup_clients()
    
    def setup_clients(self):
        """Initialize all available clients."""
        # ‡¥´‡µó‡¥£‡µç‡¥ü‡µç‡¥∞‡¥ø ‡¥≤‡µã‡¥ï‡µç‡¥ï‡µΩ ‡¥Ü‡¥∞‡¥Ç‡¥≠‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥∂‡µç‡¥∞‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
        try:
            foundry_client, foundry_model, _ = initialize_client()
            self.clients["foundry"] = foundry_client
            self.models["foundry"] = foundry_model
            print("‚úÖ Foundry Local client ready")
        except Exception as e:
            print(f"‚ùå Foundry Local setup failed: {e}")
        
        # ‡¥Ö‡¥∏‡µç‡¥Ø‡µÇ‡µº ‡¥ì‡¥™‡µç‡¥™‡µ∫‡¥é‡¥ê ‡¥Ü‡¥∞‡¥Ç‡¥≠‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥∂‡µç‡¥∞‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
        try:
            if os.environ.get("AZURE_OPENAI_ENDPOINT") and os.environ.get("AZURE_OPENAI_API_KEY"):
                azure_client, azure_model = create_azure_client()
                self.clients["azure"] = azure_client
                self.models["azure"] = azure_model
                print("‚úÖ Azure OpenAI client ready")
        except Exception as e:
            print(f"‚ùå Azure OpenAI setup failed: {e}")
    
    def compare_providers(self, prompt: str, max_tokens: int = 100) -> Dict[str, Any]:
        """Compare responses from all available providers."""
        results = {}
        
        for provider_name, client in self.clients.items():
            model = self.models[provider_name]
            print(f"\nTesting {provider_name} ({model})...")
            
            start_time = time.time()
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens
                )
                
                response_time = time.time() - start_time
                
                results[provider_name] = {
                    "model": model,
                    "response": response.choices[0].message.content,
                    "response_time": response_time,
                    "status": "success"
                }
                
                print(f"   ‚úÖ Success ({response_time:.2f}s)")
                
            except Exception as e:
                results[provider_name] = {
                    "model": model,
                    "error": str(e),
                    "status": "failed"
                }
                print(f"   ‚ùå Failed: {e}")
        
        return results
    
    def streaming_comparison(self, prompt: str, max_tokens: int = 150):
        """Compare streaming responses from providers."""
        for provider_name, client in self.clients.items():
            model = self.models[provider_name]
            print(f"\nüåä Streaming from {provider_name} ({model}):")
            print("-" * 50)
            
            try:
                stream = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    stream=True
                )
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        print(chunk.choices[0].delta.content, end="", flush=True)
                
                print("\n")
                
            except Exception as e:
                print(f"Streaming failed: {e}")


# ‡¥µ‡µç‡¥Ø‡¥æ‡¥Ø‡¥æ‡¥Æ‡¥Ç 1 ‡¥™‡µç‡¥∞‡¥µ‡µº‡¥§‡µç‡¥§‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
exercise_1 = MultiProviderSDKDemo()

test_prompt = "Explain the benefits of running AI models locally versus in the cloud."
print(f"üó∫Ô∏è Exercise 1: Multi-Provider Comparison")
print(f"Prompt: {test_prompt}")
print("=" * 60)

comparison_results = exercise_1.compare_providers(test_prompt)
exercise_1.streaming_comparison(test_prompt)
```

### ‡¥µ‡µç‡¥Ø‡¥æ‡¥Ø‡¥æ‡¥Æ‡¥Ç 2: ‡¥Ü‡¥ß‡µÅ‡¥®‡¥ø‡¥ï ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£ ‡¥Æ‡¥æ‡¥®‡µá‡¥ú‡µç‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µç

```python
# exercise_2_conversation.py
class AdvancedConversationManager:
    """Advanced conversation management with multiple features."""
    
    def __init__(self, client: OpenAI, model: str):
        self.client = client
        self.model = model
        self.conversations = {}  # ‡¥¨‡¥π‡µÅ‡¥≠‡µÇ‡¥∞‡¥ø‡¥≠‡¥æ‡¥ó‡¥Ç ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£ ‡¥∏‡µÜ‡¥∑‡¥®‡µÅ‡¥ï‡µæ
    
    def create_conversation(self, session_id: str, system_prompt: str = None) -> str:
        """Create a new conversation session."""
        self.conversations[session_id] = {
            "messages": [],
            "created_at": time.time(),
            "message_count": 0
        }
        
        if system_prompt:
            self.conversations[session_id]["messages"].append({
                "role": "system", 
                "content": system_prompt
            })
        
        return f"Conversation {session_id} created"
    
    def send_message(self, session_id: str, message: str, 
                    temperature: float = 0.7, max_tokens: int = 200) -> Dict[str, Any]:
        """Send message in a specific conversation session."""
        if session_id not in self.conversations:
            return {"error": f"Conversation {session_id} not found"}
        
        conversation = self.conversations[session_id]
        conversation["messages"].append({"role": "user", "content": message})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=conversation["messages"],
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            assistant_message = response.choices[0].message.content
            conversation["messages"].append({
                "role": "assistant", 
                "content": assistant_message
            })
            conversation["message_count"] += 2  # ‡¥â‡¥™‡¥Ø‡µã‡¥ï‡µç‡¥§‡¥æ‡¥µ‡µç + ‡¥Ö‡¥∏‡¥ø‡¥∏‡µç‡¥±‡µç‡¥±‡¥®‡µç‡¥±‡µç
            
            return {
                "session_id": session_id,
                "response": assistant_message,
                "message_count": conversation["message_count"],
                "status": "success"
            }
            
        except Exception as e:
            return {"error": str(e), "session_id": session_id}
    
    def get_conversation_summary(self, session_id: str) -> Dict[str, Any]:
        """Get summary of conversation session."""
        if session_id not in self.conversations:
            return {"error": f"Conversation {session_id} not found"}
        
        conversation = self.conversations[session_id]
        
        return {
            "session_id": session_id,
            "message_count": conversation["message_count"],
            "created_at": conversation["created_at"],
            "duration": time.time() - conversation["created_at"],
            "has_system_prompt": len(conversation["messages"]) > 0 and 
                               conversation["messages"][0]["role"] == "system"
        }
    
    def export_conversation(self, session_id: str) -> str:
        """Export conversation as formatted text."""
        if session_id not in self.conversations:
            return f"Conversation {session_id} not found"
        
        conversation = self.conversations[session_id]
        export_text = f"Conversation Export: {session_id}\n"
        export_text += "=" * 50 + "\n\n"
        
        for msg in conversation["messages"]:
            role = msg["role"].title()
            content = msg["content"]
            export_text += f"{role}: {content}\n\n"
        
        return export_text


# ‡¥µ‡µç‡¥Ø‡¥æ‡¥Ø‡¥æ‡¥Æ‡¥Ç 2 ‡¥™‡µç‡¥∞‡¥µ‡µº‡¥§‡µç‡¥§‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
client, model, provider = initialize_client()
conv_manager = AdvancedConversationManager(client, model)

# ‡¥¨‡¥π‡µÅ‡¥≠‡µÇ‡¥∞‡¥ø‡¥≠‡¥æ‡¥ó‡¥Ç ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£ ‡¥∏‡µÜ‡¥∑‡¥®‡µÅ‡¥ï‡µæ ‡¥∏‡µÉ‡¥∑‡µç‡¥ü‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
print("üí¨ Exercise 2: Advanced Conversation Management")
print("=" * 60)

# ‡¥∏‡¥æ‡¥ô‡µç‡¥ï‡µá‡¥§‡¥ø‡¥ï ‡¥ö‡µº‡¥ö‡µç‡¥ö
conv_manager.create_conversation("tech_discussion", 
    "You are a technical expert explaining AI concepts clearly.")

# ‡¥∏‡µÉ‡¥∑‡µç‡¥ü‡¥ø‡¥™‡¥∞‡¥Æ‡¥æ‡¥Ø ‡¥∏‡µÜ‡¥∑‡µª
conv_manager.create_conversation("creative_session", 
    "You are a creative writing assistant helping with storytelling.")

# ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£‡¥ô‡µç‡¥ô‡µæ ‡¥™‡¥∞‡µÄ‡¥ï‡µç‡¥∑‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
tech_questions = [
    "What is the difference between inference and training?",
    "How does quantization improve model performance?"
]

creative_prompts = [
    "Start a story about an AI that lives on an edge device.",
    "Continue the story with a plot twist."
]

# ‡¥∏‡¥æ‡¥ô‡µç‡¥ï‡µá‡¥§‡¥ø‡¥ï ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£‡¥Ç
print("\nüîß Technical Discussion:")
for question in tech_questions:
    result = conv_manager.send_message("tech_discussion", question)
    print(f"Q: {question}")
    print(f"A: {result['response'][:100]}...\n")

# ‡¥∏‡µÉ‡¥∑‡µç‡¥ü‡¥ø‡¥™‡¥∞‡¥Æ‡¥æ‡¥Ø ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£‡¥Ç
print("üé® Creative Session:")
for prompt in creative_prompts:
    result = conv_manager.send_message("creative_session", prompt, temperature=0.9)
    print(f"Prompt: {prompt}")
    print(f"Response: {result['response'][:100]}...\n")

# ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£ ‡¥∏‡¥Ç‡¥ó‡µç‡¥∞‡¥π‡¥ô‡µç‡¥ô‡µæ ‡¥ï‡¥æ‡¥£‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
print("üìä Conversation Summaries:")
for session_id in conv_manager.conversations.keys():
    summary = conv_manager.get_conversation_summary(session_id)
    print(f"   {session_id}: {summary['message_count']} messages, {summary['duration']:.1f}s")
```

### ‡¥µ‡µç‡¥Ø‡¥æ‡¥Ø‡¥æ‡¥Æ‡¥Ç 3: ‡¥™‡µç‡¥∞‡µä‡¥°‡¥ï‡µç‡¥∑‡µª-‡¥∏‡¥ú‡µç‡¥ú ‡¥π‡µÜ‡µΩ‡¥§‡µç‡¥§‡µç ‡¥Æ‡µã‡¥£‡¥ø‡¥±‡µç‡¥±‡¥±‡¥ø‡¥Ç‡¥ó‡µç

```python
# exercise_3_monitoring.py
class ProductionHealthMonitor:
    """Production-ready health monitoring for AI services."""
    
    def __init__(self):
        self.health_history = []
        self.alert_thresholds = {
            "response_time": 5.0,
            "error_rate": 10.0,
            "availability": 95.0
        }
    
    def run_comprehensive_check(self, client: OpenAI, model: str, provider: str) -> Dict[str, Any]:
        """Run comprehensive health check with detailed reporting."""
        check_results = {
            "timestamp": time.time(),
            "provider": provider,
            "model": model,
            "tests": {},
            "overall_health": "unknown"
        }
        
        # ‡¥ü‡µÜ‡¥∏‡µç‡¥±‡µç‡¥±‡µç 1: ‡¥Ö‡¥ü‡¥ø‡¥∏‡µç‡¥•‡¥æ‡¥® ‡¥ï‡¥£‡¥ï‡µç‡¥ü‡¥ø‡¥µ‡¥ø‡¥±‡µç‡¥±‡¥ø
        connectivity_result = self._test_connectivity(client)
        check_results["tests"]["connectivity"] = connectivity_result
        
        # ‡¥ü‡µÜ‡¥∏‡µç‡¥±‡µç‡¥±‡µç 2: ‡¥Æ‡µã‡¥°‡µΩ ‡¥≤‡¥≠‡µç‡¥Ø‡¥§
        model_result = self._test_model_availability(client, model)
        check_results["tests"]["model_availability"] = model_result
        
        # ‡¥ü‡µÜ‡¥∏‡µç‡¥±‡µç‡¥±‡µç 3: ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£ ‡¥∏‡¥Æ‡¥Ø‡¥Ç ‡¥¨‡µÜ‡¥û‡µç‡¥ö‡µç‡¥Æ‡¥æ‡µº‡¥ï‡µç‡¥ï‡µç
        performance_result = self._test_performance(client, model)
        check_results["tests"]["performance"] = performance_result
        
        # ‡¥ü‡µÜ‡¥∏‡µç‡¥±‡µç‡¥±‡µç 4: ‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÜ‡¥∏‡µç ‡¥ü‡µÜ‡¥∏‡µç‡¥±‡µç‡¥±‡µç
        stress_result = self._test_stress(client, model)
        check_results["tests"]["stress_test"] = stress_result
        
        # ‡¥Æ‡µä‡¥§‡µç‡¥§‡¥Ç ‡¥Ü‡¥∞‡µã‡¥ó‡µç‡¥Ø‡¥®‡¥ø‡¥≤ ‡¥ï‡¥£‡¥ï‡µç‡¥ï‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï
        check_results["overall_health"] = self._calculate_health_score(check_results["tests"])
        
        # ‡¥ü‡µç‡¥∞‡µÜ‡µª‡¥°‡¥ø‡¥ô‡µç‡¥ô‡¥ø‡¥®‡¥æ‡¥Ø‡¥ø ‡¥∏‡¥Ç‡¥≠‡¥∞‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
        self.health_history.append(check_results)
        
        return check_results
    
    def _test_connectivity(self, client: OpenAI) -> Dict[str, Any]:
        """Test basic service connectivity."""
        try:
            start_time = time.time()
            models = client.models.list()
            response_time = time.time() - start_time
            
            return {
                "status": "success",
                "response_time": response_time,
                "models_count": len(models.data)
            }
        except Exception as e:
            return {"status": "failed", "error": str(e)}
    
    def _test_model_availability(self, client: OpenAI, model: str) -> Dict[str, Any]:
        """Test specific model availability."""
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": "Health check"}],
                max_tokens=5
            )
            
            return {
                "status": "success",
                "model": model,
                "response_received": bool(response.choices[0].message.content)
            }
        except Exception as e:
            return {"status": "failed", "error": str(e)}
    
    def _test_performance(self, client: OpenAI, model: str) -> Dict[str, Any]:
        """Test response time performance."""
        response_times = []
        
        for i in range(3):
            try:
                start_time = time.time()
                client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": f"Test {i+1}"}],
                    max_tokens=10
                )
                response_time = time.time() - start_time
                response_times.append(response_time)
            except Exception:
                pass
        
        if response_times:
            avg_time = sum(response_times) / len(response_times)
            return {
                "status": "success",
                "average_response_time": avg_time,
                "min_time": min(response_times),
                "max_time": max(response_times),
                "within_threshold": avg_time < self.alert_thresholds["response_time"]
            }
        else:
            return {"status": "failed", "error": "No successful responses"}
    
    def _test_stress(self, client: OpenAI, model: str) -> Dict[str, Any]:
        """Test service under concurrent requests."""
        import concurrent.futures
        
        def single_request():
            try:
                client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": "Stress test"}],
                    max_tokens=5
                )
                return True
            except Exception:
                return False
        
        # 5 ‡¥∏‡¥Æ‡¥ï‡¥æ‡¥≤‡¥ø‡¥ï ‡¥Ö‡¥≠‡µç‡¥Ø‡µº‡¥§‡µç‡¥•‡¥®‡¥ï‡µæ ‡¥®‡¥ü‡¥§‡µç‡¥§‡µÅ‡¥ï
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(single_request) for _ in range(5)]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        success_rate = (sum(results) / len(results)) * 100
        
        return {
            "status": "success" if success_rate > 80 else "degraded",
            "concurrent_requests": len(results),
            "success_rate": success_rate,
            "within_threshold": success_rate >= self.alert_thresholds["availability"]
        }
    
    def _calculate_health_score(self, tests: Dict[str, Any]) -> str:
        """Calculate overall health score."""
        successful_tests = sum(1 for test in tests.values() if test["status"] == "success")
        total_tests = len(tests)
        health_percentage = (successful_tests / total_tests) * 100
        
        if health_percentage >= 90:
            return "healthy"
        elif health_percentage >= 70:
            return "degraded"
        else:
            return "unhealthy"
    
    def generate_health_report(self) -> str:
        """Generate formatted health report."""
        if not self.health_history:
            return "No health data available"
        
        latest = self.health_history[-1]
        report = f"Health Report - {time.ctime(latest['timestamp'])}\n"
        report += "=" * 60 + "\n"
        report += f"Provider: {latest['provider']}\n"
        report += f"Model: {latest['model']}\n"
        report += f"Overall Health: {latest['overall_health'].upper()}\n\n"
        
        for test_name, test_result in latest["tests"].items():
            status_icon = "‚úÖ" if test_result["status"] == "success" else "‚ùå"
            report += f"{status_icon} {test_name.replace('_', ' ').title()}: {test_result['status']}\n"
        
        return report


# ‡¥é‡¥ï‡µç‡¥∏‡µº‡¥∏‡µà‡¥∏‡µç 3 ‡¥™‡µç‡¥∞‡¥µ‡µº‡¥§‡µç‡¥§‡¥ø‡¥™‡µç‡¥™‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
client, model, provider = initialize_client()
health_monitor = ProductionHealthMonitor()

print("üè• Exercise 3: Production Health Monitoring")
print("=" * 60)

health_results = health_monitor.run_comprehensive_check(client, model, provider)
print(health_monitor.generate_health_report())
```

## ‡¥≠‡¥æ‡¥ó‡¥Ç 7: ‡¥∏‡¥Ç‡¥ó‡µç‡¥∞‡¥π‡¥µ‡µÅ‡¥Ç ‡¥Ö‡¥ü‡µÅ‡¥§‡µç‡¥§ ‡¥ò‡¥ü‡µç‡¥ü‡¥ô‡µç‡¥ô‡¥≥‡µÅ‡¥Ç

### ‡¥∏‡µÜ‡¥∑‡µª ‡¥®‡µá‡¥ü‡µç‡¥ü‡¥ô‡µç‡¥ô‡µæ

‡¥à ‡¥∏‡µÜ‡¥∑‡¥®‡¥ø‡µΩ, ‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ ‡¥™‡µç‡¥∞‡¥æ‡¥µ‡µÄ‡¥£‡µç‡¥Ø‡¥Ç ‡¥®‡µá‡¥ü‡¥ø‡¥Ø‡¥ø‡¥ü‡µç‡¥ü‡µÅ‡¥£‡µç‡¥ü‡µç:
- ‚úÖ **OpenAI SDK ‡¥á‡¥®‡µç‡¥±‡¥ó‡µç‡¥∞‡µá‡¥∑‡µª**: Foundry Local-‡¥®‡µÅ‡¥Ç Azure OpenAI-‡¥®‡µÅ‡¥Ç ‡¥µ‡µá‡¥£‡µç‡¥ü‡¥ø ‡¥Ü‡¥ß‡µÅ‡¥®‡¥ø‡¥ï ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡µæ
- ‚úÖ **‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÄ‡¥Æ‡¥ø‡¥Ç‡¥ó‡µç ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£‡¥ô‡µç‡¥ô‡µæ**: ‡¥Æ‡µÜ‡¥ö‡µç‡¥ö‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü ‡¥â‡¥™‡¥Ø‡µã‡¥ï‡µç‡¥§‡µÉ ‡¥Ö‡¥®‡µÅ‡¥≠‡¥µ‡¥§‡µç‡¥§‡¥ø‡¥®‡¥æ‡¥Ø‡¥ø ‡¥±‡¥ø‡¥Ø‡µΩ-‡¥ü‡µà‡¥Ç ‡¥ö‡¥æ‡¥±‡µç‡¥±‡µç ‡¥™‡µÇ‡µº‡¥§‡µç‡¥§‡µÄ‡¥ï‡¥∞‡¥£‡¥ô‡µç‡¥ô‡µæ
- ‚úÖ **‡¥Æ‡µæ‡¥ü‡µç‡¥ü‡¥ø-‡¥™‡µç‡¥∞‡µä‡¥µ‡µà‡¥°‡µº ‡¥™‡¥ø‡¥®‡µç‡¥§‡µÅ‡¥£**: ‡¥≤‡µã‡¥ï‡µç‡¥ï‡µΩ, ‡¥ï‡µç‡¥≤‡µó‡¥°‡µç AI ‡¥∏‡µá‡¥µ‡¥®‡¥ô‡µç‡¥ô‡µæ ‡¥§‡¥Æ‡µç‡¥Æ‡¥ø‡µΩ ‡¥∏‡µÅ‡¥§‡¥æ‡¥∞‡µç‡¥Ø‡¥Æ‡¥æ‡¥Ø ‡¥Æ‡¥æ‡¥±‡µΩ
- ‚úÖ **‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£ ‡¥Æ‡¥æ‡¥®‡µá‡¥ú‡µç‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µç**: ‡¥ï‡µã‡µ∫‡¥ü‡µÜ‡¥ï‡µç‡¥∏‡µç‡¥±‡µç‡¥±‡µç-‡¥Ö‡¥µ‡µÜ‡¥Ø‡µº ‡¥Æ‡µæ‡¥ü‡µç‡¥ü‡¥ø-‡¥ü‡µá‡µ∫ ‡¥∏‡¥Ç‡¥≠‡¥æ‡¥∑‡¥£‡¥ô‡µç‡¥ô‡µæ
- ‚úÖ **‡¥™‡µç‡¥∞‡¥ï‡¥ü‡¥® ‡¥®‡¥ø‡¥∞‡µÄ‡¥ï‡µç‡¥∑‡¥£‡¥Ç**: ‡¥™‡µç‡¥∞‡µä‡¥°‡¥ï‡µç‡¥∑‡µª ‡¥µ‡¥ø‡¥®‡µç‡¥Ø‡¥∏‡¥®‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥¨‡¥û‡µç‡¥ö‡µç‡¥Æ‡¥æ‡µº‡¥ï‡µç‡¥ï‡¥ø‡¥Ç‡¥ó‡µç, ‡¥π‡µÜ‡µΩ‡¥§‡µç‡¥§‡µç ‡¥ö‡µÜ‡¥ï‡µç‡¥ï‡¥ø‡¥Ç‡¥ó‡µç
- ‚úÖ **‡¥™‡µç‡¥∞‡µä‡¥°‡¥ï‡µç‡¥∑‡µª ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡µæ**: ‡¥∂‡¥ï‡µç‡¥§‡¥Æ‡¥æ‡¥Ø ‡¥™‡¥ø‡¥∂‡¥ï‡µç ‡¥ï‡µà‡¥ï‡¥æ‡¥∞‡µç‡¥Ø‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡¥≤‡µÅ‡¥Ç ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡µª ‡¥Æ‡¥æ‡¥®‡µá‡¥ú‡µç‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µÅ‡¥Ç

### ‡¥™‡µç‡¥∞‡¥ß‡¥æ‡¥® ‡¥Ü‡µº‡¥ï‡µç‡¥ï‡¥ø‡¥ü‡µÜ‡¥ï‡µç‡¥ö‡¥±‡µΩ ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡µæ

**‡¥ï‡µç‡¥≤‡¥Ø‡¥®‡µç‡¥±‡µç ‡¥´‡¥æ‡¥ï‡µç‡¥ü‡¥±‡¥ø ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡µ∫:**
```
Environment Detection ‚Üí Provider Selection ‚Üí Client Creation ‚Üí Model Configuration
        ‚Üì                    ‚Üì                  ‚Üì                 ‚Üì
    Azure/Local       Azure OpenAI/        OpenAI Client    Model Selection
    Credentials       Foundry Local        Initialization   and Validation
```

**‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÄ‡¥Æ‡¥ø‡¥Ç‡¥ó‡µç ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£ ‡¥™‡µç‡¥∞‡¥µ‡¥æ‡¥π‡¥Ç:**
```
User Input ‚Üí Chat Completion ‚Üí Stream Processing ‚Üí Real-time Display
     ‚Üì             ‚Üì                ‚Üì                 ‚Üì
   Prompt         Stream=True         Token Chunks      Progressive UI
```

### ‡¥Æ‡¥ø‡¥ï‡¥ö‡µç‡¥ö ‡¥™‡µç‡¥∞‡¥æ‡¥ï‡µç‡¥ü‡µÄ‡¥∏‡µÅ‡¥ï‡µæ ‡¥∏‡¥Ç‡¥ó‡µç‡¥∞‡¥π‡¥Ç

1. **üîÑ ‡¥é‡¥™‡µç‡¥™‡µã‡¥¥‡µÅ‡¥Ç ‡¥´‡¥æ‡µæ‡¥¨‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï‡µæ ‡¥®‡¥ü‡¥™‡µç‡¥™‡¥ø‡¥≤‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï**: Azure ‚Üí Foundry Local ‚Üí ‡¥™‡¥ø‡¥∂‡¥ï‡µç ‡¥ï‡µà‡¥ï‡¥æ‡¥∞‡µç‡¥Ø‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µΩ
2. **üåä ‡¥¶‡µÄ‡µº‡¥ò ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÄ‡¥Æ‡¥ø‡¥Ç‡¥ó‡µç ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï**: ‡¥Æ‡µÜ‡¥ö‡µç‡¥ö‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü ‡¥™‡µç‡¥∞‡¥ï‡¥ü‡¥® ‡¥ß‡¥æ‡¥∞‡¥£
3. **üõ°Ô∏è ‡¥∏‡¥Æ‡¥ó‡µç‡¥∞‡¥Æ‡¥æ‡¥Ø ‡¥™‡¥ø‡¥∂‡¥ï‡µç ‡¥ï‡µà‡¥ï‡¥æ‡¥∞‡µç‡¥Ø‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µΩ ‡¥®‡¥ü‡¥™‡µç‡¥™‡¥ø‡¥≤‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï**: ‡¥â‡¥™‡¥Ø‡µã‡¥ï‡µç‡¥§‡µÉ ‡¥∏‡µó‡¥π‡µÉ‡¥¶ ‡¥™‡¥ø‡¥∂‡¥ï‡µç ‡¥∏‡¥®‡µç‡¥¶‡µá‡¥∂‡¥ô‡µç‡¥ô‡µæ
4. **üìà ‡¥™‡µç‡¥∞‡¥ï‡¥ü‡¥®‡¥Ç ‡¥®‡¥ø‡¥∞‡µÄ‡¥ï‡µç‡¥∑‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï**: ‡¥™‡µç‡¥∞‡¥§‡¥ø‡¥ï‡¥∞‡¥£ ‡¥∏‡¥Æ‡¥Ø‡¥Ç, ‡¥µ‡¥ø‡¥ú‡¥Ø ‡¥®‡¥ø‡¥∞‡¥ï‡µç‡¥ï‡µç ‡¥ü‡µç‡¥∞‡¥æ‡¥ï‡µç‡¥ï‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï
5. **‚öôÔ∏è ‡¥™‡¥∞‡¥ø‡¥∏‡µç‡¥•‡¥ø‡¥§‡¥ø ‡¥Ö‡¥ü‡¥ø‡¥∏‡µç‡¥•‡¥æ‡¥® ‡¥ï‡µã‡µ∫‡¥´‡¥ø‡¥ó‡¥±‡µá‡¥∑‡µª**: ‡¥°‡µÜ‡¥µ‡µç/‡¥∏‡µç‡¥±‡µç‡¥±‡µá‡¥ú‡¥ø‡¥Ç‡¥ó‡µç/‡¥™‡µç‡¥∞‡µä‡¥°‡¥ø‡¥®‡¥ø‡¥ü‡¥Ø‡¥ø‡µΩ ‡¥é‡¥≥‡µÅ‡¥™‡µç‡¥™‡¥Ç ‡¥Æ‡¥æ‡¥±‡µΩ
6. **üîí ‡¥ï‡µç‡¥∞‡µÜ‡¥°‡µª‡¥∑‡µç‡¥Ø‡µΩ ‡¥Æ‡¥æ‡¥®‡µá‡¥ú‡µç‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µç ‡¥∏‡µÅ‡¥∞‡¥ï‡µç‡¥∑‡¥ø‡¥§‡¥Æ‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï**: API ‡¥ï‡µÄ‡¥ï‡µæ ‡¥π‡¥æ‡µº‡¥°‡µç‚Äå‡¥ï‡µã‡¥°‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡¥∞‡µÅ‡¥§‡µç

### ‡¥™‡µç‡¥∞‡µä‡¥µ‡µà‡¥°‡µº ‡¥§‡¥ø‡¥∞‡¥û‡µç‡¥û‡µÜ‡¥ü‡µÅ‡¥™‡µç‡¥™‡µç ‡¥Æ‡¥æ‡µº‡¥ó‡µç‡¥ó‡¥®‡¥ø‡µº‡¥¶‡µç‡¥¶‡µá‡¥∂‡¥ô‡µç‡¥ô‡µæ

| ‡¥∏‡¥æ‡¥π‡¥ö‡¥∞‡µç‡¥Ø‡¥ô‡µç‡¥ô‡µæ | ‡¥∂‡µÅ‡¥™‡¥æ‡µº‡¥∂ ‡¥ö‡µÜ‡¥Ø‡µç‡¥§ ‡¥™‡µç‡¥∞‡µä‡¥µ‡µà‡¥°‡µº | ‡¥ï‡¥æ‡¥∞‡¥£‡¥Ç |
|----------|---------------------|----------|
| **‡¥µ‡¥ø‡¥ï‡¥∏‡¥®‡¥Ç** | Foundry Local | ‡¥µ‡µá‡¥ó‡¥§‡µç‡¥§‡¥ø‡¥≤‡µÅ‡¥≥‡µç‡¥≥ ‡¥á‡¥±‡µç‡¥±‡¥±‡µá‡¥∑‡µª, API ‡¥ö‡µÜ‡¥≤‡¥µ‡¥ø‡¥≤‡µç‡¥≤‡¥æ‡¥§‡µÜ |
| **‡¥∏‡µç‡¥µ‡¥ï‡¥æ‡¥∞‡µç‡¥Ø‡¥§-‡¥™‡µç‡¥∞‡¥ß‡¥æ‡¥®‡µç‡¥Ø‡¥Ç** | Foundry Local | ‡¥°‡¥æ‡¥±‡µç‡¥± ‡¥â‡¥™‡¥ï‡¥∞‡¥£‡¥§‡µç‡¥§‡¥ø‡¥®‡µç ‡¥™‡µÅ‡¥±‡¥§‡µç‡¥§‡µá‡¥ï‡µç‡¥ï‡µç ‡¥™‡µã‡¥µ‡¥ø‡¥≤‡µç‡¥≤ |
| **‡¥â‡¥Ø‡µº‡¥®‡µç‡¥® ‡¥µ‡µã‡¥≥‡¥ø‡¥Ø‡¥Ç ‡¥™‡µç‡¥∞‡µä‡¥°‡¥ï‡µç‡¥∑‡µª** | Azure OpenAI | ‡¥Æ‡µÜ‡¥ö‡µç‡¥ö‡¥™‡µç‡¥™‡µÜ‡¥ü‡µç‡¥ü ‡¥∏‡µç‡¥ï‡µÜ‡¥Ø‡¥ø‡¥≤‡¥ø‡¥Ç‡¥ó‡µç, ‡¥é‡¥®‡µç‡¥±‡µº‡¥™‡µç‡¥∞‡µà‡¥∏‡µç SLA |
| **‡¥™‡µÅ‡¥§‡¥ø‡¥Ø ‡¥Æ‡µã‡¥°‡¥≤‡µÅ‡¥ï‡µæ** | Azure OpenAI | ‡¥è‡¥±‡µç‡¥±‡¥µ‡µÅ‡¥Ç ‡¥™‡µÅ‡¥§‡¥ø‡¥Ø ‡¥Æ‡µã‡¥°‡µΩ ‡¥±‡¥ø‡¥≤‡µÄ‡¥∏‡µÅ‡¥ï‡µæ‡¥ï‡µç‡¥ï‡µç ‡¥Ü‡¥ï‡µç‚Äå‡¥∏‡¥∏‡µç |
| **‡¥ì‡¥´‡µç‚Äå‡¥≤‡µà‡µª ‡¥Ü‡¥µ‡¥∂‡µç‡¥Ø‡¥ô‡µç‡¥ô‡µæ** | Foundry Local | ‡¥á‡¥®‡µç‡¥±‡µº‡¥®‡µÜ‡¥±‡µç‡¥±‡µç ‡¥Ü‡¥∂‡µç‡¥∞‡¥ø‡¥§‡¥Æ‡¥≤‡µç‡¥≤ |
| **‡¥ö‡µÜ‡¥≤‡¥µ‡µç-‡¥∏‡µÜ‡µª‡¥∏‡¥ø‡¥±‡µç‡¥±‡µÄ‡¥µ‡µç** | Foundry Local | ‡¥ü‡µã‡¥ï‡µç‡¥ï‡µ∫ ‡¥Ö‡¥ü‡¥ø‡¥∏‡µç‡¥•‡¥æ‡¥® ‡¥ö‡¥æ‡µº‡¥ú‡µÅ‡¥ï‡µæ ‡¥á‡¥≤‡µç‡¥≤‡¥æ‡¥§‡µÜ |

### ‡¥™‡¥∞‡¥ø‡¥∏‡µç‡¥•‡¥ø‡¥§‡¥ø ‡¥µ‡µá‡¥∞‡¥ø‡¥Ø‡¥¨‡¥ø‡¥≥‡µÅ‡¥ï‡µæ ‡¥ï‡µç‡¥µ‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥±‡¥´‡¥±‡µª‡¥∏‡µç

```cmd
REM Foundry Local (default)
set MODEL=phi-4-mini
set BASE_URL=http://localhost:8000
set API_KEY=

REM Azure OpenAI (cloud)
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
```

### ‡¥∏‡µÜ‡¥∑‡µª 3-‡¥®‡¥æ‡¥Ø‡¥ø ‡¥§‡¥Ø‡µç‡¥Ø‡¥æ‡¥±‡µÜ‡¥ü‡µÅ‡¥™‡µç‡¥™‡µç: ‡¥ì‡¥™‡µç‡¥™‡µ∫-‡¥∏‡µã‡¥¥‡µç‚Äå‡¥∏‡µç ‡¥Æ‡µã‡¥°‡¥≤‡µÅ‡¥ï‡µæ

1. **‡¥Æ‡µã‡¥°‡µΩ ‡¥ï‡¥æ‡¥±‡µç‡¥±‡¥≤‡µã‡¥ó‡µç ‡¥™‡¥∞‡¥ø‡¥∂‡µã‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï**: Foundry Local-‡µΩ ‡¥≤‡¥≠‡µç‡¥Ø‡¥Æ‡¥æ‡¥Ø ‡¥Æ‡µã‡¥°‡¥≤‡µÅ‡¥ï‡µæ ‡¥Ö‡¥µ‡¥≤‡µã‡¥ï‡¥®‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥ï
2. **‡¥Æ‡µã‡¥°‡µΩ ‡¥´‡µã‡µº‡¥Æ‡¥æ‡¥±‡µç‡¥±‡µÅ‡¥ï‡µæ ‡¥Æ‡¥®‡¥∏‡µç‡¥∏‡¥ø‡¥≤‡¥æ‡¥ï‡µç‡¥ï‡µÅ‡¥ï**: ONNX, ‡¥ï‡µç‡¥µ‡¥æ‡¥£‡µç‡¥ü‡µà‡¥∏‡µá‡¥∑‡µª, ‡¥í‡¥™‡µç‡¥±‡µç‡¥±‡¥ø‡¥Æ‡µà‡¥∏‡µá‡¥∑‡µª ‡¥é‡¥®‡µç‡¥®‡¥ø‡¥µ‡¥Ø‡µÜ‡¥ï‡µç‡¥ï‡µÅ‡¥±‡¥ø‡¥ö‡µç‡¥ö‡µç ‡¥™‡¥†‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï
3. **‡¥ï‡¥∏‡µç‡¥±‡µç‡¥±‡¥Ç ‡¥Æ‡µã‡¥°‡¥≤‡µÅ‡¥ï‡µæ ‡¥™‡¥∞‡¥ø‡¥ó‡¥£‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï**: ‡¥°‡µä‡¥Æ‡µÜ‡¥Ø‡µç‡µª-‡¥∏‡µç‡¥™‡µÜ‡¥∏‡¥ø‡¥´‡¥ø‡¥ï‡µç ‡¥Æ‡µã‡¥°‡µΩ ‡¥Ü‡¥µ‡¥∂‡µç‡¥Ø‡¥ï‡¥§‡¥ï‡µæ ‡¥ö‡¥ø‡¥®‡µç‡¥§‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï

### ‡¥™‡µç‡¥∞‡¥∂‡µç‡¥®‡¥™‡¥∞‡¥ø‡¥π‡¥æ‡¥∞ ‡¥ï‡µç‡¥µ‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥ó‡µà‡¥°‡µç

**‡¥∏‡¥æ‡¥ß‡¥æ‡¥∞‡¥£ ‡¥™‡µç‡¥∞‡¥∂‡µç‡¥®‡¥ô‡µç‡¥ô‡µæ:**
```cmd
REM Issue: Could not use Foundry SDK
pip install foundry-local-sdk

REM Issue: Connection refused
foundry service status
foundry model run phi-4-mini

REM Issue: Azure authentication failed
echo %AZURE_OPENAI_ENDPOINT%
echo %AZURE_OPENAI_API_KEY%

REM Issue: Model not found
foundry model list
curl http://localhost:8000/v1/models
```

## ‡¥±‡¥´‡¥±‡µª‡¥∏‡µÅ‡¥ï‡µæ

- **[OpenAI Python SDK ‡¥°‡µã‡¥ï‡µç‡¥Ø‡µÅ‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µá‡¥∑‡µª](https://github.com/openai/openai-python)**: ‡¥î‡¥¶‡µç‡¥Ø‡µã‡¥ó‡¥ø‡¥ï SDK ‡¥±‡¥´‡¥±‡µª‡¥∏‡µç
- **[Azure OpenAI ‡¥°‡µã‡¥ï‡µç‡¥Ø‡µÅ‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µá‡¥∑‡µª](https://learn.microsoft.com/azure/ai-services/openai/)**: Azure OpenAI ‡¥∏‡µá‡¥µ‡¥® ‡¥ó‡µà‡¥°‡µç
- **[Foundry Local SDK ‡¥±‡¥´‡¥±‡µª‡¥∏‡µç](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: ‡¥≤‡µã‡¥ï‡µç‡¥ï‡µΩ ‡¥á‡µª‡¥´‡¥±‡µª‡¥∏‡µç ‡¥°‡µã‡¥ï‡µç‡¥Ø‡µÅ‡¥Æ‡µÜ‡¥®‡µç‡¥±‡µá‡¥∑‡µª
- **[‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÄ‡¥Æ‡¥ø‡¥Ç‡¥ó‡µç ‡¥™‡µÇ‡µº‡¥§‡µç‡¥§‡µÄ‡¥ï‡¥∞‡¥£‡¥ô‡µç‡¥ô‡µæ ‡¥ó‡µà‡¥°‡µç](https://learn.microsoft.com/azure/ai-foundry/foundry-local/how-to/integrate-with-inference-sdks)**: ‡¥Ü‡¥ß‡µÅ‡¥®‡¥ø‡¥ï ‡¥∏‡µç‡¥ü‡µç‡¥∞‡µÄ‡¥Æ‡¥ø‡¥Ç‡¥ó‡µç ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡µæ
- **[‡¥∏‡¥æ‡¥Æ‡µç‡¥™‡¥ø‡µæ 01: OpenAI SDK ‡¥µ‡¥¥‡¥ø ‡¥ï‡µç‡¥µ‡¥ø‡¥ï‡µç‡¥ï‡µç ‡¥ö‡¥æ‡¥±‡µç‡¥±‡µç](samples/01/README.md)**: ‡¥Ö‡¥ü‡¥ø‡¥∏‡µç‡¥•‡¥æ‡¥® ‡¥á‡¥®‡µç‡¥±‡¥ó‡µç‡¥∞‡µá‡¥∑‡µª ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡µæ
- **[‡¥∏‡¥æ‡¥Æ‡µç‡¥™‡¥ø‡µæ 02: ‡¥Ü‡¥ß‡µÅ‡¥®‡¥ø‡¥ï SDK ‡¥á‡¥®‡µç‡¥±‡¥ó‡µç‡¥∞‡µá‡¥∑‡µª](samples/02/README.md)**: ‡¥à ‡¥∏‡µÜ‡¥∑‡¥®‡µç‡¥±‡µÜ ‡¥π‡¥æ‡µª‡¥°‡µç‚Äå‡¥∏‡µç-‡¥ì‡µ∫ ‡¥â‡¥¶‡¥æ‡¥π‡¥∞‡¥£‡¥ô‡µç‡¥ô‡µæ
- **[‡¥∏‡¥æ‡¥Æ‡µç‡¥™‡¥ø‡µæ 04: Chainlit ‡¥Ü‡¥™‡µç‡¥≤‡¥ø‡¥ï‡µç‡¥ï‡µá‡¥∑‡µª](samples/04/README.md)**: ‡¥µ‡µÜ‡¥¨‡µç UI ‡¥µ‡¥ø‡¥ï‡¥∏‡¥®‡¥Ç
- **[‡¥∏‡¥æ‡¥Æ‡µç‡¥™‡¥ø‡µæ 05: ‡¥Æ‡µæ‡¥ü‡µç‡¥ü‡¥ø-‡¥è‡¥ú‡¥®‡µç‡¥±‡µç ‡¥∏‡¥ø‡¥∏‡µç‡¥±‡µç‡¥±‡¥ô‡µç‡¥ô‡µæ](samples/05/README.md)**: ‡¥Ü‡¥ß‡µÅ‡¥®‡¥ø‡¥ï ‡¥ì‡µº‡¥ï‡µç‡¥ï‡¥∏‡µç‡¥ü‡µç‡¥∞‡µá‡¥∑‡µª ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡µæ

‡¥®‡¥ø‡¥ô‡µç‡¥ô‡µæ ‡¥á‡¥™‡µç‡¥™‡µã‡µæ ‡¥≤‡µã‡¥ï‡µç‡¥ï‡µΩ, ‡¥ï‡µç‡¥≤‡µó‡¥°‡µç AI ‡¥ï‡¥¥‡¥ø‡¥µ‡µÅ‡¥ï‡µæ ‡¥∏‡µÅ‡¥§‡¥æ‡¥∞‡µç‡¥Ø‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥á‡¥®‡µç‡¥±‡¥ó‡µç‡¥∞‡µá‡¥±‡µç‡¥±‡µç ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡µÅ‡¥®‡µç‡¥® ‡¥∏‡¥ô‡µç‡¥ï‡µÄ‡µº‡¥£‡µç‡¥£‡¥Æ‡¥æ‡¥Ø AI ‡¥Ü‡¥™‡µç‡¥≤‡¥ø‡¥ï‡µç‡¥ï‡µá‡¥∑‡¥®‡µÅ‡¥ï‡µæ ‡¥®‡¥ø‡µº‡¥Æ‡µç‡¥Æ‡¥ø‡¥ï‡µç‡¥ï‡¥æ‡µª ‡¥∏‡¥ú‡µç‡¥ú‡¥Æ‡¥æ‡¥£‡µç, ‡¥ì‡¥∞‡µã ‡¥™‡µç‡¥∞‡¥§‡µç‡¥Ø‡µá‡¥ï ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥§‡µç‡¥§‡¥ø‡¥®‡µÅ‡¥Ç ‡¥∂‡¥∞‡¥ø‡¥Ø‡¥æ‡¥Ø ‡¥™‡µç‡¥∞‡µä‡¥µ‡µà‡¥°‡µº ‡¥§‡¥ø‡¥∞‡¥û‡µç‡¥û‡µÜ‡¥ü‡µÅ‡¥ï‡µç‡¥ï‡¥æ‡¥®‡µÅ‡¥≥‡µç‡¥≥ ‡¥´‡µç‡¥≤‡µÜ‡¥ï‡µç‡¥∏‡¥ø‡¥¨‡¥ø‡¥≤‡¥ø‡¥±‡µç‡¥±‡¥ø‡¥Ø‡µÅ‡¥Ç ‡¥∏‡µç‡¥•‡¥ø‡¥∞‡¥§‡¥Ø‡µÅ‡¥≥‡µç‡¥≥ ‡¥µ‡¥ø‡¥ï‡¥∏‡¥® ‡¥™‡¥æ‡¥±‡µç‡¥±‡µá‡¥£‡µÅ‡¥ï‡¥≥‡µÅ‡¥Ç ‡¥®‡¥ø‡¥≤‡¥®‡¥ø‡µº‡¥§‡µç‡¥§‡¥ø‡¥ï‡µç‡¥ï‡µä‡¥£‡µç‡¥ü‡µç.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**‡¥Ö‡¥∏‡µÇ‡¥Ø‡¥æ**:  
‡¥à ‡¥∞‡µá‡¥ñ AI ‡¥µ‡¥ø‡¥µ‡µº‡¥§‡µç‡¥§‡¥® ‡¥∏‡µá‡¥µ‡¥®‡¥Ç [Co-op Translator](https://github.com/Azure/co-op-translator) ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ö‡µç‡¥ö‡µç ‡¥µ‡¥ø‡¥µ‡µº‡¥§‡µç‡¥§‡¥®‡¥Ç ‡¥ö‡µÜ‡¥Ø‡µç‡¥§‡¥§‡¥æ‡¥£‡µç. ‡¥®‡¥æ‡¥Ç ‡¥ï‡µÉ‡¥§‡µç‡¥Ø‡¥§‡¥Ø‡µç‡¥ï‡µç‡¥ï‡µç ‡¥∂‡µç‡¥∞‡¥Æ‡¥ø‡¥ö‡µç‡¥ö‡¥ø‡¥ü‡µç‡¥ü‡µÅ‡¥£‡µç‡¥ü‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡¥≤‡µÅ‡¥Ç, ‡¥∏‡µç‡¥µ‡¥Ø‡¥Ç ‡¥™‡µç‡¥∞‡¥µ‡µº‡¥§‡µç‡¥§‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥µ‡¥ø‡¥µ‡µº‡¥§‡µç‡¥§‡¥®‡¥ô‡µç‡¥ô‡¥≥‡¥ø‡µΩ ‡¥™‡¥ø‡¥∂‡¥ï‡µÅ‡¥ï‡µæ ‡¥Ö‡¥≤‡µç‡¥≤‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡µΩ ‡¥§‡µÜ‡¥±‡µç‡¥±‡µÅ‡¥ï‡µæ ‡¥â‡¥£‡µç‡¥ü‡¥æ‡¥ï‡¥æ‡¥Æ‡µÜ‡¥®‡µç‡¥®‡µç ‡¥¶‡¥Ø‡¥µ‡¥æ‡¥Ø‡¥ø ‡¥∂‡µç‡¥∞‡¥¶‡µç‡¥ß‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥ï. ‡¥Ö‡¥§‡¥ø‡¥®‡µç‡¥±‡µÜ ‡¥Æ‡¥æ‡¥§‡µÉ‡¥≠‡¥æ‡¥∑‡¥Ø‡¥ø‡¥≤‡µÅ‡¥≥‡µç‡¥≥ ‡¥Ø‡¥•‡¥æ‡µº‡¥§‡µç‡¥• ‡¥∞‡µá‡¥ñ ‡¥™‡µç‡¥∞‡¥æ‡¥Æ‡¥æ‡¥£‡¥ø‡¥ï‡¥Æ‡¥æ‡¥Ø ‡¥â‡¥±‡¥µ‡¥ø‡¥ü‡¥Æ‡¥æ‡¥Ø‡¥ø ‡¥ï‡¥£‡¥ï‡µç‡¥ï‡¥æ‡¥ï‡µç‡¥ï‡¥£‡¥Ç. ‡¥®‡¥ø‡µº‡¥£‡¥æ‡¥Ø‡¥ï‡¥Æ‡¥æ‡¥Ø ‡¥µ‡¥ø‡¥µ‡¥∞‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µç, ‡¥™‡µç‡¥∞‡µä‡¥´‡¥∑‡¥£‡µΩ ‡¥Æ‡¥®‡µÅ‡¥∑‡µç‡¥Ø ‡¥µ‡¥ø‡¥µ‡µº‡¥§‡µç‡¥§‡¥®‡¥Ç ‡¥∂‡µÅ‡¥™‡¥æ‡µº‡¥∂ ‡¥ö‡µÜ‡¥Ø‡µç‡¥Ø‡¥™‡µç‡¥™‡µÜ‡¥ü‡µÅ‡¥®‡µç‡¥®‡µÅ. ‡¥à ‡¥µ‡¥ø‡¥µ‡µº‡¥§‡µç‡¥§‡¥®‡¥Ç ‡¥â‡¥™‡¥Ø‡µã‡¥ó‡¥ø‡¥ï‡µç‡¥ï‡µÅ‡¥®‡µç‡¥®‡¥§‡¥ø‡µΩ ‡¥®‡¥ø‡¥®‡µç‡¥®‡µÅ‡¥£‡µç‡¥ü‡¥æ‡¥ï‡µÅ‡¥®‡µç‡¥® ‡¥è‡¥§‡µÜ‡¥ô‡µç‡¥ï‡¥ø‡¥≤‡µÅ‡¥Ç ‡¥§‡µÜ‡¥±‡µç‡¥±‡¥ø‡¥¶‡µç‡¥ß‡¥æ‡¥∞‡¥£‡¥ï‡µæ‡¥ï‡µç‡¥ï‡µã ‡¥§‡µÜ‡¥±‡µç‡¥±‡¥æ‡¥Ø ‡¥µ‡µç‡¥Ø‡¥æ‡¥ñ‡µç‡¥Ø‡¥æ‡¥®‡¥ô‡µç‡¥ô‡µæ‡¥ï‡µç‡¥ï‡µã ‡¥û‡¥ô‡µç‡¥ô‡µæ ‡¥â‡¥§‡µç‡¥§‡¥∞‡¥µ‡¥æ‡¥¶‡¥ø‡¥ï‡¥≥‡¥≤‡µç‡¥≤.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->