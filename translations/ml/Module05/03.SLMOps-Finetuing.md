<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6485323e2963241723d26eb0e0e9a492",
  "translation_date": "2025-12-15T19:45:06+00:00",
  "source_file": "Module05/03.SLMOps-Finetuing.md",
  "language_code": "ml"
}
-->
# വിഭാഗം 3: ഫൈൻ-ട്യൂണിംഗ് - പ്രത്യേക ജോലികൾക്കായി മോഡലുകൾ ഇഷ്‌ടാനുസൃതമാക്കൽ

## ഉള്ളടക്ക പട്ടിക
1. [ഫൈൻ-ട്യൂണിംഗിന്റെ പരിചയം](../../../Module05)
2. [ഫൈൻ-ട്യൂണിംഗ് എന്തുകൊണ്ട് പ്രധാനമാണ്](../../../Module05)
3. [ഫൈൻ-ട്യൂണിംഗിന്റെ തരം](../../../Module05)
4. [Microsoft Olive ഉപയോഗിച്ച് ഫൈൻ-ട്യൂണിംഗ്](../../../Module05)
5. [പ്രായോഗിക ഉദാഹരണങ്ങൾ](../../../Module05)
6. [മികച്ച പ്രാക്ടീസുകളും മാർഗ്ഗനിർദ്ദേശങ്ങളും](../../../Module05)
7. [ഉന്നത സാങ്കേതിക വിദ്യകൾ](../../../Module05)
8. [മൂല്യനിർണ്ണയം ಮತ್ತು നിരീക്ഷണം](../../../Module05)
9. [സാധാരണ വെല്ലുവിളികളും പരിഹാരങ്ങളും](../../../Module05)
10. [സംഗ്രഹം](../../../Module05)

## ഫൈൻ-ട്യൂണിംഗിന്റെ പരിചയം

**ഫൈൻ-ട്യൂണിംഗ്** ഒരു ശക്തമായ മെഷീൻ ലേണിംഗ് സാങ്കേതിക വിദ്യയാണ്, ഇത് മുൻകൂട്ടി പരിശീലിച്ച മോഡലിനെ പ്രത്യേക ജോലികൾ ചെയ്യുന്നതിനോ പ്രത്യേക ഡാറ്റാസെറ്റുകളുമായി പ്രവർത്തിക്കുന്നതിനോ അനുയോജ്യമായി മാറ്റുന്നതാണ്. ഒരു മോഡൽ പൂർത്തിയായി പരിശീലിപ്പിക്കുന്നതിന് പകരം, ഫൈൻ-ട്യൂണിംഗ് മുൻകൂട്ടി പഠിച്ച മോഡലിന്റെ അറിവ് ഉപയോഗിച്ച് നിങ്ങളുടെ പ്രത്യേക ഉപയോഗത്തിനായി അതിനെ ക്രമീകരിക്കുന്നു.

### ഫൈൻ-ട്യൂണിംഗ് എന്താണ്?

ഫൈൻ-ട്യൂണിംഗ് ഒരു തരത്തിലുള്ള **ട്രാൻസ്ഫർ ലേണിംഗ്** ആണ്, ഇതിൽ നിങ്ങൾ:
- വലിയ ഡാറ്റാസെറ്റുകളിൽ നിന്നുള്ള പൊതുവായ മാതൃകകൾ പഠിച്ച മുൻകൂട്ടി പരിശീലിച്ച മോഡലിൽ നിന്ന് ആരംഭിക്കുന്നു
- നിങ്ങളുടെ പ്രത്യേക ഡാറ്റാസെറ്റുപയോഗിച്ച് മോഡലിന്റെ ആന്തരിക പാരാമീറ്ററുകൾ ക്രമീകരിക്കുന്നു
- വിലപ്പെട്ട അറിവ് നിലനിർത്തി നിങ്ങളുടെ ജോലിക്ക് മോഡലിനെ പ്രത്യേകമാക്കുന്നു

ഒരു പരിചയസമ്പന്നനായ ഷെഫിനെ പുതിയ ഒരു പാചകശൈലി പഠിപ്പിക്കുന്നതുപോലെ ആലോചിക്കാം - അവർക്കു പാചകത്തിന്റെ അടിസ്ഥാനങ്ങൾ അറിയാം, പക്ഷേ പുതിയ ശൈലിക്ക് പ്രത്യേക സാങ്കേതിക വിദ്യകളും രുചികളും പഠിക്കേണ്ടതുണ്ട്.

### പ്രധാന ഗുണങ്ങൾ

- **സമയം ലാഭം**: പൂർത്തിയായി പരിശീലിപ്പിക്കുന്നതിനെക്കാൾ വളരെ വേഗം
- **ഡാറ്റാ ലാഭം**: നല്ല പ്രകടനത്തിന് ചെറിയ ഡാറ്റാസെറ്റുകൾ മതിയാകും
- **ചെലവ് കുറഞ്ഞത്**: കുറഞ്ഞ കംപ്യൂട്ടേഷൻ ആവശ്യകതകൾ
- **മികച്ച പ്രകടനം**: സാധാരണയായി പൂർത്തിയായി പരിശീലിപ്പിക്കുന്നതിനെക്കാൾ മികച്ച ഫലം
- **സ്രോതസ്സ് മെച്ചപ്പെടുത്തൽ**: ശക്തമായ AI ചെറിയ ടീമുകൾക്കും സ്ഥാപനങ്ങൾക്കും ലഭ്യമാക്കുന്നു

## ഫൈൻ-ട്യൂണിംഗ് എന്തുകൊണ്ട് പ്രധാനമാണ്

### യാഥാർത്ഥ്യ പ്രയോഗങ്ങൾ

ഫൈൻ-ട്യൂണിംഗ് നിരവധി സാഹചര്യങ്ങളിൽ അനിവാര്യമാണ്:

**1. ഡൊമെയ്ൻ അനുയോജ്യത**
- മെഡിക്കൽ AI: പൊതുവായ ഭാഷാ മോഡലുകൾ മെഡിക്കൽ പദസമ്പത്തിനും ക്ലിനിക്കൽ കുറിപ്പുകൾക്കും അനുയോജ്യമായി മാറ്റൽ
- നിയമ സാങ്കേതികവിദ്യ: നിയമ രേഖാ വിശകലനത്തിനും കരാർ അവലോകനത്തിനും മോഡലുകൾ പ്രത്യേകമാക്കൽ
- സാമ്പത്തിക സേവനങ്ങൾ: സാമ്പത്തിക റിപ്പോർട്ട് വിശകലനത്തിനും റിസ്ക് വിലയിരുത്തലിനും മോഡലുകൾ ഇഷ്‌ടാനുസൃതമാക്കൽ

**2. ജോലിയുടെ പ്രത്യേകത**
- ഉള്ളടക്കം സൃഷ്ടി: പ്രത്യേക എഴുത്ത് ശൈലികൾക്കോ സ്വരങ്ങളിലോ ഫൈൻ-ട്യൂൺ ചെയ്യൽ
- കോഡ് സൃഷ്ടി: പ്രത്യേക പ്രോഗ്രാമിംഗ് ഭാഷകൾക്കോ ഫ്രെയിംവർക്കുകൾക്കോ അനുയോജ്യമായി മോഡലുകൾ ക്രമീകരിക്കൽ
- വിവർത്തനം: പ്രത്യേക ഭാഷാ ജോഡികൾക്കോ സാങ്കേതിക ഡൊമെയ്‌നുകൾക്കോ മികച്ച പ്രകടനം

**3. കോർപ്പറേറ്റ് പ്രയോഗങ്ങൾ**
- ഉപഭോക്തൃ സേവനം: കമ്പനി-നിർദ്ദിഷ്ട പദസമ്പത്ത് മനസ്സിലാക്കുന്ന ചാറ്റ്ബോട്ടുകൾ സൃഷ്ടിക്കൽ
- ആന്തരിക ഡോക്യുമെന്റേഷൻ: സംഘടനാ പ്രക്രിയകൾ അറിയുന്ന AI അസിസ്റ്റന്റുകൾ നിർമ്മിക്കൽ
- വ്യവസായ-നിർദ്ദിഷ്ട പരിഹാരങ്ങൾ: മേഖലയിലെ പ്രത്യേക പദസമ്പത്തും പ്രവൃത്തിപദ്ധതികളും മനസ്സിലാക്കുന്ന മോഡലുകൾ വികസിപ്പിക്കൽ

## ഫൈൻ-ട്യൂണിംഗിന്റെ തരം

### 1. പൂർണ്ണ ഫൈൻ-ട്യൂണിംഗ് (ഇൻസ്ട്രക്ഷൻ ഫൈൻ-ട്യൂണിംഗ്)

പൂർണ്ണ ഫൈൻ-ട്യൂണിംഗിൽ, പരിശീലന സമയത്ത് എല്ലാ മോഡൽ പാരാമീറ്ററുകളും അപ്ഡേറ്റ് ചെയ്യപ്പെടുന്നു. ഈ സമീപനം:
- പരമാവധി സൗകര്യവും പ്രകടന ശേഷിയും നൽകുന്നു
- വലിയ കംപ്യൂട്ടേഷൻ സ്രോതസ്സുകൾ ആവശ്യമാണ്
- മോഡലിന്റെ പൂർണ്ണമായ പുതിയ പതിപ്പ് സൃഷ്ടിക്കുന്നു
- വലിയ പരിശീലന ഡാറ്റയും കംപ്യൂട്ടേഷൻ ശേഷിയും ഉള്ള സാഹചര്യങ്ങൾക്ക് അനുയോജ്യം

### 2. പാരാമീറ്റർ-ക്ഷമതയുള്ള ഫൈൻ-ട്യൂണിംഗ് (PEFT)

PEFT രീതികൾ ചെറിയൊരു പാരാമീറ്റർ സെറ്റ് മാത്രം അപ്ഡേറ്റ് ചെയ്യുന്നു, ഇത് പ്രക്രിയ കൂടുതൽ കാര്യക്ഷമമാക്കുന്നു:

#### ലോ-റാങ്ക് അഡാപ്റ്റേഷൻ (LoRA)
- നിലവിലുള്ള ഭാരങ്ങളിൽ ചെറിയ ട്രെയിനബിൾ റാങ്ക് ഡികമ്പോസിഷൻ മാട്രിസുകൾ ചേർക്കുന്നു
- ട്രെയിനബിൾ പാരാമീറ്ററുകളുടെ എണ്ണം നിഗണനീയമായി കുറയ്ക്കുന്നു
- പൂർണ്ണ ഫൈൻ-ട്യൂണിംഗിനോട് അടുത്ത പ്രകടനം നിലനിർത്തുന്നു
- വ്യത്യസ്ത അഡാപ്റ്റേഷനുകൾക്ക് എളുപ്പത്തിൽ മാറാൻ സഹായിക്കുന്നു

#### QLoRA (ക്വാണ്ടൈസ്ഡ് LoRA)
- LoRA-യെ ക്വാണ്ടൈസേഷൻ സാങ്കേതിക വിദ്യകളുമായി സംയോജിപ്പിക്കുന്നു
- മെമ്മറി ആവശ്യകതകൾ കൂടുതൽ കുറയ്ക്കുന്നു
- ഉപഭോക്തൃ ഹാർഡ്‌വെയറിൽ വലിയ മോഡലുകളുടെ ഫൈൻ-ട്യൂണിംഗ് സാധ്യമാക്കുന്നു
- കാര്യക്ഷമതയും പ്രകടനവും തമ്മിൽ ബാലൻസ് നിലനിർത്തുന്നു

#### അഡാപ്റ്ററുകൾ
- നിലവിലുള്ള ലെയറുകൾക്കിടയിൽ ചെറിയ ന്യൂറൽ നെറ്റ്വർക്കുകൾ ചേർക്കുന്നു
- അടിസ്ഥാന മോഡൽ ഫ്രീസാക്കിയ നിലയിൽ ലക്ഷ്യമിട്ട ഫൈൻ-ട്യൂണിംഗ് അനുവദിക്കുന്നു
- മോഡൽ ഇഷ്‌ടാനുസൃതമാക്കലിന് മോടുലാർ സമീപനം അനുവദിക്കുന്നു

### 3. ജോലിസംബന്ധിയായ ഫൈൻ-ട്യൂണിംഗ്

നിർദ്ദിഷ്ട ഡൗൺസ്ട്രീം ജോലികൾക്കായി മോഡലുകൾ ക്രമീകരിക്കുന്നതിൽ കേന്ദ്രീകരിക്കുന്നു:
- **വർഗ്ഗീകരണം**: വർഗ്ഗീകരണ ജോലികൾക്കായി മോഡലുകൾ ക്രമീകരിക്കൽ
- **സൃഷ്ടി**: ഉള്ളടക്കം സൃഷ്ടിക്കലിനും ടെക്സ്റ്റ് ജനറേഷനും അനുയോജ്യമായി മെച്ചപ്പെടുത്തൽ
- **എക്സ്ട്രാക്ഷൻ**: വിവരങ്ങൾ എടുക്കലിനും നാമം തിരിച്ചറിയലിനും ഫൈൻ-ട്യൂണിംഗ്
- **സംഗ്രഹം**: ഡോക്യുമെന്റ് സംഗ്രഹത്തിനായി മോഡലുകൾ പ്രത്യേകമാക്കൽ

## Microsoft Olive ഉപയോഗിച്ച് ഫൈൻ-ട്യൂണിംഗ്

Microsoft Olive ഒരു സമഗ്ര മോഡൽ ഓപ്റ്റിമൈസേഷൻ ടൂൾകിറ്റ് ആണ്, ഇത് ഫൈൻ-ട്യൂണിംഗ് പ്രക്രിയ ലളിതമാക്കുകയും എന്റർപ്രൈസ്-ഗ്രേഡ് സവിശേഷതകൾ നൽകുകയും ചെയ്യുന്നു.

### Microsoft Olive എന്താണ്?

Microsoft Olive ഒരു ഓപ്പൺ-സോഴ്‌സ് മോഡൽ ഓപ്റ്റിമൈസേഷൻ ടൂൾ ആണ്, ഇത്:
- വിവിധ ഹാർഡ്‌വെയർ ലക്ഷ്യങ്ങളിലേക്കുള്ള ഫൈൻ-ട്യൂണിംഗ് പ്രവൃത്തികൾ ലളിതമാക്കുന്നു
- പ്രശസ്ത മോഡൽ ആർക്കിടെക്ചറുകൾക്ക് (Llama, Phi, Qwen, Gemma) ബിൽറ്റ്-ഇൻ പിന്തുണ നൽകുന്നു
- ക്ലൗഡ്, ലോക്കൽ ഡിപ്ലോയ്മെന്റ് ഓപ്ഷനുകൾ നൽകുന്നു
- Azure ML, മറ്റ് Microsoft AI സേവനങ്ങളുമായി സുതാര്യമായി സംയോജിക്കുന്നു
- സ്വയം ഓപ്റ്റിമൈസേഷൻ, ക്വാണ്ടൈസേഷൻ പിന്തുണ നൽകുന്നു

### പ്രധാന സവിശേഷതകൾ

- **ഹാർഡ്‌വെയർ-അവെയർ ഓപ്റ്റിമൈസേഷൻ**: പ്രത്യേക ഹാർഡ്‌വെയറുകൾക്കായി (CPU, GPU, NPU) മോഡലുകൾ സ്വയം ഓപ്റ്റിമൈസ് ചെയ്യുന്നു
- **മൾട്ടി-ഫോർമാറ്റ് പിന്തുണ**: PyTorch, Hugging Face, ONNX മോഡലുകളുമായി പ്രവർത്തിക്കുന്നു
- **സ്വയം പ്രവർത്തിക്കുന്ന പ്രവൃത്തികൾ**: മാനുവൽ കോൺഫിഗറേഷൻ കുറയ്ക്കുന്നു, പരീക്ഷണ-പിശക് കുറയ്ക്കുന്നു
- **എന്റർപ്രൈസ് സംയോജനം**: Azure ML, ക്ലൗഡ് ഡിപ്ലോയ്മെന്റുകൾക്ക് ബിൽറ്റ്-ഇൻ പിന്തുണ
- **വ്യാപകമായ ആർക്കിടെക്ചർ**: ഇഷ്‌ടാനുസൃത ഓപ്റ്റിമൈസേഷൻ സാങ്കേതിക വിദ്യകൾ അനുവദിക്കുന്നു

### ഇൻസ്റ്റലേഷൻ & സെറ്റപ്പ്

#### അടിസ്ഥാന ഇൻസ്റ്റലേഷൻ

```bash
# ഒരു വെർച്വൽ എൻവയോൺമെന്റ് സൃഷ്ടിക്കുക
python -m venv olive-env
source olive-env/bin/activate  # വിൻഡോസ്: olive-env\Scripts\activate

# ഓട്ടോ-ഓപ്റ്റിമൈസേഷൻ ഫീച്ചറുകളോടെ Olive ഇൻസ്റ്റാൾ ചെയ്യുക
pip install olive-ai[auto-opt]

# അധിക ആശ്രിതങ്ങൾ ഇൻസ്റ്റാൾ ചെയ്യുക
pip install transformers onnxruntime-genai
```

#### ഐച്ഛിക ആശ്രിതങ്ങൾ

```bash
# CPU മെച്ചപ്പെടുത്തലിനായി
pip install olive-ai[cpu]

# GPU മെച്ചപ്പെടുത്തലിനായി
pip install olive-ai[gpu]

# DirectML (Windows) നായി
pip install olive-ai[directml]

# Azure ML സംയോജനത്തിനായി
pip install olive-ai[azureml]
```

#### ഇൻസ്റ്റലേഷൻ സ്ഥിരീകരിക്കുക

```bash
# ഒലീവ് CLI ലഭ്യമാണ് എന്ന് പരിശോധിക്കുക
olive --help

# ഇൻസ്റ്റലേഷൻ സ്ഥിരീകരിക്കുക
python -c "import olive; print('Olive installed successfully')"
```

## പ്രായോഗിക ഉദാഹരണങ്ങൾ

### ഉദാഹരണം 1: Olive CLI ഉപയോഗിച്ച് അടിസ്ഥാന ഫൈൻ-ട്യൂണിംഗ്

ഈ ഉദാഹരണം ഒരു ചെറിയ ഭാഷാ മോഡൽ വാചക വർഗ്ഗീകരണത്തിനായി ഫൈൻ-ട്യൂൺ ചെയ്യുന്നത് കാണിക്കുന്നു:

#### ഘട്ടം 1: നിങ്ങളുടെ പരിസ്ഥിതി തയ്യാറാക്കുക

```bash
# പരിസ്ഥിതി സജ്ജമാക്കുക
mkdir fine-tuning-project
cd fine-tuning-project

# സാമ്പിൾ ഡാറ്റ ഡൗൺലോഡ് ചെയ്യുക (ഐച്ഛികം - ഒലീവ് സ്വയം ഡാറ്റ എടുക്കാം)
huggingface-cli login  # സ്വകാര്യ ഡാറ്റാസെറ്റുകൾ ഉപയോഗിക്കുന്നുവെങ്കിൽ
```

#### ഘട്ടം 2: മോഡൽ ഫൈൻ-ട്യൂൺ ചെയ്യുക

```bash
# അടിസ്ഥാന ഫൈൻ-ട്യൂണിംഗ് കമാൻഡ്
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --trust_remote_code \
  --output_path models/llama/ft \
  --data_name xxyyzzz/phrase_classification \
  --text_template "<|start_header_id|>user<|end_header_id|>\n{phrase}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n{tone}" \
  --method lora \
  --max_steps 100 \
  --log_level 1
```

#### ഘട്ടം 3: ഡിപ്ലോയ്മെന്റിനായി ഓപ്റ്റിമൈസ് ചെയ്യുക

```bash
# മെച്ചപ്പെടുത്തിയ ഇൻഫറൻസിനായി ONNX ഫോർമാറ്റിലേക്ക് മാറ്റുക
olive auto-opt \
  --model_name_or_path models/llama/ft/model \
  --adapter_path models/llama/ft/adapter \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --output_path models/llama/onnx \
  --log_level 1
```

### ഉദാഹരണം 2: ഇഷ്‌ടാനുസൃത ഡാറ്റാസെറ്റുമായി ഉന്നത കോൺഫിഗറേഷൻ

#### ഘട്ടം 1: ഇഷ്‌ടാനുസൃത ഡാറ്റാസെറ്റ് തയ്യാറാക്കുക

നിങ്ങളുടെ പരിശീലന ഡാറ്റയോടെ JSON ഫയൽ സൃഷ്ടിക്കുക:

```json
[
  {
    "input": "What is machine learning?",
    "output": "Machine learning is a subset of artificial intelligence that enables computers to learn and improve from experience without being explicitly programmed."
  },
  {
    "input": "Explain neural networks",
    "output": "Neural networks are computing systems inspired by biological neural networks that learn from data through interconnected nodes or neurons."
  }
]
```

#### ഘട്ടം 2: കോൺഫിഗറേഷൻ ഫയൽ സൃഷ്ടിക്കുക

```yaml
# olive-config.yaml
model:
  type: PyTorchModel
  config:
    model_path: "microsoft/DialoGPT-medium"
    task: "text-generation"

data_configs:
  - name: "custom_dataset"
    type: "HuggingfaceContainer"
    load_dataset_config:
      data_files: "path/to/your/dataset.json"
      split: "train"
    pre_process_data_config:
      text_template: "User: {input}\nAssistant: {output}"

passes:
  lora:
    type: LoRA
    config:
      r: 16
      lora_alpha: 32
      target_modules: ["c_attn", "c_proj"]
      modules_to_save: ["ln_f", "lm_head"]
```

#### ഘട്ടം 3: ഫൈൻ-ട്യൂണിംഗ് നടപ്പിലാക്കുക

```bash
# കസ്റ്റം കോൺഫിഗറേഷനോടെ പ്രവർത്തിപ്പിക്കുക
olive run --config olive-config.yaml --setup
```

### ഉദാഹരണം 3: മെമ്മറി കാര്യക്ഷമതയ്ക്കായി QLoRA ഫൈൻ-ട്യൂണിംഗ്

```bash
# മെമ്മറി കാര്യക്ഷമത മെച്ചപ്പെടുത്താൻ QLoRA ഉപയോഗിച്ച് ഫൈൻ-ട്യൂൺ ചെയ്യുക
olive finetune \
  --method qlora \
  --model_name_or_path meta-llama/Meta-Llama-3-8B \
  --data_name nampdn-ai/tiny-codes \
  --train_split "train[:4096]" \
  --eval_split "train[4096:4224]" \
  --text_template "### Language: {programming_language} \n### Question: {prompt} \n### Answer: {response}" \
  --per_device_train_batch_size 16 \
  --per_device_eval_batch_size 16 \
  --max_steps 150 \
  --logging_steps 50 \
  --output_path adapters/tiny-codes
```

## മികച്ച പ്രാക്ടീസുകളും മാർഗ്ഗനിർദ്ദേശങ്ങളും

### ഡാറ്റാ തയ്യാറെടുപ്പ്

**1. ഡാറ്റാ ഗുണമേന്മ പ്രധാനമാണ്**
- വലിയ അളവിലുള്ള മോശം ഡാറ്റയേക്കാൾ ഉയർന്ന ഗുണമേന്മയുള്ള, വൈവിധ്യമാർന്ന ഉദാഹരണങ്ങൾ മുൻഗണന നൽകുക
- ഡാറ്റ നിങ്ങളുടെ ലക്ഷ്യ ഉപയോഗത്തിന് പ്രതിനിധാനം ചെയ്യുന്നതായിരിക്കണം
- ഡാറ്റ സ്ഥിരമായി ശുദ്ധീകരിക്കുകയും പ്രീപ്രോസസ് ചെയ്യുകയും ചെയ്യുക

**2. ഡാറ്റാ ഫോർമാറ്റും ടെംപ്ലേറ്റുകളും**
- എല്ലാ പരിശീലന ഉദാഹരണങ്ങളിലും സ്ഥിരമായ ഫോർമാറ്റിംഗ് ഉപയോഗിക്കുക
- നിങ്ങളുടെ ഉപയോഗത്തിന് അനുയോജ്യമായ വ്യക്തമായ ഇൻപുട്ട്-ഔട്ട്പുട്ട് ടെംപ്ലേറ്റുകൾ സൃഷ്ടിക്കുക
- ഇൻസ്ട്രക്ഷൻ-ട്യൂൺ ചെയ്ത മോഡലുകൾക്കായി അനുയോജ്യമായ നിർദ്ദേശ ഫോർമാറ്റിംഗ് ഉൾപ്പെടുത്തുക

**3. ഡാറ്റാസെറ്റ് വിഭജനം**
- 10-20% ഡാറ്റ വാലിഡേഷനായി സംരക്ഷിക്കുക
- ട്രെയിൻ/വാലിഡേഷൻ വിഭജനം സമാനമായ വിതരണങ്ങൾ നിലനിർത്തുക
- വർഗ്ഗീകരണ ജോലികൾക്കായി സ്ട്രാറ്റിഫൈഡ് സാമ്പ്ലിംഗ് പരിഗണിക്കുക

### പരിശീലന കോൺഫിഗറേഷൻ

**1. ലേണിംഗ് റേറ്റ് തിരഞ്ഞെടുപ്പ്**
- ഫൈൻ-ട്യൂണിംഗിനായി ചെറിയ ലേണിംഗ് റേറ്റുകൾ (1e-5 മുതൽ 1e-4 വരെ) ഉപയോഗിച്ച് ആരംഭിക്കുക
- മികച്ച കൺവെർജൻസിനായി ലേണിംഗ് റേറ്റ് ഷെഡ്യൂളിംഗ് ഉപയോഗിക്കുക
- നഷ്ടം വളർച്ച നിരീക്ഷിച്ച് റേറ്റുകൾ ക്രമീകരിക്കുക

**2. ബാച്ച് സൈസ് ഓപ്റ്റിമൈസേഷൻ**
- ലഭ്യമായ മെമ്മറിയുമായി ബാച്ച് സൈസ് ബാലൻസ് ചെയ്യുക
- വലിയ ഫലപ്രദമായ ബാച്ച് സൈസിനായി ഗ്രേഡിയന്റ് അക്ക്യൂമുലേഷൻ ഉപയോഗിക്കുക
- ബാച്ച് സൈസും ലേണിംഗ് റേറ്റും തമ്മിലുള്ള ബന്ധം പരിഗണിക്കുക

**3. പരിശീലന ദൈർഘ്യം**
- ഓവർഫിറ്റിംഗ് ഒഴിവാക്കാൻ വാലിഡേഷൻ മെട്രിക്‌സ് നിരീക്ഷിക്കുക
- വാലിഡേഷൻ പ്രകടനം സ്ഥിരമായാൽ എർലി സ്റ്റോപ്പിംഗ് ഉപയോഗിക്കുക
- പുനരുദ്ധാരണത്തിനും വിശകലനത്തിനും ചेक്പോയിന്റുകൾ സ്ഥിരമായി സേവ് ചെയ്യുക

### മോഡൽ തിരഞ്ഞെടുപ്പ്

**1. അടിസ്ഥാന മോഡൽ തിരഞ്ഞെടുപ്പ്**
- സാധ്യമായപ്പോൾ സമാന ഡൊമെയ്‌നുകളിൽ മുൻകൂട്ടി പരിശീലിച്ച മോഡലുകൾ തിരഞ്ഞെടുക്കുക
- നിങ്ങളുടെ കംപ്യൂട്ടേഷൻ പരിധികൾക്ക് അനുയോജ്യമായ മോഡൽ വലിപ്പം പരിഗണിക്കുക
- വാണിജ്യ ഉപയോഗത്തിനുള്ള ലൈസൻസിംഗ് ആവശ്യകതകൾ വിലയിരുത്തുക

**2. ഫൈൻ-ട്യൂണിംഗ് രീതി തിരഞ്ഞെടുപ്പ്**
- സ്രോതസ്സ് പരിമിതമായ സാഹചര്യങ്ങൾക്ക് LoRA/QLoRA ഉപയോഗിക്കുക
- പരമാവധി പ്രകടനം ആവശ്യമായപ്പോൾ പൂർണ്ണ ഫൈൻ-ട്യൂണിംഗ് തിരഞ്ഞെടുക്കുക
- ബഹുജോലി സാഹചര്യങ്ങൾക്ക് അഡാപ്റ്റർ അടിസ്ഥാന സമീപനങ്ങൾ പരിഗണിക്കുക

### സ്രോതസ്സ് മാനേജ്മെന്റ്

**1. ഹാർഡ്‌വെയർ ഓപ്റ്റിമൈസേഷൻ**
- നിങ്ങളുടെ മോഡൽ വലിപ്പത്തിനും രീതിക്കും അനുയോജ്യമായ ഹാർഡ്‌വെയർ തിരഞ്ഞെടുക്കുക
- ഗ്രേഡിയന്റ് ചെക്ക്പോയിന്റിംഗ് ഉപയോഗിച്ച് GPU മെമ്മറി കാര്യക്ഷമമായി ഉപയോഗിക്കുക
- വലിയ മോഡലുകൾക്കായി ക്ലൗഡ് അടിസ്ഥാന പരിഹാരങ്ങൾ പരിഗണിക്കുക

**2. മെമ്മറി മാനേജ്മെന്റ്**
- ലഭ്യമായപ്പോൾ മിക്സഡ് പ്രിസിഷൻ പരിശീലനം ഉപയോഗിക്കുക
- മെമ്മറി പരിമിതികൾക്കായി ഗ്രേഡിയന്റ് അക്ക്യൂമുലേഷൻ നടപ്പിലാക്കുക
- പരിശീലന സമയത്ത് GPU മെമ്മറി ഉപയോഗം നിരീക്ഷിക്കുക

## ഉന്നത സാങ്കേതിക വിദ്യകൾ

### മൾട്ടി-അഡാപ്റ്റർ പരിശീലനം

അടിസ്ഥാന മോഡൽ പങ്കുവെച്ച് വ്യത്യസ്ത ജോലികൾക്കായി നിരവധി അഡാപ്റ്ററുകൾ പരിശീലിപ്പിക്കുക:

```bash
# ബഹുഭൂരിഭാഗം LoRA അഡാപ്റ്ററുകൾ പരിശീലിപ്പിക്കുക
olive finetune --method lora --task_name "classification" --output_path adapters/classifier
olive finetune --method lora --task_name "generation" --output_path adapters/generator

# ബഹുഅഡാപ്റ്റർ ONNX മോഡൽ സൃഷ്ടിക്കുക
olive generate-adapter \
  --base_model_path models/base \
  --adapter_paths adapters/classifier,adapters/generator \
  --output_path models/multi-adapter
```

### ഹൈപ്പർപാരാമീറ്റർ ഓപ്റ്റിമൈസേഷൻ

സിസ്റ്റമാറ്റിക് ഹൈപ്പർപാരാമീറ്റർ ട്യൂണിംഗ് നടപ്പിലാക്കുക:

```yaml
# hyperparameter-search.yaml
search_strategy:
  type: "random"
  num_trials: 20

search_space:
  learning_rate:
    type: "float"
    low: 1e-6
    high: 1e-3
    log: true
  
  lora_r:
    type: "int"
    low: 8
    high: 64
  
  batch_size:
    type: "choice"
    values: [8, 16, 32]
```

### ഇഷ്‌ടാനുസൃത നഷ്ട ഫംഗ്ഷനുകൾ

ഡൊമെയ്ൻ-നിർദ്ദിഷ്ട നഷ്ട ഫംഗ്ഷനുകൾ നടപ്പിലാക്കുക:

```python
# കസ്റ്റം_ലോസ്.py
import torch
import torch.nn as nn

class CustomContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(CustomContrastiveLoss, self).__init__()
        self.margin = margin
        
    def forward(self, output1, output2, label):
        euclidean_distance = nn.functional.pairwise_distance(output1, output2)
        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +
                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))
        return loss_contrastive
```

## മൂല്യനിർണ്ണയം & നിരീക്ഷണം

### മെട്രിക്‌സ് & മൂല്യനിർണ്ണയം

**1. സ്റ്റാൻഡേർഡ് മെട്രിക്‌സ്**
- **ശുദ്ധത**: വർഗ്ഗീകരണ ജോലികൾക്കുള്ള മൊത്തം ശരിയായതിന്റെ അളവ്
- **പർപ്ലെക്സിറ്റി**: ഭാഷാ മോഡലിംഗ് ഗുണനിലവാര അളവ്
- **BLEU/ROUGE**: ടെക്സ്റ്റ് സൃഷ്ടി, സംഗ്രഹം ഗുണനിലവാരം
- **F1 സ്കോർ**: വർഗ്ഗീകരണത്തിനുള്ള ബാലൻസ്ഡ് പ്രിസിഷനും റീക്കോളും

**2. ഡൊമെയ്ൻ-നിർദ്ദിഷ്ട മെട്രിക്‌സ്**
- **ജോലി-നിർദ്ദിഷ്ട ബെഞ്ച്മാർക്കുകൾ**: നിങ്ങളുടെ ഡൊമെയ്‌നിനുള്ള സ്ഥാപിത ബെഞ്ച്മാർക്കുകൾ ഉപയോഗിക്കുക
- **മാനവ മൂല്യനിർണ്ണയം**: വിഷയപരമായ ജോലികൾക്കായി മാനവ വിലയിരുത്തൽ ഉൾപ്പെടുത്തുക
- **ബിസിനസ് മെട്രിക്‌സ്**: യഥാർത്ഥ ബിസിനസ് ലക്ഷ്യങ്ങളുമായി പൊരുത്തപ്പെടുത്തുക

**3. മൂല്യനിർണ്ണയം സെറ്റപ്പ്**

```python
# evaluation_script.py
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import load_dataset
import torch

def evaluate_model(model_path, test_dataset, metric_type="accuracy"):
    """
    Evaluate fine-tuned model performance
    """
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)
    
    # വിലയിരുത്തൽ ലജിക് ഇവിടെ
    results = {}
    
    for example in test_dataset:
        # ഉദാഹരണം പ്രോസസ്സ് ചെയ്ത് മെട്രിക്കുകൾ കണക്കാക്കുക
        pass
    
    return results
```

### പരിശീലന പുരോഗതി നിരീക്ഷണം

**1. നഷ്ടം ട്രാക്കിംഗ്**

```bash
# വിശദമായ ലോഗിംഗ് സജ്ജമാക്കുക
olive finetune \
  --logging_steps 10 \
  --eval_steps 50 \
  --save_steps 100 \
  --logging_dir ./logs \
  --report_to tensorboard
```

**2. വാലിഡേഷൻ നിരീക്ഷണം**
- പരിശീലന നഷ്ടത്തോടൊപ്പം വാലിഡേഷൻ നഷ്ടം ട്രാക്ക് ചെയ്യുക
- ഓവർഫിറ്റിംഗ് ലക്ഷണങ്ങൾ നിരീക്ഷിക്കുക (വാലിഡേഷൻ നഷ്ടം വർദ്ധിക്കുന്നപ്പോൾ പരിശീലന നഷ്ടം കുറയുന്നത്)
- വാലിഡേഷൻ മെട്രിക്‌സിന്റെ അടിസ്ഥാനത്തിൽ എർലി സ്റ്റോപ്പിംഗ് ഉപയോഗിക്കുക

**3. സ്രോതസ്സ് നിരീക്ഷണം**
- GPU/CPU ഉപയോഗം നിരീക്ഷിക്കുക
- മെമ്മറി ഉപയോഗ പാറ്റേണുകൾ ട്രാക്ക് ചെയ്യുക
- പരിശീലന വേഗതയും ത്രൂപുട്ടും നിരീക്ഷിക്കുക

## സാധാരണ വെല്ലുവിളികളും പരിഹാരങ്ങളും

### വെല്ലുവിളി 1: ഓവർഫിറ്റിംഗ്

**ലക്ഷണങ്ങൾ:**
- പരിശീലന നഷ്ടം കുറയുമ്പോൾ വാലിഡേഷൻ നഷ്ടം വർദ്ധിക്കുന്നു
- പരിശീലനവും വാലിഡേഷനും പ്രകടനത്തിൽ വലിയ വ്യത്യാസം
- പുതിയ ഡാറ്റയിൽ മോശം ജനറലൈസേഷൻ

**പരിഹാരങ്ങൾ:**
```yaml
# Regularization techniques
passes:
  lora:
    type: LoRA
    config:
      r: 16  # Reduce rank to prevent overfitting
      lora_alpha: 16  # Lower alpha value
      lora_dropout: 0.1  # Add dropout
      weight_decay: 0.01  # L2 regularization
```

### വെല്ലുവിളി 2: മെമ്മറി പരിമിതികൾ

**പരിഹാരങ്ങൾ:**
- ഗ്രേഡിയന്റ് ചെക്ക്പോയിന്റിംഗ് ഉപയോഗിക്കുക
- ഗ്രേഡിയന്റ് അക്ക്യൂമുലേഷൻ നടപ്പിലാക്കുക
- പാരാമീറ്റർ-ക്ഷമതയുള്ള രീതികൾ (LoRA, QLoRA) തിരഞ്ഞെടുക്കുക
- വലിയ മോഡലുകൾക്കായി മോഡൽ പാരലലിസം ഉപയോഗിക്കുക

```bash
# മെമ്മറി-ക്ഷമമായ പരിശീലനം
olive finetune \
  --method qlora \
  --gradient_checkpointing true \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16
```

### വെല്ലുവിളി 3: മന്ദഗതിയിലുള്ള പരിശീലനം

**പരിഹാരങ്ങൾ:**
- ഡാറ്റാ ലോഡിംഗ് പൈപ്പ്‌ലൈൻകൾ ഓപ്റ്റിമൈസ് ചെയ്യുക
- മിക്സഡ് പ്രിസിഷൻ പരിശീലനം ഉപയോഗിക്കുക
- കാര്യക്ഷമമായ ബാച്ചിംഗ് തന്ത്രങ്ങൾ നടപ്പിലാക്കുക
- വലിയ ഡാറ്റാസെറ്റുകൾക്കായി വിതരണ പരിശീലനം പരിഗണിക്കുക

```yaml
# Performance optimization
training_config:
  fp16: true  # Mixed precision
  dataloader_num_workers: 4
  optim: "adamw_torch"
  lr_scheduler_type: "cosine"
```

### വെല്ലുവിളി 4: മോശം പ്രകടനം

**നിരീക്ഷണ ഘട്ടങ്ങൾ:**
1. ഡാറ്റാ ഗുണമേന്മയും ഫോർമാറ്റിംഗും പരിശോധിക്കുക
2. ലേണിംഗ് റേറ്റ്, പരിശീലന ദൈർഘ്യം പരിശോധിക്കുക
3. അടിസ്ഥാന മോഡൽ തിരഞ്ഞെടുപ്പ് വിലയിരുത്തുക
4. പ്രീപ്രോസസ്സിംഗ്, ടോക്കണൈസേഷൻ അവലോകനം ചെയ്യുക

**പരിഹാരങ്ങൾ:**
- പരിശീലന ഡാറ്റയുടെ വൈവിധ്യം വർദ്ധിപ്പിക്കുക
- ലേണിംഗ് റേറ്റ് ഷെഡ്യൂൾ ക്രമീകരിക്കുക
- വ്യത്യസ്ത അടിസ്ഥാന മോഡലുകൾ പരീക്ഷിക്കുക
- ഡാറ്റാ ഓഗ്മെന്റേഷൻ സാങ്കേതിക വിദ്യകൾ നടപ്പിലാക്കുക

## സംഗ്രഹം

ഫൈൻ-ട്യൂണിംഗ് ഒരു ശക്തമായ സാങ്കേതിക വിദ്യയാണ്, ഇത് ആധുനിക AI കഴിവുകൾക്ക് ജനപ്രവേശ്യം നൽകുന്നു. Microsoft Olive പോലുള്ള ഉപകരണങ്ങൾ ഉപയോഗിച്ച്, സ്ഥാപനങ്ങൾ അവരുടെ പ്രത്യേക ആവശ്യങ്ങൾക്കായി മുൻകൂട്ടി പരിശീലിച്ച മോഡലുകൾ കാര്യക്ഷമമായി ക്രമീകരിക്കാനും പ്രകടനവും സ്രോതസ്സ് പരിമിതികളും പരിഗണിച്ച് ഓപ്റ്റിമൈസ് ചെയ്യാനും കഴിയും.

### പ്രധാനപ്പെട്ട കാര്യങ്ങൾ

1. **ശരിയായ സമീപനം തിരഞ്ഞെടുക്കുക**: നിങ്ങളുടെ കംപ്യൂട്ടേഷൻ സ്രോതസ്സുകളും പ്രകടന ആവശ്യകതകളും അടിസ്ഥാനമാക്കി ഫൈൻ-ട്യൂണിംഗ് രീതി തിരഞ്ഞെടുക്കുക
2. **ഡാറ്റാ ഗുണമേന്മ പ്രധാനമാണ്**: ഉയർന്ന ഗുണമേന്മയുള്ള, പ്രതിനിധാനപരമായ പരിശീലന ഡാറ്റയിൽ നിക്ഷേപിക്കുക
3. **നിരീക്ഷിക്കുകയും പുനരാവർത്തിക്കുകയും ചെയ്യുക**: നിങ്ങളുടെ മോഡലുകൾ തുടർച്ചയായി മൂല്യനിർണ്ണയിച്ച് മെച്ചപ്പെടുത്തുക
4. **ഉപകരണങ്ങൾ ഉപയോഗിക്കുക**: പ്രക്രിയ ലളിതമാക്കാനും ഓപ്റ്റിമൈസ് ചെയ്യാനും Olive പോലുള്ള ഫ്രെയിംവർക്ക് ഉപയോഗിക്കുക
5. **ഡിപ്ലോയ്മെന്റ് പരിഗണിക്കുക**: തുടക്കത്തിൽ തന്നെ മോഡൽ ഓപ്റ്റിമൈസേഷനും ഡിപ്ലോയ്മെന്റും പദ്ധതിയിടുക


## ➡️ അടുത്തത് എന്താണ്

- [04: ഡിപ്ലോയ്മെന്റ് - പ്രൊഡക്ഷൻ-റെഡി മോഡൽ നടപ്പാക്കൽ](./04.SLMOps.Deployment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**അസൂയാപത്രം**:  
ഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖ അധികാരപരമായ ഉറവിടമായി കണക്കാക്കപ്പെടണം. നിർണായക വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനത്തിന്റെ ഉപയോഗത്തിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ തെറ്റായ വ്യാഖ്യാനങ്ങൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->