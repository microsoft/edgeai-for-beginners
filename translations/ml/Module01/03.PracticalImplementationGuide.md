# ഭാഗം 3: പ്രായോഗിക നടപ്പാക്കൽ ഗൈഡ്

## അവലോകനം

എഡ്ജ് ഡിവൈസുകളിൽ കാര്യക്ഷമമായി പ്രവർത്തിക്കുന്ന പ്രായോഗിക AI പരിഹാരങ്ങൾ നിർമ്മിക്കുന്നതിൽ കേന്ദ്രീകരിക്കുന്ന EdgeAI കോഴ്സിനായി നിങ്ങൾ തയ്യാറെടുക്കാൻ ഈ സമഗ്ര ഗൈഡ് സഹായിക്കും. കോഴ്സ് ആധുനിക ഫ്രെയിംവർക്കുകളും എഡ്ജ് ഡിപ്ലോയ്മെന്റിനായി ഒപ്റ്റിമൈസ് ചെയ്ത സ്റ്റേറ്റ്-ഓഫ്-ദി-ആർട്ട് മോഡലുകളും ഉപയോഗിച്ച് ഹാൻഡ്‌സ്-ഓൺ ഡെവലപ്മെന്റിനെ ഊന്നിപ്പറയുന്നു.

## 1. ഡെവലപ്മെന്റ് പരിസ്ഥിതി സജ്ജീകരണം

### പ്രോഗ്രാമിംഗ് ഭാഷകളും ഫ്രെയിംവർക്കുകളും

**Python പരിസ്ഥിതി**
- **വർഷൻ**: Python 3.10 അല്ലെങ്കിൽ അതിനുമുകളിൽ (ശുപാർശ: Python 3.11)
- **പാക്കേജ് മാനേജർ**: pip അല്ലെങ്കിൽ conda
- **വിർച്വൽ പരിസ്ഥിതി**: ഐസൊലേഷനായി venv അല്ലെങ്കിൽ conda പരിസ്ഥിതികൾ ഉപയോഗിക്കുക
- **പ്രധാന ലൈബ്രറികൾ**: കോഴ്സ് സമയത്ത് പ്രത്യേക EdgeAI ലൈബ്രറികൾ ഇൻസ്റ്റാൾ ചെയ്യും

**Microsoft .NET പരിസ്ഥിതി**
- **വർഷൻ**: .NET 8 അല്ലെങ്കിൽ അതിനുമുകളിൽ
- **IDE**: Visual Studio 2022, Visual Studio Code, അല്ലെങ്കിൽ JetBrains Rider
- **SDK**: ക്രോസ്-പ്ലാറ്റ്ഫോം ഡെവലപ്മെന്റിനായി .NET SDK ഇൻസ്റ്റാൾ ചെയ്തിട്ടുണ്ടെന്ന് ഉറപ്പാക്കുക

### ഡെവലപ്മെന്റ് ടൂളുകൾ

**കോഡ് എഡിറ്ററുകളും IDEകളും**
- Visual Studio Code (ക്രോസ്-പ്ലാറ്റ്ഫോം ഡെവലപ്മെന്റിനായി ശുപാർശ)
- PyCharm അല്ലെങ്കിൽ Visual Studio (ഭാഷ-നിർദിഷ്ട ഡെവലപ്മെന്റിനായി)
- Jupyter Notebooks ഇന്ററാക്ടീവ് ഡെവലപ്മെന്റിനും പ്രോട്ടോടൈപ്പിംഗിനും

**വർഷൻ കൺട്രോൾ**
- Git (അടുത്തകാലത്തെ പതിപ്പ്)
- GitHub അക്കൗണ്ട് റിപോസിറ്ററികൾ ആക്‌സസ് ചെയ്യാനും സഹകരണത്തിനും

## 2. ഹാർഡ്‌വെയർ ആവശ്യകതകളും ശുപാർശകളും

### കുറഞ്ഞ സിസ്റ്റം ആവശ്യകതകൾ
- **CPU**: മൾട്ടി-കോർ പ്രോസസർ (Intel i5/AMD Ryzen 5 അല്ലെങ്കിൽ തുല്യമായത്)
- **RAM**: കുറഞ്ഞത് 8GB, ശുപാർശ 16GB
- **സ്റ്റോറേജ്**: മോഡലുകൾക്കും ഡെവലപ്മെന്റ് ടൂളുകൾക്കും 50GB ലഭ്യമായ സ്ഥലം
- **ഓഎസ്**: Windows 10/11, macOS 10.15+, അല്ലെങ്കിൽ Linux (Ubuntu 20.04+)

### കംപ്യൂട്ട് റിസോഴ്‌സസ് തന്ത്രം
വിവിധ ഹാർഡ്‌വെയർ കോൺഫിഗറേഷനുകളിൽ ആക്സസിബിൾ ആക്കാൻ കോഴ്സ് രൂപകൽപ്പന ചെയ്തിരിക്കുന്നു:

**ലോകൽ ഡെവലപ്മെന്റ് (CPU/NPU ഫോകസ്)**
- പ്രാഥമിക ഡെവലപ്മെന്റ് CPU, NPU ആക്സിലറേഷൻ ഉപയോഗിച്ച് നടത്തും
- മിക്ക ആധുനിക ലാപ്‌ടോപ്പുകളും ഡെസ്ക്ടോപ്പുകളും അനുയോജ്യം
- കാര്യക്ഷമതയിലും പ്രായോഗിക ഡിപ്ലോയ്മെന്റ് സീനാരിയോകളിലും ശ്രദ്ധ കേന്ദ്രീകരിക്കുക

**ക്ലൗഡ് GPU റിസോഴ്‌സുകൾ (ഐച്ഛികം)**
- **Azure Machine Learning**: തീവ്രമായ ട്രെയിനിംഗിനും പരീക്ഷണത്തിനും
- **Google Colab**: വിദ്യാഭ്യാസ ആവശ്യങ്ങൾക്കായി സൗജന്യ ടിയർ ലഭ്യമാണ്
- **Kaggle Notebooks**: ബദൽ ക്ലൗഡ് കംപ്യൂട്ടിംഗ് പ്ലാറ്റ്ഫോം

### എഡ്ജ് ഡിവൈസ് പരിഗണനകൾ
- ARM അടിസ്ഥാന പ്രോസസറുകളുടെ മനസ്സിലാക്കൽ
- മൊബൈൽ, IoT ഹാർഡ്‌വെയർ നിയന്ത്രണങ്ങളുടെ അറിവ്
- പവർ കൺസംപ്ഷൻ ഒപ്റ്റിമൈസേഷൻ പരിചയം

## 3. കോർ മോഡൽ കുടുംബങ്ങളും റിസോഴ്‌സുകളും

### പ്രാഥമിക മോഡൽ കുടുംബങ്ങൾ

**Microsoft Phi-4 കുടുംബം**
- **വിവരണം**: എഡ്ജ് ഡിപ്ലോയ്മെന്റിനായി രൂപകൽപ്പന ചെയ്ത കംപാക്റ്റ്, കാര്യക്ഷമ മോഡലുകൾ
- **ശക്തികൾ**: മികച്ച പ്രകടനവും വലുപ്പവും അനുപാതം, റീസണിംഗ് ടാസ്കുകൾക്ക് ഒപ്റ്റിമൈസ് ചെയ്തത്
- **റിസോഴ്‌സ്**: [Phi-4 Collection on Hugging Face](https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4)
- **ഉപയോഗ കേസുകൾ**: കോഡ് ജനറേഷൻ, ഗണിത റീസണിംഗ്, പൊതുവായ സംഭാഷണം

**Qwen-3 കുടുംബം**
- **വിവരണം**: അലി‌ബാബയുടെ ഏറ്റവും പുതിയ ബഹുഭാഷാ മോഡൽ തലമുറ
- **ശക്തികൾ**: ശക്തമായ ബഹുഭാഷാ കഴിവുകൾ, കാര്യക്ഷമ ആർക്കിടെക്ചർ
- **റിസോഴ്‌സ്**: [Qwen-3 Collection on Hugging Face](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f)
- **ഉപയോഗ കേസുകൾ**: ബഹുഭാഷാ ആപ്ലിക്കേഷനുകൾ, സാംസ്‌കാരിക AI പരിഹാരങ്ങൾ

**Google Gemma-3n കുടുംബം**
- **വിവരണം**: എഡ്ജ് ഡിപ്ലോയ്മെന്റിനായി ഒപ്റ്റിമൈസ് ചെയ്ത ഗൂഗിളിന്റെ ലഘു മോഡലുകൾ
- **ശക്തികൾ**: വേഗത്തിലുള്ള ഇൻഫറൻസ്, മൊബൈൽ-ഫ്രണ്ട്ലി ആർക്കിടെക്ചർ
- **റിസോഴ്‌സ്**: [Gemma-3n Collection on Hugging Face](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **ഉപയോഗ കേസുകൾ**: മൊബൈൽ ആപ്ലിക്കേഷനുകൾ, റിയൽ-ടൈം പ്രോസസ്സിംഗ്

### മോഡൽ തിരഞ്ഞെടുപ്പ് മാനദണ്ഡങ്ങൾ
- **പ്രകടനം vs വലുപ്പം ട്രേഡ്-ഓഫുകൾ**: ചെറിയ മോഡലുകൾ തിരഞ്ഞെടുക്കേണ്ട സമയവും വലിയ മോഡലുകൾ തിരഞ്ഞെടുക്കേണ്ട സമയവും മനസ്സിലാക്കൽ
- **ടാസ്‌ക്-സ്പെസിഫിക് ഒപ്റ്റിമൈസേഷൻ**: പ്രത്യേക ഉപയോഗ കേസുകൾക്കായി മോഡലുകൾ പൊരുത്തപ്പെടുത്തൽ
- **ഡിപ്ലോയ്മെന്റ് നിയന്ത്രണങ്ങൾ**: മെമ്മറി, ലാറ്റൻസി, പവർ കൺസംപ്ഷൻ പരിഗണനകൾ

## 4. ക്വാണ്ടൈസേഷൻ & ഒപ്റ്റിമൈസേഷൻ ടൂളുകൾ

### Llama.cpp ഫ്രെയിംവർക്ക്
- **റിപ്പോസിറ്ററി**: [Llama.cpp on GitHub](https://github.com/ggml-org/llama.cpp)
- **ഉദ്ദേശ്യം**: LLMs-ക്കായി ഉയർന്ന പ്രകടന ഇൻഫറൻസ് എഞ്ചിൻ
- **പ്രധാന സവിശേഷതകൾ**:
  - CPU-ഓപ്റ്റിമൈസ് ചെയ്ത ഇൻഫറൻസ്
  - ബഹുവിധ ക്വാണ്ടൈസേഷൻ ഫോർമാറ്റുകൾ (Q4, Q5, Q8)
  - ക്രോസ്-പ്ലാറ്റ്ഫോം അനുയോജ്യത
  - മെമ്മറി കാര്യക്ഷമമായ എക്സിക്യൂഷൻ
- **ഇൻസ്റ്റലേഷൻ & അടിസ്ഥാന ഉപയോഗം**:
  ```bash
  # റിപ്പോസിറ്ററി ക്ലോൺ ചെയ്യുക
  git clone https://github.com/ggml-org/llama.cpp.git
  cd llama.cpp
  
  # ഓപ്റ്റിമൈസേഷനുകളോടെ പ്രോജക്ട് നിർമ്മിക്കുക
  mkdir build && cd build
  cmake .. -DCMAKE_BUILD_TYPE=Release
  cmake --build . --config Release
  
  # ഒരു മോഡൽ ക്വാണ്ടൈസ് ചെയ്യുക (GGUF ഫോർമാറ്റിൽ നിന്ന് 4-ബിറ്റ് ക്വാണ്ടൈസേഷനിലേക്ക്)
  ./quantize ../models/original-model.gguf ../models/quantized-model-q4_0.gguf q4_0
  
  # ക്വാണ്ടൈസ്ഡ് മോഡലുമായി ഇൻഫറൻസ് നടത്തുക
  ./main -m ../models/quantized-model-q4_0.gguf -n 512 -p "Write a function to calculate fibonacci numbers in Python:"
  ```

### Microsoft Olive
- **റിപ്പോസിറ്ററി**: [Microsoft Olive on GitHub](https://github.com/microsoft/olive)
- **ഉദ്ദേശ്യം**: എഡ്ജ് ഡിപ്ലോയ്മെന്റിനായി മോഡൽ ഒപ്റ്റിമൈസേഷൻ ടൂൾകിറ്റ്
- **പ്രധാന സവിശേഷതകൾ**:
  - ഓട്ടോമേറ്റഡ് മോഡൽ ഒപ്റ്റിമൈസേഷൻ വർക്ക്‌ഫ്ലോകൾ
  - ഹാർഡ്‌വെയർ-അവെയർ ഒപ്റ്റിമൈസേഷൻ
  - ONNX Runtime-നൊപ്പം ഇന്റഗ്രേഷൻ
  - പ്രകടന ബഞ്ച്മാർക്കിംഗ് ടൂളുകൾ
- **ഇൻസ്റ്റലേഷൻ & അടിസ്ഥാന ഉപയോഗം**:
  ```bash
  # ഒലീവ് ഇൻസ്റ്റാൾ ചെയ്യുക
  pip install olive-ai
  ```
  
  # മോഡൽ ഒപ്റ്റിമൈസേഷനിനുള്ള ഉദാഹരണ Python സ്ക്രിപ്റ്റ്
  ```python
  from olive.model import ONNXModel
  from olive.workflows import run_workflow
  
  # മോഡൽ மற்றும் ഓപ്റ്റിമൈസേഷൻ കോൺഫിഗർ നിർവചിക്കുക
  model = ONNXModel("original_model.onnx")
  config = {
      "input_model": model,
      "systems": {
          "local_system": {
              "type": "LocalSystem"
          }
      },
      "engine": {
          "log_severity_level": 0,
          "cache_dir": "cache"
      },
      "passes": {
          "quantization": {
              "type": "OrtQuantization",
              "config": {
                  "quant_mode": "static",
                  "activation_type": "int8",
                  "weight_type": "int8"
              }
          }
      }
  }
  
  # ഓപ്റ്റിമൈസേഷൻ വർക്ക്‌ഫ്ലോ പ്രവർത്തിപ്പിക്കുക
  result = run_workflow(config)
  optimized_model = result.optimized_model
  
  # ഓപ്റ്റിമൈസ്ഡ് മോഡൽ സേവ് ചെയ്യുക
  optimized_model.save("optimized_model.onnx")
  ```

### Apple MLX (macOS ഉപയോക്താക്കൾക്ക്)
- **റിപ്പോസിറ്ററി**: [Apple MLX on GitHub](https://github.com/ml-explore/mlx)
- **ഉദ്ദേശ്യം**: Apple Silicon-ക്കായി മെഷീൻ ലേണിംഗ് ഫ്രെയിംവർക്ക്
- **പ്രധാന സവിശേഷതകൾ**:
  - നേറ്റീവ് Apple Silicon ഒപ്റ്റിമൈസേഷൻ
  - മെമ്മറി കാര്യക്ഷമമായ ഓപ്പറേഷനുകൾ
  - PyTorch പോലുള്ള API
  - യൂണിഫൈഡ് മെമ്മറി ആർക്കിടെക്ചർ പിന്തുണ
- **ഇൻസ്റ്റലേഷൻ & അടിസ്ഥാന ഉപയോഗം**:
  ```bash
  # MLX ഇൻസ്റ്റാൾ ചെയ്യുക
  pip install mlx
  ```
  
  ```python
  # മോഡൽ ലോഡ് ചെയ്യാനും ഓപ്റ്റിമൈസ് ചെയ്യാനും ഉദാഹരണ പൈതൺ സ്ക്രിപ്റ്റ്
  import mlx.core as mx
  import mlx.nn as nn
  from mlx.utils import tree_flatten
  
  # മുൻകൂട്ടി പരിശീലിപ്പിച്ച വെയ്റ്റുകൾ ലോഡ് ചെയ്യുക (സാധാരണ MLP ഉപയോഗിച്ച് ഉദാഹരണം)
  class MLP(nn.Module):
      def __init__(self, dim=768, hidden_dim=3072):
          super().__init__()
          self.fc1 = nn.Linear(dim, hidden_dim)
          self.fc2 = nn.Linear(hidden_dim, dim)
          
      def __call__(self, x):
          return self.fc2(mx.maximum(0, self.fc1(x)))
  
  # മോഡൽ സൃഷ്ടിച്ച് വെയ്റ്റുകൾ ലോഡ് ചെയ്യുക
  model = MLP()
  weights = mx.load("original_weights.npz")
  model.update(weights)
  
  # മോഡൽ വെയ്റ്റുകൾ FP16 ആയി ക്വാണ്ടൈസ് ചെയ്യുക
  def quantize_weights(model):
      params = {}
      for k, v in tree_flatten(model.parameters()):
          params[k] = v.astype(mx.float16)
      model.update(params)
      return model
  
  quantized_model = quantize_weights(model)
  
  # ക്വാണ്ടൈസ് ചെയ്ത മോഡൽ സേവ് ചെയ്യുക
  mx.save("quantized_model.npz", quantized_model.parameters())
  
  # ഇൻഫറൻസ് നടത്തുക
  input_data = mx.random.normal((1, 768))
  output = quantized_model(input_data)
  ```

### ONNX Runtime
- **റിപ്പോസിറ്ററി**: [ONNX Runtime on GitHub](https://github.com/microsoft/onnxruntime)
- **ഉദ്ദേശ്യം**: ONNX മോഡലുകൾക്കായി ക്രോസ്-പ്ലാറ്റ്ഫോം ഇൻഫറൻസ് ആക്സിലറേഷൻ
- **പ്രധാന സവിശേഷതകൾ**:
  - ഹാർഡ്‌വെയർ-സ്പെസിഫിക് ഒപ്റ്റിമൈസേഷനുകൾ (CPU, GPU, NPU)
  - ഇൻഫറൻസിനുള്ള ഗ്രാഫ് ഒപ്റ്റിമൈസേഷനുകൾ
  - ക്വാണ്ടൈസേഷൻ പിന്തുണ
  - ക്രോസ്-ഭാഷാ പിന്തുണ (Python, C++, C#, JavaScript)
- **ഇൻസ്റ്റലേഷൻ & അടിസ്ഥാന ഉപയോഗം**:
  ```bash
  # ONNX റൺടൈം ഇൻസ്റ്റാൾ ചെയ്യുക
  pip install onnxruntime
  
  # GPU പിന്തുണയ്ക്കായി
  pip install onnxruntime-gpu
  ```
  
  ```python
  import onnxruntime as ort
  import numpy as np
  
  # മെച്ചപ്പെടുത്തലുകളോടെ ഇൻഫറൻസ് സെഷൻ സൃഷ്ടിക്കുക
  sess_options = ort.SessionOptions()
  sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
  sess_options.enable_profiling = True  # പ്രകടന പ്രൊഫൈലിംഗ് സജീവമാക്കുക
  
  # ഹാർഡ്‌വെയർ ആക്സലറേഷനായി പ്രൊവൈഡർ തിരഞ്ഞെടുക്കുന്നതോടെ സെഷൻ സൃഷ്ടിക്കുക
  providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']  # GPU ലഭ്യമായാൽ ഉപയോഗിക്കുക
  session = ort.InferenceSession("model.onnx", sess_options, providers=providers)
  
  # ഇൻപുട്ട് ഡാറ്റ തയ്യാറാക്കുക
  input_name = session.get_inputs()[0].name
  input_shape = session.get_inputs()[0].shape
  input_data = np.random.rand(*input_shape).astype(np.float32)
  
  # ഇൻഫറൻസ് നടത്തുക
  outputs = session.run(None, {input_name: input_data})
  
  # പ്രൊഫൈലിംഗ് ഡാറ്റ നേടുക
  prof_file = session.end_profiling()
  print(f"Profiling data saved to: {prof_file}")
  ```
## 5. ശുപാർശ ചെയ്ത വായനയും റിസോഴ്‌സുകളും

### അനിവാര്യ ഡോക്യുമെന്റേഷൻ
- **ONNX Runtime ഡോക്യുമെന്റേഷൻ**: ക്രോസ്-പ്ലാറ്റ്ഫോം ഇൻഫറൻസ് മനസ്സിലാക്കൽ
- **Hugging Face Transformers ഗൈഡ്**: മോഡൽ ലോഡിംഗും ഇൻഫറൻസും
- **Edge AI ഡിസൈൻ പാറ്റേണുകൾ**: എഡ്ജ് ഡിപ്ലോയ്മെന്റിനുള്ള മികച്ച പ്രാക്ടീസുകൾ

### സാങ്കേതിക പേപ്പറുകൾ
- "Efficient Edge AI: A Survey of Quantization Techniques"
- "Model Compression for Mobile and Edge Devices"
- "Optimizing Transformer Models for Edge Computing"

### കമ്മ്യൂണിറ്റി റിസോഴ്‌സുകൾ
- **EdgeAI Slack/Discord കമ്മ്യൂണിറ്റികൾ**: കൂട്ടായ്മ പിന്തുണയും ചർച്ചകളും
- **GitHub റിപോസിറ്ററികൾ**: ഉദാഹരണ നടപ്പാക്കലുകളും ട്യൂട്ടോറിയലുകളും
- **YouTube ചാനലുകൾ**: സാങ്കേതിക ഡീപ്-ഡൈവും ട്യൂട്ടോറിയലുകളും

## 6. അസസ്മെന്റ് & സ്ഥിരീകരണം

### പ്രീ-കോഴ്സ് ചെക്ക്ലിസ്റ്റ്
- [ ] Python 3.10+ ഇൻസ്റ്റാൾ ചെയ്ത് സ്ഥിരീകരിച്ചു
- [ ] .NET 8+ ഇൻസ്റ്റാൾ ചെയ്ത് സ്ഥിരീകരിച്ചു
- [ ] ഡെവലപ്മെന്റ് പരിസ്ഥിതി കോൺഫിഗർ ചെയ്തു
- [ ] Hugging Face അക്കൗണ്ട് സൃഷ്ടിച്ചു
- [ ] ലക്ഷ്യ മോഡൽ കുടുംബങ്ങളുമായി അടിസ്ഥാന പരിചയം
- [ ] ക്വാണ്ടൈസേഷൻ ടൂളുകൾ ഇൻസ്റ്റാൾ ചെയ്ത് പരീക്ഷിച്ചു
- [ ] ഹാർഡ്‌വെയർ ആവശ്യകതകൾ പാലിച്ചു
- [ ] ക്ലൗഡ് കംപ്യൂട്ടിംഗ് അക്കൗണ്ടുകൾ സജ്ജമാക്കി (ആവശ്യമായാൽ)

## പ്രധാന പഠന ലക്ഷ്യങ്ങൾ

ഈ ഗൈഡ് അവസാനിപ്പിക്കുമ്പോൾ, നിങ്ങൾക്ക് കഴിയും:

1. EdgeAI ആപ്ലിക്കേഷൻ ഡെവലപ്മെന്റിനായി സമ്പൂർണ ഡെവലപ്മെന്റ് പരിസ്ഥിതി സജ്ജമാക്കുക
2. മോഡൽ ഒപ്റ്റിമൈസേഷനിനുള്ള ആവശ്യമായ ടൂളുകളും ഫ്രെയിംവർക്കുകളും ഇൻസ്റ്റാൾ ചെയ്ത് കോൺഫിഗർ ചെയ്യുക
3. നിങ്ങളുടെ EdgeAI പ്രോജക്ടുകൾക്കായി അനുയോജ്യമായ ഹാർഡ്‌വെയർ, സോഫ്റ്റ്വെയർ കോൺഫിഗറേഷനുകൾ തിരഞ്ഞെടുക്കുക
4. എഡ്ജ് ഡിവൈസുകളിൽ AI മോഡലുകൾ ഡിപ്ലോയ് ചെയ്യുന്നതിനുള്ള പ്രധാന പരിഗണനകൾ മനസ്സിലാക്കുക
5. കോഴ്സിലെ ഹാൻഡ്‌സ്-ഓൺ അഭ്യാസങ്ങൾക്ക് നിങ്ങളുടെ സിസ്റ്റം തയ്യാറാക്കുക

## അധിക റിസോഴ്‌സുകൾ

### ഔദ്യോഗിക ഡോക്യുമെന്റേഷൻ
- **Python ഡോക്യുമെന്റേഷൻ**: ഔദ്യോഗിക Python ഭാഷ ഡോക്യുമെന്റേഷൻ
- **Microsoft .NET ഡോക്യുമെന്റേഷൻ**: ഔദ്യോഗിക .NET ഡെവലപ്മെന്റ് റിസോഴ്‌സുകൾ
- **ONNX Runtime ഡോക്യുമെന്റേഷൻ**: ONNX Runtime-നുള്ള സമഗ്ര ഗൈഡ്
- **TensorFlow Lite ഡോക്യുമെന്റേഷൻ**: ഔദ്യോഗിക TensorFlow Lite ഡോക്യുമെന്റേഷൻ

### ഡെവലപ്മെന്റ് ടൂളുകൾ
- **Visual Studio Code**: AI ഡെവലപ്മെന്റ് എക്സ്റ്റൻഷനുകളുള്ള ലഘു കോഡ് എഡിറ്റർ
- **Jupyter Notebooks**: ML പരീക്ഷണത്തിനുള്ള ഇന്ററാക്ടീവ് കംപ്യൂട്ടിംഗ് പരിസ്ഥിതി
- **Docker**: സ്ഥിരതയുള്ള ഡെവലപ്മെന്റ് പരിസ്ഥിതികൾക്കുള്ള കണ്ടെയ്‌നറൈസേഷൻ പ്ലാറ്റ്ഫോം
- **Git**: കോഡ് മാനേജ്മെന്റിനുള്ള വർഷൻ കൺട്രോൾ സിസ്റ്റം

### പഠന റിസോഴ്‌സുകൾ
- **EdgeAI ഗവേഷണ പേപ്പറുകൾ**: കാര്യക്ഷമ മോഡലുകളിലെ ഏറ്റവും പുതിയ അക്കാദമിക് ഗവേഷണം
- **ഓൺലൈൻ കോഴ്സുകൾ**: AI ഒപ്റ്റിമൈസേഷനിൽ സഹായകമായ പഠന സാമഗ്രികൾ
- **കമ്മ്യൂണിറ്റി ഫോറങ്ങൾ**: EdgeAI ഡെവലപ്മെന്റ് വെല്ലുവിളികൾക്കുള്ള Q&A പ്ലാറ്റ്ഫോമുകൾ
- **ബഞ്ച്മാർക്ക് ഡാറ്റാസെറ്റുകൾ**: മോഡൽ പ്രകടനം വിലയിരുത്തുന്നതിനുള്ള സ്റ്റാൻഡേർഡ് ഡാറ്റാസെറ്റുകൾ

## പഠന ഫലങ്ങൾ

ഈ തയ്യാറെടുപ്പ് ഗൈഡ് പൂർത്തിയാക്കിയ ശേഷം, നിങ്ങൾക്ക്:

1. EdgeAI ഡെവലപ്മെന്റിനായി പൂർണ്ണമായി കോൺഫിഗർ ചെയ്ത ഡെവലപ്മെന്റ് പരിസ്ഥിതി ഉണ്ടാകും
2. വ്യത്യസ്ത ഡിപ്ലോയ്മെന്റ് സീനാരിയോകൾക്കുള്ള ഹാർഡ്‌വെയർ, സോഫ്റ്റ്വെയർ ആവശ്യകതകൾ മനസ്സിലാകും
3. കോഴ്സിൽ ഉപയോഗിക്കുന്ന പ്രധാന ഫ്രെയിംവർക്കുകളും ടൂളുകളും പരിചിതമാകും
4. ഡിവൈസ് നിയന്ത്രണങ്ങളും ആവശ്യകതകളും അടിസ്ഥാനമാക്കി അനുയോജ്യമായ മോഡലുകൾ തിരഞ്ഞെടുക്കാൻ കഴിയും
5. എഡ്ജ് ഡിപ്ലോയ്മെന്റിനുള്ള ഒപ്റ്റിമൈസേഷൻ സാങ്കേതികവിദ്യകളുടെ അടിസ്ഥാന അറിവ് ഉണ്ടാകും

## ➡️ അടുത്തത് എന്താണ്

- [04: EdgeAI Hardware and Deployment](04.EdgeDeployment.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**അസൂയാ**:  
ഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖ അധികാരപരമായ ഉറവിടമായി കണക്കാക്കപ്പെടണം. നിർണായക വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനത്തിന്റെ ഉപയോഗത്തിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ തെറ്റായ വ്യാഖ്യാനങ്ങൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->