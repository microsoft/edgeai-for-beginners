<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-12-15T23:08:41+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "ml"
}
-->
# Section 3 : Microsoft Olive Optimization Suite

## Table of Contents
1. [പരിചയം](../../../Module04)
2. [Microsoft Olive എന്താണ്?](../../../Module04)
3. [ഇൻസ്റ്റലേഷൻ](../../../Module04)
4. [ക്വിക്ക് സ്റ്റാർട്ട് ഗൈഡ്](../../../Module04)
5. [ഉദാഹരണം: Qwen3-നെ ONNX INT4-ലേക്ക് മാറ്റൽ](../../../Module04)
6. [അഡ്വാൻസ്ഡ് ഉപയോഗം](../../../Module04)
7. [Olive റെസിപ്പികൾ റിപോസിറ്ററി](../../../Module04)
8. [മികച്ച പ്രാക്ടീസുകൾ](../../../Module04)
9. [പ്രശ്നപരിഹാരം](../../../Module04)
10. [കൂടുതൽ വിഭവങ്ങൾ](../../../Module04)

## Introduction

Microsoft Olive ഒരു ശക്തമായ, ഉപയോഗിക്കാൻ എളുപ്പമുള്ള ഹാർഡ്‌വെയർ-അവെയർ മോഡൽ ഓപ്റ്റിമൈസേഷൻ ടൂൾകിറ്റ് ആണ്, വിവിധ ഹാർഡ്‌വെയർ പ്ലാറ്റ്‌ഫോമുകളിൽ വിന്യസിക്കാനുള്ള മെഷീൻ ലേണിംഗ് മോഡലുകൾ ഓപ്റ്റിമൈസ് ചെയ്യാനുള്ള പ്രക്രിയ ലളിതമാക്കുന്നു. നിങ്ങൾ CPU, GPU, അല്ലെങ്കിൽ പ്രത്യേക AI ആക്സിലറേറ്ററുകൾ ലക്ഷ്യമിടുകയാണെങ്കിൽ, Olive മോഡൽ കൃത്യത നിലനിർത്തിക്കൊണ്ട് മികച്ച പ്രകടനം നേടാൻ സഹായിക്കുന്നു.

## What is Microsoft Olive?

Olive ഒരു ഉപയോഗിക്കാൻ എളുപ്പമുള്ള ഹാർഡ്‌വെയർ-അവെയർ മോഡൽ ഓപ്റ്റിമൈസേഷൻ ടൂൾ ആണ്, മോഡൽ കംപ്രഷൻ, ഓപ്റ്റിമൈസേഷൻ, കോമ്പൈലേഷൻ എന്നിവയിൽ വ്യവസായത്തിലെ മുൻനിര സാങ്കേതിക വിദ്യകൾ സംയോജിപ്പിക്കുന്നു. ഇത് ONNX Runtime-നൊപ്പം E2E ഇൻഫറൻസ് ഓപ്റ്റിമൈസേഷൻ പരിഹാരമായി പ്രവർത്തിക്കുന്നു.

### പ്രധാന സവിശേഷതകൾ

- **ഹാർഡ്‌വെയർ-അവെയർ ഓപ്റ്റിമൈസേഷൻ**: നിങ്ങളുടെ ലക്ഷ്യ ഹാർഡ്‌വെയറിനായി മികച്ച ഓപ്റ്റിമൈസേഷൻ സാങ്കേതിക വിദ്യകൾ സ്വയം തിരഞ്ഞെടുക്കുന്നു
- **40+ ഇൻബിൽറ്റ് ഓപ്റ്റിമൈസേഷൻ ഘടകങ്ങൾ**: മോഡൽ കംപ്രഷൻ, ക്വാണ്ടൈസേഷൻ, ഗ്രാഫ് ഓപ്റ്റിമൈസേഷൻ എന്നിവ ഉൾപ്പെടുന്നു
- **സിമ്പിൾ CLI ഇന്റർഫേസ്**: സാധാരണ ഓപ്റ്റിമൈസേഷൻ ടാസ്കുകൾക്കുള്ള ലളിതമായ കമാൻഡുകൾ
- **മൾട്ടി-ഫ്രെയിംവർക്ക് പിന്തുണ**: PyTorch, Hugging Face മോഡലുകൾ, ONNX എന്നിവയുമായി പ്രവർത്തിക്കുന്നു
- **പ്രസിദ്ധമായ മോഡൽ പിന്തുണ**: Llama, Phi, Qwen, Gemma തുടങ്ങിയ പ്രശസ്ത മോഡൽ ആർക്കിടെക്ചറുകൾ സ്വയം ഓപ്റ്റിമൈസ് ചെയ്യാൻ Olive കഴിയും

### ഗുണങ്ങൾ

- **വികസന സമയം കുറവ്**: വ്യത്യസ്ത ഓപ്റ്റിമൈസേഷൻ സാങ്കേതിക വിദ്യകൾ കൈമാറി പരീക്ഷിക്കേണ്ടതില്ല
- **പ്രകടന വർദ്ധനവ്**: ചില സാഹചര്യങ്ങളിൽ 6x വരെ വേഗത വർദ്ധനവ്
- **ക്രോസ്-പ്ലാറ്റ്‌ഫോം വിന്യാസം**: വ്യത്യസ്ത ഹാർഡ്‌വെയർ, ഓപ്പറേറ്റിംഗ് സിസ്റ്റങ്ങളിൽ ഓപ്റ്റിമൈസ് ചെയ്ത മോഡലുകൾ പ്രവർത്തിക്കുന്നു
- **കൃത്യത നിലനിർത്തൽ**: പ്രകടനം മെച്ചപ്പെടുത്തുമ്പോഴും മോഡൽ ഗുണമേന്മ നിലനിർത്തുന്നു

## Installation

### മുൻ‌അവശ്യങ്ങൾ

- Python 3.8 അല്ലെങ്കിൽ അതിനുമുകളിൽ
- pip പാക്കേജ് മാനേജർ
- വെർച്വൽ എൻവയോൺമെന്റ് (ശുപാർശ ചെയ്യുന്നു)

### അടിസ്ഥാന ഇൻസ്റ്റലേഷൻ

ഒരു വെർച്വൽ എൻവയോൺമെന്റ് സൃഷ്ടിച്ച് സജീവമാക്കുക:

```bash
# വെർച്വൽ എൻവയോൺമെന്റ് സൃഷ്ടിക്കുക
python -m venv olive-env

# വെർച്വൽ എൻവയോൺമെന്റ് സജീവമാക്കുക
# വിൻഡോസ്-ൽ:
olive-env\Scripts\activate
# മാക്‌ഓഎസ്/ലിനക്സ്-ൽ:
source olive-env/bin/activate
```

ഓട്ടോ-ഓപ്റ്റിമൈസേഷൻ ഫീച്ചറുകളോടെ Olive ഇൻസ്റ്റാൾ ചെയ്യുക:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### ഐച്ഛിക ആശ്രിതങ്ങൾ

കൂടുതൽ ഫീച്ചറുകൾക്കായി Olive വിവിധ ഐച്ഛിക ആശ്രിതങ്ങൾ നൽകുന്നു:

```bash
# Azure ML സംയോജനത്തിനായി
pip install olive-ai[azureml]

# DirectML (Windows GPU വേഗത വർദ്ധനവിന്)
pip install olive-ai[directml]

# CPU മെച്ചപ്പെടുത്തലിനായി
pip install olive-ai[cpu]

# എല്ലാ സവിശേഷതകൾക്കും
pip install olive-ai[all]
```

### ഇൻസ്റ്റലേഷൻ സ്ഥിരീകരിക്കുക

```bash
olive --help
```

വിജയകരമായാൽ, Olive CLI സഹായ സന്ദേശം കാണും.

## Quick Start Guide

### നിങ്ങളുടെ ആദ്യ ഓപ്റ്റിമൈസേഷൻ

Olive-ന്റെ ഓട്ടോ-ഓപ്റ്റിമൈസേഷൻ ഫീച്ചർ ഉപയോഗിച്ച് ഒരു ചെറിയ ഭാഷാ മോഡൽ ഓപ്റ്റിമൈസ് ചെയ്യാം:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ഈ കമാൻഡ് ചെയ്യുന്നത്

ഓപ്റ്റിമൈസേഷൻ പ്രക്രിയയിൽ ഉൾപ്പെടുന്നത്: മോഡൽ ലോക്കൽ കാഷെ നിന്ന് നേടൽ, ONNX ഗ്രാഫ് പിടിച്ച് ONNX ഡാറ്റ ഫയലിൽ വെയ്റ്റുകൾ സംഭരിക്കൽ, ONNX ഗ്രാഫ് ഓപ്റ്റിമൈസ് ചെയ്യൽ, RTN രീതിയിൽ മോഡൽ int4 ആയി ക്വാണ്ടൈസ് ചെയ്യൽ.

### കമാൻഡ് പാരാമീറ്ററുകളുടെ വിശദീകരണം

- `--model_name_or_path`: Hugging Face മോഡൽ ഐഡന്റിഫയർ അല്ലെങ്കിൽ ലോക്കൽ പാത
- `--output_path`: ഓപ്റ്റിമൈസ് ചെയ്ത മോഡൽ സേവ് ചെയ്യാനുള്ള ഡയറക്ടറി
- `--device`: ലക്ഷ്യ ഉപകരണം (cpu, gpu)
- `--provider`: എക്സിക്യൂഷൻ പ്രൊവൈഡർ (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ഇൻഫറൻസിനായി ONNX Runtime Generate AI ഉപയോഗിക്കുക
- `--precision`: ക്വാണ്ടൈസേഷൻ പ്രിസിഷൻ (int4, int8, fp16)
- `--log_level`: ലോഗിംഗ് വിശദാംശം (0=കുറഞ്ഞത്, 1=വിവരവിവരം)

## Example: Converting Qwen3 to ONNX INT4

[lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) എന്ന Hugging Face ഉദാഹരണത്തെ അടിസ്ഥാനമാക്കി, Qwen3 മോഡൽ ഓപ്റ്റിമൈസ് ചെയ്യുന്നതിന്റെ രീതി:

### ഘട്ടം 1: മോഡൽ ഡൗൺലോഡ് (ഐച്ഛികം)

ഡൗൺലോഡ് സമയം കുറയ്ക്കാൻ, ആവശ്യമായ ഫയലുകൾ മാത്രം കാഷെ ചെയ്യുക:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### ഘട്ടം 2: Qwen3 മോഡൽ ഓപ്റ്റിമൈസ് ചെയ്യുക

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ഘട്ടം 3: ഓപ്റ്റിമൈസ് ചെയ്ത മോഡൽ ടെസ്റ്റ് ചെയ്യുക

നിങ്ങളുടെ ഓപ്റ്റിമൈസ് ചെയ്ത മോഡൽ ടെസ്റ്റ് ചെയ്യാൻ ഒരു ലളിതമായ Python സ്ക്രിപ്റ്റ് സൃഷ്ടിക്കുക:

```python
import onnxruntime_genai as og

# മെച്ചപ്പെടുത്തിയ മോഡൽ ലോഡ് ചെയ്യുക
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# ഒരു ചാറ്റ് ടെംപ്ലേറ്റ് സൃഷ്ടിക്കുക
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# ടെക്സ്റ്റ് സൃഷ്ടിക്കുക
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### ഔട്ട്പുട്ട് ഘടന

ഓപ്റ്റിമൈസേഷനു ശേഷം, നിങ്ങളുടെ ഔട്ട്പുട്ട് ഡയറക്ടറിയിൽ ഇതെല്ലാം ഉണ്ടാകും:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Advanced Usage

### കോൺഫിഗറേഷൻ ഫയലുകൾ

കൂടുതൽ സങ്കീർണ്ണമായ ഓപ്റ്റിമൈസേഷൻ പ്രവാഹങ്ങൾക്കായി JSON കോൺഫിഗറേഷൻ ഫയലുകൾ ഉപയോഗിക്കാം:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

കോൺഫിഗറേഷനോടെ പ്രവർത്തിപ്പിക്കുക:

```bash
olive run --config config.json
```

### GPU ഓപ്റ്റിമൈസേഷൻ

CUDA GPU ഓപ്റ്റിമൈസേഷനായി:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) വേണ്ടി:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Olive ഉപയോഗിച്ച് ഫൈൻ-ട്യൂണിംഗ്

Olive മോഡലുകൾ ഫൈൻ-ട്യൂൺ ചെയ്യാനും പിന്തുണ നൽകുന്നു:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Best Practices

### 1. മോഡൽ തിരഞ്ഞെടുപ്പ്
- പരീക്ഷണത്തിനായി ചെറിയ മോഡലുകൾ ഉപയോഗിക്കുക (ഉദാ: 0.5B-7B പാരാമീറ്ററുകൾ)
- നിങ്ങളുടെ ലക്ഷ്യ മോഡൽ ആർക്കിടെക്ചർ Olive പിന്തുണയ്ക്കുന്നുണ്ടെന്ന് ഉറപ്പാക്കുക

### 2. ഹാർഡ്‌വെയർ പരിഗണനകൾ
- നിങ്ങളുടെ വിന്യാസ ഹാർഡ്‌വെയറിനനുസരിച്ച് ഓപ്റ്റിമൈസേഷൻ ലക്ഷ്യമിടുക
- CUDA-സഹായമുള്ള ഹാർഡ്‌വെയർ ഉണ്ടെങ്കിൽ GPU ഓപ്റ്റിമൈസേഷൻ ഉപയോഗിക്കുക
- Windows യന്ത്രങ്ങളിൽ ഇന്റഗ്രേറ്റഡ് ഗ്രാഫിക്സിനായി DirectML പരിഗണിക്കുക

### 3. പ്രിസിഷൻ തിരഞ്ഞെടുപ്പ്
- **INT4**: പരമാവധി കംപ്രഷൻ, ചെറിയ കൃത്യത നഷ്ടം
- **INT8**: വലുപ്പവും കൃത്യതയും നല്ല ബാലൻസ്
- **FP16**: കുറഞ്ഞ കൃത്യത നഷ്ടം, മിതമായ വലുപ്പ കുറവ്

### 4. ടെസ്റ്റിംഗ് & വാലിഡേഷൻ
- നിങ്ങളുടെ പ്രത്യേക ഉപയോഗ കേസുകളുമായി ഓപ്റ്റിമൈസ് ചെയ്ത മോഡലുകൾ എല്ലായ്പ്പോഴും ടെസ്റ്റ് ചെയ്യുക
- പ്രകടന മെട്രിക്‌സ് (ലേറ്റൻസി, ത്രൂപുട്ട്, കൃത്യത) താരതമ്യം ചെയ്യുക
- മൂല്യനിർണയത്തിന് പ്രതിനിധി ഇൻപുട്ട് ഡാറ്റ ഉപയോഗിക്കുക

### 5. ആവർത്തന ഓപ്റ്റിമൈസേഷൻ
- വേഗത്തിലുള്ള ഫലങ്ങൾക്ക് ഓട്ടോ-ഓപ്റ്റിമൈസേഷൻ ഉപയോഗിച്ച് ആരംഭിക്കുക
- സൂക്ഷ്മ നിയന്ത്രണത്തിനായി കോൺഫിഗറേഷൻ ഫയലുകൾ ഉപയോഗിക്കുക
- വ്യത്യസ്ത ഓപ്റ്റിമൈസേഷൻ പാസുകൾ പരീക്ഷിക്കുക

## Troubleshooting

### സാധാരണ പ്രശ്നങ്ങൾ

#### 1. ഇൻസ്റ്റലേഷൻ പ്രശ്നങ്ങൾ
```bash
# നിങ്ങൾ ആശ്രിതത്വ സംഘർഷങ്ങൾ നേരിടുകയാണെങ്കിൽ:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU പ്രശ്നങ്ങൾ
```bash
# CUDA ഇൻസ്റ്റലേഷൻ സ്ഥിരീകരിക്കുക:
nvidia-smi

# ശരിയായ ONNX റൺടൈം GPU പാക്കേജ് ഇൻസ്റ്റാൾ ചെയ്യുക:
pip install onnxruntime-gpu
```

#### 3. മെമ്മറി പ്രശ്നങ്ങൾ
- ഓപ്റ്റിമൈസേഷനിൽ ചെറിയ ബാച്ച് സൈസുകൾ ഉപയോഗിക്കുക
- ആദ്യം ഉയർന്ന പ്രിസിഷനോടെ ക്വാണ്ടൈസേഷൻ പരീക്ഷിക്കുക (int8 int4-നേക്കാൾ)
- മോഡൽ കാഷെ ചെയ്യുന്നതിനായി മതിയായ ഡിസ്ക് സ്പേസ് ഉറപ്പാക്കുക

#### 4. മോഡൽ ലോഡിംഗ് പിശകുകൾ
- മോഡൽ പാതയും ആക്‌സസ് അനുമതികളും പരിശോധിക്കുക
- മോഡലിന് `trust_remote_code=True` ആവശ്യമുണ്ടോ എന്ന് പരിശോധിക്കുക
- ആവശ്യമായ എല്ലാ മോഡൽ ഫയലുകളും ഡൗൺലോഡ് ചെയ്തിട്ടുണ്ടെന്ന് ഉറപ്പാക്കുക

### സഹായം നേടുക

- **ഡോക്യുമെന്റേഷൻ**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **ഉദാഹരണങ്ങൾ**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Recipes Repository

### Olive Recipes-ന്റെ പരിചയം

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) റിപോസിറ്ററി പ്രധാന Olive ടൂൾകിറ്റിനെ പൂരിപ്പിച്ച് പ്രശസ്ത AI മോഡലുകൾക്കുള്ള ഉപയോഗത്തിന് തയ്യാറാക്കിയ ഓപ്റ്റിമൈസേഷൻ റെസിപ്പികളുടെ സമ്പൂർണ്ണ ശേഖരം നൽകുന്നു. പൊതുവായി ലഭ്യമായ മോഡലുകൾ ഓപ്റ്റിമൈസ് ചെയ്യാനും സ്വകാര്യ മോഡലുകൾക്കുള്ള ഓപ്റ്റിമൈസേഷൻ പ്രവാഹങ്ങൾ സൃഷ്ടിക്കാനും ഇത് പ്രായോഗിക റഫറൻസായി സേവിക്കുന്നു.

### പ്രധാന സവിശേഷതകൾ

- **100+ മുൻകൂട്ടി നിർമ്മിച്ച റെസിപ്പികൾ**: പ്രശസ്ത മോഡലുകൾക്കുള്ള ഉപയോഗത്തിന് തയ്യാറാക്കിയ ഓപ്റ്റിമൈസേഷൻ കോൺഫിഗറേഷനുകൾ
- **മൾട്ടി-ആർക്കിടെക്ചർ പിന്തുണ**: ട്രാൻസ്ഫോർമർ മോഡലുകൾ, വിഷൻ മോഡലുകൾ, മൾട്ടിമോഡൽ ആർക്കിടെക്ചറുകൾ ഉൾപ്പെടുന്നു
- **ഹാർഡ്‌വെയർ-സ്പെസിഫിക് ഓപ്റ്റിമൈസേഷനുകൾ**: CPU, GPU, പ്രത്യേക ആക്സിലറേറ്ററുകൾക്കായി റെസിപ്പികൾ
- **പ്രസിദ്ധ മോഡൽ കുടുംബങ്ങൾ**: Phi, Llama, Qwen, Gemma, Mistral തുടങ്ങിയവ ഉൾപ്പെടുന്നു

### പിന്തുണയുള്ള മോഡൽ കുടുംബങ്ങൾ

റിപോസിറ്ററിയിൽ ഉൾപ്പെടുന്ന ഓപ്റ്റിമൈസേഷൻ റെസിപ്പികൾ:

#### ഭാഷാ മോഡലുകൾ
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 സീരീസ് (0.5B മുതൽ 14B വരെ)
- **Google Gemma**: വിവിധ Gemma മോഡൽ കോൺഫിഗറേഷനുകൾ
- **Mistral AI**: Mistral-7B സീരീസ്
- **DeepSeek**: R1-Distill സീരീസ് മോഡലുകൾ

#### വിഷൻ & മൾട്ടിമോഡൽ മോഡലുകൾ
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP മോഡലുകൾ**: വിവിധ CLIP-ViT കോൺഫിഗറേഷനുകൾ
- **ResNet**: ResNet-50 ഓപ്റ്റിമൈസേഷനുകൾ
- **Vision Transformers**: ViT-base-patch16-224

#### പ്രത്യേക മോഡലുകൾ
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: ബേസ് & മൾട്ടിലിംഗ്വൽ വകഭേദങ്ങൾ
- **Sentence Transformers**: all-MiniLM-L6-v2

### Olive Recipes ഉപയോഗിക്കൽ

#### രീതി 1: പ്രത്യേക റെസിപ്പി ക്ലോൺ ചെയ്യുക

```bash
# റെസിപ്പീസ് റിപോസിറ്ററി ക്ലോൺ ചെയ്യുക
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# ഒരു പ്രത്യേക മോഡൽ റെസിപ്പിയിലേക്ക് നാവിഗേറ്റ് ചെയ്യുക
cd microsoft-Phi-4-mini-instruct

# ഓപ്റ്റിമൈസേഷൻ നടത്തുക
olive run --config olive_config.json
```

#### രീതി 2: റെസിപ്പി ടെംപ്ലേറ്റായി ഉപയോഗിക്കുക

```bash
# നിങ്ങളുടെ മോഡലിനുള്ള ഒരു റെസിപ്പി കോൺഫിഗറേഷൻ പകർത്തുക
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# നിങ്ങളുടെ ആവശ്യങ്ങൾക്കായി കോൺഫിഗറേഷൻ മാറ്റുക
# മോഡൽ പാതകൾ, ഓപ്റ്റിമൈസേഷൻ പാരാമീറ്ററുകൾ എന്നിവ അപ്ഡേറ്റ് ചെയ്യുക

# നിങ്ങളുടെ കസ്റ്റം കോൺഫിഗറേഷനോടെ പ്രവർത്തിപ്പിക്കുക
olive run --config my_config.json
```

### റെസിപ്പി ഘടന

ഓരോ റെസിപ്പി ഡയറക്ടറിയിലും സാധാരണയായി ഇതെല്ലാം ഉണ്ടാകും:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### ഉദാഹരണം: Phi-4-mini റെസിപ്പി ഉപയോഗിക്കൽ

Phi-4-mini റെസിപ്പി ഉദാഹരണമായി ഉപയോഗിക്കാം:

```bash
# റിപ്പോസിറ്ററി ക്ലോൺ ചെയ്യുക
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# ആശ്രിതങ്ങൾ ഇൻസ്റ്റാൾ ചെയ്യുക
pip install -r requirements.txt

# ഓപ്റ്റിമൈസേഷൻ പ്രവർത്തിപ്പിക്കുക
olive run --config olive_config.json
```

കോൺഫിഗറേഷൻ ഫയലിൽ സാധാരണയായി ഉൾപ്പെടുന്നത്:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### റെസിപ്പികൾ കസ്റ്റമൈസ് ചെയ്യൽ

#### ലക്ഷ്യ ഹാർഡ്‌വെയർ മാറ്റൽ

`systems` സെക്ഷൻ അപ്ഡേറ്റ് ചെയ്യുക:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### ഓപ്റ്റിമൈസേഷൻ പാരാമീറ്ററുകൾ ക്രമീകരിക്കൽ

വ്യത്യസ്ത ഓപ്റ്റിമൈസേഷൻ നിലകൾക്കായി `passes` സെക്ഷൻ മാറ്റുക:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### നിങ്ങളുടെ സ്വന്തം റെസിപ്പി സൃഷ്ടിക്കൽ

1. **സമാന മോഡലിൽ നിന്നു ആരംഭിക്കുക**: സമാന ആർക്കിടെക്ചറുള്ള ഒരു റെസിപ്പി കണ്ടെത്തുക
2. **മോഡൽ കോൺഫിഗറേഷൻ അപ്ഡേറ്റ് ചെയ്യുക**: കോൺഫിഗറേഷനിൽ മോഡൽ നാമം/പാത മാറ്റുക
3. **പാരാമീറ്ററുകൾ ക്രമീകരിക്കുക**: ആവശ്യാനുസരണം ഓപ്റ്റിമൈസേഷൻ പാരാമീറ്ററുകൾ മാറ്റുക
4. **ടെസ്റ്റ് & വാലിഡേറ്റ് ചെയ്യുക**: ഓപ്റ്റിമൈസേഷൻ നടത്തുകയും ഫലങ്ങൾ പരിശോധിക്കുകയും ചെയ്യുക
5. **പുനർനിർമ്മാണം നൽകുക**: നിങ്ങളുടെ റെസിപ്പി റിപോസിറ്ററിയിലേക്ക് സംഭാവന നൽകുക

### റെസിപ്പികൾ ഉപയോഗിക്കുന്നതിനുള്ള ഗുണങ്ങൾ

#### 1. **പരിശോധിച്ച കോൺഫിഗറേഷനുകൾ**
- പ്രത്യേക മോഡലുകൾക്കുള്ള പരീക്ഷിച്ച ഓപ്റ്റിമൈസേഷൻ സെറ്റിങ്ങുകൾ
- മികച്ച പാരാമീറ്ററുകൾ കണ്ടെത്തുന്നതിൽ പരീക്ഷണ-പിശക് ഒഴിവാക്കുന്നു

#### 2. **ഹാർഡ്‌വെയർ-സ്പെസിഫിക് ട്യൂണിംഗ്**
- വ്യത്യസ്ത എക്സിക്യൂഷൻ പ്രൊവൈഡറുകൾക്കായി മുൻകൂട്ടി ഓപ്റ്റിമൈസ് ചെയ്തിരിക്കുന്നു
- CPU, GPU, NPU ലക്ഷ്യങ്ങളിലേക്കുള്ള റെഡി-ടു-യൂസ് കോൺഫിഗറേഷനുകൾ

#### 3. **സമ്പൂർണ്ണ കവർേജ്**
- ഏറ്റവും പ്രശസ്തമായ ഓപ്പൺ സോഴ്‌സ് മോഡലുകൾക്ക് പിന്തുണ
- പുതിയ മോഡൽ റിലീസുകളുമായി സ്ഥിരം അപ്ഡേറ്റുകൾ

#### 4. **കമ്മ്യൂണിറ്റി സംഭാവനകൾ**
- AI കമ്മ്യൂണിറ്റിയുമായി സഹകരിച്ച് വികസനം
- പങ്കുവെച്ച അറിവും മികച്ച പ്രാക്ടീസുകളും

### Olive Recipes-ലേക്ക് സംഭാവന നൽകൽ

നിങ്ങൾ ഓപ്റ്റിമൈസ് ചെയ്ത മോഡൽ റിപോസിറ്ററിയിൽ ഉൾപ്പെടാത്തതാണെങ്കിൽ:

1. **റിപോസിറ്ററി ഫോർക്ക് ചെയ്യുക**: olive-recipes-ന്റെ നിങ്ങളുടെ സ്വന്തം ഫോർക്ക് സൃഷ്ടിക്കുക
2. **റെസിപ്പി ഡയറക്ടറി സൃഷ്ടിക്കുക**: നിങ്ങളുടെ മോഡലിനായി പുതിയ ഡയറക്ടറി ചേർക്കുക
3. **കോൺഫിഗറേഷൻ ഉൾപ്പെടുത്തുക**: olive_config.json ഉൾപ്പെടെ പിന്തുണയുള്ള ഫയലുകൾ ചേർക്കുക
4. **ഉപയോഗം രേഖപ്പെടുത്തുക**: വ്യക്തമായ README നിർദ്ദേശങ്ങളോടെ നൽകുക
5. **പുൾ റിക്വസ്റ്റ് സമർപ്പിക്കുക**: കമ്മ്യൂണിറ്റിയിലേക്ക് സംഭാവന നൽകുക

### പ്രകടന ബഞ്ച്മാർക്കുകൾ

അനേകം റെസിപ്പികൾ പ്രകടന ബഞ്ച്മാർക്കുകൾ ഉൾക്കൊള്ളുന്നു:
- **ലേറ്റൻസി മെച്ചപ്പെടുത്തലുകൾ**: സാധാരണ 2-6x വേഗത വർദ്ധനവ്
- **മെമ്മറി കുറവ്**: ക്വാണ്ടൈസേഷനോടെ 50-75% മെമ്മറി ഉപയോഗം കുറവ്
- **കൃത്യത നിലനിർത്തൽ**: 95-99% കൃത്യത സംരക്ഷണം

### AI ടൂൾകിറ്റുമായി ഇന്റഗ്രേഷൻ

റെസിപ്പികൾ സുതാര്യമായി പ്രവർത്തിക്കുന്നു:
- **VS Code AI Toolkit**: മോഡൽ ഓപ്റ്റിമൈസേഷനായി നേരിട്ട് ഇന്റഗ്രേഷൻ
- **Azure Machine Learning**: ക്ലൗഡ് അടിസ്ഥാന ഓപ്റ്റിമൈസേഷൻ പ്രവാഹങ്ങൾ
- **ONNX Runtime**: ഓപ്റ്റിമൈസ് ചെയ്ത ഇൻഫറൻസ് വിന്യാസം

## Additional Resources

### ഔദ്യോഗിക ലിങ്കുകൾ
- **GitHub റിപോസിറ്ററി**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipes റിപോസിറ്ററി**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime ഡോക്യുമെന്റേഷൻ**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face ഉദാഹരണം**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### കമ്മ്യൂണിറ്റി ഉദാഹരണങ്ങൾ
- **Jupyter നോട്ട്ബുക്കുകൾ**: Olive GitHub റിപോസിറ്ററിയിൽ ലഭ്യമാണ് — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code എക്സ്റ്റൻഷൻ**: VS Code-ക്കുള്ള AI Toolkit അവലോകനം — https://learn.microsoft.com/azure/ai-toolkit/overview
- **ബ്ലോഗ് പോസ്റ്റുകൾ**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### ബന്ധപ്പെട്ട ടൂളുകൾ
- **ONNX Runtime**: ഉയർന്ന പ്രകടന ഇൻഫറൻസ് എഞ്ചിൻ — https://onnxruntime.ai/
- **Hugging Face Transformers**: അനേകം അനുയോജ്യമായ മോഡലുകളുടെ ഉറവിടം — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: ക്ലൗഡ് അടിസ്ഥാന ഓപ്റ്റിമൈസേഷൻ പ്രവാഹങ്ങൾ — https://learn.microsoft.com/azure/machine-learning/


## ➡️ What's next

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**അസൂയാപത്രം**:  
ഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖയാണ് പ്രാമാണികമായ ഉറവിടം എന്ന് പരിഗണിക്കേണ്ടതാണ്. നിർണായകമായ വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനം ഉപയോഗിക്കുന്നതിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ തെറ്റായ വ്യാഖ്യാനങ്ങൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->