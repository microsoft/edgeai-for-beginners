<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7084ef66b0602790ea6a864a89b6fd0c",
  "translation_date": "2025-12-15T23:24:06+00:00",
  "source_file": "Module04/05.AppleMLX.md",
  "language_code": "ml"
}
-->
# Section 4 : ആപ്പിൾ MLX ഫ്രെയിംവർക്ക് ഡീപ്പ് ഡൈവ്

## Table of Contents
1. [ആപ്പിൾ MLX-ലേക്കുള്ള പരിചയം](../../../Module04)
2. [LLM വികസനത്തിനുള്ള പ്രധാന സവിശേഷതകൾ](../../../Module04)
3. [ഇൻസ്റ്റലേഷൻ ഗൈഡ്](../../../Module04)
4. [MLX ഉപയോഗിച്ച് ആരംഭിക്കൽ](../../../Module04)
5. [MLX-LM: ഭാഷാ മോഡലുകൾ](../../../Module04)
6. [വലിയ ഭാഷാ മോഡലുകളുമായി പ്രവർത്തിക്കൽ](../../../Module04)
7. [Hugging Face ഇന്റഗ്രേഷൻ](../../../Module04)
8. [മോഡൽ പരിവർത്തനം மற்றும் ക്വാണ്ടൈസേഷൻ](../../../Module04)
9. [ഭാഷാ മോഡലുകളുടെ ഫൈൻ-ട്യൂണിംഗ്](../../../Module04)
10. [അഡ്വാൻസ്ഡ് LLM സവിശേഷതകൾ](../../../Module04)
11. [LLM-കൾക്കുള്ള മികച്ച പ്രാക്ടീസുകൾ](../../../Module04)
12. [പ്രശ്നപരിഹാരം](../../../Module04)
13. [അധിക വിഭവങ്ങൾ](../../../Module04)

## ആപ്പിൾ MLX-ലേക്കുള്ള പരിചയം

ആപ്പിൾ MLX ആപ്പിൾ സിലിക്കണിൽ കാര്യക്ഷമവും ഫ്ലെക്സിബിളുമായ മെഷീൻ ലേണിങ്ങിനായി പ്രത്യേകമായി രൂപകൽപ്പന ചെയ്ത ഒരു അറേ ഫ്രെയിംവർക്ക് ആണ്, ആപ്പിൾ മെഷീൻ ലേണിങ് റിസർച്ച് വികസിപ്പിച്ചിരിക്കുന്നു. 2023 ഡിസംബറിൽ പുറത്തിറങ്ങിയ MLX, PyTorch, TensorFlow പോലുള്ള ഫ്രെയിംവർക്കുകൾക്ക് ആപ്പിളിന്റെ മറുപടി ആയി, മാക് കമ്പ്യൂട്ടറുകളിൽ ശക്തമായ വലിയ ഭാഷാ മോഡൽ കഴിവുകൾ സജ്ജമാക്കുന്നതിൽ പ്രത്യേക ശ്രദ്ധ നൽകുന്നു.

### LLM-കൾക്കായി MLX എന്തുകൊണ്ട് പ്രത്യേകമാണ്?

MLX ആപ്പിൾ സിലിക്കണിന്റെ ഏകീകൃത മെമ്മറി ആർക്കിടെക്ചർ പൂർണ്ണമായി പ്രയോജനപ്പെടുത്താൻ രൂപകൽപ്പന ചെയ്തതാണ്, ഇത് മാക് കമ്പ്യൂട്ടറുകളിൽ വലിയ ഭാഷാ മോഡലുകൾ ലോക്കലായി ഓടിക്കാനും ഫൈൻ-ട്യൂൺ ചെയ്യാനും വളരെ അനുയോജ്യമാണ്. ഈ ഫ്രെയിംവർക്ക് മാക് ഉപയോക്താക്കൾക്ക് LLM-കളുമായി പ്രവർത്തിക്കുമ്പോൾ നേരിട്ടിരുന്ന അനേകം പൊരുത്തക്കേടുകൾ ഒഴിവാക്കുന്നു.

### LLM-കൾക്കായി MLX ആരാണ് ഉപയോഗിക്കേണ്ടത്?

- ക്ലൗഡ് ആശ്രിതത്വമില്ലാതെ LLM-കൾ ലോക്കലായി ഓടിക്കാൻ ആഗ്രഹിക്കുന്ന **മാക് ഉപയോക്താക്കൾ**
- ഭാഷാ മോഡൽ ഫൈൻ-ട്യൂണിംഗ്, കസ്റ്റമൈസേഷൻ പരീക്ഷിക്കുന്ന **ഗവേഷകർ**
- ഭാഷാ മോഡൽ കഴിവുകളുള്ള AI ആപ്ലിക്കേഷനുകൾ നിർമ്മിക്കുന്ന **ഡെവലപ്പർമാർ**
- ടെക്സ്റ്റ് ജനറേഷൻ, ചാറ്റ്, ഭാഷാ ടാസ്കുകൾക്കായി ആപ്പിൾ സിലിക്കൺ പ്രയോജനപ്പെടുത്താൻ ആഗ്രഹിക്കുന്ന **ഏവരും**

## LLM വികസനത്തിനുള്ള പ്രധാന സവിശേഷതകൾ

### 1. ഏകീകൃത മെമ്മറി ആർക്കിടെക്ചർ
ആപ്പിൾ സിലിക്കണിന്റെ ഏകീകൃത മെമ്മറി MLX-ന് വലിയ ഭാഷാ മോഡലുകൾ മെമ്മറി കോപ്പി ഓവർഹെഡ് ഇല്ലാതെ കാര്യക്ഷമമായി കൈകാര്യം ചെയ്യാൻ അനുവദിക്കുന്നു. അതായത്, ഒരേ ഹാർഡ്‌വെയറിൽ വലിയ മോഡലുകളുമായി പ്രവർത്തിക്കാം.

### 2. നേറ്റീവ് ആപ്പിൾ സിലിക്കൺ ഓപ്റ്റിമൈസേഷൻ
MLX ആപ്പിൾ M-സീരീസ് ചിപ്പുകൾക്കായി പൂർണ്ണമായും നിർമ്മിച്ചിരിക്കുന്നു, ഭാഷാ മോഡലുകളിൽ സാധാരണയായി ഉപയോഗിക്കുന്ന ട്രാൻസ്ഫോർമർ ആർക്കിടെക്ചറുകൾക്കായി മികച്ച പ്രകടനം നൽകുന്നു.

### 3. ക്വാണ്ടൈസേഷൻ പിന്തുണ
4-ബിറ്റ്, 8-ബിറ്റ് ക്വാണ്ടൈസേഷനുകൾക്ക് ഇൻബിൽറ്റ് പിന്തുണ, മെമ്മറി ആവശ്യകത കുറയ്ക്കുന്നു, മോഡൽ ഗുണനിലവാരം നിലനിർത്തിക്കൊണ്ട് ഉപഭോക്തൃ ഹാർഡ്‌വെയറിൽ വലിയ മോഡലുകൾ ഓടിക്കാൻ സാധിക്കുന്നു.

### 4. Hugging Face ഇന്റഗ്രേഷൻ
Hugging Face ഇക്കോസിസ്റ്റവുമായി സുതാര്യമായ ഇന്റഗ്രേഷൻ, ആയിരക്കണക്കിന് പ്രീ-ട്രെയിൻ ചെയ്ത ഭാഷാ മോഡലുകൾക്ക് ലളിതമായ പരിവർത്തന ഉപകരണങ്ങൾ വഴി ആക്‌സസ് നൽകുന്നു.

### 5. LoRA ഫൈൻ-ട്യൂണിംഗ്
ലോ-റാങ്ക് അഡാപ്റ്റേഷൻ (LoRA) പിന്തുണ, കുറഞ്ഞ കംപ്യൂട്ടേഷണൽ വിഭവങ്ങളോടെ വലിയ മോഡലുകളുടെ ഫൈൻ-ട്യൂണിംഗ് കാര്യക്ഷമമാക്കുന്നു.

## ഇൻസ്റ്റലേഷൻ ഗൈഡ്

### സിസ്റ്റം ആവശ്യകതകൾ
- **macOS 13.0+** (ആപ്പിൾ സിലിക്കൺ ഓപ്റ്റിമൈസേഷനായി)
- **Python 3.8+**
- **ആപ്പിൾ സിലിക്കൺ** (M1, M2, M3, M4 സീരീസ്)
- **നേറ്റീവ് ARM പരിസ്ഥിതി** (Rosetta-യിൽ ഓടിക്കരുത്)
- **8GB+ RAM** (വലിയ മോഡലുകൾക്കായി 16GB+ ശുപാർശ)

### LLM-കൾക്കായി ക്വിക്ക് ഇൻസ്റ്റലേഷൻ

ഭാഷാ മോഡലുകളുമായി ആരംഭിക്കാൻ എളുപ്പമാർഗം MLX-LM ഇൻസ്റ്റാൾ ചെയ്യുകയാണ്:

```bash
pip install mlx-lm
```

ഈ ഒറ്റ കമാൻഡ് കോർ MLX ഫ്രെയിംവർക്കും ഭാഷാ മോഡൽ ഉപകരണങ്ങൾക്കും ഇൻസ്റ്റാൾ ചെയ്യുന്നു.

### വെർച്വൽ എൻവയോൺമെന്റ് സജ്ജമാക്കൽ (ശുപാർശ ചെയ്യുന്നു)

```bash
# വെർച്വൽ എൻവയോൺമെന്റ് സൃഷ്ടിച്ച് സജീവമാക്കുക
python -m venv mlx-llm-env
source mlx-llm-env/bin/activate

# MLX-LM ഇൻസ്റ്റാൾ ചെയ്യുക
pip install mlx-lm

# ഇൻസ്റ്റലേഷൻ സ്ഥിരീകരിക്കുക
python -c "from mlx_lm import load; print('MLX-LM installed successfully')"
```

### ഓഡിയോ മോഡലുകൾക്കുള്ള അധിക ആശ്രിതങ്ങൾ

Whisper പോലുള്ള സ്പീച്ച് മോഡലുകളുമായി പ്രവർത്തിക്കാൻ ആഗ്രഹിക്കുന്നുവെങ്കിൽ:

```bash
pip install mlx-lm[whisper]
# അല്ലെങ്കിൽ
pip install mlx-lm ffmpeg-python
```

## MLX ഉപയോഗിച്ച് ആരംഭിക്കൽ

### നിങ്ങളുടെ ആദ്യ ഭാഷാ മോഡൽ

ഒരു ലളിതമായ ടെക്സ്റ്റ് ജനറേഷൻ ഉദാഹരണം ഓടിച്ച് തുടങ്ങാം:

```bash
# കമാൻഡ് ലൈൻ നിന്ന് വേഗത്തിലുള്ള ടെക്സ്റ്റ് ജനറേഷൻ
python -m mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt "Explain artificial intelligence in simple terms:"
```

### പൈത്തൺ API ഉദാഹരണം

```python
from mlx_lm import load, generate

# ക്വാണ്ടൈസ്ഡ് മോഡൽ ലോഡ് ചെയ്യുക (കുറഞ്ഞ മെമ്മറി ഉപയോഗിക്കുന്നു)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# ടെക്സ്റ്റ് സൃഷ്ടിക്കുക
prompt = "Write a short story about a robot learning to understand emotions:"
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True,
    max_tokens=300,
    temp=0.7
)
print(response)
```

### മോഡൽ ലോഡിംഗ് മനസ്സിലാക്കൽ

```python
from mlx_lm import load

# മോഡലുകൾ ലോഡ് ചെയ്യാനുള്ള വ്യത്യസ്ത മാർഗങ്ങൾ
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # പൂർണ്ണ കൃത്യത
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # ക്വാണ്ടൈസ്ഡ്

# ഇഷ്ടാനുസൃത ക്രമീകരണങ്ങളോടെ ലോഡ് ചെയ്യുക
model, tokenizer = load(
    "qwen/Qwen-7B-Chat",
    tokenizer_config={
        "eos_token": "<|endoftext|>",
        "trust_remote_code": True
    }
)
```

## MLX-LM: ഭാഷാ മോഡലുകൾ

### പിന്തുണയുള്ള മോഡൽ ആർക്കിടെക്ചറുകൾ

MLX-LM നിരവധി ജനപ്രിയ ഭാഷാ മോഡൽ ആർക്കിടെക്ചറുകൾ പിന്തുണയ്ക്കുന്നു:

- **LLaMA and LLaMA 2** - മെറ്റയുടെ അടിസ്ഥാന മോഡലുകൾ
- **Mistral and Mixtral** - കാര്യക്ഷമവും ശക്തവുമായ മോഡലുകൾ
- **Phi-3** - മൈക്രോസോഫ്റ്റിന്റെ കോംപാക്റ്റ് ഭാഷാ മോഡലുകൾ
- **Qwen** - അലി ബാബയുടെ ബഹുഭാഷാ മോഡലുകൾ
- **Code Llama** - കോഡ് ജനറേഷനിനായി പ്രത്യേകിച്ചുള്ളത്
- **Gemma** - ഗൂഗിളിന്റെ ഓപ്പൺ ഭാഷാ മോഡലുകൾ

### കമാൻഡ് ലൈൻ ഇന്റർഫേസ്

MLX-LM കമാൻഡ് ലൈൻ ഇന്റർഫേസ് ഭാഷാ മോഡലുകളുമായി പ്രവർത്തിക്കാൻ ശക്തമായ ഉപകരണങ്ങൾ നൽകുന്നു:

```bash
# അടിസ്ഥാന ടെക്സ്റ്റ് ജനറേഷൻ
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "Hello, how are you?"

# പ്രത്യേക പാരാമീറ്ററുകളോടെ ജനറേറ്റ് ചെയ്യുക
python -m mlx_lm.generate \
    --model mlx-community/CodeLlama-7b-Instruct-hf-4bit \
    --prompt "Write a Python function to calculate fibonacci numbers:" \
    --max-tokens 500 \
    --temp 0.3

# ഇന്ററാക്ടീവ് ചാറ്റ് മോഡ്
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "You are a helpful assistant." --max-tokens 100

# എല്ലാ ഓപ്ഷനുകൾക്കും സഹായം ലഭിക്കുക
python -m mlx_lm.generate --help
```

### അഡ്വാൻസ്ഡ് ഉപയോഗത്തിനുള്ള പൈത്തൺ API

```python
from mlx_lm import load, generate

# ഒന്നിലധികം തലമുറകൾക്കായി മോഡൽ ഒരിക്കൽ ലോഡ് ചെയ്യുക
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# ഒറ്റ പ്രോംപ്റ്റ് തലമുറ
def generate_response(prompt, max_tokens=200, temperature=0.7):
    return generate(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temperature,
        verbose=True
    )

# ബാച്ച് തലമുറ
prompts = [
    "Explain quantum computing:",
    "Write a haiku about technology:",
    "What are the benefits of renewable energy?"
]

responses = [generate_response(prompt) for prompt in prompts]
```

## വലിയ ഭാഷാ മോഡലുകളുമായി പ്രവർത്തിക്കൽ

### ടെക്സ്റ്റ് ജനറേഷൻ പാറ്റേണുകൾ

#### സിംഗിൾ-ടേൺ ജനറേഷൻ
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Summarize the key principles of sustainable development:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=300)
```

#### ഇൻസ്ട്രക്ഷൻ ഫോളോ ചെയ്യൽ
```python
# നിർദ്ദേശങ്ങൾ പാലിക്കുന്ന മോഡലുകൾക്കായി പ്രോംപ്റ്റുകൾ ഫോർമാറ്റ് ചെയ്യുക
instruction_prompt = """<s>[INST] You are a helpful coding assistant. 
Write a Python function that takes a list of numbers and returns the median value. 
Include comments explaining your code. [/INST]"""

response = generate(model, tokenizer, prompt=instruction_prompt, max_tokens=400)
```

#### ക്രിയേറ്റീവ് റൈറ്റിംഗ്
```python
creative_prompt = """Write a creative story beginning with: 
"The last library on Earth had been closed for fifty years when Sarah discovered the hidden door..."
Continue the story for about 200 words."""

story = generate(
    model, 
    tokenizer, 
    prompt=creative_prompt, 
    max_tokens=250, 
    temp=0.8  # കൂടുതൽ സൃഷ്ടിപരമായതിനായി ഉയർന്ന താപനില
)
```

### മൾട്ടി-ടേൺ സംഭാഷണങ്ങൾ

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# സംഭാഷണ ചരിത്രം മാനേജ് ചെയ്യൽ
class Conversation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def generate_response(self, user_input):
        self.add_message("user", user_input)
        
        # മോഡലിനായി സംഭാഷണം ഫോർമാറ്റ് ചെയ്യുക
        conversation_text = self.format_conversation()
        
        response = generate(
            self.model,
            self.tokenizer,
            prompt=conversation_text,
            max_tokens=300,
            temp=0.7
        )
        
        self.add_message("assistant", response)
        return response
    
    def format_conversation(self):
        formatted = ""
        for message in self.history:
            if message["role"] == "user":
                formatted += f"[INST] {message['content']} [/INST]"
            else:
                formatted += f" {message['content']} "
        return formatted

# ഉപയോഗം
chat = Conversation(model, tokenizer)
response1 = chat.generate_response("What is machine learning?")
response2 = chat.generate_response("Can you give me a practical example?")
```

## Hugging Face ഇന്റഗ്രേഷൻ

### MLX-സഹജമായ മോഡലുകൾ കണ്ടെത്തൽ

MLX Hugging Face ഇക്കോസിസ്റ്റവുമായി സുതാര്യമായി പ്രവർത്തിക്കുന്നു:

- **MLX മോഡലുകൾ ബ്രൗസ് ചെയ്യുക**: https://huggingface.co/models?library=mlx&sort=trending
- **MLX കമ്മ്യൂണിറ്റി**: https://huggingface.co/mlx-community (പ്രീ-കൺവേർട്ടഡ് മോഡലുകൾ)
- **ഓറിജിനൽ മോഡലുകൾ**: LLaMA, Mistral, Phi, Qwen മോഡലുകളുടെ ഭൂരിഭാഗവും പരിവർത്തനത്തോടെ പ്രവർത്തിക്കുന്നു

### Hugging Face-ൽ നിന്നുള്ള മോഡലുകൾ ലോഡ് ചെയ്യൽ

```python
from mlx_lm import load

# മുൻകൂട്ടി പരിവർത്തനം ചെയ്ത MLX മോഡലുകൾ ലോഡ് ചെയ്യുക (ശുപാർശ ചെയ്യുന്നു)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")

# ഒറിജിനൽ Hugging Face മോഡലുകൾ ലോഡ് ചെയ്യുക (സ്വയം പരിവർത്തനം ചെയ്യും)
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")
model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")
```

### ഓഫ്‌ലൈൻ ഉപയോഗത്തിനായി മോഡലുകൾ ഡൗൺലോഡ് ചെയ്യൽ

```bash
# Hugging Face CLI ഇൻസ്റ്റാൾ ചെയ്യുക
pip install huggingface_hub

# ഓഫ്ലൈൻ ഉപയോഗത്തിനായി ഒരു മോഡൽ ഡൗൺലോഡ് ചെയ്യുക
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit --local-dir ./models/mistral-7b

# ഡൗൺലോഡ് ചെയ്ത മോഡൽ ഉപയോഗിക്കുക
python -m mlx_lm.generate --model ./models/mistral-7b --prompt "Hello world"
```

## മോഡൽ പരിവർത്തനം மற்றும் ക്വാണ്ടൈസേഷൻ

### Hugging Face മോഡലുകൾ MLX-ലേക്ക് പരിവർത്തനം ചെയ്യൽ

```bash
# അടിസ്ഥാന പരിവർത്തനം
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2

# ക്വാണ്ടൈസേഷനോടുകൂടി പരിവർത്തനം ചെയ്യുക (മെമ്മറി കാര്യക്ഷമതയ്ക്കായി ശുപാർശ ചെയ്യുന്നു)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q

# പരിവർത്തനം ചെയ്ത് Hugging Face Hub-ലേക്ക് അപ്‌ലോഡ് ചെയ്യുക
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --upload-repo your-username/phi-3-mini-4k-instruct-mlx
```

### ക്വാണ്ടൈസേഷൻ മനസ്സിലാക്കൽ

ക്വാണ്ടൈസേഷൻ മോഡലിന്റെ വലിപ്പവും മെമ്മറി ഉപയോഗവും കുറയ്ക്കുന്നു, കുറഞ്ഞ ഗുണനിലവാര നഷ്ടത്തോടെ:

```python
# മോഡൽ വലിപ്പങ്ങളും മെമ്മറി ഉപയോഗവും താരതമ്യം

# ഒറിജിനൽ മോഡൽ (float32): 7B പാരാമീറ്ററുകൾക്ക് ഏകദേശം 14GB
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")

# 4-ബിറ്റ് ക്വാണ്ടൈസ്ഡ്: 7B പാരാമീറ്ററുകൾക്ക് ഏകദേശം 4GB
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# 8-ബിറ്റ് ക്വാണ്ടൈസ്ഡ്: 7B പാരാമീറ്ററുകൾക്ക് ഏകദേശം 7GB (4-ബിറ്റിനേക്കാൾ മികച്ച ഗുണമേന്മ)
# python -m mlx_lm.convert --hf-path model_name --quantize-bits 8
```

### കസ്റ്റം ക്വാണ്ടൈസേഷൻ

```bash
# വ്യത്യസ്ത ക്വാണ്ടൈസേഷൻ ഓപ്ഷനുകൾ
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 4
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 8

# ഗ്രൂപ്പ് സൈസ് ക്വാണ്ടൈസേഷൻ (കൂടുതൽ കൃത്യമായ)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q --q-group-size 64
```

## ഭാഷാ മോഡലുകളുടെ ഫൈൻ-ട്യൂണിംഗ്

### LoRA (ലോ-റാങ്ക് അഡാപ്റ്റേഷൻ) ഫൈൻ-ട്യൂണിംഗ്

MLX LoRA ഉപയോഗിച്ച് കാര്യക്ഷമമായ ഫൈൻ-ട്യൂണിംഗ് പിന്തുണയ്ക്കുന്നു, കുറഞ്ഞ കംപ്യൂട്ടേഷണൽ വിഭവങ്ങളോടെ വലിയ മോഡലുകൾ അനുയോജ്യമായി മാറ്റാൻ സാധിക്കുന്നു:

```python
# അടിസ്ഥാന LoRA ഫൈൻ-ട്യൂണിംഗ് ക്രമീകരണം
from mlx_lm import load
from mlx_lm.utils import load_dataset

# അടിസ്ഥാന മോഡൽ ലോഡ് ചെയ്യുക
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# നിങ്ങളുടെ ഡാറ്റാസെറ്റ് (JSON ഫോർമാറ്റ്) തയ്യാറാക്കുക
# ഓരോ എൻട്രിയിലും നിങ്ങളുടെ പരിശീലന ഉദാഹരണങ്ങളുള്ള 'text' ഫീൽഡ് ഉണ്ടായിരിക്കണം
dataset_path = "your_training_data.json"
```

### ട്രെയിനിംഗ് ഡാറ്റ തയ്യാറാക്കൽ

നിങ്ങളുടെ ട്രെയിനിംഗ് ഉദാഹരണങ്ങളോടുകൂടിയ JSON ഫയൽ സൃഷ്ടിക്കുക:

```json
[
    {
        "text": "[INST] What is the capital of France? [/INST] The capital of France is Paris."
    },
    {
        "text": "[INST] Explain photosynthesis briefly. [/INST] Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen."
    }
]
```

### ഫൈൻ-ട്യൂണിംഗ് കമാൻഡ്

```bash
# LoRA ഉപയോഗിച്ച് ഫൈൻ-ട്യൂൺ ചെയ്യുക
python -m mlx_lm.lora \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --train \
    --data your_training_data.json \
    --lora-layers 16 \
    --batch-size 4 \
    --learning-rate 1e-5 \
    --steps 1000 \
    --save-every 100 \
    --adapter-path ./fine_tuned_model
```

### ഫൈൻ-ട്യൂൺ ചെയ്ത മോഡലുകൾ ഉപയോഗിക്കൽ

```python
from mlx_lm import load

# ഫൈൻ-ട്യൂൺ ചെയ്ത അഡാപ്റ്ററോടുകൂടി ബേസ് മോഡൽ ലോഡ് ചെയ്യുക
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="./fine_tuned_model"
)

# നിങ്ങളുടെ ഫൈൻ-ട്യൂൺ ചെയ്ത മോഡലിൽ നിന്ന് ജനറേറ്റ് ചെയ്യുക
response = generate(model, tokenizer, prompt="Your custom prompt", max_tokens=200)
```

## അഡ്വാൻസ്ഡ് LLM സവിശേഷതകൾ

### കാര്യക്ഷമതയ്ക്കായി പ്രോംപ്റ്റ് കാഷിംഗ്

ഒരേ കോൺടെക്സ്റ്റ് ആവർത്തിച്ച് ഉപയോഗിക്കുമ്പോൾ പ്രകടനം മെച്ചപ്പെടുത്താൻ MLX പ്രോംപ്റ്റ് കാഷിംഗ് പിന്തുണയ്ക്കുന്നു:

```bash
# ഒരു സിസ്റ്റം പ്രോംപ്റ്റ് സൃഷ്ടിച്ച് കാഷ് ചെയ്യുക
python -m mlx_lm.generate \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --prompt "You are a helpful coding assistant. You provide clean, well-commented code solutions." \
    --save-prompt-cache coding_assistant.safetensors

# പുതിയ ക്വെറിയുകളുമായി കാഷ് ചെയ്ത പ്രോംപ്റ്റ് ഉപയോഗിക്കുക
python -m mlx_lm.generate \
    --prompt-cache-file coding_assistant.safetensors \
    --prompt "Write a Python function to sort a list of dictionaries by a specific key."
```

### സ്ട്രീമിംഗ് ടെക്സ്റ്റ് ജനറേഷൻ

```python
from mlx_lm import load, stream_generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a detailed explanation of renewable energy sources:"

# ടോക്കണുകൾ സൃഷ്ടിക്കുമ്പോൾ സ്ട്രീം ചെയ്യുക
for token in stream_generate(model, tokenizer, prompt, max_tokens=500):
    print(token, end='', flush=True)
```

### കോഡ് ജനറേഷൻ മോഡലുകളുമായി പ്രവർത്തിക്കൽ

```python
from mlx_lm import load, generate

# കോഡ്-വിശേഷമായ മോഡൽ ലോഡ് ചെയ്യുക
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")

# കോഡ് ജനറേഷൻ പ്രോംപ്റ്റ്
code_prompt = """Write a Python class that implements a simple cache with the following features:
- Get and set methods
- Maximum size limit
- LRU (Least Recently Used) eviction policy
Include proper documentation and error handling."""

code_response = generate(
    model, 
    tokenizer, 
    prompt=code_prompt, 
    max_tokens=800,
    temp=0.3  # കൂടുതൽ കൃത്യമായ കോഡിനായി താപനില കുറയ്ക്കുക
)

print(code_response)
```

### ചാറ്റ് മോഡലുകളുമായി പ്രവർത്തിക്കൽ

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# മിസ്ട്രാൽ മോഡലുകൾക്കുള്ള ശരിയായ ചാറ്റ് ഫോർമാറ്റിംഗ്
def format_chat_prompt(messages):
    formatted_prompt = ""
    for message in messages:
        if message["role"] == "user":
            formatted_prompt += f"[INST] {message['content']} [/INST]"
        elif message["role"] == "assistant":
            formatted_prompt += f" {message['content']} "
    return formatted_prompt

# ബഹു-മുറി സംഭാഷണം
messages = [
    {"role": "user", "content": "What are the main components of a computer?"},
    {"role": "assistant", "content": "The main components of a computer include the CPU, RAM, storage, motherboard, and power supply."},
    {"role": "user", "content": "Can you explain what RAM does in more detail?"}
]

chat_prompt = format_chat_prompt(messages)
response = generate(model, tokenizer, prompt=chat_prompt, max_tokens=300)
```

## LLM-കൾക്കുള്ള മികച്ച പ്രാക്ടീസുകൾ

### മെമ്മറി മാനേജ്മെന്റ്

```python
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")
    print(f"Available memory: {memory.available / (1024**3):.2f} GB")

# വലിയ മോഡലുകൾ ലോഡ് ചെയ്യുന്നതിന് മുമ്പ് മെമ്മറി പരിശോധിക്കുക
check_memory_usage()

# മെമ്മറി കാര്യക്ഷമതയ്ക്കായി ക്വാണ്ടൈസ്ഡ് മോഡലുകൾ ഉപയോഗിക്കുക
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # ~4GB
# എതിരായി
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB
```

### മോഡൽ തിരഞ്ഞെടുപ്പ് മാർഗ്ഗനിർദ്ദേശങ്ങൾ

**പരീക്ഷണത്തിനും പഠനത്തിനും:**
- 4-ബിറ്റ് ക്വാണ്ടൈസ്ഡ് മോഡലുകൾ ഉപയോഗിക്കുക (ഉദാ: `mlx-community/Mistral-7B-Instruct-v0.3-4bit`)
- Phi-3-mini പോലുള്ള ചെറിയ മോഡലുകളിൽ നിന്ന് ആരംഭിക്കുക

**പ്രൊഡക്ഷൻ ആപ്ലിക്കേഷനുകൾക്കായി:**
- മോഡൽ വലിപ്പവും ഗുണനിലവാരവും തമ്മിലുള്ള ട്രേഡ്-ഓഫ് പരിഗണിക്കുക
- ക്വാണ്ടൈസ്ഡ് മോഡലുകളും ഫുൾ-പ്രിസിഷൻ മോഡലുകളും പരീക്ഷിക്കുക
- നിങ്ങളുടെ പ്രത്യേക ഉപയോഗ കേസുകളിൽ ബെഞ്ച്മാർക്ക് നടത്തുക

**നിർദ്ദിഷ്ട ടാസ്കുകൾക്കായി:**
- **കോഡ് ജനറേഷൻ**: CodeLlama, Code Llama Instruct
- **ജനറൽ ചാറ്റ്**: Mistral-7B-Instruct, Phi-3
- **ബഹുഭാഷാ**: Qwen മോഡലുകൾ
- **ക്രിയേറ്റീവ് റൈറ്റിംഗ്**: Mistral അല്ലെങ്കിൽ LLaMA ഉപയോഗിച്ച് ഉയർന്ന താപനില സെറ്റിംഗുകൾ

### പ്രോംപ്റ്റ് എഞ്ചിനീയറിംഗ് മികച്ച പ്രാക്ടീസുകൾ

```python
# നിർദ്ദേശങ്ങൾ പാലിക്കുന്ന മോഡലുകൾക്കുള്ള നല്ല പ്രോംപ്റ്റ് ഘടന
def create_instruction_prompt(instruction, context="", examples=""):
    prompt = f"[INST] "
    
    if context:
        prompt += f"Context: {context}\n\n"
    
    if examples:
        prompt += f"Examples:\n{examples}\n\n"
    
    prompt += f"Instruction: {instruction} [/INST]"
    
    return prompt

# ഉദാഹരണ ഉപയോഗം
prompt = create_instruction_prompt(
    instruction="Summarize the following text in 2-3 sentences:",
    context="You are a helpful assistant that provides concise summaries.",
    examples="Text: 'Long article...' Summary: 'Brief summary...'"
)
```

### പ്രകടന ഓപ്റ്റിമൈസേഷൻ

```python
# ഉപയോഗ കേസിന്റെ അടിസ്ഥാനത്തിൽ ജനറേഷൻ പാരാമീറ്ററുകൾ മെച്ചപ്പെടുത്തുക
def optimize_for_use_case(use_case):
    params = {
        "max_tokens": 200,
        "temp": 0.7,
        "top_p": 0.9
    }
    
    if use_case == "code_generation":
        params.update({"temp": 0.3, "max_tokens": 500})
    elif use_case == "creative_writing":
        params.update({"temp": 0.9, "max_tokens": 800})
    elif use_case == "factual_qa":
        params.update({"temp": 0.3, "max_tokens": 150})
    elif use_case == "summarization":
        params.update({"temp": 0.5, "max_tokens": 300})
    
    return params

# ഉപയോഗം
code_params = optimize_for_use_case("code_generation")
response = generate(model, tokenizer, prompt=prompt, **code_params)
```

## പ്രശ്നപരിഹാരം

### സാധാരണ പ്രശ്നങ്ങളും പരിഹാരങ്ങളും

#### ഇൻസ്റ്റലേഷൻ പ്രശ്നങ്ങൾ

**പ്രശ്നം**: "mlx-lm-ക്കായി യോജിക്കുന്ന ഡിസ്‌ട്രിബ്യൂഷൻ കണ്ടെത്തിയില്ല"
```bash
# പൈത്തൺ ആർക്കിടെക്ചർ പരിശോധിക്കുക
python -c "import platform; print(platform.processor())"
# 'i386' അല്ല, 'arm' ഔട്ട്പുട്ട് നൽകണം

# ഔട്ട്പുട്ട് 'i386' ആണെങ്കിൽ, നിങ്ങൾ Rosetta കീഴിൽ x86 പൈത്തൺ ഉപയോഗിക്കുന്നു
# നേറ്റീവ് ARM പൈത്തൺ ഇൻസ്റ്റാൾ ചെയ്യുക അല്ലെങ്കിൽ Conda ഉപയോഗിക്കുക
```

**പരിഹാരം**: നേറ്റീവ് ARM പൈത്തൺ അല്ലെങ്കിൽ മിനികോണ്ട ഉപയോഗിക്കുക:
```bash
# ARM64-ന് വേണ്ടി Miniconda ഇൻസ്റ്റാൾ ചെയ്യുക
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh

# പുതിയ പരിസ്ഥിതി സൃഷ്ടിക്കുക
conda create -n mlx python=3.11
conda activate mlx
pip install mlx-lm
```

#### മെമ്മറി പ്രശ്നങ്ങൾ

**പ്രശ്നം**: "RuntimeError: Out of memory"
```python
# ചെറുതായോ ക്വാണ്ടൈസ്ഡ് മോഡലുകളോ ഉപയോഗിക്കുക
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")  # ~2GB
# പകരം
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB

# macOS 15+ ന് വേണ്ടി, വയർഡ് മെമ്മറി പരിധി വർദ്ധിപ്പിക്കുക
# sudo sysctl iogpu.wired_limit_mb=8192  # നിങ്ങളുടെ RAM അനുസരിച്ച് ക്രമീകരിക്കുക
```

#### മോഡൽ ലോഡിംഗ് പ്രശ്നങ്ങൾ

**പ്രശ്നം**: മോഡൽ ലോഡ് ചെയ്യാൻ പരാജയപ്പെടുന്നു അല്ലെങ്കിൽ മോശം ഔട്ട്പുട്ട് ഉത്പാദിപ്പിക്കുന്നു
```python
# മോഡൽ അഖണ്ഡത പരിശോധിക്കുക
from mlx_lm import load

try:
    model, tokenizer = load("model_name")
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model: {e}")
    
# ഒരു ലളിതമായ പ്രോംപ്റ്റ് ഉപയോഗിച്ച് പരീക്ഷിക്കുക
test_response = generate(model, tokenizer, prompt="Hello", max_tokens=10)
print(f"Test response: {test_response}")
```

#### പ്രകടന പ്രശ്നങ്ങൾ

**പ്രശ്നം**: സ്ലോ ജനറേഷൻ സ്പീഡ്
- മറ്റ് മെമ്മറി-ഇന്റൻസീവ് ആപ്ലിക്കേഷനുകൾ അടയ്ക്കുക
- സാധ്യമായപ്പോൾ ക്വാണ്ടൈസ്ഡ് മോഡലുകൾ ഉപയോഗിക്കുക
- Rosetta-യിൽ ഓടിക്കുന്നില്ലെന്ന് ഉറപ്പാക്കുക
- മോഡലുകൾ ലോഡ് ചെയ്യുന്നതിന് മുമ്പ് ലഭ്യമായ മെമ്മറി പരിശോധിക്കുക

### ഡീബഗ്ഗിംഗ് ടിപ്പുകൾ

```python
# ഡീബഗിംഗിനായി വിശദമായ ഔട്ട്പുട്ട് സജ്ജമാക്കുക
response = generate(
    model, 
    tokenizer, 
    prompt="Test prompt", 
    verbose=True,  # ജനറേഷൻ പുരോഗതി കാണിക്കുന്നു
    max_tokens=50
)

# സിസ്റ്റം വിഭവങ്ങൾ നിരീക്ഷിക്കുക
import psutil
import time

def monitor_generation():
    start_time = time.time()
    start_memory = psutil.virtual_memory().percent
    
    response = generate(model, tokenizer, prompt="Long prompt...", max_tokens=200)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().percent
    
    print(f"Generation time: {end_time - start_time:.2f} seconds")
    print(f"Memory change: {end_memory - start_memory:.1f}%")
    
    return response
```

## അധിക വിഭവങ്ങൾ

### ഔദ്യോഗിക ഡോക്യുമെന്റേഷൻ, റിപോസിറ്ററികൾ

- **MLX GitHub റിപോസിറ്ററി**: https://github.com/ml-explore/mlx
- **MLX-LM ഉദാഹരണങ്ങൾ**: https://github.com/ml-explore/mlx-examples/tree/main/llms
- **MLX ഡോക്യുമെന്റേഷൻ**: https://ml-explore.github.io/mlx/
- **Hugging Face MLX ഇന്റഗ്രേഷൻ**: https://huggingface.co/docs/hub/en/mlx

### മോഡൽ ശേഖരങ്ങൾ

- **MLX കമ്മ്യൂണിറ്റി മോഡലുകൾ**: https://huggingface.co/mlx-community
- **ട്രെൻഡിംഗ് MLX മോഡലുകൾ**: https://huggingface.co/models?library=mlx&sort=trending

### ഉദാഹരണ ആപ്ലിക്കേഷനുകൾ

1. **പേഴ്സണൽ AI അസിസ്റ്റന്റ്**: സംഭാഷണ മെമ്മറിയുള്ള ലോക്കൽ ചാറ്റ്ബോട്ട് നിർമ്മിക്കുക
2. **കോഡ് ഹെൽപ്പർ**: നിങ്ങളുടെ ഡെവലപ്പ്മെന്റ് വർക്ക്‌ഫ്ലോയ്ക്ക് കോഡിംഗ് അസിസ്റ്റന്റ് സൃഷ്ടിക്കുക
3. **കണ്ടന്റ് ജനറേറ്റർ**: എഴുത്ത്, സംഗ്രഹം, ഉള്ളടക്ക സൃഷ്ടിക്കുള്ള ഉപകരണങ്ങൾ വികസിപ്പിക്കുക
4. **കസ്റ്റം ഫൈൻ-ട്യൂൺ ചെയ്ത മോഡലുകൾ**: ഡൊമെയ്ൻ-സ്പെസിഫിക് ടാസ്കുകൾക്കായി മോഡലുകൾ അനുയോജ്യപ്പെടുത്തുക
5. **മൾട്ടി-മോഡൽ ആപ്ലിക്കേഷനുകൾ**: ടെക്സ്റ്റ് ജനറേഷൻ മറ്റ് MLX കഴിവുകളുമായി സംയോജിപ്പിക്കുക

### കമ്മ്യൂണിറ്റി, പഠനം

- **MLX കമ്മ്യൂണിറ്റി ചർച്ചകൾ**: GitHub Issues and Discussions
- **Hugging Face ഫോറങ്ങൾ**: കമ്മ്യൂണിറ്റി പിന്തുണയും മോഡൽ പങ്കുവെപ്പും
- **ആപ്പിൾ ഡെവലപ്പർ ഡോക്യുമെന്റേഷൻ**: ഔദ്യോഗിക ആപ്പിൾ ML വിഭവങ്ങൾ

### ഉദ്ധരണി

നിങ്ങൾ നിങ്ങളുടെ ഗവേഷണത്തിൽ MLX ഉപയോഗിക്കുന്നുവെങ്കിൽ, ദയവായി ഉദ്ധരിക്കുക:

```bibtex
@software{mlx2023,
    author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
    title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
    url = {https://github.com/ml-explore},
    version = {0.26.5},
    year = {2023},
}
```

---

## നിഗമനം

ആപ്പിൾ MLX മാക് കമ്പ്യൂട്ടറുകളിൽ വലിയ ഭാഷാ മോഡലുകൾ ഓടിക്കുന്ന രംഗത്ത് വിപ്ലവം സൃഷ്ടിച്ചു. നേറ്റീവ് ആപ്പിൾ സിലിക്കൺ ഓപ്റ്റിമൈസേഷൻ, സുതാര്യമായ Hugging Face ഇന്റഗ്രേഷൻ, ക്വാണ്ടൈസേഷൻ, LoRA ഫൈൻ-ട്യൂണിംഗ് പോലുള്ള ശക്തമായ സവിശേഷതകൾ നൽകിക്കൊണ്ട്, MLX സങ്കീർണ്ണമായ ഭാഷാ മോഡലുകൾ ലോക്കലായി മികച്ച പ്രകടനത്തോടെ ഓടിക്കാൻ സാധ്യമാക്കുന്നു.

നിങ്ങൾ ചാറ്റ്ബോട്ടുകൾ, കോഡ് അസിസ്റ്റന്റുകൾ, ഉള്ളടക്ക ജനറേറ്ററുകൾ, അല്ലെങ്കിൽ കസ്റ്റം ഫൈൻ-ട്യൂൺ ചെയ്ത മോഡലുകൾ നിർമ്മിക്കുകയാണെങ്കിൽ, MLX ആപ്പിൾ സിലിക്കൺ മാക് ഉപയോഗിച്ച് ഭാഷാ മോഡൽ ആപ്ലിക്കേഷനുകളുടെ പൂർണ്ണ ശേഷി പ്രയോജനപ്പെടുത്താൻ ആവശ്യമായ ഉപകരണങ്ങളും പ്രകടനവും നൽകുന്നു. കാര്യക്ഷമതയിലും ഉപയോഗസൗകര്യത്തിലും ഫ്രെയിംവർക്ക് ശ്രദ്ധ കേന്ദ്രീകരിച്ചതിനാൽ ഗവേഷണത്തിനും പ്രൊഡക്ഷൻ ആപ്ലിക്കേഷനുകൾക്കുമായി ഇത് മികച്ച തിരഞ്ഞെടുപ്പാണ്.

ഈ ട്യൂട്ടോറിയലിലെ അടിസ്ഥാന ഉദാഹരണങ്ങളിൽ നിന്ന് ആരംഭിച്ച്, Hugging Face-ൽ പ്രീ-കൺവേർട്ടഡ് മോഡലുകളുടെ സമൃദ്ധമായ ഇക്കോസിസ്റ്റം അന്വേഷിച്ച്, ഫൈൻ-ട്യൂണിംഗ്, കസ്റ്റം മോഡൽ വികസനം പോലുള്ള കൂടുതൽ അഡ്വാൻസ്ഡ് സവിശേഷതകളിലേക്ക് ക്രമമായി മുന്നോട്ട് പോവുക. MLX ഇക്കോസിസ്റ്റം വളരുന്നതിനൊപ്പം, ആപ്പിൾ ഹാർഡ്‌വെയറിൽ ഭാഷാ മോഡൽ വികസനത്തിനായി ഇത് കൂടുതൽ ശക്തമായ പ്ലാറ്റ്ഫോമായി മാറുകയാണ്.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**അസൂയാപത്രം**:  
ഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, യന്ത്രം ചെയ്ത വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖ അധികാരപരമായ ഉറവിടമായി കണക്കാക്കപ്പെടണം. നിർണായക വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനം ഉപയോഗിക്കുന്നതിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ തെറ്റായ വ്യാഖ്യാനങ്ങൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->