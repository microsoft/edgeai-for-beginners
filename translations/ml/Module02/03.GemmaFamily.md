<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "c6ab7eb1b8009ae984eaf6b85568f77a",
  "translation_date": "2025-12-15T18:06:29+00:00",
  "source_file": "Module02/03.GemmaFamily.md",
  "language_code": "ml"
}
-->
# വിഭാഗം 3: ജെമ്മ ഫാമിലി അടിസ്ഥാനങ്ങൾ

ജെമ്മ മോഡൽ ഫാമിലി ഗൂഗിളിന്റെ ഓപ്പൺ-സോഴ്‌സ് വലിയ ഭാഷാ മോഡലുകളും മൾട്ടിമോഡൽ എഐയും സംബന്ധിച്ച സമഗ്ര സമീപനത്തെ പ്രതിനിധീകരിക്കുന്നു, വിവിധ സാഹചര്യങ്ങളിൽ മൊബൈൽ ഉപകരണങ്ങളിൽ നിന്ന് എന്റർപ്രൈസ് വർക്ക്‌സ്റ്റേഷനുകളിലേക്കുള്ള വിന്യാസം സാധ്യമാക്കുന്ന അതുല്യ പ്രകടനം കൈവരിക്കാൻ ലഭ്യമായ മോഡലുകൾ സാധ്യമാക്കുന്നു. ജെമ്മ ഫാമിലി ശക്തമായ എഐ കഴിവുകൾ ഫ്ലെക്സിബിൾ വിന്യാസ ഓപ്ഷനുകളോടെ എങ്ങനെ സജ്ജമാക്കുന്നുവെന്ന് മനസ്സിലാക്കുക, മത്സരാധിഷ്ഠിത പ്രകടനവും ഉത്തരവാദിത്വമുള്ള എഐ പ്രാക്ടീസുകളും നിലനിർത്തിക്കൊണ്ട്.

## പരിചയം

ഈ ട്യൂട്ടോറിയലിൽ, നാം ഗൂഗിളിന്റെ ജെമ്മ മോഡൽ ഫാമിലി അതിന്റെ അടിസ്ഥാന ആശയങ്ങൾ പരിശോധിക്കും. ജെമ്മ ഫാമിലിയുടെ വികാസം, ജെമ്മ മോഡലുകൾ ഫലപ്രദമാക്കുന്ന നവീന പരിശീലന രീതികൾ, ഫാമിലിയിലെ പ്രധാന വകഭേദങ്ങൾ, വിവിധ വിന്യാസ സാഹചര്യങ്ങളിലെ പ്രായോഗിക ഉപയോഗങ്ങൾ എന്നിവ ഉൾപ്പെടും.

## പഠന ലക്ഷ്യങ്ങൾ

ഈ ട്യൂട്ടോറിയലിന്റെ അവസാനം, നിങ്ങൾക്ക് കഴിയും:

- ഗൂഗിളിന്റെ ജെമ്മ മോഡൽ ഫാമിലിയുടെ രൂപകൽപ്പന തത്ത്വവും വികാസവും മനസ്സിലാക്കുക
- വിവിധ പാരാമീറ്റർ വലുപ്പങ്ങളിൽ ജെമ്മ മോഡലുകൾ ഉയർന്ന പ്രകടനം കൈവരിക്കാൻ സഹായിക്കുന്ന പ്രധാന നവീകരണങ്ങൾ തിരിച്ചറിയുക
- ജെമ്മ മോഡൽ വകഭേദങ്ങളുടെ ഗുണങ്ങളും പരിമിതികളും തിരിച്ചറിയുക
- യഥാർത്ഥ ലോക സാഹചര്യങ്ങൾക്ക് അനുയോജ്യമായ വകഭേദങ്ങൾ തിരഞ്ഞെടുക്കാൻ ജെമ്മ മോഡലുകളുടെ അറിവ് പ്രയോഗിക്കുക

## ആധുനിക എഐ മോഡൽ ലാൻഡ്‌സ്‌കേപ്പ് മനസ്സിലാക്കൽ

എഐ രംഗം വളരെ വികസിച്ചിട്ടുണ്ട്, വിവിധ സംഘടനകൾ ഭാഷാ മോഡൽ വികസനത്തിന് വ്യത്യസ്ത സമീപനങ്ങൾ സ്വീകരിക്കുന്നു. ചിലർ APIകൾ വഴി മാത്രമേ ലഭ്യമാകുന്ന സ്വകാര്യ ക്ലോസ്ഡ്-സോഴ്‌സ് മോഡലുകളിൽ ശ്രദ്ധ കേന്ദ്രീകരിക്കുന്നുവെങ്കിൽ, മറ്റുള്ളവർ ഓപ്പൺ-സോഴ്‌സ് ലഭ്യതക്കും പാരദർശിത്വത്തിനും പ്രാധാന്യം നൽകുന്നു. പരമ്പരാഗത സമീപനം വലിയ സ്വകാര്യ മോഡലുകൾക്കോ തുടർച്ചയായ ചെലവുകൾക്കോ അല്ലെങ്കിൽ വിന്യാസത്തിന് സാങ്കേതിക വിദഗ്ധത ആവശ്യമായ ഓപ്പൺ-സോഴ്‌സ് മോഡലുകൾക്കോ ആശ്രയിച്ചിരിക്കുന്നു.

ഈ മാതൃക ശക്തമായ എഐ കഴിവുകൾ തേടുന്ന സംഘടനകൾക്ക് അവരുടെ ഡാറ്റ, ചെലവുകൾ, വിന്യാസ ഫ്ലെക്സിബിലിറ്റി എന്നിവ നിയന്ത്രിക്കാനുള്ള വെല്ലുവിളികൾ സൃഷ്ടിക്കുന്നു. പരമ്പരാഗത സമീപനം ആധുനിക പ്രകടനവും പ്രായോഗിക വിന്യാസ പരിഗണനകളും തമ്മിൽ തിരഞ്ഞെടുക്കേണ്ടി വരും.

## ലഭ്യമായ എഐ മികവിന്റെ വെല്ലുവിളി

വിവിധ സാഹചര്യങ്ങളിൽ ഉയർന്ന നിലവാരമുള്ള, ലഭ്യമായ എഐ ആവശ്യകത വർദ്ധിച്ചുവരുന്നു. വ്യത്യസ്ത സംഘടനാ ആവശ്യങ്ങൾക്കായി ഫ്ലെക്സിബിൾ വിന്യാസ ഓപ്ഷനുകൾ ആവശ്യമായ അപ്ലിക്കേഷനുകൾ, API ചെലവുകൾ വലിയതാകുന്ന ചെലവുകുറഞ്ഞ നടപ്പാക്കലുകൾ, സമഗ്രമായ മനസ്സിലാക്കലിനുള്ള മൾട്ടിമോഡൽ കഴിവുകൾ, മൊബൈൽ, എഡ്ജ് ഉപകരണങ്ങളിൽ പ്രത്യേക വിന്യാസം എന്നിവ പരിഗണിക്കുക.

### പ്രധാന വിന്യാസ ആവശ്യകതകൾ

ആധുനിക എഐ വിന്യാസങ്ങൾ പ്രായോഗിക പ്രയോഗം പരിമിതപ്പെടുത്തുന്ന ചില അടിസ്ഥാന ആവശ്യകതകൾ നേരിടുന്നു:

- **ലഭ്യത**: പാരദർശിത്വത്തിനും ഇഷ്ടാനുസൃതമാക്കലിനും ഓപ്പൺ-സോഴ്‌സ് ലഭ്യത
- **ചെലവ് ഫലപ്രദത**: വിവിധ ബജറ്റുകൾക്കായി യുക്തമായ കംപ്യൂട്ടേഷണൽ ആവശ്യകതകൾ
- **ഫ്ലെക്സിബിലിറ്റി**: വ്യത്യസ്ത വിന്യാസ സാഹചര്യങ്ങൾക്ക് അനുസരിച്ച് വിവിധ മോഡൽ വലുപ്പങ്ങൾ
- **മൾട്ടിമോഡൽ മനസ്സിലാക്കൽ**: ദൃശ്യ, വാചക, ശബ്ദ പ്രോസസ്സിംഗ് കഴിവുകൾ
- **എഡ്ജ് വിന്യാസം**: മൊബൈൽ, സ്രോതസ്സ്-പരിമിത ഉപകരണങ്ങളിൽ മെച്ചപ്പെട്ട പ്രകടനം

## ജെമ്മ മോഡൽ തത്ത്വശാസ്ത്രം

ജെമ്മ മോഡൽ ഫാമിലി ഗൂഗിളിന്റെ സമഗ്ര എഐ മോഡൽ വികസന സമീപനത്തെ പ്രതിനിധീകരിക്കുന്നു, ഓപ്പൺ-സോഴ്‌സ് ലഭ്യത, മൾട്ടിമോഡൽ കഴിവുകൾ, പ്രായോഗിക വിന്യാസം എന്നിവ മുൻഗണന നൽകുന്നു, മത്സരാധിഷ്ഠിത പ്രകടന സവിശേഷതകൾ നിലനിർത്തുന്നു. ജെമ്മ മോഡലുകൾ വ്യത്യസ്ത മോഡൽ വലുപ്പങ്ങൾ, ജെമിനി ഗവേഷണത്തിൽ നിന്നുള്ള ഉയർന്ന നിലവാരമുള്ള പരിശീലന രീതികൾ, വ്യത്യസ്ത ഡൊമെയ്ൻ, വിന്യാസ സാഹചര്യങ്ങൾക്കായി പ്രത്യേക വകഭേദങ്ങൾ എന്നിവ വഴി ഇത് കൈവരിക്കുന്നു.

ജെമ്മ ഫാമിലി പ്രകടന-ക്ഷമത സ്പെക്ട്രത്തിൽ ഓപ്ഷനുകൾ നൽകാൻ രൂപകൽപ്പന ചെയ്ത വിവിധ സമീപനങ്ങൾ ഉൾക്കൊള്ളുന്നു, മൊബൈൽ ഉപകരണങ്ങളിൽ നിന്ന് എന്റർപ്രൈസ് സെർവറുകളിലേക്കുള്ള വിന്യാസം സാധ്യമാക്കുന്നു, അതേസമയം അർത്ഥപൂർണ എഐ കഴിവുകൾ നൽകുന്നു. ഉയർന്ന നിലവാരമുള്ള എഐ സാങ്കേതികവിദ്യക്ക് ജനാധിപത്യ ആക്‌സസ് നൽകുകയും വിന്യാസ തിരഞ്ഞെടുപ്പുകളിൽ ഫ്ലെക്സിബിലിറ്റി നൽകുകയും ചെയ്യുകയാണ് ലക്ഷ്യം.

### പ്രധാന ജെമ്മ രൂപകൽപ്പന സിദ്ധാന്തങ്ങൾ

ജെമ്മ മോഡലുകൾ മറ്റ് ഭാഷാ മോഡൽ ഫാമിലികളിൽ നിന്ന് വ്യത്യസ്തമാക്കുന്ന ചില അടിസ്ഥാന സിദ്ധാന്തങ്ങളിൽ നിർമ്മിച്ചിരിക്കുന്നു:

- **ഓപ്പൺ സോഴ്‌സ് പ്രഥമം**: ഗവേഷണത്തിനും വാണിജ്യ ഉപയോഗത്തിനും പൂർണ്ണ പാരദർശിത്വവും ലഭ്യതയും
- **ഗവേഷണ-നിർമ്മിത വികസനം**: ജെമിനി മോഡലുകൾക്ക് ശക്തി നൽകുന്ന അതേ ഗവേഷണവും സാങ്കേതികവിദ്യയും ഉപയോഗിച്ച് നിർമ്മിച്ചത്
- **സ്കെയിലബിൾ ആർക്കിടെക്ചർ**: വ്യത്യസ്ത കംപ്യൂട്ടേഷണൽ ആവശ്യകതകൾക്കായി നിരവധി മോഡൽ വലുപ്പങ്ങൾ
- **ഉത്തരവാദിത്വമുള്ള എഐ**: സംയോജിത സുരക്ഷാ നടപടികളും ഉത്തരവാദിത്വമുള്ള വികസന പ്രാക്ടീസുകളും

## ജെമ്മ ഫാമിലി സജ്ജമാക്കുന്ന പ്രധാന സാങ്കേതികവിദ്യകൾ

### പുരോഗമനപരമായ പരിശീലന രീതികൾ

ജെമ്മ ഫാമിലിയുടെ നിർണ്ണായക ഘടകങ്ങളിൽ ഒന്നാണ് ഗൂഗിളിന്റെ ജെമിനി ഗവേഷണത്തിൽ നിന്നുള്ള സങ്കീർണ്ണമായ പരിശീലന സമീപനം. ജെമ്മ മോഡലുകൾ വലിയ മോഡലുകളിൽ നിന്നുള്ള ഡിസ്റ്റിലേഷൻ, മനുഷ്യ പ്രതികരണത്തിൽ നിന്നുള്ള റീ ഇൻഫോഴ്‌സ്‌മെന്റ് ലേണിംഗ് (RLHF), മോഡൽ മേഴ്ജിംഗ് സാങ്കേതികവിദ്യകൾ ഉപയോഗിച്ച് ഗണിതം, കോഡിംഗ്, നിർദ്ദേശങ്ങൾ പാലിക്കൽ എന്നിവയിൽ മെച്ചപ്പെട്ട പ്രകടനം കൈവരിക്കുന്നു.

പരിശീലന പ്രക്രിയയിൽ വലിയ ഇൻസ്ട്രക്റ്റ് മോഡലുകളിൽ നിന്നുള്ള ഡിസ്റ്റിലേഷൻ, മനുഷ്യ ഇഷ്ടാനുസരണം പൊരുത്തപ്പെടുത്താൻ RLHF, ഗണിത തർക്കത്തിന് മെഷീൻ ഫീഡ്ബാക്കിൽ നിന്നുള്ള റീ ഇൻഫോഴ്‌സ്‌മെന്റ് ലേണിംഗ് (RLMF), കോഡിംഗ് കഴിവുകൾക്കായി എക്സിക്യൂഷൻ ഫീഡ്ബാക്കിൽ നിന്നുള്ള റീ ഇൻഫോഴ്‌സ്‌മെന്റ് ലേണിംഗ് (RLEF) എന്നിവ ഉൾപ്പെടുന്നു.

### മൾട്ടിമോഡൽ സംയോജനം, മനസ്സിലാക്കൽ

സമകാലീന ജെമ്മ മോഡലുകൾ വിവിധ ഇൻപുട്ട് തരംകളിൽ സമഗ്രമായ മനസ്സിലാക്കൽ സാധ്യമാക്കുന്ന സങ്കീർണ്ണമായ മൾട്ടിമോഡൽ കഴിവുകൾ ഉൾക്കൊള്ളുന്നു:

**ദൃശ്യ-ഭാഷ സംയോജനം (ജെമ്മ 3)**: ജെമ്മ 3 വാചകവും ചിത്രങ്ങളും ഒരേസമയം പ്രോസസ് ചെയ്യാൻ കഴിയും, ചിത്രങ്ങൾ വിശകലനം ചെയ്യുക, ദൃശ്യ ഉള്ളടക്കത്തെക്കുറിച്ച് ചോദ്യങ്ങൾക്ക് ഉത്തരം നൽകുക, ചിത്രങ്ങളിൽ നിന്നുള്ള വാചകം എടുക്കുക, സങ്കീർണ്ണമായ ദൃശ്യ ഡാറ്റ മനസ്സിലാക്കുക എന്നിവ സാധ്യമാക്കുന്നു.

**ശബ്ദ പ്രോസസ്സിംഗ് (ജെമ്മ 3n)**: ജെമ്മ 3n സ്വയംകൃതമായ വാചക തിരിച്ചറിയൽ (ASR)യും സ്വയംകൃതമായ വാചക വിവർത്തനവും (AST) ഉൾക്കൊള്ളുന്നു, പ്രത്യേകിച്ച് ഇംഗ്ലീഷ്-സ്പാനിഷ്, ഫ്രഞ്ച്, ഇറ്റാലിയൻ, പോർച്ചുഗീസ് ഭാഷകളിൽ വിവർത്തനത്തിൽ ശക്തമായ പ്രകടനം.

**ഇന്റർലീവ് ഇൻപുട്ട് പ്രോസസ്സിംഗ്**: ജെമ്മ മോഡലുകൾ മൾട്ടിമോഡൽ ഇടപെടലുകളിൽ വാചകം, ചിത്രം, ശബ്ദം ഒരുമിച്ച് പ്രോസസ് ചെയ്യാൻ പിന്തുണ നൽകുന്നു.

### ആർക്കിടെക്ചറൽ നവീകരണങ്ങൾ

ജെമ്മ ഫാമിലി പ്രകടനത്തിനും ക്ഷമതയ്ക്കും വേണ്ടി രൂപകൽപ്പന ചെയ്ത ചില ആർക്കിടെക്ചറൽ മെച്ചപ്പെടുത്തലുകൾ ഉൾക്കൊള്ളുന്നു:

**കോൺടെക്സ്റ്റ് വിൻഡോ വിപുലീകരണം**: ജെമ്മ 3 മോഡലുകൾ 128K-ടോക്കൺ കോൺടെക്സ്റ്റ് വിൻഡോ ഉപയോഗിക്കുന്നു, മുൻ ജെമ്മ മോഡലുകളേക്കാൾ 16 മടങ്ങ് വലുത്, നിരവധി ഡോക്യുമെന്റുകളും നൂറുകണക്കിന് ചിത്രങ്ങളും പ്രോസസ് ചെയ്യാൻ കഴിയും.

**മൊബൈൽ-ഫസ്റ്റ് ആർക്കിടെക്ചർ (ജെമ്മ 3n)**: ജെമ്മ 3n പർ-ലെയർ എംബെഡിംഗ്സ് (PLE) സാങ്കേതികവിദ്യയും MatFormer ആർക്കിടെക്ചറും ഉപയോഗിച്ച് വലിയ മോഡലുകൾ ചെറിയ പരമ്പരാഗത മോഡലുകളെപ്പോലെ മെമ്മറി ഉപയോഗിച്ച് പ്രവർത്തിപ്പിക്കുന്നു.

**ഫംഗ്ഷൻ കോളിംഗ് കഴിവുകൾ**: ജെമ്മ 3 ഫംഗ്ഷൻ കോളിംഗ് പിന്തുണയ്ക്കുന്നു, ഡെവലപ്പർമാർക്ക് പ്രോഗ്രാമിംഗ് ഇന്റർഫേസുകൾക്കായി നാചുറൽ ലാംഗ്വേജ് ഇന്റർഫേസുകൾ നിർമ്മിക്കാനും ബുദ്ധിമുട്ടില്ലാത്ത ഓട്ടോമേഷൻ സിസ്റ്റങ്ങൾ സൃഷ്ടിക്കാനും സാധിക്കുന്നു.

## മോഡൽ വലുപ്പവും വിന്യാസ ഓപ്ഷനുകളും

ആധുനിക വിന്യാസ പരിസ്ഥിതികൾ ജെമ്മ മോഡലുകളുടെ വ്യത്യസ്ത കംപ്യൂട്ടേഷണൽ ആവശ്യകതകളെ അനുസരിച്ച് ഫ്ലെക്സിബിലിറ്റിയിൽ നിന്ന് പ്രയോജനം നേടുന്നു:

### ചെറിയ മോഡലുകൾ (0.6B-4B)

ജെമ്മ ചെറു മോഡലുകൾ എഡ്ജ് വിന്യാസം, മൊബൈൽ അപ്ലിക്കേഷനുകൾ, സ്രോതസ്സ്-പരിമിത പരിസ്ഥിതികൾക്കായി ഫലപ്രദമാണ്, അതേസമയം ശ്രദ്ധേയമായ കഴിവുകൾ നിലനിർത്തുന്നു. 1B മോഡൽ ചെറിയ അപ്ലിക്കേഷനുകൾക്കായി അനുയോജ്യമാണ്, 4B മോഡൽ മൾട്ടിമോഡൽ പിന്തുണയോടെ പ്രകടനവും ഫ്ലെക്സിബിലിറ്റിയും സമന്വയിപ്പിക്കുന്നു.

### മധ്യവലുപ്പ മോഡലുകൾ (8B-14B)

മധ്യവലുപ്പ മോഡലുകൾ പ്രൊഫഷണൽ അപ്ലിക്കേഷനുകൾക്കായി മെച്ചപ്പെട്ട കഴിവുകൾ നൽകുന്നു, വർക്ക്‌സ്റ്റേഷൻ, സെർവർ വിന്യാസത്തിനായി പ്രകടനവും കംപ്യൂട്ടേഷണൽ ആവശ്യകതകളും തമ്മിൽ മികച്ച ബാലൻസ് നൽകുന്നു.

### വലിയ മോഡലുകൾ (27B+)

പൂർണ്ണ വലുപ്പ മോഡലുകൾ ആവശ്യമായ ഏറ്റവും ഉയർന്ന കഴിവ് ആവശ്യപ്പെടുന്ന അപ്ലിക്കേഷനുകൾ, ഗവേഷണം, എന്റർപ്രൈസ് വിന്യാസങ്ങൾക്കായി ആധുനിക പ്രകടനം നൽകുന്നു. 27B മോഡൽ ഒറ്റ GPUയിൽ പ്രവർത്തിക്കാൻ കഴിയുന്ന ഏറ്റവും ശേഷിയുള്ള ഓപ്ഷനാണ്.

### മൊബൈൽ-ഓപ്റ്റിമൈസ്ഡ് മോഡലുകൾ (ജെമ്മ 3n)

ജെമ്മ 3n E2B, E4B മോഡലുകൾ പ്രത്യേകിച്ച് മൊബൈൽ, എഡ്ജ് വിന്യാസത്തിനായി രൂപകൽപ്പന ചെയ്തതാണ്, 2B, 4B ഫലപ്രദ പാരാമീറ്റർ കണക്കുകൾ ഉപയോഗിക്കുന്നു, മെമ്മറി ഫുട്പ്രിന്റ് E2Bയ്ക്ക് 2GB, E4Bയ്ക്ക് 3GB വരെ കുറയ്ക്കാൻ നവീന ആർക്കിടെക്ചർ ഉപയോഗിക്കുന്നു.

## ജെമ്മ മോഡൽ ഫാമിലിയുടെ ഗുണങ്ങൾ

### ഓപ്പൺ സോഴ്‌സ് ലഭ്യത

ജെമ്മ മോഡലുകൾ പൂർണ്ണ പാരദർശിത്വവും ഇഷ്ടാനുസൃതമാക്കലും നൽകുന്നു, ഉത്തരവാദിത്വമുള്ള വാണിജ്യ ഉപയോഗത്തിന് തുറന്ന വെയ്റ്റുകൾ അനുവദിക്കുന്നു, സംഘടനകൾക്ക് അവരെ സ്വന്തം പ്രോജക്റ്റുകളിലും അപ്ലിക്കേഷനുകളിലും ട്യൂൺ ചെയ്ത് വിന്യസിക്കാൻ സാധിക്കുന്നു.

### വിന്യാസ ഫ്ലെക്സിബിലിറ്റി

വിവിധ മോഡൽ വലുപ്പങ്ങൾ മൊബൈൽ ഉപകരണങ്ങളിൽ നിന്ന് ഉയർന്ന നിലവാരമുള്ള സെർവറുകളിലേക്കുള്ള വ്യത്യസ്ത ഹാർഡ്‌വെയർ കോൺഫിഗറേഷനുകളിൽ വിന്യാസം സാധ്യമാക്കുന്നു, ഗൂഗിൾ ക്ലൗഡ് TPUകൾ, NVIDIA GPUകൾ, ROCm വഴി AMD GPUകൾ, Gemma.cpp വഴി CPU എക്സിക്യൂഷൻ എന്നിവ ഉൾപ്പെടെ വിവിധ പ്ലാറ്റ്‌ഫോമുകൾക്കായി ഒപ്റ്റിമൈസ് ചെയ്തിരിക്കുന്നു.

### ബഹുഭാഷാ മികവ്

ജെമ്മ മോഡലുകൾ 140-ലധികം ഭാഷകളിൽ ബഹുഭാഷാ മനസ്സിലാക്കലിലും സൃഷ്ടിയിലും മികവുറ്റവയാണ്, ആഗോള അപ്ലിക്കേഷനുകൾക്കായി അനുയോജ്യമാണ്.

### മത്സരാധിഷ്ഠിത പ്രകടനം

ജെമ്മ മോഡലുകൾ ബഞ്ച്മാർക്കുകളിൽ സ്ഥിരമായി മത്സരാധിഷ്ഠിത ഫലങ്ങൾ കൈവരിക്കുന്നു, ജെമ്മ 3 ഉപയോക്തൃ ഇഷ്ടാനുസരണം വിലയിരുത്തലുകളിൽ പ്രശസ്ത സ്വകാര്യവും ഓപ്പൺ മോഡലുകളും തമ്മിൽ ഉയർന്ന റാങ്ക് നേടുന്നു.

### പ്രത്യേക കഴിവുകൾ

ഡൊമെയ്ൻ-നിർദ്ദിഷ്ട അപ്ലിക്കേഷനുകൾ ജെമ്മയുടെ മൾട്ടിമോഡൽ മനസ്സിലാക്കൽ, ഫംഗ്ഷൻ കോളിംഗ് കഴിവുകൾ, വിവിധ ഹാർഡ്‌വെയർ പ്ലാറ്റ്‌ഫോമുകളിൽ ഒപ്റ്റിമൈസ്ഡ് പ്രകടനം എന്നിവയിൽ നിന്നു പ്രയോജനം നേടുന്നു.

## പ്രായോഗിക ഉദാഹരണങ്ങളും ഉപയോഗ കേസുകളും

സാങ്കേതിക വിശദാംശങ്ങളിൽ പ്രവേശിക്കുന്നതിന് മുമ്പ്, ജെമ്മ മോഡലുകൾ എന്തെല്ലാം സാധ്യമാക്കുന്നുവെന്ന് ചില വ്യക്തമായ ഉദാഹരണങ്ങൾ പരിശോധിക്കാം:

### മൾട്ടിമോഡൽ മനസ്സിലാക്കൽ ഉദാഹരണം

ജെമ്മ 3 വാചകവും ചിത്രങ്ങളും ഒരേസമയം പ്രോസസ് ചെയ്ത് സമഗ്ര വിശകലനം നടത്തുന്നു:

```
User: [Image of a complex chart showing financial data] 
What trends can you identify in this quarterly revenue chart, and what might be causing the fluctuation in Q3?

Gemma 3: Looking at this quarterly revenue chart, I can identify several key patterns:

**Revenue Trends:**
1. **Steady Growth Pattern**: Q1 and Q2 show consistent upward trajectory with approximately 15% quarter-over-quarter growth
2. **Q3 Anomaly**: Notable dip of roughly 8% compared to Q2, breaking the growth pattern
3. **Q4 Recovery**: Strong rebound with 22% growth, reaching new highs

**Potential Q3 Factors:**
- **Seasonal Business Cycles**: Many industries experience summer slowdowns
- **Market Disruption**: Possible competitive pressure or economic uncertainty
- **Operational Changes**: Internal restructuring or supply chain adjustments
- **Product Lifecycle**: End-of-cycle for key products before Q4 launches

**Strategic Implications:**
The Q4 recovery suggests the Q3 dip was temporary rather than systemic, indicating resilient underlying business fundamentals with effective adaptation strategies.
```

### ബഹുഭാഷാ പിന്തുണ ഉദാഹരണം

ജെമ്മ മോഡലുകൾ 140-ലധികം ഭാഷകളിൽ ശക്തമായ ബഹുഭാഷാ കഴിവുകൾ കാണിക്കുന്നു:

```
User: Explain the concept of sustainable development in English, Spanish, and Chinese.

Gemma: **English:**
Sustainable development refers to meeting the needs of the present without compromising the ability of future generations to meet their own needs. It balances economic growth, environmental protection, and social equity to create long-term prosperity while preserving natural resources and ecosystems for future use.

**Español:**
El desarrollo sostenible se refiere a satisfacer las necesidades del presente sin comprometer la capacidad de las generaciones futuras para satisfacer sus propias necesidades. Equilibra el crecimiento económico, la protección ambiental y la equidad social para crear prosperidad a largo plazo mientras preserva los recursos naturales y ecosistemas para uso futuro.

**中文:**
可持续发展是指在满足当代人需求的同时，不损害后代满足其需求的能力。它平衡经济增长、环境保护和社会公平，创造长期繁荣，同时为未来使用保护自然资源和生态系统。
```

### ഫംഗ്ഷൻ കോളിംഗ് ഉദാഹരണം

ജെമ്മ 3 നാചുറൽ ലാംഗ്വേജ് ഇന്റർഫേസുകൾ നിർമ്മിക്കാൻ ഫംഗ്ഷൻ കോളിംഗ് പിന്തുണയ്ക്കുന്നു:

```python
# ഉദാഹരണ ഫംഗ്ഷൻ നിർവചനങ്ങൾ
available_functions = {
    "get_weather": {
        "description": "Get current weather for a location",
        "parameters": {
            "location": "string",
            "units": "string (optional, default: celsius)"
        }
    },
    "schedule_meeting": {
        "description": "Schedule a meeting with participants",
        "parameters": {
            "title": "string",
            "participants": "array of strings",
            "datetime": "ISO datetime string",
            "duration": "integer (minutes)"
        }
    }
}

User: "Can you check the weather in Tokyo and schedule a team meeting for tomorrow at 2 PM about project updates?"

Gemma 3: I'll help you with both tasks. Let me check the weather and schedule your meeting.

Function Calls:
1. get_weather(location="Tokyo", units="celsius")
2. schedule_meeting(
   title="Project Updates Team Meeting",
   participants=["user", "team"],
   datetime="2025-07-08T14:00:00",
   duration=60
)

Based on the results: Tokyo currently has partly cloudy weather at 24°C. Your project updates meeting has been scheduled for tomorrow (July 8th) at 2:00 PM for one hour. Meeting invitations will be sent to team participants.
```

### മൊബൈൽ വിന്യാസ ഉദാഹരണം (ജെമ്മ 3n)

ജെമ്മ 3n ഫലപ്രദമായ മെമ്മറി ഉപയോഗത്തോടെ മൊബൈൽ, എഡ്ജ് വിന്യാസത്തിന് ഒപ്റ്റിമൈസ്ഡ് ആണ്:

```python
# ജെമ്മ 3n ഉപയോഗിച്ച് മൊബൈൽ-ഓപ്റ്റിമൈസ്ഡ് ഇൻഫറൻസ്
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# മൊബൈൽ ഡിപ്ലോയ്മെന്റിനായി ജെമ്മ 3n ലോഡ് ചെയ്യുക
model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-3n-E2B-it",
    torch_dtype=torch.float16,
    device_map="auto",
    load_in_8bit=True  # മൊബൈലിനായി കൂടുതൽ ഓപ്റ്റിമൈസ് ചെയ്യുക
)

tokenizer = AutoTokenizer.from_pretrained("google/gemma-3n-E2B-it")

def mobile_inference(prompt, max_tokens=100):
    """Optimized inference for mobile devices"""
    inputs = tokenizer(
        prompt, 
        return_tensors="pt", 
        max_length=512, 
        truncation=True
    )
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=True,
            temperature=0.7,
            early_stopping=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()

# ഉദാഹരണ മൊബൈൽ ഉപയോഗം
user_query = "Quick summary of renewable energy benefits"
response = mobile_inference(user_query)
print(f"Mobile Response: {response}")
```

### ശബ്ദ പ്രോസസ്സിംഗ് ഉദാഹരണം (ജെമ്മ 3n)

ജെമ്മ 3n വാചക തിരിച്ചറിയലും വിവർത്തനവും ഉൾപ്പെടുന്ന പുരോഗമന ശബ്ദ കഴിവുകൾ ഉൾക്കൊള്ളുന്നു:

```python
# ജെമ്മ 3n ഉപയോഗിച്ച് ഓഡിയോ പ്രോസസ്സിംഗ്
def process_audio_input(audio_file_path, task="transcribe", target_language="en"):
    """
    Process audio input for transcription or translation
    
    Args:
        audio_file_path: Path to audio file
        task: "transcribe" or "translate"
        target_language: Target language for translation
    """
    
    # ജെമ്മ 3n ഓഡിയോ പ്രോസസ്സിംഗ് പൈപ്പ്‌ലൈൻ
    if task == "transcribe":
        prompt = f"<audio>{audio_file_path}</audio>\nTranscribe this audio:"
    elif task == "translate":
        prompt = f"<audio>{audio_file_path}</audio>\nTranslate this audio to {target_language}:"
    
    # ജെമ്മ 3n ഉപയോഗിച്ച് പ്രോസസ്സ് ചെയ്യുക
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=256)
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return result.replace(prompt, "").strip()

# ഉദാഹരണ ഉപയോഗം
# സ്പാനിഷ് ഓഡിയോയെ ടെക്സ്റ്റായി ട്രാൻസ്ക്രൈബ് ചെയ്യുക
transcription = process_audio_input("spanish_audio.wav", task="transcribe")
print(f"Transcription: {transcription}")

# സ്പാനിഷ് സംസാരത്തെ ഇംഗ്ലീഷ് ടെക്സ്റ്റായി പരിഭാഷപ്പെടുത്തുക
translation = process_audio_input("spanish_audio.wav", task="translate", target_language="English")
print(f"Translation: {translation}")
```

## ജെമ്മ ഫാമിലിയുടെ വികാസം

### ജെമ്മ 1.0, 2.0: അടിസ്ഥാന മോഡലുകൾ

ആദ്യകാല ജെമ്മ മോഡലുകൾ ഓപ്പൺ-സോഴ്‌സ് ലഭ്യതയും പ്രായോഗിക വിന്യാസവും സംബന്ധിച്ച അടിസ്ഥാന സിദ്ധാന്തങ്ങൾ സ്ഥാപിച്ചു:

- **ജെമ്മ-2B, 7B**: കാര്യക്ഷമമായ ഭാഷാ മനസ്സിലാക്കലിൽ പ്രാരംഭ റിലീസ്
- **ജെമ്മ 1.5 സീരീസ്**: കോൺടെക്സ്റ്റ് കൈകാര്യം വർദ്ധിപ്പിക്കുകയും പ്രകടനം മെച്ചപ്പെടുത്തുകയും ചെയ്തു
- **ജെമ്മ 2 ഫാമിലി**: മൾട്ടിമോഡൽ കഴിവുകൾ പരിചയപ്പെടുത്തി, മോഡൽ വലുപ്പങ്ങൾ വിപുലീകരിച്ചു

### ജെമ്മ 3: മൾട്ടിമോഡൽ മികവ്

ജെമ്മ 3 സീരീസ് മൾട്ടിമോഡൽ കഴിവുകളിലും പ്രകടനത്തിലും വലിയ പുരോഗതി രേഖപ്പെടുത്തി. ജെമിനി 2.0 മോഡലുകൾക്ക് ശക്തി നൽകുന്ന അതേ ഗവേഷണവും സാങ്കേതികവിദ്യയും ഉപയോഗിച്ച് നിർമ്മിച്ച ജെമ്മ 3 ദൃശ്യ-ഭാഷ മനസ്സിലാക്കൽ, 128K-ടോക്കൺ കോൺടെക്സ്റ്റ് വിൻഡോ, ഫംഗ്ഷൻ കോളിംഗ്, 140-ലധികം ഭാഷകൾക്കുള്ള പിന്തുണ എന്നിവ അവതരിപ്പിച്ചു.

പ്രധാന ജെമ്മ 3 സവിശേഷതകൾ:
- **ജെമ്മ 3-1B മുതൽ 27B വരെ**: വിവിധ വിന്യാസ ആവശ്യങ്ങൾക്കായി സമഗ്ര പരിധി
- **മൾട്ടിമോഡൽ മനസ്സിലാക്കൽ**: പുരോഗമന വാചകവും ദൃശ്യവും തർക്കശേഷി
- **വിപുലീകരിച്ച കോൺടെക്സ്റ്റ്**: 128K-ടോക്കൺ പ്രോസസ്സിംഗ് കഴിവ്
- **ഫംഗ്ഷൻ കോളിംഗ്**: നാചുറൽ ലാംഗ്വേജ് ഇന്റർഫേസ് നിർമ്മാണം
- **മെച്ചപ്പെട്ട പരിശീലനം**: ഡിസ്റ്റിലേഷൻ, റീ ഇൻഫോഴ്‌സ്‌മെന്റ് ലേണിംഗ് ഉപയോഗിച്ച് ഒപ്റ്റിമൈസ്ഡ്

### ജെമ്മ 3n: മൊബൈൽ-ഫസ്റ്റ് നവീകരണം

ജെമ്മ 3n മൊബൈൽ-ഫസ്റ്റ് എഐ ആർക്കിടെക്ചറിൽ ഒരു വിപ്ലവം പ്രതിനിധീകരിക്കുന്നു, പർ-ലെയർ എംബെഡിംഗ്സ് (PLE) സാങ്കേതികവിദ്യ, കംപ്യൂട്ട് ഫ്ലെക്സിബിലിറ്റിക്ക് MatFormer ആർക്കിടെക്ചർ, ശബ്ദ പ്രോസസ്സിംഗ് ഉൾപ്പെടുന്ന സമഗ്ര മൾട്ടിമോഡൽ കഴിവുകൾ എന്നിവ ഉൾക്കൊള്ളുന്നു.

ജെമ്മ 3n നവീകരണങ്ങൾ:
- **E2B, E4B മോഡലുകൾ**: കുറച്ച മെമ്മറി ഉപയോഗിച്ച് ഫലപ്രദമായ 2B, 4B പാരാമീറ്റർ പ്രകടനം
- **ശബ്ദ കഴിവുകൾ**: ഉയർന്ന നിലവാരമുള്ള ASR, വാചക വിവർത്തനം
- **വീഡിയോ മനസ്സിലാക്കൽ**: വളരെ മെച്ചപ്പെട്ട വീഡിയോ പ്രോസസ്സിംഗ് കഴിവുകൾ
- **മൊബൈൽ ഒപ്റ്റിമൈസേഷൻ**: ഫോണുകളിലും ടാബ്ലറ്റുകളിലും റിയൽ-ടൈം എഐക്ക് രൂപകൽപ്പന ചെയ്തത്

## ജെമ്മ മോഡലുകളുടെ പ്രയോഗങ്ങൾ

### എന്റർപ്രൈസ് അപ്ലിക്കേഷനുകൾ

സംഘടനകൾ ദൃശ്യ ഉള്ളടക്കമുള്ള ഡോക്യുമെന്റ് വിശകലനം, മൾട്ടിമോഡൽ പിന്തുണയുള്ള കസ്റ്റമർ സർവീസ് ഓട്ടോമേഷൻ, ബുദ്ധിമുട്ടില്ലാത്ത കോഡിംഗ് സഹായം, ബിസിനസ് ഇന്റലിജൻസ് അപ്ലിക്കേഷനുകൾ എന്നിവയ്ക്ക് ജെമ്മ മോഡലുകൾ ഉപയോഗിക്കുന്നു. ഓപ്പൺ-സോഴ്‌സ് സ്വഭാവം പ്രത്യേക ബിസിനസ് ആവശ്യങ്ങൾക്കായി ഇഷ്ടാനുസൃതമാക്കലിനും ഡാറ്റാ സ്വകാര്യതയും നിയന്ത്രണവും നിലനിർത്തുന്നതിനും സഹായിക്കുന്നു.

### മൊബൈൽ, എഡ്ജ് കംപ്യൂട്ടിംഗ്

മൊബൈൽ അപ്ലിക്കേഷനുകൾ ജെമ്മ 3n ഉപയോഗിച്ച് ഉപകരണങ്ങളിൽ നേരിട്ട് പ്രവർത്തിക്കുന്ന റിയൽ-ടൈം എഐ സജ്ജമാക്കുന്നു, വ്യക്തിഗതവും സ്വകാര്യവുമായ അനുഭവങ്ങൾ വേഗത്തിൽ മൾട്ടിമോഡൽ എഐ കഴിവുകളോടെ നൽകുന്നു. റിയൽ-ടൈം വിവർത്തനം, ബുദ്ധിമുട്ടില്ലാത്ത അസിസ്റ്റന്റുകൾ, ഉള്ളടക്കം സൃഷ്ടിക്കൽ, വ്യക്തിഗത ശുപാർശകൾ എന്നിവ ഉൾപ്പെടുന്നു.

### വിദ്യാഭ്യാസ സാങ്കേതികവിദ്യ

വിദ്യാഭ്യാസ പ്ലാറ്റ്‌ഫോമുകൾ മൾട്ടിമോഡൽ ട്യൂട്ടറിംഗ് അനുഭവങ്ങൾ, ദൃശ്യ ഘടകങ്ങളോടുകൂടിയ സ്വയംകൃത ഉള്ളടക്കം സൃഷ്ടിക്കൽ, ശബ്ദ പ്രോസസ്സിംഗ് ഉപയോഗിച്ചുള്ള ഭാഷാ പഠന സഹായം, വാചകം, ചിത്രം, വാചക സംയോജനം എന്നിവയുള്ള ഇന്ററാക്ടീവ് വിദ്യാഭ്യാസ അനുഭവങ്ങൾക്കായി ജെമ്മ മോഡലുകൾ ഉപയോഗിക്കുന്നു.

### ആഗോള അപ്ലിക്കേഷനുകൾ

അന്താരാഷ്ട്ര അപ്ലിക്കേഷനുകൾ ജെമ്മ മോഡലുകളുടെ ശക്തമായ ബഹുഭാഷാ, സാംസ്കാരിക മികവ്, ദൃശ്യ-ശബ്ദ മനസ്സിലാക്കൽ എന്നിവയിൽ നിന്നു പ്രയോജനം നേടുന്നു, വ്യത്യസ്ത ഭാഷകളിലും സാംസ്കാരിക പശ്ചാത്തലങ്ങളിലും സ്ഥിരതയുള്ള എഐ അനുഭവങ്ങൾ നൽകുന്നു.

## വെല്ലുവിളികളും പരിമിതികളും

### കംപ്യൂട്ടേഷണൽ ആവശ്യകതകൾ

ജെമ്മ വ്യത്യസ്ത വലുപ്പത്തിലുള്ള മോഡലുകൾ നൽകുന്നുവെങ്കിലും, വലിയ വകഭേദങ്ങൾ മികച്ച പ്രകടനത്തിനായി വലിയ കംപ്യൂട്ടേഷണൽ വിഭവങ്ങൾ ആവശ്യപ്പെടുന്നു. മെമ്മറി ആവശ്യകതകൾ ക്വാണ്ടൈസ്ഡ് ചെറിയ മോഡലുകൾക്കായി ഏകദേശം 2GB മുതൽ ഏറ്റവും വലിയ 27B മോഡലിനായി 54GB വരെ വ്യത്യാസപ്പെടുന്നു.

### പ്രത്യേക ഡൊമെയ്ൻ പ്രകടനം

ജെമ്മ മോഡലുകൾ പൊതുവായ ഡൊമെയ്‌നുകളിലും മൾട്ടിമോഡൽ ടാസ്കുകളിലും നല്ല പ്രകടനം കാണിക്കുന്നുവെങ്കിലും, വളരെ പ്രത്യേകമായ അപ്ലിക്കേഷനുകൾക്ക് ഡൊമെയ്ൻ-നിർദ്ദിഷ്ട ഫൈൻ-ട്യൂണിംഗ് അല്ലെങ്കിൽ ടാസ്‌ക്-നിർദ്ദിഷ്ട ഒപ്റ്റിമൈസേഷൻ പ്രയോജനകരമായിരിക്കാം.

### മോഡൽ തിരഞ്ഞെടുപ്പ് സങ്കീർണ്ണത

വ്യത്യസ്ത മോഡലുകൾ, വകഭേദങ്ങൾ, വിന്യാസ ഓപ്ഷനുകൾ എന്നിവയുടെ വ്യാപകമായ പരിധി പുതിയ ഉപയോക്താക്കൾക്ക് തിരഞ്ഞെടുപ്പ് ബുദ്ധിമുട്ടാക്കാം, പ്രകടന-ക്ഷമത വ്യാപാരങ്ങൾ സൂക്ഷ്മമായി പരിഗണിക്കേണ്ടതുണ്ട്.

### ഹാർഡ്‌വെയർ ഒപ്റ്റിമൈസേഷൻ

ജെമ്മ മോഡലുകൾ NVIDIA GPUകൾ, ഗൂഗിൾ ക്ലൗഡ് TPUകൾ, AMD GPUകൾ ഉൾപ്പെടെ വിവിധ പ്ലാറ്റ്‌ഫോമുകൾക്കായി ഒപ്റ്റിമൈസ് ചെയ്തിട്ടുണ്ടെങ്കിലും, വ്യത്യസ്ത ഹാർഡ്‌വെയർ കോൺഫിഗറേഷനുകളിൽ പ്രകടനം വ്യത്യാസപ്പെടാം.

## ജെമ്മ മോഡൽ ഫാമിലിയുടെ ഭാവി

ജെമ്മ മോഡൽ ഫാമിലി ജനാധിപത്യവത്കൃത, ഉയർന്ന നിലവാരമുള്ള എഐയിലേക്ക് തുടർച്ചയായ വികാസത്തെ പ്രതിനിധീകരിക്കുന്നു, മെച്ചപ്പെട്ട ക്ഷമത ഒപ്റ്റിമൈസേഷനുകൾ, വിപുലമായ മൾട്ടിമോഡൽ കഴിവുകൾ, വ്യത്യസ്ത വിന്യാസ സാഹചര്യങ്ങളിലേക്കുള്ള മികച്ച സംയോജനം എന്നിവയുടെ തുടർച്ചയായ വികസനത്തോടെ.

ഭാവിയിൽ ജെമ്മ 3n ആർക്കിടെക്ചർ ആൻഡ്രോയിഡ്, ക്രോം പോലുള്ള പ്രധാന പ്ലാറ്റ്‌ഫോമുകളിൽ സംയോജിപ്പിക്കപ്പെടും, വിപുലമായ ഉപകരണങ്ങളിലെയും അപ്ലിക്കേഷനുകളിലെയും ലഭ്യമായ എഐ അനുഭവങ്ങൾ സാധ്യമാക്കും.

സാങ്കേതികവിദ്യ തുടർച്ചയായി വികസിക്കുമ്പോൾ, ജെമ്മ മോഡലുകൾ അവരുടെ ഓപ്പൺ-സോഴ്‌സ് ലഭ്യത നിലനിർത്തിക്കൊണ്ട് കൂടുതൽ ശേഷിയുള്ളതായും മൊബൈൽ അപ്ലിക്കേഷനുകളിൽ നിന്ന് എന്റർപ്രൈസ് സിസ്റ്റങ്ങളിലേക്കുള്ള വ്യത്യസ്ത സാഹചര്യങ്ങളിലും ഉപയോഗിക്കാവുന്നതുമായ എഐ വിന്യാസം സാധ്യമാക്കുമെന്നും പ്രതീക്ഷിക്കാം.

## വികസനവും സംയോജന ഉദാഹരണങ്ങളും

### ട്രാൻസ്ഫോർമേഴ്സുമായി ക്വിക്ക് സ്റ്റാർട്ട്

ഹഗ്ഗിംഗ് ഫെയ്‌സ് ട്രാൻസ്ഫോർമേഴ്സ് ലൈബ്രറി ഉപയോഗിച്ച് ജെമ്മ മോഡലുകൾ എങ്ങനെ ആരംഭിക്കാമെന്ന് കാണാം:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# ജെമ്മ 3-8ബി മോഡൽ ലോഡ് ചെയ്യുക
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto",
    trust_remote_code=True
)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# സംഭാഷണം തയ്യാറാക്കുക
messages = [
    {"role": "user", "content": "Explain quantum computing and its potential applications."}
]

# പ്രതികരണം സൃഷ്ടിക്കുക
input_text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7,
    top_p=0.9
)

# പ്രതികരണം എടുക്കുകയും പ്രദർശിപ്പിക്കുകയും ചെയ്യുക
output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()
response = tokenizer.decode(output_ids, skip_special_tokens=True)
print(response)
```

### ജെമ്മ 3 ഉപയോഗിച്ച് മൾട്ടിമോഡൽ ഉപയോഗം

```python
from transformers import AutoProcessor, AutoModelForVision2Seq
from PIL import Image

# ജെമ്മ 3 വിഷൻ മോഡൽ ലോഡ് ചെയ്യുക
model_name = "google/gemma-3-4b-it"
model = AutoModelForVision2Seq.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)
processor = AutoProcessor.from_pretrained(model_name)

# ചിത്രം மற்றும் ടെക്സ്റ്റ് ഇൻപുട്ട് പ്രോസസ് ചെയ്യുക
image = Image.open("chart.jpg")
messages = [
    {
        "role": "user", 
        "content": [
            {"type": "image"},
            {"type": "text", "text": "Analyze this chart and explain the key trends you observe."}
        ]
    }
]

# ഇൻപുട്ടുകൾ തയ്യാറാക്കുക
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
model_inputs = processor(text=text, images=image, return_tensors="pt").to(model.device)

# മൾട്ടിമോഡൽ പ്രതികരണം സൃഷ്ടിക്കുക
generated_ids = model.generate(
    **model_inputs,
    max_new_tokens=512,
    do_sample=True,
    temperature=0.7
)

response = processor.decode(generated_ids[0], skip_special_tokens=True)
print(response)
```

### ഫംഗ്ഷൻ കോളിംഗ് നടപ്പാക്കൽ

```python
import json

# ലഭ്യമായ ഫംഗ്ഷനുകൾ നിർവചിക്കുക
functions = [
    {
        "name": "get_current_weather",
        "description": "Get the current weather in a given location",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {"type": "string", "description": "City and state, e.g. San Francisco, CA"},
                "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
            },
            "required": ["location"]
        }
    },
    {
        "name": "calculate_math",
        "description": "Perform mathematical calculations",
        "parameters": {
            "type": "object",
            "properties": {
                "expression": {"type": "string", "description": "Mathematical expression to evaluate"}
            },
            "required": ["expression"]
        }
    }
]

def gemma_function_calling(user_query, available_functions):
    """Implement function calling with Gemma 3"""
    
    # ഫംഗ്ഷൻ നിർവചനങ്ങളോടുകൂടിയ സിസ്റ്റം പ്രോംപ്റ്റ് സൃഷ്ടിക്കുക
    system_prompt = f"""You are a helpful assistant with access to the following functions:

{json.dumps(available_functions, indent=2)}

When the user asks for something that requires a function call, respond with a JSON object containing:
- "function_name": the name of the function to call
- "parameters": the parameters to pass to the function

If no function call is needed, respond normally."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    # ജെമ്മയുമായി പ്രോസസ്സ് ചെയ്യുക
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)
    
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=256,
        temperature=0.3  # കൂടുതൽ ഘടനാപരമായ ഔട്ട്പുട്ടിനായി താപനില കുറയ്ക്കുക
    )
    
    response = tokenizer.decode(generated_ids[0][len(model_inputs.input_ids[0]):], skip_special_tokens=True)
    
    # ഫംഗ്ഷൻ കോൾ ഉണ്ടെങ്കിൽ പാഴ്‌സ് ചെയ്യുക
    try:
        function_call = json.loads(response.strip())
        if "function_name" in function_call:
            return function_call
    except json.JSONDecodeError:
        pass
    
    return {"response": response}

# ഉദാഹരണ ഉപയോഗം
user_request = "What's the weather like in Tokyo and calculate 15% of 850"
result = gemma_function_calling(user_request, functions)
print(result)
```

### 📱 ജെമ്മ 3n ഉപയോഗിച്ച് മൊബൈൽ വിന്യാസം

```python
# മൊബൈൽ വിന്യാസം മെച്ചപ്പെടുത്തിയത്
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

class MobileGemmaService:
    """Optimized Gemma 3n service for mobile deployment"""
    
    def __init__(self, model_name="google/gemma-3n-E2B-it"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self._load_optimized_model()
    
    def _load_optimized_model(self):
        """Load model with mobile optimizations"""
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        
        # മൊബൈലിനായി മെച്ചപ്പെടുത്തലുകളോടെ ലോഡ് ചെയ്യുക
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            load_in_8bit=True,  # കാര്യക്ഷമതയ്ക്കായി ക്വാണ്ടൈസേഷൻ
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # ഇൻഫറൻസിനായി മെച്ചപ്പെടുത്തുക
        self.model.eval()
        if torch.cuda.is_available():
            self.model = torch.jit.trace(self.model, example_inputs=...)  # JIT മെച്ചപ്പെടുത്തൽ
    
    def mobile_chat(self, user_input, max_tokens=150):
        """Optimized chat for mobile devices"""
        messages = [{"role": "user", "content": user_input}]
        
        input_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            max_length=512,
            truncation=True
        )
        
        inputs = self.tokenizer(input_text, return_tensors="pt", max_length=512, truncation=True)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                early_stopping=True,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.replace(input_text, "").strip()
    
    def process_audio(self, audio_path, task="transcribe"):
        """Process audio input with Gemma 3n"""
        if task == "transcribe":
            prompt = f"<audio>{audio_path}</audio>\nTranscribe:"
        elif task == "translate":
            prompt = f"<audio>{audio_path}</audio>\nTranslate to English:"
        
        inputs = self.tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=200,
                temperature=0.3
            )
        
        result = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return result.replace(prompt, "").strip()

# മൊബൈൽ സേവനം ആരംഭിക്കുക
mobile_gemma = MobileGemmaService()

# ഉദാഹരണ മൊബൈൽ ഉപയോഗം
quick_response = mobile_gemma.mobile_chat("Summarize renewable energy benefits in 2 sentences")
print(f"Mobile Response: {quick_response}")

# ഓഡിയോ പ്രോസസ്സിംഗ് ഉദാഹരണം
# audio_transcript = mobile_gemma.process_audio("voice_note.wav", task="transcribe")
# print(f"ഓഡിയോ ട്രാൻസ്ക്രിപ്റ്റ്: {audio_transcript}")
```

### vLLM ഉപയോഗിച്ച് API വിന്യാസം

```python
from vllm import LLM, SamplingParams
import asyncio
from typing import List, Dict

class GemmaAPIService:
    """High-performance Gemma API service using vLLM"""
    
    def __init__(self, model_name="google/gemma-3-8b-it"):
        self.llm = LLM(
            model=model_name,
            tensor_parallel_size=1,
            gpu_memory_utilization=0.8,
            trust_remote_code=True
        )
        
        self.sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=512,
            stop_token_ids=[self.llm.get_tokenizer().eos_token_id]
        )
    
    def format_messages(self, messages: List[Dict[str, str]]) -> str:
        """Format messages for API processing"""
        tokenizer = self.llm.get_tokenizer()
        return tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
    
    async def generate_batch(self, batch_messages: List[List[Dict[str, str]]]) -> List[str]:
        """Generate responses for batch of conversations"""
        formatted_prompts = [self.format_messages(messages) for messages in batch_messages]
        
        # പ്രതികരണങ്ങൾ സൃഷ്ടിക്കുക
        outputs = self.llm.generate(formatted_prompts, self.sampling_params)
        
        # പ്രതികരണങ്ങൾ എടുക്കുക
        responses = []
        for output in outputs:
            response = output.outputs[0].text.strip()
            responses.append(response)
        
        return responses
    
    async def stream_generate(self, messages: List[Dict[str, str]]):
        """Stream generation for real-time applications"""
        formatted_prompt = self.format_messages(messages)
        
        # കുറിപ്പ്: vLLM സ്ട്രീമിംഗ് നടപ്പാക്കൽ ഇവിടെ വരും
        # ഇത് ഒരു ലളിതമായ ഉദാഹരണമാണ്
        outputs = self.llm.generate([formatted_prompt], self.sampling_params)
        response = outputs[0].outputs[0].text
        
        # ചങ്കുകൾ നൽകിക്കൊണ്ട് സ്ട്രീമിംഗ് അനുകരിക്കുക
        words = response.split()
        for i in range(0, len(words), 3):  # ഒരിക്കൽ 3 വാക്കുകൾ നൽകുക
            chunk = " ".join(words[i:i+3])
            yield chunk
            await asyncio.sleep(0.1)  # സ്ട്രീമിംഗ് വൈകിപ്പിക്കൽ അനുകരിക്കുക

# ഉദാഹരണ API ഉപയോഗം
async def api_example():
    api_service = GemmaAPIService()
    
    # ബാച്ച് പ്രോസസ്സിംഗ്
    batch_conversations = [
        [{"role": "user", "content": "Explain machine learning briefly"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}],
        [{"role": "user", "content": "How does photosynthesis work?"}]
    ]
    
    responses = await api_service.generate_batch(batch_conversations)
    for i, response in enumerate(responses):
        print(f"Response {i+1}: {response}")
    
    # സ്ട്രീമിംഗ് ഉദാഹരണം
    print("\nStreaming response:")
    stream_messages = [{"role": "user", "content": "Write a short story about AI and humans working together"}]
    async for chunk in api_service.stream_generate(stream_messages):
        print(chunk, end=" ", flush=True)

# API ഉദാഹരണം പ്രവർത്തിപ്പിക്കുക
# asyncio.run(api_example())
```

## പ്രകടന ബഞ്ച്മാർക്കുകളും നേട്ടങ്ങളും

ജെമ്മ മോഡൽ ഫാമിലി വിവിധ ബഞ്ച്മാർക്കുകളിൽ അതുല്യ പ്രകടനം കൈവരിച്ചിട്ടുണ്ട്, ഓപ്പൺ-സോഴ്‌സ് ലഭ്യതയും ഫലപ്രദ വിന്യാസ സവിശേഷതകളും നിലനിർത്തിക്കൊണ്ട്:

### പ്രധാന പ്രകടന ഹൈലൈറ്റുകൾ

**മൾട്ടിമോഡൽ മികവ്:**
- Gemma 3 വികസിത ടെക്സ്റ്റ്, ദൃശ്യ നിരീക്ഷണ ശേഷികളോടെ ശക്തമായ കഴിവുകൾ ഡെവലപ്പർമാർക്ക് നൽകുന്നു, മൾട്ടിമോഡൽ മനസ്സിലാക്കലിനായി ചിത്രം, ടെക്സ്റ്റ് ഇൻപുട്ട് പിന്തുണയ്ക്കുന്നു
- Gemma 3n ചാറ്റ്ബോട്ട് അരീന ഇലോ സ്കോറുകളിൽ ജനപ്രിയ പ്രോപ്രൈറ്ററി, ഓപ്പൺ മോഡലുകൾക്കിടയിൽ ഉയർന്ന റാങ്ക് നേടുന്നു, ശക്തമായ ഉപയോക്തൃ മുൻഗണന സൂചിപ്പിക്കുന്നു

**ക്ഷമതാ നേട്ടങ്ങൾ:**
- Gemma 3 മോഡലുകൾ 128K ടോക്കൺ വരെ പ്രോംപ്റ്റ് ഇൻപുട്ടുകൾ കൈകാര്യം ചെയ്യാൻ കഴിയും, മുൻ Gemma മോഡലുകളേക്കാൾ 16 മടങ്ങ് വലിയ കോൺടെക്സ്റ്റ് വിൻഡോ
- Gemma 3n Per-Layer Embeddings (PLE) ഉപയോഗിച്ച് RAM ഉപയോഗം ഗണ്യമായി കുറയ്ക്കുന്നു, വലിയ മോഡൽ കഴിവുകൾ നിലനിർത്തുന്നു

**മൊബൈൽ ഓപ്റ്റിമൈസേഷൻ:**
- Gemma 3n E2B 2GB മെമ്മറിയോടെ പ്രവർത്തിക്കുന്നു, E4B 3GB മാത്രം ആവശ്യമാണ്, 5B, 8B റോ പാരാമീറ്റർ കണക്കുകൾ ഉള്ളതിനിടയിലും
- സ്വകാര്യത മുൻഗണനയുള്ള, ഓഫ്‌ലൈൻ-സജ്ജമായ പ്രവർത്തനത്തോടെ മൊബൈൽ ഉപകരണങ്ങളിൽ നേരിട്ടുള്ള റിയൽ-ടൈം AI കഴിവുകൾ

**പരിശീലന സ്കെയിൽ:**
- Gemma 3 1B മോഡലിന് 2T ടോക്കൺ, 4B-ന് 4T, 12B-ന് 12T, 27B-ന് 14T ടോക്കണുകൾ Google TPUകൾ, JAX ഫ്രെയിംവർക്ക് ഉപയോഗിച്ച് പരിശീലിപ്പിച്ചു

### മോഡൽ താരതമ്യ മാട്രിക്സ്

| മോഡൽ സീരീസ് | പാരാമീറ്ററുകളുടെ പരിധി | കോൺടെക്സ്റ്റ് നീളം | പ്രധാന ശക്തികൾ | മികച്ച ഉപയോഗ കേസുകൾ |
|--------------|------------------|----------------|---------------|----------------|
| **Gemma 3** | 1B-27B | 128K | മൾട്ടിമോഡൽ മനസ്സിലാക്കൽ, ഫംഗ്ഷൻ കോൾ ചെയ്യൽ | പൊതുവായ ആപ്ലിക്കേഷനുകൾ, ദൃശ്യ-ഭാഷാ പ്രവർത്തനങ്ങൾ |
| **Gemma 3n** | E2B (5B), E4B (8B) | വ്യത്യസ്തം | മൊബൈൽ ഓപ്റ്റിമൈസേഷൻ, ഓഡിയോ പ്രോസസ്സിംഗ് | മൊബൈൽ ആപ്പുകൾ, എഡ്ജ് കംപ്യൂട്ടിംഗ്, റിയൽ-ടൈം AI |
| **Gemma 2.5** | 0.5B-72B | 32K-128K | ബാലൻസ്ഡ് പ്രകടനം, ബഹുഭാഷാ പിന്തുണ | പ്രൊഡക്ഷൻ ഡിപ്ലോയ്മെന്റ്, നിലവിലുള്ള വർക്ക്‌ഫ്ലോകൾ |
| **Gemma-VL** | വ്യത്യസ്തം | വ്യത്യസ്തം | ദൃശ്യ-ഭാഷാ പ്രത്യേകത | ചിത്രം വിശകലനം, ദൃശ്യ ചോദ്യോത്തരങ്ങൾ |

## മോഡൽ തിരഞ്ഞെടുപ്പ് മാർഗ്ഗനിർദ്ദേശം

### അടിസ്ഥാന ആപ്ലിക്കേഷനുകൾക്കായി
- **Gemma 3-1B**: ലഘു ടെക്സ്റ്റ് ടാസ്കുകൾ, ലളിതമായ മൊബൈൽ ആപ്പുകൾ
- **Gemma 3-4B**: മൾട്ടിമോഡൽ പിന്തുണയോടെ ബാലൻസ്ഡ് പ്രകടനം പൊതുവായ ഉപയോഗത്തിനായി

### മൾട്ടിമോഡൽ ആപ്ലിക്കേഷനുകൾക്കായി
- **Gemma 3-4B/12B**: ചിത്രം മനസ്സിലാക്കൽ, ദൃശ്യ ചോദ്യോത്തരങ്ങൾ
- **Gemma 3n**: ഓഡിയോ പ്രോസസ്സിംഗ് കഴിവുകളുള്ള മൊബൈൽ മൾട്ടിമോഡൽ ആപ്പുകൾ

### മൊബൈൽ, എഡ്ജ് ഡിപ്ലോയ്മെന്റിനായി
- **Gemma 3n E2B**: വിഭവപരിമിതിയുള്ള ഉപകരണങ്ങൾ, റിയൽ-ടൈം മൊബൈൽ AI
- **Gemma 3n E4B**: ഓഡിയോ കഴിവുകളോടെ മെച്ചപ്പെട്ട മൊബൈൽ പ്രകടനം

### എന്റർപ്രൈസ് ഡിപ്ലോയ്മെന്റിനായി
- **Gemma 3-12B/27B**: ഉയർന്ന പ്രകടന ഭാഷാ, ദൃശ്യ മനസ്സിലാക്കൽ
- **ഫംഗ്ഷൻ കോൾ ചെയ്യൽ കഴിവുകൾ**: ബുദ്ധിമുട്ടുള്ള ഓട്ടോമേഷൻ സിസ്റ്റങ്ങൾ നിർമ്മിക്കാൻ

### ആഗോള ആപ്ലിക്കേഷനുകൾക്കായി
- **ഏതെങ്കിലും Gemma 3 വകഭേദം**: 140+ ഭാഷാ പിന്തുണ, സാംസ്കാരിക മനസ്സിലാക്കൽ
- **Gemma 3n**: ഓഡിയോ വിവർത്തനത്തോടെ മൊബൈൽ-ഫസ്റ്റ് ആഗോള ആപ്ലിക്കേഷനുകൾ

## ഡിപ്ലോയ്മെന്റ് പ്ലാറ്റ്ഫോമുകളും ആക്സസിബിലിറ്റിയും

### ക്ലൗഡ് പ്ലാറ്റ്ഫോമുകൾ
- **Vertex AI**: സർവർലെസ് അനുഭവത്തോടെ എന്റു-ടു-എൻഡ് MLOps കഴിവുകൾ
- **Google Kubernetes Engine (GKE)**: സങ്കീർണ്ണ വർക്ക്‌ലോഡുകൾക്കായി സ്കെയിലബിൾ കണ്ടെയ്‌നർ ഡിപ്ലോയ്മെന്റ്
- **Google GenAI API**: വേഗത്തിലുള്ള പ്രോട്ടോടൈപ്പിംഗിനായി നേരിട്ടുള്ള API ആക്‌സസ്
- **NVIDIA API Catalog**: NVIDIA GPUകളിൽ മെച്ചപ്പെട്ട പ്രകടനം

### ലോക്കൽ ഡെവലപ്പ്മെന്റ് ഫ്രെയിംവർക്ക്
- **Hugging Face Transformers**: ഡെവലപ്പ്മെന്റിനായി സ്റ്റാൻഡേർഡ് ഇന്റഗ്രേഷൻ
- **Ollama**: ലളിതമായ ലോക്കൽ ഡിപ്ലോയ്മെന്റ്, മാനേജ്മെന്റ്
- **vLLM**: പ്രൊഡക്ഷൻ-ലേവൽ ഉയർന്ന പ്രകടന സർവിംഗ്
- **Gemma.cpp**: CPU-ഓപ്റ്റിമൈസ്ഡ് എക്സിക്യൂഷൻ
- **Google AI Edge**: മൊബൈൽ, എഡ്ജ് ഡിപ്ലോയ്മെന്റ് ഓപ്റ്റിമൈസേഷൻ

### പഠന വിഭവങ്ങൾ
- **Google AI Studio**: കുറച്ച് ക്ലിക്കുകളോടെ Gemma മോഡലുകൾ പരീക്ഷിക്കുക
- **Kaggle, Hugging Face**: മോഡൽ വെയിറ്റുകൾ, കമ്മ്യൂണിറ്റി ഉദാഹരണങ്ങൾ ഡൗൺലോഡ് ചെയ്യുക
- **ടെക്നിക്കൽ റിപ്പോർട്ടുകൾ**: സമഗ്രമായ ഡോക്യുമെന്റേഷൻ, ഗവേഷണ പേപ്പറുകൾ
- **കമ്മ്യൂണിറ്റി ഫോറങ്ങൾ**: സജീവ കമ്മ്യൂണിറ്റി പിന്തുണ, ചർച്ചകൾ

### Gemma മോഡലുകളുമായി ആരംഭിക്കൽ

#### ഡെവലപ്പ്മെന്റ് പ്ലാറ്റ്ഫോമുകൾ
1. **Google AI Studio**: വെബ് അടിസ്ഥാനത്തിലുള്ള പരീക്ഷണങ്ങൾ ആരംഭിക്കുക
2. **Hugging Face Hub**: മോഡലുകളും കമ്മ്യൂണിറ്റി ഇംപ്ലിമെന്റേഷനുകളും പരിശോധിക്കുക
3. **ലോക്കൽ ഡിപ്ലോയ്മെന്റ്**: Ollama അല്ലെങ്കിൽ Transformers ഉപയോഗിച്ച് ഡെവലപ്പ്മെന്റ്

#### പഠന പാത
1. **പ്രധാന ആശയങ്ങൾ മനസ്സിലാക്കുക**: മൾട്ടിമോഡൽ കഴിവുകളും ഡിപ്ലോയ്മെന്റ് ഓപ്ഷനുകളും പഠിക്കുക
2. **വിവിധ വകഭേദങ്ങൾ പരീക്ഷിക്കുക**: വ്യത്യസ്ത മോഡൽ വലിപ്പങ്ങളും പ്രത്യേക പതിപ്പുകളും പരീക്ഷിക്കുക
3. **ഇംപ്ലിമെന്റേഷൻ അഭ്യാസം**: ഡെവലപ്പ്മെന്റ് പരിസരങ്ങളിൽ മോഡലുകൾ ഡിപ്ലോയ് ചെയ്യുക
4. **പ്രൊഡക്ഷനായി ഓപ്റ്റിമൈസ് ചെയ്യുക**: പ്രത്യേക ഉപയോഗ കേസുകൾക്കും പ്ലാറ്റ്ഫോമുകൾക്കും ഫൈൻ-ട്യൂൺ ചെയ്യുക

#### മികച്ച പ്രാക്ടീസുകൾ
- **ചെറുതായി തുടങ്ങുക**: തുടക്കത്തിൽ Gemma 3-4B ഉപയോഗിച്ച് വികസനം, ടെസ്റ്റിംഗ്
- **അധികൃത ടെംപ്ലേറ്റുകൾ ഉപയോഗിക്കുക**: മികച്ച ഫലങ്ങൾക്ക് ശരിയായ ചാറ്റ് ടെംപ്ലേറ്റുകൾ പ്രയോഗിക്കുക
- **വിഭവങ്ങൾ നിരീക്ഷിക്കുക**: മെമ്മറി ഉപയോഗവും ഇൻഫറൻസ് പ്രകടനവും ട്രാക്ക് ചെയ്യുക
- **പ്രത്യേകത പരിഗണിക്കുക**: മൾട്ടിമോഡൽ അല്ലെങ്കിൽ മൊബൈൽ ആവശ്യങ്ങൾക്കായി അനുയോജ്യമായ വകഭേദങ്ങൾ തിരഞ്ഞെടുക്കുക

## പുരോഗമന ഉപയോഗ മാതൃകകൾ

### ഫൈൻ-ട്യൂണിംഗ് ഉദാഹരണങ്ങൾ

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments
from peft import LoraConfig, get_peft_model
from trl import SFTTrainer
from datasets import load_dataset
import torch

# ഫൈൻ-ട്യൂണിംഗിനായി ജെമ്മ മോഡൽ ലോഡ് ചെയ്യുക
model_name = "google/gemma-3-8b-it"
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

# ഫലപ്രദമായ ഫൈൻ-ട്യൂണിംഗിനായി ലോറ ക്രമീകരിക്കുക
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
)

# മോഡലിൽ ലോറ പ്രയോഗിക്കുക
model = get_peft_model(model, peft_config)

# ജെമ്മയ്ക്ക് അനുയോജ്യമായ പരിശീലന ക്രമീകരണം
training_args = TrainingArguments(
    output_dir="./gemma-finetuned",
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    evaluation_strategy="steps",
    eval_steps=500,
    bf16=True,
    remove_unused_columns=False,
    dataloader_pin_memory=False
)

def format_gemma_instruction(example):
    """Format instruction for Gemma chat template"""
    messages = [
        {"role": "user", "content": example['instruction']},
        {"role": "assistant", "content": example['output']}
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

# ഡാറ്റാസെറ്റ് ലോഡ് ചെയ്ത് തയ്യാറാക്കുക
dataset = load_dataset("your-custom-dataset")
dataset = dataset.map(format_gemma_instruction, remove_columns=dataset["train"].column_names)

# ട്രെയിനർ ആരംഭിക്കുക
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    tokenizer=tokenizer,
    max_seq_length=2048,
    packing=True
)

# ഫൈൻ-ട്യൂണിംഗ് ആരംഭിക്കുക
trainer.train()

# ഫൈൻ-ട്യൂൺ ചെയ്ത മോഡൽ സേവ് ചെയ്യുക
model.save_pretrained("./gemma-custom-model")
tokenizer.save_pretrained("./gemma-custom-model")
```

### പ്രത്യേക പ്രോംപ്റ്റ് എഞ്ചിനീയറിംഗ്

**മൾട്ടിമോഡൽ ടാസ്കുകൾക്കായി:**
```python
def create_multimodal_prompt(text_query, image_description="", task_type="analysis"):
    """Create structured prompt for multimodal tasks"""
    
    if task_type == "analysis":
        system_msg = """You are Gemma, an AI assistant with advanced vision capabilities. 
        When analyzing images, provide detailed, accurate descriptions and insights.
        Structure your analysis with clear sections for visual elements, context, and implications."""
    
    elif task_type == "comparison":
        system_msg = """You are Gemma, an AI assistant specialized in visual comparison tasks.
        Compare images systematically, highlighting similarities, differences, and key insights.
        Organize your comparison with clear categories and specific observations."""
    
    messages = [
        {"role": "system", "content": system_msg},
        {
            "role": "user", 
            "content": [
                {"type": "image"},
                {"type": "text", "text": f"{text_query}\n\nAdditional context: {image_description}"}
            ]
        }
    ]
    
    return messages

# ചിത്രം വിശകലനത്തിനുള്ള ഉദാഹരണ ഉപയോഗം
analysis_prompt = create_multimodal_prompt(
    "Analyze this business chart and identify key trends, patterns, and actionable insights.",
    "This appears to be a quarterly revenue chart spanning 2 years",
    task_type="analysis"
)
```

**കോൺടെക്സ്റ്റോടുകൂടിയ ഫംഗ്ഷൻ കോൾ ചെയ്യലിനായി:**
```python
def create_function_calling_prompt(user_query, available_functions, context=""):
    """Create structured prompt for function calling with Gemma 3"""
    
    function_descriptions = []
    for func in available_functions:
        func_desc = f"- {func['name']}: {func['description']}"
        if 'parameters' in func:
            params = ", ".join([f"{k} ({v.get('type', 'string')})" 
                              for k, v in func['parameters'].get('properties', {}).items()])
            func_desc += f"\n  Parameters: {params}"
        function_descriptions.append(func_desc)
    
    system_prompt = f"""You are Gemma, an AI assistant with access to specific functions.

Available Functions:
{chr(10).join(function_descriptions)}

When a user request requires function calls:
1. Identify which function(s) to use
2. Extract appropriate parameters from the user's request
3. Respond with function calls in this format:
   FUNCTION_CALL: function_name(param1="value1", param2="value2")

If multiple functions are needed, list them separately.
If no function is needed, respond normally.

{f"Context: {context}" if context else ""}"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_query}
    ]
    
    return messages

# ഉദാഹരണ ഫംഗ്ഷൻ നിർവചനങ്ങൾ
weather_functions = [
    {
        "name": "get_weather_forecast",
        "description": "Get weather forecast for a specific location and time period",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "days": {"type": "integer", "description": "Number of days (1-7)"},
                "units": {"type": "string", "description": "Temperature units (celsius/fahrenheit)"}
            }
        }
    },
    {
        "name": "get_weather_alerts",
        "description": "Get current weather alerts and warnings",
        "parameters": {
            "properties": {
                "location": {"type": "string", "description": "City and country"},
                "severity": {"type": "string", "description": "Minimum alert severity"}
            }
        }
    }
]

# ഫംഗ്ഷൻ വിളിക്കുന്ന പ്രോംപ്റ്റ് സൃഷ്ടിക്കുക
user_request = "I'm traveling to Tokyo next week. Can you check the 5-day weather forecast and any severe weather warnings?"
prompt = create_function_calling_prompt(user_request, weather_functions)
```

### സാംസ്കാരിക കോൺടെക്സ്റ്റോടുകൂടിയ ബഹുഭാഷാ ആപ്ലിക്കേഷനുകൾ

```python
def create_culturally_aware_prompt(query, target_cultures, response_style="formal"):
    """Create prompts that consider cultural context"""
    
    cultural_guidelines = {
        "japanese": "Use respectful honorifics, avoid direct confrontation, emphasize group harmony",
        "american": "Be direct and practical, focus on individual benefits, use casual tone",
        "german": "Be precise and thorough, provide detailed explanations, maintain professionalism",
        "brazilian": "Be warm and personable, use inclusive language, consider social aspects",
        "indian": "Show respect for hierarchy, consider diverse regional perspectives, be comprehensive"
    }
    
    style_instructions = {
        "formal": "Use professional language, proper grammar, and respectful tone",
        "casual": "Use conversational language while remaining informative",
        "educational": "Explain concepts clearly with examples and context"
    }
    
    cultural_notes = []
    for culture in target_cultures:
        if culture.lower() in cultural_guidelines:
            cultural_notes.append(f"For {culture} audience: {cultural_guidelines[culture.lower()]}")
    
    system_prompt = f"""You are Gemma, a culturally-aware AI assistant. Provide responses that are 
    appropriate for different cultural contexts while maintaining accuracy and helpfulness.

    Response Style: {style_instructions.get(response_style, style_instructions['formal'])}
    
    Cultural Considerations:
    {chr(10).join(cultural_notes)}
    
    Provide your response in the requested languages with cultural appropriateness."""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": query}
    ]
    
    return messages

# ഉദാഹരണമായി സാംസ്കാരികമായി ബോധമുള്ള ഉപയോഗം
multicultural_query = """Explain the concept of work-life balance and provide practical advice 
for maintaining it. Please provide responses suitable for Japanese and American workplace cultures."""

culturally_aware_prompt = create_culturally_aware_prompt(
    multicultural_query, 
    target_cultures=["Japanese", "American"],
    response_style="educational"
)
```

### പ്രൊഡക്ഷൻ ഡിപ്ലോയ്മെന്റ് മാതൃകകൾ

```python
import asyncio
import logging
from typing import List, Dict, Optional, Union
from dataclasses import dataclass
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from PIL import Image
import time

@dataclass
class GenerationConfig:
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.9
    repetition_penalty: float = 1.05
    do_sample: bool = True

@dataclass
class MultimodalInput:
    text: str
    images: Optional[List[Union[str, Image.Image]]] = None
    audio_path: Optional[str] = None

class ProductionGemmaService:
    """Production-ready Gemma service with multimodal support"""
    
    def __init__(self, model_name: str, device: str = "auto", enable_multimodal: bool = True):
        self.model_name = model_name
        self.device = device
        self.enable_multimodal = enable_multimodal
        self.model = None
        self.tokenizer = None
        self.processor = None
        self.logger = self._setup_logging()
        self._load_model()
    
    def _setup_logging(self):
        """Setup production logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        return logging.getLogger(f"GemmaService-{self.model_name}")
    
    def _load_model(self):
        """Load model and tokenizer with error handling"""
        try:
            self.logger.info(f"Loading model: {self.model_name}")
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name,
                trust_remote_code=True
            )
            
            if self.enable_multimodal and "gemma-3" in self.model_name.lower():
                # മൾട്ടിമോഡൽ വേരിയന്റ് ലോഡ് ചെയ്യുക
                from transformers import AutoProcessor
                self.processor = AutoProcessor.from_pretrained(self.model_name)
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            else:
                # ടെക്സ്റ്റ്-ഓൺലി മോഡൽ ലോഡ് ചെയ്യുക
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device,
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
            
            # ഇൻഫറൻസ് ഓപ്റ്റിമൈസ് ചെയ്യുക
            self.model.eval()
            self.logger.info("Model loaded successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to load model: {str(e)}")
            raise
    
    def _prepare_multimodal_input(self, multimodal_input: MultimodalInput) -> Dict:
        """Prepare multimodal input for processing"""
        if not self.enable_multimodal or not self.processor:
            # ടെക്സ്റ്റ്-ഓൺലി പ്രോസസ്സിംഗ്
            return {"text": multimodal_input.text}
        
        inputs = {"text": multimodal_input.text}
        
        if multimodal_input.images:
            # ചിത്രങ്ങൾ പ്രോസസ്സ് ചെയ്യുക
            processed_images = []
            for img in multimodal_input.images:
                if isinstance(img, str):
                    img = Image.open(img)
                processed_images.append(img)
            inputs["images"] = processed_images
        
        if multimodal_input.audio_path:
            # ഓഡിയോ പ്രോസസ്സ് ചെയ്യുക (ജെമ്മ 3എൻ പ്രത്യേകമായി)
            inputs["audio"] = multimodal_input.audio_path
        
        return inputs
    
    async def generate_async(
        self, 
        messages: List[Dict[str, str]], 
        config: GenerationConfig = GenerationConfig(),
        multimodal_input: Optional[MultimodalInput] = None
    ) -> str:
        """Async generation with multimodal support"""
        
        start_time = time.time()
        
        try:
            if multimodal_input and self.enable_multimodal:
                # മൾട്ടിമോഡൽ പ്രോസസ്സിംഗ്
                processed_input = self._prepare_multimodal_input(multimodal_input)
                
                if self.processor:
                    # മൾട്ടിമോഡൽ ഇൻപുട്ടിനായി പ്രോസസർ ഉപയോഗിക്കുക
                    text = self.processor.apply_chat_template(
                        messages, 
                        tokenize=False, 
                        add_generation_prompt=True
                    )
                    
                    model_inputs = self.processor(
                        text=text,
                        images=processed_input.get("images"),
                        return_tensors="pt"
                    ).to(self.model.device)
                else:
                    # ടെക്സ്റ്റ്-ഓൺലിക്ക് ഫാൾബാക്ക്
                    formatted_text = self.tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    )
                    model_inputs = self.tokenizer(
                        formatted_text,
                        return_tensors="pt"
                    ).to(self.model.device)
            else:
                # ടെക്സ്റ്റ്-ഓൺലി പ്രോസസ്സിംഗ്
                formatted_text = self.tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                model_inputs = self.tokenizer(
                    formatted_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=4096
                ).to(self.model.device)
            
            # പ്രതികരണം സൃഷ്ടിക്കുക
            with torch.no_grad():
                outputs = await asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: self.model.generate(
                        **model_inputs,
                        max_new_tokens=config.max_tokens,
                        temperature=config.temperature,
                        top_p=config.top_p,
                        repetition_penalty=config.repetition_penalty,
                        do_sample=config.do_sample,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                )
            
            # സൃഷ്ടിച്ച ടെക്സ്റ്റ് എടുക്കുക
            if self.processor and multimodal_input:
                generated_text = self.processor.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            else:
                generated_text = self.tokenizer.decode(
                    outputs[0][model_inputs["input_ids"].shape[1]:],
                    skip_special_tokens=True
                )
            
            generation_time = time.time() - start_time
            self.logger.info(f"Generation completed in {generation_time:.2f}s")
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"Generation failed: {str(e)}")
            raise
    
    def health_check(self) -> Dict[str, Union[str, bool, float]]:
        """Comprehensive health check"""
        health_status = {
            "status": "healthy",
            "model_loaded": False,
            "multimodal_enabled": self.enable_multimodal,
            "response_time": None,
            "memory_usage": None,
            "errors": []
        }
        
        try:
            # മോഡൽ ലോഡ് ചെയ്തിട്ടുണ്ടോ എന്ന് പരിശോധിക്കുക
            if self.model is not None and self.tokenizer is not None:
                health_status["model_loaded"] = True
            else:
                health_status["errors"].append("Model not properly loaded")
                health_status["status"] = "unhealthy"
                return health_status
            
            # അടിസ്ഥാന പ്രവർത്തനം പരിശോധിക്കുക
            start_time = time.time()
            test_messages = [{"role": "user", "content": "Hello"}]
            
            # ഹെൽത്ത് ചെക്കിനായി സിങ്ക്രോണസ് ടെസ്റ്റ്
            formatted_text = self.tokenizer.apply_chat_template(
                test_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_text, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=10,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response_time = time.time() - start_time
            health_status["response_time"] = response_time
            
            # മെമ്മറി ഉപയോഗം പരിശോധിക്കുക
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024 / 1024  # എംബി
                health_status["memory_usage"] = memory_used
            
            # പ്രതികരണ സമയം വിലയിരുത്തുക
            if response_time > 5.0:  # സെക്കൻഡുകൾ
                health_status["errors"].append("Response time exceeds threshold")
                health_status["status"] = "degraded"
            
        except Exception as e:
            health_status["status"] = "unhealthy"
            health_status["errors"].append(f"Health check failed: {str(e)}")
        
        return health_status

# ഉദാഹരണ പ്രൊഡക്ഷൻ ഉപയോഗം
async def production_example():
    # പ്രൊഡക്ഷൻ സർവീസ് ആരംഭിക്കുക
    gemma_service = ProductionGemmaService(
        model_name="google/gemma-3-8b-it",
        enable_multimodal=True
    )
    
    # ഹെൽത്ത് ചെക്ക്
    health = gemma_service.health_check()
    print(f"Service Health: {health}")
    
    if health["status"] != "healthy":
        print("Service not healthy, aborting")
        return
    
    # ടെക്സ്റ്റ്-ഓൺലി ജനറേഷൻ
    text_messages = [
        {"role": "user", "content": "Explain the importance of sustainable development"}
    ]
    
    response = await gemma_service.generate_async(text_messages)
    print(f"Text Response: {response}")
    
    # മൾട്ടിമോഡൽ ജനറേഷൻ (സഹായിക്കുന്നുവെങ്കിൽ)
    if gemma_service.enable_multimodal:
        multimodal_messages = [
            {
                "role": "user", 
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": "Describe what you see in this image and explain its significance"}
                ]
            }
        ]
        
        # ചിത്രത്തോടെ ഉദാഹരണം
        multimodal_input = MultimodalInput(
            text="Analyze this business chart",
            images=["path/to/chart.jpg"]
        )
        
        multimodal_response = await gemma_service.generate_async(
            multimodal_messages,
            multimodal_input=multimodal_input
        )
        print(f"Multimodal Response: {multimodal_response}")

# പ്രൊഡക്ഷൻ ഉദാഹരണം പ്രവർത്തിപ്പിക്കുക
# asyncio.run(production_example())
```

## പ്രകടന ഓപ്റ്റിമൈസേഷൻ തന്ത്രങ്ങൾ

### മെമ്മറി ഓപ്റ്റിമൈസേഷൻ

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
import torch

# ജെമ്മ മോഡലുകൾക്കായി മെമ്മറി-ക്ഷമമായ ലോഡിംഗ് തന്ത്രങ്ങൾ
def load_optimized_gemma(model_name, optimization_level="balanced"):
    """Load Gemma with various optimization levels"""
    
    if optimization_level == "maximum_efficiency":
        # പരമാവധി മെമ്മറി കാര്യക്ഷമതയ്ക്കായി 4-ബിറ്റ് ക്വാണ്ടൈസേഷൻ
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
    elif optimization_level == "balanced":
        # സമതുലിത പ്രകടനത്തിനായി 8-ബിറ്റ് ക്വാണ്ടൈസേഷൻ
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
        
    elif optimization_level == "performance":
        # പരമാവധി പ്രകടനത്തിനായി പൂർണ്ണ കൃത്യത
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            device_map="auto",
            torch_dtype=torch.bfloat16,
            trust_remote_code=True
        )
    
    return model

# ഉദാഹരണ ഉപയോഗം
efficient_model = load_optimized_gemma("google/gemma-3-8b-it", "maximum_efficiency")
```

### ഇൻഫറൻസ് ഓപ്റ്റിമൈസേഷൻ

```python
import torch
from torch.nn.attention import SDPABackend, sdpa_kernel
from transformers import AutoTokenizer
import time

class OptimizedGemmaInference:
    """Optimized inference class for Gemma models"""
    
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self._setup_optimizations()
    
    def _setup_optimizations(self):
        """Configure various inference optimizations"""
        
        # മെച്ചപ്പെടുത്തിയ ശ്രദ്ധാ യന്ത്രങ്ങൾ സജീവമാക്കുക
        if torch.cuda.is_available():
            torch.backends.cuda.enable_flash_sdp(True)
            torch.backends.cuda.enable_math_sdp(True)
            torch.backends.cuda.enable_mem_efficient_sdp(True)
        
        # CPU പ്രവർത്തനങ്ങൾക്ക് അനുയോജ്യമായ ത്രെഡിംഗ് സജ്ജമാക്കുക
        torch.set_num_threads(min(8, torch.get_num_threads()))
        
        # ആവർത്തിക്കുന്ന മാതൃകകൾക്കായി JIT സംയോജനം സജീവമാക്കുക
        if hasattr(torch.jit, 'set_fusion_strategy'):
            torch.jit.set_fusion_strategy([('STATIC', 3), ('DYNAMIC', 20)])
        
        # ഇൻഫറൻസിനായി മോഡൽ മെച്ചപ്പെടുത്തുക
        self.model.eval()
        
        # ലഭ്യമായെങ്കിൽ torch.compile സജീവമാക്കുക (PyTorch 2.0+)
        if hasattr(torch, 'compile'):
            try:
                self.model = torch.compile(self.model, mode="reduce-overhead")
            except Exception as e:
                print(f"Torch compile failed: {e}")
    
    def fast_generate(self, messages, max_tokens=256, use_cache=True):
        """Optimized generation with various performance enhancements"""
        
        start_time = time.time()
        
        # ഇൻപുട്ട് ഫോർമാറ്റ് ചെയ്യുക
        formatted_text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(
            formatted_text,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        # മെച്ചപ്പെടുത്തിയ ശ്രദ്ധാ ബാക്ക്എൻഡ് ഉപയോഗിക്കുക
        with torch.no_grad():
            if torch.cuda.is_available():
                with sdpa_kernel(SDPABackend.FLASH_ATTENTION):
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=max_tokens,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        use_cache=use_cache,
                        pad_token_id=self.tokenizer.eos_token_id,
                        early_stopping=True
                    )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    use_cache=use_cache,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
        
        # പ്രതികരണം എടുക്കുക
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        generation_time = time.time() - start_time
        tokens_generated = outputs.shape[1] - inputs.input_ids.shape[1]
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        return {
            "response": response.strip(),
            "generation_time": generation_time,
            "tokens_per_second": tokens_per_second,
            "tokens_generated": tokens_generated
        }
    
    def batch_generate(self, batch_messages, max_tokens=256):
        """Optimized batch generation"""
        
        # എല്ലാ ഇൻപുട്ടുകളും ഫോർമാറ്റ് ചെയ്യുക
        formatted_texts = []
        for messages in batch_messages:
            formatted_text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            formatted_texts.append(formatted_text)
        
        # പാഡിങ്ങോടുകൂടിയ ബാച്ച് ടോക്കൺ ചെയ്യുക
        inputs = self.tokenizer(
            formatted_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=2048
        ).to(self.model.device)
        
        start_time = time.time()
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                use_cache=True,
                pad_token_id=self.tokenizer.eos_token_id,
                early_stopping=True
            )
        
        # എല്ലാ പ്രതികരണങ്ങളും എടുക്കുക
        responses = []
        for i, output in enumerate(outputs):
            response = self.tokenizer.decode(
                output[inputs.input_ids[i].shape[0]:],
                skip_special_tokens=True
            )
            responses.append(response.strip())
        
        generation_time = time.time() - start_time
        
        return {
            "responses": responses,
            "batch_generation_time": generation_time,
            "average_time_per_item": generation_time / len(batch_messages)
        }

# ഉദാഹരണ ഉപയോഗം
tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
model = load_optimized_gemma("google/gemma-3-8b-it", "balanced")

optimized_inference = OptimizedGemmaInference(model, tokenizer)

# ഒറ്റ മെച്ചപ്പെടുത്തിയ ജനറേഷൻ
test_messages = [{"role": "user", "content": "Explain machine learning in simple terms"}]
result = optimized_inference.fast_generate(test_messages)
print(f"Response: {result['response']}")
print(f"Speed: {result['tokens_per_second']:.1f} tokens/second")
```

## മികച്ച പ്രാക്ടീസുകളും മാർഗ്ഗനിർദ്ദേശങ്ങളും

### സുരക്ഷയും സ്വകാര്യതയും

```python
import hashlib
import time
import re
from typing import List, Dict, Optional
import logging

class SecureGemmaService:
    """Security-focused Gemma service implementation"""
    
    def __init__(self, model_name: str, max_requests_per_hour: int = 100):
        self.model_name = model_name
        self.max_requests_per_hour = max_requests_per_hour
        self.model = None
        self.tokenizer = None
        self.request_logs = {}
        self.logger = logging.getLogger("SecureGemmaService")
        self._load_model()
    
    def _load_model(self):
        """Load model with security considerations"""
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            trust_remote_code=True  # വിശ്വസനീയമായ മോഡലുകൾക്കായി മാത്രമേ സജീവമാക്കൂ
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
    
    def _sanitize_input(self, text: str) -> str:
        """Comprehensive input sanitization"""
        if not isinstance(text, str):
            raise ValueError("Input must be a string")
        
        # അപകടകരമായ പാറ്റേണുകൾ നീക്കം ചെയ്യുക
        dangerous_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"data:text/html",
            r"<iframe[^>]*>.*?</iframe>",
            r"<object[^>]*>.*?</object>",
            r"<embed[^>]*>.*?</embed>"
        ]
        
        sanitized = text
        for pattern in dangerous_patterns:
            sanitized = re.sub(pattern, "", sanitized, flags=re.IGNORECASE | re.DOTALL)
        
        # വിഭവ ക്ഷയം തടയാൻ നീളം പരിധി നിശ്ചയിക്കുക
        max_length = 8192
        if len(sanitized) > max_length:
            sanitized = sanitized[:max_length]
            self.logger.warning(f"Input truncated to {max_length} characters")
        
        return sanitized
    
    def _rate_limit_check(self, user_id: str) -> bool:
        """Advanced rate limiting with sliding window"""
        current_time = time.time()
        window_size = 3600  # 1 മണിക്കൂർ സെക്കൻഡുകളിൽ
        
        if user_id not in self.request_logs:
            self.request_logs[user_id] = []
        
        # സമയപരിധിക്ക് പുറത്തുള്ള പഴയ അഭ്യർത്ഥനകൾ ശുചിയാക്കുക
        self.request_logs[user_id] = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        # പരിധി ലംഘിച്ചിട്ടുണ്ടോ എന്ന് പരിശോധിക്കുക
        if len(self.request_logs[user_id]) >= self.max_requests_per_hour:
            self.logger.warning(f"Rate limit exceeded for user {user_id[:8]}...")
            return False
        
        # നിലവിലെ അഭ്യർത്ഥന ലോഗ് ചെയ്യുക
        self.request_logs[user_id].append(current_time)
        return True
    
    def _validate_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Validate and sanitize message structure"""
        if not isinstance(messages, list):
            raise ValueError("Messages must be a list")
        
        if len(messages) == 0:
            raise ValueError("Messages list cannot be empty")
        
        if len(messages) > 50:  # യുക്തിസഹമായ സംഭാഷണ പരിധി
            raise ValueError("Too many messages in conversation")
        
        validated_messages = []
        for message in messages:
            if not isinstance(message, dict):
                raise ValueError("Each message must be a dictionary")
            
            if "role" not in message or "content" not in message:
                raise ValueError("Each message must have 'role' and 'content' fields")
            
            # റോളിന്റെ സാധുത പരിശോധിക്കുക
            valid_roles = ["user", "assistant", "system"]
            if message["role"] not in valid_roles:
                raise ValueError(f"Invalid role: {message['role']}")
            
            # ഉള്ളടക്കം ശുദ്ധീകരിക്കുക
            sanitized_content = self._sanitize_input(message["content"])
            
            validated_messages.append({
                "role": message["role"],
                "content": sanitized_content
            })
        
        return validated_messages
    
    def _content_filter(self, text: str) -> tuple[bool, str]:
        """Basic content filtering for inappropriate content"""
        # ഇത് ഒരു ലളിതമായ ഉദാഹരണമാണ് - പ്രൊഡക്ഷനിൽ കൂടുതൽ സങ്കീർണ്ണമായ ഫിൽട്ടറിംഗ് ഉപയോഗിക്കുക
        prohibited_keywords = [
            "violence", "hate", "illegal", "harmful",
            # നിങ്ങളുടെ ഉപയോഗത്തിനായി ആവശ്യമായത്ര കൂടി ചേർക്കുക
        ]
        
        text_lower = text.lower()
        for keyword in prohibited_keywords:
            if keyword in text_lower:
                return False, f"Content contains prohibited keyword: {keyword}"
        
        return True, ""
    
    def _hash_sensitive_data(self, data: str) -> str:
        """Hash sensitive data for logging"""
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def secure_generate(
        self, 
        messages: List[Dict[str, str]], 
        user_id: str,
        max_tokens: int = 512
    ) -> Dict[str, any]:
        """Generate response with comprehensive security measures"""
        
        try:
            # നിരക്ക് പരിധി
            if not self._rate_limit_check(user_id):
                return {
                    "success": False,
                    "error": "Rate limit exceeded. Please try again later.",
                    "error_code": "RATE_LIMIT_EXCEEDED"
                }
            
            # ഇൻപുട്ട് സാധുത പരിശോധന
            validated_messages = self._validate_messages(messages)
            
            # ഉള്ളടക്കം ഫിൽട്ടറിംഗ്
            for message in validated_messages:
                is_safe, filter_message = self._content_filter(message["content"])
                if not is_safe:
                    self.logger.warning(f"Content filtered for user {user_id[:8]}...: {filter_message}")
                    return {
                        "success": False,
                        "error": "Content violates safety guidelines",
                        "error_code": "CONTENT_FILTERED"
                    }
            
            # അഭ്യർത്ഥന ലോഗ് ചെയ്യുക (സ്വകാര്യതയ്ക്കായി ഹാഷ് ചെയ്ത ഉള്ളടക്കത്തോടെ)
            content_hash = self._hash_sensitive_data(str(validated_messages))
            self.logger.info(f"Processing request from user {user_id[:8]}... Content hash: {content_hash}")
            
            # ടോക്കൺ പരിധി സാധുത പരിശോധിക്കുക
            max_allowed_tokens = min(max_tokens, 1024)
            
            # പ്രതികരണം സൃഷ്ടിക്കുക
            start_time = time.time()
            
            formatted_prompt = self.tokenizer.apply_chat_template(
                validated_messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer(formatted_prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_allowed_tokens,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.05,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            response = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:],
                skip_special_tokens=True
            ).strip()
            
            generation_time = time.time() - start_time
            
            # പ്രതികരണ ഉള്ളടക്കം ഫിൽട്ടർ ചെയ്യുക
            is_safe_response, filter_message = self._content_filter(response)
            if not is_safe_response:
                self.logger.warning(f"Response filtered for user {user_id[:8]}...: {filter_message}")
                return {
                    "success": False,
                    "error": "Generated response violates safety guidelines",
                    "error_code": "RESPONSE_FILTERED"
                }
            
            # വിജയകരമായ സൃഷ്ടി ലോഗ് ചെയ്യുക
            self.logger.info(f"Successful generation for user {user_id[:8]}... in {generation_time:.2f}s")
            
            return {
                "success": True,
                "response": response,
                "generation_time": generation_time,
                "tokens_used": max_allowed_tokens
            }
            
        except ValueError as e:
            self.logger.error(f"Validation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": f"Input validation failed: {str(e)}",
                "error_code": "VALIDATION_ERROR"
            }
        
        except Exception as e:
            self.logger.error(f"Generation error for user {user_id[:8]}...: {str(e)}")
            return {
                "success": False,
                "error": "An error occurred while processing your request",
                "error_code": "GENERATION_ERROR"
            }
    
    def get_usage_statistics(self, user_id: str) -> Dict[str, any]:
        """Get usage statistics for a user"""
        current_time = time.time()
        window_size = 3600  # 1 മണിക്കൂർ
        
        if user_id not in self.request_logs:
            return {
                "requests_last_hour": 0,
                "remaining_requests": self.max_requests_per_hour,
                "reset_time": current_time + window_size
            }
        
        # അടുത്തിടെ ഉണ്ടായ അഭ്യർത്ഥനകൾ എണ്ണുക
        recent_requests = [
            req_time for req_time in self.request_logs[user_id]
            if current_time - req_time < window_size
        ]
        
        requests_last_hour = len(recent_requests)
        remaining_requests = max(0, self.max_requests_per_hour - requests_last_hour)
        
        # പുനഃസജ്ജീകരണ സമയം കണക്കാക്കുക (ഏറ്റവും പഴയ അഭ്യർത്ഥന കാലഹരണപ്പെടുമ്പോൾ)
        reset_time = min(recent_requests) + window_size if recent_requests else current_time
        
        return {
            "requests_last_hour": requests_last_hour,
            "remaining_requests": remaining_requests,
            "reset_time": reset_time
        }

# ഉദാഹരണ സുരക്ഷിത ഉപയോഗം
secure_service = SecureGemmaService("google/gemma-3-8b-it", max_requests_per_hour=50)

# സുരക്ഷിത സൃഷ്ടി
user_messages = [
    {"role": "user", "content": "Explain the benefits of renewable energy"}
]

result = secure_service.secure_generate(user_messages, user_id="user123")

if result["success"]:
    print(f"Secure Response: {result['response']}")
else:
    print(f"Error: {result['error']} (Code: {result['error_code']})")

# ഉപയോഗ സ്ഥിതിവിവരക്കണക്കുകൾ പരിശോധിക്കുക
usage_stats = secure_service.get_usage_statistics("user123")
print(f"Usage Statistics: {usage_stats}")
```

### നിരീക്ഷണവും മൂല്യനിർണയവും

```python
import time
import psutil
import torch
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional
import json
from datetime import datetime
import statistics

@dataclass
class PerformanceMetrics:
    """Comprehensive performance metrics for Gemma models"""
    timestamp: float
    response_time: float
    memory_usage_mb: float
    gpu_memory_mb: float
    token_count: int
    tokens_per_second: float
    model_name: str
    input_length: int
    success: bool
    error_message: Optional[str] = None

@dataclass
class QualityMetrics:
    """Quality assessment metrics"""
    relevance_score: float  # 0-1
    coherence_score: float  # 0-1
    safety_score: float     # 0-1
    factual_accuracy: float # 0-1
    user_satisfaction: Optional[float] = None  # 0-1

class GemmaMonitoringService:
    """Comprehensive monitoring service for Gemma models"""
    
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.performance_history: List[PerformanceMetrics] = []
        self.quality_history: List[QualityMetrics] = []
        self.alert_thresholds = {
            "max_response_time": 10.0,  # സെക്കൻഡുകൾ
            "max_memory_usage": 8192,   # എംബി
            "min_tokens_per_second": 5.0,
            "min_success_rate": 0.95    # 95%
        }
    
    def measure_performance(
        self, 
        model, 
        tokenizer, 
        messages: List[Dict[str, str]],
        expected_tokens: int = 256
    ) -> PerformanceMetrics:
        """Comprehensive performance measurement"""
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # എംബി
        
        # GPU മെട്രിക്‌സ്
        gpu_memory = 0
        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()
            gpu_memory = torch.cuda.memory_allocated() / 1024 / 1024  # എംബി
        
        success = True
        error_message = None
        token_count = 0
        
        try:
            # ഇൻപുട്ട് ഫോർമാറ്റ് ചെയ്യുക
            formatted_text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            input_length = len(tokenizer.encode(formatted_text))
            
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            # പ്രതികരണം സൃഷ്ടിക്കുക
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=expected_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )
            
            token_count = outputs.shape[1] - inputs.input_ids.shape[1]
            
        except Exception as e:
            success = False
            error_message = str(e)
            input_length = 0
        
        # അന്തിമ മെട്രിക്‌സ് കണക്കാക്കുക
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        response_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.max_memory_allocated() / 1024 / 1024
        
        tokens_per_second = token_count / response_time if response_time > 0 and success else 0
        
        metrics = PerformanceMetrics(
            timestamp=start_time,
            response_time=response_time,
            memory_usage_mb=memory_usage,
            gpu_memory_mb=gpu_memory,
            token_count=token_count,
            tokens_per_second=tokens_per_second,
            model_name=self.model_name,
            input_length=input_length,
            success=success,
            error_message=error_message
        )
        
        self.performance_history.append(metrics)
        return metrics
    
    def evaluate_quality(
        self, 
        input_text: str, 
        generated_text: str,
        reference_text: Optional[str] = None
    ) -> QualityMetrics:
        """Evaluate response quality using various metrics"""
        
        # ലളിതമായ ഗുണനിലവാര വിലയിരുത്തൽ (ഉത്പാദനത്തിൽ, കൂടുതൽ സങ്കീർണ്ണമായ രീതികൾ ഉപയോഗിക്കുക)
        relevance_score = self._assess_relevance(input_text, generated_text)
        coherence_score = self._assess_coherence(generated_text)
        safety_score = self._assess_safety(generated_text)
        factual_accuracy = self._assess_factual_accuracy(generated_text, reference_text)
        
        quality_metrics = QualityMetrics(
            relevance_score=relevance_score,
            coherence_score=coherence_score,
            safety_score=safety_score,
            factual_accuracy=factual_accuracy
        )
        
        self.quality_history.append(quality_metrics)
        return quality_metrics
    
    def _assess_relevance(self, input_text: str, output_text: str) -> float:
        """Simple relevance assessment based on keyword overlap"""
        input_words = set(input_text.lower().split())
        output_words = set(output_text.lower().split())
        
        if len(input_words) == 0:
            return 0.0
        
        overlap = len(input_words.intersection(output_words))
        return min(1.0, overlap / len(input_words) * 2)  # യോജിച്ച രീതിയിൽ സ്കെയിൽ ചെയ്യുക
    
    def _assess_coherence(self, text: str) -> float:
        """Simple coherence assessment"""
        sentences = text.split('.')
        if len(sentences) < 2:
            return 0.8  # ചെറുതായുള്ള വാചകം, യുക്തിപൂർവ്വം സുസംബന്ധിതമാണെന്ന് കരുതുക
        
        # ലളിതമായ ഹ്യൂറിസ്റ്റിക്: യുക്തിപൂർവ്വമായ വാചക ദൈർഘ്യം പരിശോധിക്കുക
        avg_sentence_length = sum(len(s.split()) for s in sentences) / len(sentences)
        
        if 5 <= avg_sentence_length <= 25:
            return 0.9
        elif 3 <= avg_sentence_length <= 30:
            return 0.7
        else:
            return 0.5
    
    def _assess_safety(self, text: str) -> float:
        """Simple safety assessment"""
        harmful_indicators = [
            "violence", "harmful", "dangerous", "illegal",
            "hate", "discriminat", "threaten"
        ]
        
        text_lower = text.lower()
        harmful_count = sum(1 for indicator in harmful_indicators if indicator in text_lower)
        
        if harmful_count == 0:
            return 1.0
        elif harmful_count <= 2:
            return 0.7
        else:
            return 0.3
    
    def _assess_factual_accuracy(self, text: str, reference: Optional[str] = None) -> float:
        """Simple factual accuracy assessment"""
        if reference is None:
            # റഫറൻസ് ഇല്ലാതെ, ലളിതമായ ഹ്യൂറിസ്റ്റിക്‌സ് ഉപയോഗിക്കുക
            confidence_indicators = [
                "according to", "research shows", "studies indicate",
                "data suggests", "evidence shows"
            ]
            
            text_lower = text.lower()
            confidence_count = sum(1 for indicator in confidence_indicators if indicator in text_lower)
            
            return min(0.8, 0.5 + confidence_count * 0.1)
        
        # റഫറൻസോടുകൂടി, സാദൃശ്യത കണക്കാക്കുക (സരളീകരിച്ചത്)
        ref_words = set(reference.lower().split())
        text_words = set(text.lower().split())
        
        if len(ref_words) == 0:
            return 0.5
        
        overlap = len(ref_words.intersection(text_words))
        return min(1.0, overlap / len(ref_words))
    
    def get_performance_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        if not self.performance_history:
            return {"error": "No performance data available"}
        
        recent_metrics = self.performance_history[-last_n:]
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            return {"error": "No successful operations in recent history"}
        
        summary = {
            "total_requests": len(recent_metrics),
            "successful_requests": len(successful_metrics),
            "success_rate": len(successful_metrics) / len(recent_metrics),
            "avg_response_time": statistics.mean(m.response_time for m in successful_metrics),
            "avg_memory_usage": statistics.mean(m.memory_usage_mb for m in successful_metrics),
            "avg_tokens_per_second": statistics.mean(m.tokens_per_second for m in successful_metrics),
            "p95_response_time": statistics.quantiles([m.response_time for m in successful_metrics], n=20)[18],
            "total_tokens_generated": sum(m.token_count for m in successful_metrics),
        }
        
        if torch.cuda.is_available():
            summary["avg_gpu_memory"] = statistics.mean(m.gpu_memory_mb for m in successful_metrics)
        
        return summary
    
    def get_quality_summary(self, last_n: int = 100) -> Dict[str, Any]:
        """Get quality metrics summary"""
        if not self.quality_history:
            return {"error": "No quality data available"}
        
        recent_quality = self.quality_history[-last_n:]
        
        return {
            "total_evaluations": len(recent_quality),
            "avg_relevance": statistics.mean(q.relevance_score for q in recent_quality),
            "avg_coherence": statistics.mean(q.coherence_score for q in recent_quality),
            "avg_safety": statistics.mean(q.safety_score for q in recent_quality),
            "avg_factual_accuracy": statistics.mean(q.factual_accuracy for q in recent_quality),
            "overall_quality": statistics.mean([
                (q.relevance_score + q.coherence_score + q.safety_score + q.factual_accuracy) / 4
                for q in recent_quality
            ])
        }
    
    def check_alerts(self) -> List[Dict[str, Any]]:
        """Check for performance alerts"""
        alerts = []
        
        if not self.performance_history:
            return alerts
        
        recent_metrics = self.performance_history[-10:]  # അവസാന 10 അഭ്യർത്ഥനകൾ
        successful_metrics = [m for m in recent_metrics if m.success]
        
        if not successful_metrics:
            alerts.append({
                "type": "error",
                "message": "No successful requests in recent history",
                "severity": "high",
                "timestamp": time.time()
            })
            return alerts
        
        # പ്രതികരണ സമയം പരിശോധിക്കുക
        avg_response_time = statistics.mean(m.response_time for m in successful_metrics)
        if avg_response_time > self.alert_thresholds["max_response_time"]:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {avg_response_time:.2f}s (threshold: {self.alert_thresholds['max_response_time']}s)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # മെമ്മറി ഉപയോഗം പരിശോധിക്കുക
        avg_memory = statistics.mean(m.memory_usage_mb for m in successful_metrics)
        if avg_memory > self.alert_thresholds["max_memory_usage"]:
            alerts.append({
                "type": "memory",
                "message": f"High memory usage: {avg_memory:.1f}MB (threshold: {self.alert_thresholds['max_memory_usage']}MB)",
                "severity": "medium",
                "timestamp": time.time()
            })
        
        # സെക്കൻഡിൽ ടോക്കണുകൾ പരിശോധിക്കുക
        avg_tps = statistics.mean(m.tokens_per_second for m in successful_metrics)
        if avg_tps < self.alert_thresholds["min_tokens_per_second"]:
            alerts.append({
                "type": "performance",
                "message": f"Low generation speed: {avg_tps:.1f} tokens/s (threshold: {self.alert_thresholds['min_tokens_per_second']} tokens/s)",
                "severity": "low",
                "timestamp": time.time()
            })
        
        # വിജയ നിരക്ക് പരിശോധിക്കുക
        success_rate = len(successful_metrics) / len(recent_metrics)
        if success_rate < self.alert_thresholds["min_success_rate"]:
            alerts.append({
                "type": "reliability",
                "message": f"Low success rate: {success_rate:.2%} (threshold: {self.alert_thresholds['min_success_rate']:.2%})",
                "severity": "high",
                "timestamp": time.time()
            })
        
        return alerts
    
    def export_metrics(self, filepath: str):
        """Export metrics to JSON file"""
        export_data = {
            "model_name": self.model_name,
            "export_timestamp": datetime.now().isoformat(),
            "performance_metrics": [asdict(m) for m in self.performance_history],
            "quality_metrics": [asdict(q) for q in self.quality_history],
            "performance_summary": self.get_performance_summary(),
            "quality_summary": self.get_quality_summary(),
            "current_alerts": self.check_alerts()
        }
        
        with open(filepath, 'w') as f:
            json.dump(export_data, f, indent=2)

# ഉദാഹരണ നിരീക്ഷണ ഉപയോഗം
def monitoring_example():
    """Example of comprehensive Gemma monitoring"""
    
    # നിരീക്ഷണം ആരംഭിക്കുക
    monitor = GemmaMonitoringService("google/gemma-3-8b-it")
    
    # പരിശോധനയ്ക്കായി മോഡൽ ലോഡ് ചെയ്യുക
    from transformers import AutoModelForCausalLM, AutoTokenizer
    
    model = AutoModelForCausalLM.from_pretrained(
        "google/gemma-3-8b-it",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-8b-it")
    
    # വിവിധ സാഹചര്യങ്ങൾ പരീക്ഷിക്കുക
    test_scenarios = [
        [{"role": "user", "content": "What is machine learning?"}],
        [{"role": "user", "content": "Explain quantum computing in simple terms"}],
        [{"role": "user", "content": "Write a short story about AI"}],
        [{"role": "user", "content": "How does photosynthesis work?"}],
        [{"role": "user", "content": "What are the benefits of renewable energy?"}]
    ]
    
    print("Running performance tests...")
    for i, messages in enumerate(test_scenarios):
        print(f"Test {i+1}/5: {messages[0]['content'][:50]}...")
        
        # പ്രകടനം അളക്കുക
        perf_metrics = monitor.measure_performance(model, tokenizer, messages)
        print(f"  Response time: {perf_metrics.response_time:.2f}s")
        print(f"  Tokens/sec: {perf_metrics.tokens_per_second:.1f}")
        print(f"  Success: {perf_metrics.success}")
        
        if perf_metrics.success:
            # ഗുണനിലവാര വിലയിരുത്തലിനായി യഥാർത്ഥ പ്രതികരണം സൃഷ്ടിക്കുക
            formatted_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
            inputs = tokenizer(formatted_text, return_tensors="pt").to(model.device)
            
            with torch.no_grad():
                outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)
            
            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
            
            # ഗുണനിലവാരം വിലയിരുത്തുക
            quality_metrics = monitor.evaluate_quality(messages[0]['content'], response)
            print(f"  Quality score: {(quality_metrics.relevance_score + quality_metrics.coherence_score + quality_metrics.safety_score + quality_metrics.factual_accuracy) / 4:.2f}")
        
        print()
    
    # സമഗ്രമായ സംഗ്രഹം നേടുക
    perf_summary = monitor.get_performance_summary()
    quality_summary = monitor.get_quality_summary()
    alerts = monitor.check_alerts()
    
    print("=== Performance Summary ===")
    print(f"Success Rate: {perf_summary.get('success_rate', 0):.2%}")
    print(f"Average Response Time: {perf_summary.get('avg_response_time', 0):.2f}s")
    print(f"Average Tokens/Second: {perf_summary.get('avg_tokens_per_second', 0):.1f}")
    print(f"Total Tokens Generated: {perf_summary.get('total_tokens_generated', 0)}")
    
    print("\n=== Quality Summary ===")
    print(f"Overall Quality Score: {quality_summary.get('overall_quality', 0):.2f}")
    print(f"Average Relevance: {quality_summary.get('avg_relevance', 0):.2f}")
    print(f"Average Safety: {quality_summary.get('avg_safety', 0):.2f}")
    
    print(f"\n=== Alerts ({len(alerts)}) ===")
    for alert in alerts:
        print(f"[{alert['severity'].upper()}] {alert['type']}: {alert['message']}")
    
    # മെട്രിക്‌സ് എക്സ്പോർട്ട് ചെയ്യുക
    monitor.export_metrics("gemma_monitoring_report.json")
    print("\nMetrics exported to gemma_monitoring_report.json")

# നിരീക്ഷണ ഉദാഹരണം പ്രവർത്തിപ്പിക്കുക
# monitoring_example()
```

## സമാപനം

Gemma മോഡൽ കുടുംബം Google-ന്റെ സമഗ്രമായ AI സാങ്കേതികവിദ്യ ജനസാമാന്യത്തിന് എത്തിക്കുന്ന സമീപനത്തെ പ്രതിനിധീകരിക്കുന്നു, വ്യത്യസ്ത ആപ്ലിക്കേഷനുകളിലും ഡിപ്ലോയ്മെന്റ് സാഹചര്യങ്ങളിലും മത്സരാധിഷ്ഠിത പ്രകടനം നിലനിർത്തുന്നു. ഓപ്പൺ-സോഴ്‌സ് ആക്സസിബിലിറ്റി, മൾട്ടിമോഡൽ കഴിവുകൾ, നവീന ആർക്കിടെക്ചറൽ ഡിസൈനുകൾ എന്നിവയിലൂടെ Gemma സംഘടനകൾക്കും ഡെവലപ്പർമാർക്കും ശക്തമായ AI കഴിവുകൾ ഉപയോഗപ്പെടുത്താൻ സഹായിക്കുന്നു, അവരുടെ വിഭവങ്ങളോ പ്രത്യേക ആവശ്യങ്ങളോ എന്തായാലും.

### പ്രധാന ആശയങ്ങൾ

**ഓപ്പൺ സോഴ്‌സ് മികവ്**: Gemma ഓപ്പൺ-സോഴ്‌സ് മോഡലുകൾ പ്രോപ്രൈറ്ററി മോഡലുകളുമായി മത്സരിക്കുന്ന പ്രകടനം കൈവരിക്കാമെന്ന് തെളിയിക്കുന്നു, AI ഡിപ്ലോയ്മെന്റിൽ പാരദർശിത്വം, ഇഷ്ടാനുസൃതം, നിയന്ത്രണം നൽകുന്നു.

**മൾട്ടിമോഡൽ നവീകരണം**: Gemma 3, Gemma 3n-ൽ ടെക്സ്റ്റ്, ദൃശ്യ, ഓഡിയോ കഴിവുകളുടെ സംയോജനം ആക്സസിബിൾ മൾട്ടിമോഡൽ AI-യിൽ വലിയ പുരോഗതിയാണ്, വ്യത്യസ്ത ഇൻപുട്ട് തരംകളിൽ സമഗ്രമായ മനസ്സിലാക്കൽ സാധ്യമാക്കുന്നു.

**മൊബൈൽ-ഫസ്റ്റ് ആർക്കിടെക്ചർ**: Gemma 3n-ന്റെ Per-Layer Embeddings (PLE) സാങ്കേതികവിദ്യയും മൊബൈൽ ഓപ്റ്റിമൈസേഷനും ശക്തമായ AI വിഭവപരിമിതിയുള്ള ഉപകരണങ്ങളിലും കാര്യക്ഷമമായി പ്രവർത്തിക്കാമെന്ന് തെളിയിക്കുന്നു.

**സ്കെയിലബിൾ ഡിപ്ലോയ്മെന്റ്**: 1B മുതൽ 27B പാരാമീറ്ററുകൾ വരെ, പ്രത്യേക മൊബൈൽ വകഭേദങ്ങളോടെ, വിവിധ കംപ്യൂട്ടേഷൻ പരിസരങ്ങളിൽ സ്ഥിരതയുള്ള ഗുണമേന്മയും പ്രകടനവും നിലനിർത്തുന്നു.

**ഉത്തരവാദിത്വമുള്ള AI സംയോജനം**: ShieldGemma 2 വഴി ഉൾപ്പെടുത്തിയ സുരക്ഷാ നടപടികളും ഉത്തരവാദിത്വമുള്ള വികസന പ്രാക്ടീസുകളും ശക്തമായ AI കഴിവുകൾ സുരക്ഷിതവും നൈതികവുമായ രീതിയിൽ ഡിപ്ലോയ് ചെയ്യാൻ ഉറപ്പാക്കുന്നു.

### ഭാവി ദിശ

Gemma കുടുംബം വികസനത്തിലേക്ക് തുടർന്നുകൊണ്ടിരിക്കുമ്പോൾ പ്രതീക്ഷിക്കാം:

**മൊബൈൽ കഴിവുകളുടെ മെച്ചപ്പെടുത്തൽ**: Android, Chrome പോലുള്ള പ്രധാന പ്ലാറ്റ്ഫോമുകളിൽ Gemma 3n ആർക്കിടെക്ചർ സംയോജിപ്പിച്ച് മൊബൈൽ, എഡ്ജ് ഡിപ്ലോയ്മെന്റിനായി കൂടുതൽ ഓപ്റ്റിമൈസേഷൻ.

**മൾട്ടിമോഡൽ മനസ്സിലാക്കലിന്റെ വിപുലീകരണം**: ദൃശ്യ-ഭാഷ-ഓഡിയോ സംയോജനത്തിൽ തുടർച്ചയായ പുരോഗതി കൂടുതൽ സമഗ്ര AI അനുഭവങ്ങൾക്കായി.

**ക്ഷമത മെച്ചപ്പെടുത്തൽ**: മികച്ച പ്രകടന-പാരാമീറ്റർ അനുപാതങ്ങൾ, കുറവായ കംപ്യൂട്ടേഷൻ ആവശ്യകതകൾ നൽകാൻ ആർക്കിടെക്ചറൽ നവീകരണങ്ങൾ.

**വ്യാപകമായ ഇക്കോസിസ്റ്റം സംയോജനം**: വികസന ഫ്രെയിംവർക്ക്, ക്ലൗഡ് പ്ലാറ്റ്ഫോം, ഡിപ്ലോയ്മെന്റ് ടൂളുകൾ എന്നിവയിൽ മെച്ചപ്പെട്ട പിന്തുണ, നിലവിലുള്ള വർക്ക്‌ഫ്ലോകളിലേക്ക് സുതാര്യമായ സംയോജനം.

**കമ്മ്യൂണിറ്റി വളർച്ച**: Gemmaverse-ന്റെ തുടർച്ചയായ വിപുലീകരണം, കമ്മ്യൂണിറ്റി സൃഷ്ടിച്ച മോഡലുകൾ, ടൂളുകൾ, ആപ്ലിക്കേഷനുകൾ മുഖേന മുൾക്കാഴ്ചകൾ വിപുലീകരിക്കുന്നു.

### അടുത്ത ഘട്ടങ്ങൾ

റിയൽ-ടൈം AI കഴിവുകളുള്ള മൊബൈൽ ആപ്പുകൾ നിർമ്മിക്കുകയോ, മൾട്ടിമോഡൽ വിദ്യാഭ്യാസ ഉപകരണങ്ങൾ വികസിപ്പിക്കുകയോ, ബുദ്ധിമുട്ടുള്ള ഓട്ടോമേഷൻ സിസ്റ്റങ്ങൾ സൃഷ്ടിക്കുകയോ, ബഹുഭാഷാ പിന്തുണ ആവശ്യമായ ആഗോള ആപ്ലിക്കേഷനുകളിൽ പ്രവർത്തിക്കുകയോ ചെയ്യുമ്പോൾ Gemma കുടുംബം ശക്തമായ കമ്മ്യൂണിറ്റി പിന്തുണയോടും സമഗ്രമായ ഡോക്യുമെന്റേഷനോടും കൂടിയ സ്കെയിലബിൾ പരിഹാരങ്ങൾ നൽകുന്നു.

**ആരംഭിക്കാൻ ശുപാർശകൾ:**
1. **Google AI Studio-യിൽ പരീക്ഷണം നടത്തുക** ഉടൻ കൈകാര്യം ചെയ്യാനുള്ള അനുഭവത്തിനായി
2. **Hugging Face-ൽ നിന്നുള്ള മോഡലുകൾ ഡൗൺലോഡ് ചെയ്യുക** ലോക്കൽ ഡെവലപ്പ്മെന്റിനും ഇഷ്ടാനുസൃതത്തിനും
3. **Gemma 3n പോലുള്ള പ്രത്യേക വകഭേദങ്ങൾ പരിശോധിക്കുക** മൊബൈൽ ആപ്ലിക്കേഷനുകൾക്കായി
4. **മൾട്ടിമോഡൽ കഴിവുകൾ നടപ്പിലാക്കുക** സമഗ്ര AI അനുഭവങ്ങൾക്കായി
5. **സുരക്ഷാ മികച്ച പ്രാക്ടീസുകൾ പാലിക്കുക** പ്രൊഡക്ഷൻ ഡിപ്ലോയ്മെന്റിനായി

**മൊബൈൽ ഡെവലപ്പ്മെന്റിനായി**: ഓഡിയോ, ദൃശ്യ കഴിവുകളോടെ വിഭവക്ഷമമായ ഡിപ്ലോയ്മെന്റിനായി Gemma 3n E2B-ൽ തുടങ്ങുക.

**എന്റർപ്രൈസ് ആപ്ലിക്കേഷനുകൾക്കായി**: ഫംഗ്ഷൻ കോൾ ചെയ്യലും വികസിത നിരീക്ഷണവും ഉള്ള Gemma 3-12B അല്ലെങ്കിൽ 27B മോഡലുകൾ പരിഗണിക്കുക.

**ആഗോള ആപ്ലിക്കേഷനുകൾക്കായി**: സാംസ്കാരിക ബോധമുള്ള പ്രോംപ്റ്റ് എഞ്ചിനീയറിംഗോടെ Gemma-യുടെ 140+ ഭാഷാ പിന്തുണ ഉപയോഗിക്കുക.

**പ്രത്യേക ഉപയോഗ കേസുകൾക്കായി**: ഫൈൻ-ട്യൂണിംഗ് സമീപനങ്ങളും ഡൊമെയ്ൻ-സ്പെസിഫിക് ഓപ്റ്റിമൈസേഷൻ തന്ത്രങ്ങളും പരിശോധിക്കുക.

### 🔮 AI-യുടെ ജനാധിപത്യവൽക്കരണം

Gemma കുടുംബം ശക്തമായ, കഴിവുള്ള മോഡലുകൾ വ്യക്തിഗത ഡെവലപ്പർമാരിൽ നിന്നും വലിയ എന്റർപ്രൈസുകളിലേക്കും എല്ലാവർക്കും ലഭ്യമാക്കുന്ന AI വികസനത്തിന്റെ ഭാവിയെ പ്രതിനിധീകരിക്കുന്നു. ആധുനിക ഗവേഷണവും ഓപ്പൺ-സോഴ്‌സ് ആക്സസിബിലിറ്റിയും സംയോജിപ്പിച്ച് Google എല്ലാ മേഖലയിലും സ്കെയിലിലും നവീകരണം സാധ്യമാക്കുന്ന ഒരു അടിസ്ഥാനമുണ്ടാക്കി.

100 മില്യൺ ഡൗൺലോഡുകളും 60,000+ കമ്മ്യൂണിറ്റി വകഭേദങ്ങളും ഉള്ള Gemma-യുടെ വിജയം AI സാങ്കേതികവിദ്യ മുന്നോട്ടു കൊണ്ടുപോകുന്നതിൽ തുറന്ന സഹകരണത്തിന്റെ ശക്തി തെളിയിക്കുന്നു. ഭാവിയിൽ Gemma കുടുംബം AI നവീകരണത്തിന് പ്രേരകമായി തുടരുകയും, മുൻപ് പ്രോപ്രൈറ്ററി, ചെലവേറിയ മോഡലുകളാൽ മാത്രമേ സാധ്യമായിരുന്ന ആപ്ലിക്കേഷനുകൾ വികസിപ്പിക്കാൻ സഹായിക്കുകയും ചെയ്യും.

AI-യുടെ ഭാവി തുറന്നതും, ആക്സസിബിളും, ശക്തവുമാണ് – Gemma കുടുംബം ഈ ദർശനം യാഥാർത്ഥ്യമാക്കുന്നതിൽ മുന്നണിയിലാണ്.

## അധിക വിഭവങ്ങൾ

**അധികൃത ഡോക്യുമെന്റേഷൻ, മോഡലുകൾ:**
- **Google AI Studio**: [Gemma മോഡലുകൾ നേരിട്ട് പരീക്ഷിക്കുക](https://aistudio.google.com)
- **Hugging Face Collections**: 
  - [Gemma 3 റിലീസ്](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d)
  - [Gemma 3n പ്രിവ്യൂ](https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4)
- **Google AI ഡെവലപ്പർ ഡോക്യുമെന്റേഷൻ**: [സമഗ്ര Gemma ഗൈഡുകൾ](https://ai.google.dev/gemma)
- **Vertex AI ഡോക്യുമെന്റേഷൻ**: [എന്റർപ്രൈസ് ഡിപ്ലോയ്മെന്റ് ഗൈഡുകൾ](https://cloud.google.com/vertex-ai/generative-ai/docs/open-models/use-gemma)

**ടെക്നിക്കൽ വിഭവങ്ങൾ:**
- **ഗവേഷണ പേപ്പറുകളും ടെക്നിക്കൽ റിപ്പോർട്ടുകളും**: [Google DeepMind പ്രസിദ്ധീകരണങ്ങൾ](https://deepmind.google/models/gemma/)
- **ഡെവലപ്പർ ബ്ലോഗ് പോസ്റ്റുകൾ**: [അടുത്തകാല പ്രഖ്യാപനങ്ങളും ട്യൂട്ടോറിയലുകളും](https://developers.googleblog.com)
- **മോഡൽ കാർഡുകൾ**: വിശദമായ സാങ്കേതിക പ്രത്യേകതകളും പ്രകടന ബഞ്ച്മാർക്കുകളും

**കമ്മ്യൂണിറ്റി, പിന്തുണ:**
- **Hugging Face കമ്മ്യൂണിറ്റി**: സജീവ ചർച്ചകളും കമ്മ്യൂണിറ്റി ഉദാഹരണങ്ങളും
- **GitHub റിപോസിറ്ററികൾ**: ഓപ്പൺ-സോഴ്‌സ് ഇംപ്ലിമെന്റേഷനുകളും ടൂളുകളും
- **ഡെവലപ്പർ ഫോറങ്ങൾ**: Google AI ഡെവലപ്പർ കമ്മ്യൂണിറ്റി പിന്തുണ
- **Stack Overflow**: ടാഗ് ചെയ്ത ചോദ്യങ്ങളും കമ്മ്യൂണിറ്റി പരിഹാരങ്ങളും

**ഡെവലപ്പ്മെന്റ് ടൂളുകൾ:**
- **Ollama**: [ലളിതമായ ലോക്കൽ ഡിപ്ലോയ്മെന്റ്](https://ollama.ai)
- **vLLM**: [ഉയർന്ന പ്രകടന സർവിംഗ്](https://github.com/vllm-project/vllm)
- **Transformers ലൈബ്രറി**: [Hugging Face ഇന്റഗ്രേഷൻ](https://huggingface.co/docs/transformers)
- **Google AI Edge**: മൊബൈൽ, എഡ്ജ് ഡിപ്ലോയ്മെന്റ് ഓപ്റ്റിമൈസേഷൻ

**പഠന പാതകൾ:**
- **ആരംഭകർ**: Google AI Studio → Hugging Face ഉദാഹരണങ്ങൾ → ലോക്കൽ ഡിപ്ലോയ്മെന്റ്
- **ഡെവലപ്പർമാർ**: Transformers ഇന്റഗ്രേഷൻ → ഇഷ്ടാനുസൃത ആപ്ലിക്കേഷനുകൾ → പ്രൊഡക്ഷൻ ഡിപ്ലോയ്മെന്റ്
- **ഗവേഷകർ**: ടെക്നിക്കൽ പേപ്പറുകൾ → ഫൈൻ-ട്യൂണിംഗ് → നവീന ആപ്ലിക്കേഷനുകൾ
- **എന്റർപ്രൈസ്**: Vertex AI ഡിപ്ലോയ്മെന്റ് → സുരക്ഷ നടപ്പാക്കൽ → സ്കെയിൽ ഓപ്റ്റിമൈസേഷൻ

Gemma മോഡൽ കുടുംബം AI മോഡലുകളുടെ ഒരു ശേഖരം മാത്രമല്ല, ആക്സസിബിൾ, ശക്തമായ, ഉത്തരവാദിത്വമുള്ള AI ആപ്ലിക്കേഷനുകളുടെ ഭാവി നിർമ്മിക്കുന്ന സമഗ്ര ഇക്കോസിസ്റ്റമാണ്. ഇന്ന് തന്നെ അന്വേഷനം ആരംഭിച്ച്, ഓപ്പൺ-സോഴ്‌സ് AI-യുടെ സാധ്യതകളെ മുന്നോട്ട് കൊണ്ടുപോകുന്ന ഡെവലപ്പർമാരുടെയും ഗവേഷകരുടെയും വളരുന്ന കമ്മ്യൂണിറ്റിയിൽ ചേരുക.



## അധിക വിഭവങ്ങൾ

### അധികൃത ഡോക്യുമെന്റേഷൻ
- Google Gemma ടെക്നിക്കൽ ഡോക്യുമെന്റേഷൻ
- മോഡൽ കാർഡുകളും ഉപയോഗ മാർഗ്ഗനിർദ്ദേശങ്ങളും
- ഉത്തരവാദിത്വമുള്ള AI നടപ്പാക്കൽ ഗൈഡ്
- Google Vertex AI സംയോജനം ഗൈഡ്

### ഡെവലപ്പ്മെന്റ് ടൂളുകൾ
- ക്ലൗഡ് ഡിപ്ലോയ്മെന്റിനായി Google AI Studio
- മോഡൽ ഇന്റഗ്രേഷനായി Hugging Face Transformers
- ഉയർന്ന പ്രകടന സർവിംഗിനായി vLLM
- CPU-ഓപ്റ്റിമൈസ്ഡ് ഇൻഫറൻസിനായി Gemma.cpp

### പഠന വിഭവങ്ങൾ
- Gemma 3, Gemma 3n ടെക്നിക്കൽ പേപ്പറുകൾ
- Google AI ബ്ലോഗ്, ട്യൂട്ടോറിയലുകൾ
- മോഡൽ ഓപ്റ്റിമൈസേഷൻ, ക്വാണ്ടൈസേഷൻ ഗൈഡുകൾ
- കമ്മ്യൂണിറ്റി ഫോറങ്ങൾ, ചർച്ചാ ഗ്രൂപ്പുകൾ

## പഠന ഫലങ്ങൾ

ഈ മോഡ്യൂൾ പൂർത്തിയാക്കിയ ശേഷം, നിങ്ങൾക്ക് കഴിയും:

1. Gemma മോഡൽ കുടുംബത്തിന്റെ ആർക്കിടെക്ചറൽ നേട്ടങ്ങളും ഓപ്പൺ-സോഴ്‌സ് സമീപനവും വിശദീകരിക്കുക
2. പ്രത്യേക ആപ്ലിക്കേഷൻ ആവശ്യകതകളും ഹാർഡ്‌വെയർ പരിധികളും അടിസ്ഥാനമാക്കി അനുയോജ്യമായ Gemma വകഭേദം തിരഞ്ഞെടുക്കുക
3. മൊബൈൽ മുതൽ ക്ലൗഡ് വരെ വിവിധ ഡിപ്ലോയ്മെന്റ് സാഹചര്യങ്ങളിൽ Gemma മോഡലുകൾ നടപ്പിലാക്കുക, ഓപ്റ്റിമൈസ്ഡ് കോൺഫിഗറേഷനുകൾ ഉപയോഗിച്ച്
4. Gemma മോഡൽ പ്രകടനം മെച്ചപ്പെടുത്താൻ ക്വാണ്ടൈസേഷൻ, ഓപ്റ്റിമൈസേഷൻ തന്ത്രങ്ങൾ പ്രയോഗിക്കുക
5. മോഡൽ വലിപ്പം, പ്രകടനം, കഴിവുകൾ എന്നിവയുടെ ഇടയിൽ വ്യാപാര-offs Gemma കുടുംബത്തിൽ വിലയിരുത്തുക

## അടുത്തത്

- [04: BitNET Family Fundamentals](04.BitNETFamily.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**അസൂയാ**:  
ഈ രേഖ AI വിവർത്തന സേവനം [Co-op Translator](https://github.com/Azure/co-op-translator) ഉപയോഗിച്ച് വിവർത്തനം ചെയ്തതാണ്. നാം കൃത്യതയ്ക്ക് ശ്രമിച്ചിട്ടുണ്ടെങ്കിലും, സ്വയം പ്രവർത്തിക്കുന്ന വിവർത്തനങ്ങളിൽ പിശകുകൾ അല്ലെങ്കിൽ തെറ്റുകൾ ഉണ്ടാകാമെന്ന് ദയവായി ശ്രദ്ധിക്കുക. അതിന്റെ മാതൃഭാഷയിലുള്ള യഥാർത്ഥ രേഖയാണ് പ്രാമാണികമായ ഉറവിടം എന്ന് പരിഗണിക്കേണ്ടതാണ്. നിർണായകമായ വിവരങ്ങൾക്ക്, പ്രൊഫഷണൽ മനുഷ്യ വിവർത്തനം ശുപാർശ ചെയ്യപ്പെടുന്നു. ഈ വിവർത്തനം ഉപയോഗിക്കുന്നതിൽ നിന്നുണ്ടാകുന്ന ഏതെങ്കിലും തെറ്റിദ്ധാരണകൾക്കോ തെറ്റായ വ്യാഖ്യാനങ്ങൾക്കോ ഞങ്ങൾ ഉത്തരവാദികളല്ല.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->