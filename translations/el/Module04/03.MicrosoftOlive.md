# Ενότητα 3: Microsoft Olive Optimization Suite

## Πίνακας Περιεχομένων
1. [Εισαγωγή](../../../Module04)
2. [Τι είναι το Microsoft Olive;](../../../Module04)
3. [Εγκατάσταση](../../../Module04)
4. [Οδηγός Γρήγορης Εκκίνησης](../../../Module04)
5. [Παράδειγμα: Μετατροπή Qwen3 σε ONNX INT4](../../../Module04)
6. [Προχωρημένη Χρήση](../../../Module04)
7. [Αποθετήριο Συνταγών Olive](../../../Module04)
8. [Καλές Πρακτικές](../../../Module04)
9. [Αντιμετώπιση Προβλημάτων](../../../Module04)
10. [Πρόσθετοι Πόροι](../../../Module04)

## Εισαγωγή

Το Microsoft Olive είναι ένα ισχυρό και εύχρηστο εργαλείο βελτιστοποίησης μοντέλων που λαμβάνει υπόψη το υλικό, διευκολύνοντας τη διαδικασία βελτιστοποίησης μοντέλων μηχανικής μάθησης για ανάπτυξη σε διαφορετικές πλατφόρμες υλικού. Είτε στοχεύετε CPUs, GPUs ή εξειδικευμένους επιταχυντές AI, το Olive σας βοηθά να επιτύχετε βέλτιστη απόδοση διατηρώντας την ακρίβεια του μοντέλου.

## Τι είναι το Microsoft Olive;

Το Olive είναι ένα εύχρηστο εργαλείο βελτιστοποίησης μοντέλων που λαμβάνει υπόψη το υλικό και συνδυάζει κορυφαίες τεχνικές στη συμπίεση, βελτιστοποίηση και μεταγλώττιση μοντέλων. Λειτουργεί με το ONNX Runtime ως λύση βελτιστοποίησης από άκρο σε άκρο για την εκτέλεση.

### Βασικά Χαρακτηριστικά

- **Βελτιστοποίηση με βάση το υλικό**: Επιλέγει αυτόματα τις καλύτερες τεχνικές βελτιστοποίησης για το υλικό στόχο σας
- **40+ Ενσωματωμένα Στοιχεία Βελτιστοποίησης**: Καλύπτει συμπίεση μοντέλων, ποσοτικοποίηση, βελτιστοποίηση γραφημάτων και άλλα
- **Εύχρηστη Διεπαφή CLI**: Απλές εντολές για κοινές εργασίες βελτιστοποίησης
- **Υποστήριξη Πολλαπλών Πλαισίων**: Συνεργάζεται με PyTorch, μοντέλα Hugging Face και ONNX
- **Υποστήριξη Δημοφιλών Μοντέλων**: Το Olive μπορεί να βελτιστοποιήσει αυτόματα δημοφιλείς αρχιτεκτονικές μοντέλων όπως Llama, Phi, Qwen, Gemma κ.λπ.

### Οφέλη

- **Μειωμένος Χρόνος Ανάπτυξης**: Δεν χρειάζεται να πειραματιστείτε χειροκίνητα με διαφορετικές τεχνικές βελτιστοποίησης
- **Αυξήσεις Απόδοσης**: Σημαντικές βελτιώσεις ταχύτητας (έως και 6 φορές σε ορισμένες περιπτώσεις)
- **Ανάπτυξη σε Πολλαπλές Πλατφόρμες**: Τα βελτιστοποιημένα μοντέλα λειτουργούν σε διαφορετικό υλικό και λειτουργικά συστήματα
- **Διατήρηση Ακρίβειας**: Οι βελτιστοποιήσεις διατηρούν την ποιότητα του μοντέλου ενώ βελτιώνουν την απόδοση

## Εγκατάσταση

### Προαπαιτούμενα

- Python 3.8 ή νεότερη έκδοση
- Διαχειριστής πακέτων pip
- Εικονικό περιβάλλον (συνιστάται)

### Βασική Εγκατάσταση

Δημιουργήστε και ενεργοποιήστε ένα εικονικό περιβάλλον:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Εγκαταστήστε το Olive με δυνατότητες αυτόματης βελτιστοποίησης:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Προαιρετικές Εξαρτήσεις

Το Olive προσφέρει διάφορες προαιρετικές εξαρτήσεις για πρόσθετες δυνατότητες:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Επαλήθευση Εγκατάστασης

```bash
olive --help
```

Εάν η εγκατάσταση είναι επιτυχής, θα πρέπει να δείτε το μήνυμα βοήθειας του Olive CLI.

## Οδηγός Γρήγορης Εκκίνησης

### Η Πρώτη σας Βελτιστοποίηση

Ας βελτιστοποιήσουμε ένα μικρό γλωσσικό μοντέλο χρησιμοποιώντας τη δυνατότητα αυτόματης βελτιστοποίησης του Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Τι Κάνει Αυτή η Εντολή

Η διαδικασία βελτιστοποίησης περιλαμβάνει: λήψη του μοντέλου από την τοπική προσωρινή μνήμη, καταγραφή του ONNX Graph και αποθήκευση των βαρών σε ένα αρχείο δεδομένων ONNX, βελτιστοποίηση του ONNX Graph και ποσοτικοποίηση του μοντέλου σε int4 χρησιμοποιώντας τη μέθοδο RTN.

### Επεξήγηση Παραμέτρων Εντολής

- `--model_name_or_path`: Αναγνωριστικό μοντέλου Hugging Face ή τοπική διαδρομή
- `--output_path`: Κατάλογος όπου θα αποθηκευτεί το βελτιστοποιημένο μοντέλο
- `--device`: Στόχος συσκευής (cpu, gpu)
- `--provider`: Πάροχος εκτέλεσης (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Χρήση του ONNX Runtime Generate AI για εκτέλεση
- `--precision`: Ακρίβεια ποσοτικοποίησης (int4, int8, fp16)
- `--log_level`: Επίπεδο λεπτομέρειας καταγραφής (0=ελάχιστο, 1=λεπτομερές)

## Παράδειγμα: Μετατροπή Qwen3 σε ONNX INT4

Βάσει του παραδείγματος που παρέχεται από το Hugging Face στο [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), δείτε πώς να βελτιστοποιήσετε ένα μοντέλο Qwen3:

### Βήμα 1: Λήψη Μοντέλου (Προαιρετικό)

Για να ελαχιστοποιήσετε τον χρόνο λήψης, αποθηκεύστε μόνο τα απαραίτητα αρχεία:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Βήμα 2: Βελτιστοποίηση Μοντέλου Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Βήμα 3: Δοκιμή του Βελτιστοποιημένου Μοντέλου

Δημιουργήστε ένα απλό σενάριο Python για να δοκιμάσετε το βελτιστοποιημένο μοντέλο σας:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Δομή Εξόδου

Μετά τη βελτιστοποίηση, ο κατάλογος εξόδου σας θα περιέχει:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Προχωρημένη Χρήση

### Αρχεία Ρυθμίσεων

Για πιο σύνθετες ροές εργασίας βελτιστοποίησης, μπορείτε να χρησιμοποιήσετε αρχεία ρυθμίσεων JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Εκτέλεση με ρυθμίσεις:

```bash
olive run --config config.json
```

### Βελτιστοποίηση GPU

Για βελτιστοποίηση CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Για DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fine-tuning με το Olive

Το Olive υποστηρίζει επίσης fine-tuning μοντέλων:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Καλές Πρακτικές

### 1. Επιλογή Μοντέλου
- Ξεκινήστε με μικρότερα μοντέλα για δοκιμές (π.χ., 0.5B-7B παραμέτρους)
- Βεβαιωθείτε ότι η αρχιτεκτονική του μοντέλου σας υποστηρίζεται από το Olive

### 2. Σκέψεις για το Υλικό
- Ταιριάξτε τον στόχο βελτιστοποίησης με το υλικό ανάπτυξης σας
- Χρησιμοποιήστε βελτιστοποίηση GPU αν διαθέτετε υλικό συμβατό με CUDA
- Εξετάστε το DirectML για μηχανήματα Windows με ενσωματωμένα γραφικά

### 3. Επιλογή Ακρίβειας
- **INT4**: Μέγιστη συμπίεση, μικρή απώλεια ακρίβειας
- **INT8**: Καλή ισορροπία μεγέθους και ακρίβειας
- **FP16**: Ελάχιστη απώλεια ακρίβειας, μέτρια μείωση μεγέθους

### 4. Δοκιμή και Επικύρωση
- Πάντα να δοκιμάζετε τα βελτιστοποιημένα μοντέλα με τις συγκεκριμένες περιπτώσεις χρήσης σας
- Συγκρίνετε μετρικές απόδοσης (καθυστέρηση, ρυθμός μετάδοσης, ακρίβεια)
- Χρησιμοποιήστε αντιπροσωπευτικά δεδομένα εισόδου για αξιολόγηση

### 5. Επαναληπτική Βελτιστοποίηση
- Ξεκινήστε με αυτόματη βελτιστοποίηση για γρήγορα αποτελέσματα
- Χρησιμοποιήστε αρχεία ρυθμίσεων για λεπτομερή έλεγχο
- Πειραματιστείτε με διαφορετικά περάσματα βελτιστοποίησης

## Αντιμετώπιση Προβλημάτων

### Συνηθισμένα Προβλήματα

#### 1. Προβλήματα Εγκατάστασης
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Προβλήματα CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Προβλήματα Μνήμης
- Χρησιμοποιήστε μικρότερα μεγέθη παρτίδας κατά τη βελτιστοποίηση
- Δοκιμάστε ποσοτικοποίηση με υψηλότερη ακρίβεια πρώτα (int8 αντί για int4)
- Βεβαιωθείτε ότι υπάρχει αρκετός χώρος στο δίσκο για προσωρινή αποθήκευση μοντέλων

#### 4. Σφάλματα Φόρτωσης Μοντέλου
- Επαληθεύστε τη διαδρομή του μοντέλου και τα δικαιώματα πρόσβασης
- Ελέγξτε αν το μοντέλο απαιτεί `trust_remote_code=True`
- Βεβαιωθείτε ότι έχουν ληφθεί όλα τα απαραίτητα αρχεία μοντέλου

### Λήψη Βοήθειας

- **Τεκμηρίωση**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Θέματα GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Παραδείγματα**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Αποθετήριο Συνταγών Olive

### Εισαγωγή στις Συνταγές Olive

Το [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) αποθετήριο συμπληρώνει το κύριο εργαλείο Olive παρέχοντας μια ολοκληρωμένη συλλογή έτοιμων προς χρήση συνταγών βελτιστοποίησης για δημοφιλή μοντέλα AI. Αυτό το αποθετήριο λειτουργεί ως πρακτική αναφορά τόσο για τη βελτιστοποίηση δημόσιων μοντέλων όσο και για τη δημιουργία ροών εργασίας βελτιστοποίησης για ιδιόκτητα μοντέλα.

### Βασικά Χαρακτηριστικά

- **100+ Προκατασκευασμένες Συνταγές**: Έτοιμες ρυθμίσεις βελτιστοποίησης για δημοφιλή μοντέλα
- **Υποστήριξη Πολλαπλών Αρχιτεκτονικών**: Καλύπτει μοντέλα μετασχηματιστών, μοντέλα όρασης και πολυτροπικές αρχιτεκτονικές
- **Βελτιστοποιήσεις με βάση το υλικό**: Συνταγές προσαρμοσμένες για CPU, GPU και εξειδικευμένους επιταχυντές
- **Δημοφιλείς Οικογένειες Μοντέλων**: Περιλαμβάνει Phi, Llama, Qwen, Gemma, Mistral και πολλά άλλα

### Υποστηριζόμενες Οικογένειες Μοντέλων

Το αποθετήριο περιλαμβάνει συνταγές βελτιστοποίησης για:

#### Γλωσσικά Μοντέλα
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 σειρά (0.5B έως 14B)
- **Google Gemma**: Διάφορες διαμορφώσεις Gemma
- **Mistral AI**: Σειρά Mistral-7B
- **DeepSeek**: Μοντέλα σειράς R1-Distill

#### Μοντέλα Όρασης και Πολυτροπικά
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP Models**: Διάφορες διαμορφώσεις CLIP-ViT
- **ResNet**: Βελτιστοποιήσεις ResNet-50
- **Vision Transformers**: ViT-base-patch16-224

#### Εξειδικευμένα Μοντέλα
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Βασικές και πολυγλωσσικές παραλλαγές
- **Sentence Transformers**: all-MiniLM-L6-v2

### Χρήση Συνταγών Olive

#### Μέθοδος 1: Κλωνοποίηση Συγκεκριμένης Συνταγής

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Μέθοδος 2: Χρήση Συνταγής ως Πρότυπο

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Δομή Συνταγής

Κάθε κατάλογος συνταγής περιέχει συνήθως:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Παράδειγμα: Χρήση Συνταγής Phi-4-mini

Ας χρησιμοποιήσουμε τη συνταγή Phi-4-mini ως παράδειγμα:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Το αρχείο ρυθμίσεων περιλαμβάνει συνήθως:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Προσαρμογή Συνταγών

#### Τροποποίηση Στόχου Υλικού

Για να αλλάξετε το υλικό στόχο, ενημερώστε την ενότητα `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Ρύθμιση Παραμέτρων Βελτιστοποίησης

Τροποποιήστε την ενότητα `passes` για διαφορετικά επίπεδα βελτιστοποίησης:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Δημιουργία Δικής σας Συνταγής

1. **Ξεκινήστε με Παρόμοιο Μοντέλο**: Βρείτε μια συνταγή για μοντέλο με παρόμοια αρχιτεκτονική
2. **Ενημερώστε τη Ρύθμιση Μοντέλου**: Αλλάξτε το όνομα/διαδρομή του μοντέλου στη ρύθμιση
3. **Ρυθμίστε Παραμέτρους**: Τροποποιήστε τις παραμέτρους βελτιστοποίησης όπως απαιτείται
4. **Δοκιμή και Επικύρωση**: Εκτελέστε τη βελτιστοποίηση και επικυρώστε τα αποτελέσματα
5. **Συνεισφέρετε Πίσω**: Σκεφτείτε να συνεισφέρετε τη συνταγή σας στο αποθετήριο

### Οφέλη από τη Χρήση Συνταγών

#### 1. **Αποδεδειγμένες Ρυθμίσεις**
- Δοκιμασμένες ρυθμίσεις βελτιστοποίησης για συγκεκριμένα μοντέλα
- Αποφεύγεται η δοκιμή και το λάθος στην εύρεση βέλτιστων παραμέτρων

#### 2. **Ρύθμιση με βάση το Υλικό**
- Προ-βελτιστοποιημένες για διαφορετικούς παρόχους εκτέλεσης
- Έτοιμες ρυθμίσεις για στόχους CPU, GPU και NPU

#### 3. **Πλήρης Κάλυψη**
- Υποστηρίζει τα πιο δημοφιλή μοντέλα ανοιχτού κώδικα
- Τακτικές ενημερώσεις με νέες κυκλοφορίες μοντέλων

#### 4. **Συμβολή της Κοινότητας**
- Συνεργατική ανάπτυξη με την κοινότητα AI
- Κοινή γνώση και καλές πρακτικές

### Συνεισφορά στις Συνταγές Olive

Εάν έχετε βελτιστοποιήσει ένα μοντέλο που δεν καλύπτεται στο αποθετήριο:

1. **Fork το Αποθετήριο**: Δημιουργήστε το δικό σας fork του olive-recipes
2. **Δημιουργία Καταλόγου Συντα

---

**Αποποίηση ευθύνης**:  
Αυτό το έγγραφο έχει μεταφραστεί χρησιμοποιώντας την υπηρεσία μετάφρασης AI [Co-op Translator](https://github.com/Azure/co-op-translator). Παρόλο που καταβάλλουμε προσπάθειες για ακρίβεια, παρακαλούμε να έχετε υπόψη ότι οι αυτοματοποιημένες μεταφράσεις ενδέχεται να περιέχουν λάθη ή ανακρίβειες. Το πρωτότυπο έγγραφο στη μητρική του γλώσσα θα πρέπει να θεωρείται η αυθεντική πηγή. Για κρίσιμες πληροφορίες, συνιστάται επαγγελματική ανθρώπινη μετάφραση. Δεν φέρουμε ευθύνη για τυχόν παρεξηγήσεις ή εσφαλμένες ερμηνείες που προκύπτουν από τη χρήση αυτής της μετάφρασης.