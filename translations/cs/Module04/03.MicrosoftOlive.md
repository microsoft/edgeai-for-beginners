# Sekce 3: Microsoft Olive Optimization Suite

## Obsah
1. [Úvod](../../../Module04)
2. [Co je Microsoft Olive?](../../../Module04)
3. [Instalace](../../../Module04)
4. [Rychlý průvodce](../../../Module04)
5. [Příklad: Konverze Qwen3 na ONNX INT4](../../../Module04)
6. [Pokročilé použití](../../../Module04)
7. [Repozitář Olive Recipes](../../../Module04)
8. [Nejlepší postupy](../../../Module04)
9. [Řešení problémů](../../../Module04)
10. [Další zdroje](../../../Module04)

## Úvod

Microsoft Olive je výkonný a snadno použitelný nástroj pro optimalizaci modelů strojového učení, který zjednodušuje proces optimalizace modelů pro nasazení na různých hardwarových platformách. Ať už cílíte na CPU, GPU nebo specializované AI akcelerátory, Olive vám pomůže dosáhnout optimálního výkonu při zachování přesnosti modelu.

## Co je Microsoft Olive?

Olive je snadno použitelný nástroj pro optimalizaci modelů, který je přizpůsobený hardwaru a kombinuje špičkové techniky v oblasti komprese, optimalizace a kompilace modelů. Funguje s ONNX Runtime jako komplexní řešení pro optimalizaci inferencí.

### Klíčové vlastnosti

- **Optimalizace přizpůsobená hardwaru**: Automaticky vybírá nejlepší techniky optimalizace pro cílový hardware
- **Více než 40 vestavěných optimalizačních komponent**: Zahrnuje kompresi modelů, kvantizaci, optimalizaci grafů a další
- **Jednoduché CLI rozhraní**: Snadné příkazy pro běžné úkoly optimalizace
- **Podpora více frameworků**: Funguje s PyTorch, modely Hugging Face a ONNX
- **Podpora populárních modelů**: Olive dokáže automaticky optimalizovat oblíbené architektury modelů jako Llama, Phi, Qwen, Gemma a další

### Výhody

- **Zkrácení doby vývoje**: Není nutné ručně experimentovat s různými technikami optimalizace
- **Zvýšení výkonu**: Významné zrychlení (až 6x v některých případech)
- **Nasazení napříč platformami**: Optimalizované modely fungují na různém hardwaru a operačních systémech
- **Zachování přesnosti**: Optimalizace zachovávají kvalitu modelu při zlepšení výkonu

## Instalace

### Předpoklady

- Python 3.8 nebo novější
- Správce balíčků pip
- Virtuální prostředí (doporučeno)

### Základní instalace

Vytvořte a aktivujte virtuální prostředí:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Nainstalujte Olive s funkcemi automatické optimalizace:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Volitelné závislosti

Olive nabízí různé volitelné závislosti pro další funkce:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Ověření instalace

```bash
olive --help
```

Pokud je instalace úspěšná, zobrazí se nápověda Olive CLI.

## Rychlý průvodce

### Vaše první optimalizace

Optimalizujme malý jazykový model pomocí funkce automatické optimalizace Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Co tento příkaz dělá

Proces optimalizace zahrnuje: získání modelu z lokálního úložiště, zachycení ONNX grafu a uložení vah do ONNX datového souboru, optimalizaci ONNX grafu a kvantizaci modelu na int4 pomocí metody RTN.

### Vysvětlení parametrů příkazu

- `--model_name_or_path`: Identifikátor modelu Hugging Face nebo lokální cesta
- `--output_path`: Adresář, kam bude uložen optimalizovaný model
- `--device`: Cílové zařízení (cpu, gpu)
- `--provider`: Poskytovatel exekuce (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Použití ONNX Runtime Generate AI pro inferenci
- `--precision`: Kvantizační přesnost (int4, int8, fp16)
- `--log_level`: Úroveň podrobnosti logování (0=minimální, 1=podrobné)

## Příklad: Konverze Qwen3 na ONNX INT4

Na základě příkladu Hugging Face na [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) zde uvádíme, jak optimalizovat model Qwen3:

### Krok 1: Stažení modelu (volitelné)

Pro minimalizaci doby stahování uložte pouze nezbytné soubory:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Krok 2: Optimalizace modelu Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Krok 3: Testování optimalizovaného modelu

Vytvořte jednoduchý Python skript pro testování vašeho optimalizovaného modelu:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Struktura výstupu

Po optimalizaci bude váš výstupní adresář obsahovat:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Pokročilé použití

### Konfigurační soubory

Pro složitější pracovní postupy optimalizace můžete použít JSON konfigurační soubory:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Spuštění s konfigurací:

```bash
olive run --config config.json
```

### Optimalizace GPU

Pro optimalizaci CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Pro DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Doladění s Olive

Olive také podporuje doladění modelů:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Nejlepší postupy

### 1. Výběr modelu
- Začněte s menšími modely pro testování (např. 0.5B-7B parametrů)
- Ujistěte se, že cílová architektura modelu je podporována Olive

### 2. Hardwarové úvahy
- Přizpůsobte optimalizaci cílovému hardwaru pro nasazení
- Použijte optimalizaci GPU, pokud máte hardware kompatibilní s CUDA
- Zvažte DirectML pro Windows zařízení s integrovanou grafikou

### 3. Výběr přesnosti
- **INT4**: Maximální komprese, mírná ztráta přesnosti
- **INT8**: Dobrá rovnováha mezi velikostí a přesností
- **FP16**: Minimální ztráta přesnosti, střední redukce velikosti

### 4. Testování a validace
- Vždy testujte optimalizované modely na vašich konkrétních případech použití
- Porovnávejte metriky výkonu (latence, propustnost, přesnost)
- Používejte reprezentativní vstupní data pro hodnocení

### 5. Iterativní optimalizace
- Začněte s automatickou optimalizací pro rychlé výsledky
- Použijte konfigurační soubory pro jemné nastavení
- Experimentujte s různými optimalizačními kroky

## Řešení problémů

### Běžné problémy

#### 1. Problémy s instalací
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Problémy s CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Problémy s pamětí
- Použijte menší velikosti batchů během optimalizace
- Zkuste kvantizaci s vyšší přesností nejdříve (int8 místo int4)
- Ujistěte se, že máte dostatek místa na disku pro ukládání modelů

#### 4. Chyby při načítání modelu
- Ověřte cestu k modelu a oprávnění k přístupu
- Zkontrolujte, zda model vyžaduje `trust_remote_code=True`
- Ujistěte se, že všechny potřebné soubory modelu jsou staženy

### Získání pomoci

- **Dokumentace**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Příklady**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Repozitář Olive Recipes

### Úvod do Olive Recipes

Repozitář [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) doplňuje hlavní nástroj Olive a poskytuje rozsáhlou sbírku připravených optimalizačních receptů pro populární AI modely. Tento repozitář slouží jako praktický odkaz pro optimalizaci veřejně dostupných modelů i pro vytváření optimalizačních pracovních postupů pro vlastní modely.

### Klíčové vlastnosti

- **Více než 100 předpřipravených receptů**: Optimalizační konfigurace připravené k použití pro populární modely
- **Podpora více architektur**: Zahrnuje transformery, vizuální modely a multimodální architektury
- **Optimalizace přizpůsobené hardwaru**: Recepty přizpůsobené pro CPU, GPU a specializované akcelerátory
- **Populární rodiny modelů**: Zahrnuje Phi, Llama, Qwen, Gemma, Mistral a mnoho dalších

### Podporované rodiny modelů

Repozitář obsahuje optimalizační recepty pro:

#### Jazykové modely
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 série (0.5B až 14B)
- **Google Gemma**: Různé konfigurace modelů Gemma
- **Mistral AI**: Série Mistral-7B
- **DeepSeek**: Modely série R1-Distill

#### Vizuální a multimodální modely
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP modely**: Různé konfigurace CLIP-ViT
- **ResNet**: Optimalizace ResNet-50
- **Vision Transformers**: ViT-base-patch16-224

#### Specializované modely
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Základní a vícejazyčné varianty
- **Sentence Transformers**: all-MiniLM-L6-v2

### Použití Olive Recipes

#### Metoda 1: Klonování konkrétního receptu

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Metoda 2: Použití receptu jako šablony

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Struktura receptu

Každý adresář receptu obvykle obsahuje:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Příklad: Použití receptu Phi-4-mini

Použijme jako příklad recept Phi-4-mini:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Konfigurační soubor obvykle obsahuje:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Úprava receptů

#### Změna cílového hardwaru

Pro změnu cílového hardwaru upravte sekci `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Úprava parametrů optimalizace

Upravte sekci `passes` pro různé úrovně optimalizace:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Vytvoření vlastního receptu

1. **Začněte s podobným modelem**: Najděte recept pro model s podobnou architekturou
2. **Aktualizujte konfiguraci modelu**: Změňte název/cestu modelu v konfiguraci
3. **Upravte parametry**: Přizpůsobte parametry optimalizace podle potřeby
4. **Testujte a validujte**: Spusťte optimalizaci a ověřte výsledky
5. **Přispějte zpět**: Zvažte přidání svého receptu do repozitáře

### Výhody použití receptů

#### 1. **Ověřené konfigurace**
- Testované nastavení optimalizace pro konkrétní modely
- Vyhýbá se pokusům a omylům při hledání optimálních parametrů

#### 2. **Ladění přizpůsobené hardwaru**
- Předem optimalizováno pro různé poskytovatele exekuce
- Konfigurace připravené k použití pro CPU, GPU a NPU cíle

#### 3. **Komplexní pokrytí**
- Podpora nejpopulárnějších open-source modelů
- Pravidelné aktualizace s novými verzemi modelů

#### 4. **Příspěvky komunity**
- Spolupráce s AI komunitou
- Sdílené znalosti a osvědčené postupy

### Přispívání do Olive Recipes

Pokud jste optimalizovali model, který není pokryt v repozitáři:

1. **Fork repozitáře**: Vytvořte vlastní fork olive-recipes
2. **Vytvořte adresář receptu**: Přidejte nový adresář pro váš model
3. **Přidejte konfiguraci**: Přidejte olive_config.json a podpůrné soubory
4. **Dokumentujte použití**: Poskytněte jasný README s instrukcemi
5. **Odešlete Pull Request**: Přispějte zpět komunitě

### Výkonnostní benchmarky

Mnoho receptů obsahuje výkonnostní benchmarky ukazující:
- **Zlepšení latence**: Typické zrychlení 2-6x oproti základnímu modelu
- **Snížení paměti**: 50-75% snížení využití paměti díky kvantizaci
- **Zachování přesnosti**: 95-99% zachování přesnosti

### Integrace s AI nástroji

Recepty fungují bez problémů s:
- **VS Code AI Toolkit**: Přímá integrace pro optimalizaci modelů
- **Azure Machine Learning**: Optimalizační pracovní postupy v cloudu
- **ONNX Runtime**: Optimalizované nasazení inferencí

## Další zdroje

### Oficiální odkazy
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Repozitář Olive Recipes**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime Dokumentace**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Příklad**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Příklady komunity
- **Jupyter Notebooks**: Dostupné v Olive GitHub repozitáři — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: AI Toolkit pro VS Code přehled — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogové příspěvky**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Související nástroje
- **ONNX Runtime**: Vysoce výkonný inference engine — https://onnxruntime.ai/
- **Hugging Face Transformers**: Zdroj mnoha kompatibilních modelů — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Optimalizační pracovní postupy v cloudu — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Co dál

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Prohlášení**:  
Tento dokument byl přeložen pomocí služby AI pro překlady [Co-op Translator](https://github.com/Azure/co-op-translator). I když se snažíme o přesnost, mějte prosím na paměti, že automatizované překlady mohou obsahovat chyby nebo nepřesnosti. Původní dokument v jeho původním jazyce by měl být považován za autoritativní zdroj. Pro důležité informace se doporučuje profesionální lidský překlad. Neodpovídáme za žádná nedorozumění nebo nesprávné interpretace vyplývající z použití tohoto překladu.