# AI agenti a mal√© jazykov√© modely: Komplexn√≠ pr≈Øvodce

## √övod

V tomto tutori√°lu se pod√≠v√°me na AI agenty a mal√© jazykov√© modely (SLM) a jejich pokroƒçil√© implementaƒçn√≠ strategie pro prost≈ôed√≠ edge computingu. Probereme z√°kladn√≠ koncepty agentn√≠ AI, optimalizaƒçn√≠ techniky SLM, praktick√© strategie nasazen√≠ pro za≈ô√≠zen√≠ s omezen√Ωmi zdroji a Microsoft Agent Framework pro vytv√°≈ôen√≠ produkƒçnƒõ p≈ôipraven√Ωch agentn√≠ch syst√©m≈Ø.

Oblast umƒõl√© inteligence za≈æ√≠v√° v roce 2025 paradigmatick√Ω posun. Zat√≠mco rok 2023 byl rokem chatbot≈Ø a rok 2024 p≈ôinesl boom kopilot≈Ø, rok 2025 pat≈ô√≠ AI agent≈Øm ‚Äî inteligentn√≠m syst√©m≈Øm, kter√© mysl√≠, pl√°nuj√≠, pou≈æ√≠vaj√≠ n√°stroje a vykon√°vaj√≠ √∫koly s minim√°ln√≠m lidsk√Ωm vstupem, st√°le v√≠ce poh√°nƒõn√© efektivn√≠mi mal√Ωmi jazykov√Ωmi modely. Microsoft Agent Framework se st√°v√° p≈ôedn√≠m ≈ôe≈°en√≠m pro vytv√°≈ôen√≠ tƒõchto inteligentn√≠ch syst√©m≈Ø s offline schopnostmi na edge za≈ô√≠zen√≠ch.

## C√≠le uƒçen√≠

Na konci tohoto tutori√°lu budete schopni:

- ü§ñ Porozumƒõt z√°kladn√≠m koncept≈Øm AI agent≈Ø a agentn√≠ch syst√©m≈Ø
- üî¨ Identifikovat v√Ωhody mal√Ωch jazykov√Ωch model≈Ø oproti velk√Ωm jazykov√Ωm model≈Øm v agentn√≠ch aplikac√≠ch
- üöÄ Nauƒçit se pokroƒçil√© strategie nasazen√≠ SLM pro prost≈ôed√≠ edge computingu
- üì± Implementovat praktick√© agenty poh√°nƒõn√© SLM pro re√°ln√© aplikace
- üèóÔ∏è Vytvo≈ôit produkƒçnƒõ p≈ôipraven√© agenty pomoc√≠ Microsoft Agent Framework
- üåê Nasadit offline agenty na edge za≈ô√≠zen√≠ch s integrac√≠ lok√°ln√≠ch LLM a SLM
- üîß Integrovat Microsoft Agent Framework s Foundry Local pro nasazen√≠ na edge

## Porozumƒõn√≠ AI agent≈Øm: Z√°klady a klasifikace

### Definice a z√°kladn√≠ koncepty

Umƒõl√Ω inteligentn√≠ agent oznaƒçuje syst√©m nebo program, kter√Ω je schopen autonomnƒõ vykon√°vat √∫koly jm√©nem u≈æivatele nebo jin√©ho syst√©mu t√≠m, ≈æe navrhuje sv≈Øj pracovn√≠ postup a vyu≈æ√≠v√° dostupn√© n√°stroje. Na rozd√≠l od tradiƒçn√≠ AI, kter√° pouze odpov√≠d√° na va≈°e ot√°zky, agent m≈Ø≈æe jednat nez√°visle na dosa≈æen√≠ c√≠l≈Ø.

### R√°mec klasifikace agent≈Ø

Porozumƒõn√≠ hranic√≠m agent≈Ø pom√°h√° p≈ôi v√Ωbƒõru vhodn√Ωch typ≈Ø agent≈Ø pro r≈Øzn√© sc√©n√°≈ôe v√Ωpoƒçetn√≠ techniky:

- **üî¨ Jednoduch√© reflexn√≠ agenty**: Syst√©my zalo≈æen√© na pravidlech, kter√© reaguj√≠ na okam≈æit√© vn√≠m√°n√≠ (termostaty, z√°kladn√≠ automatizace)
- **üì± Agenti zalo≈æen√≠ na modelu**: Syst√©my, kter√© udr≈æuj√≠ vnit≈ôn√≠ stav a pamƒõ≈• (robotick√© vysavaƒçe, navigaƒçn√≠ syst√©my)
- **‚öñÔ∏è Agenti zalo≈æen√≠ na c√≠lech**: Syst√©my, kter√© pl√°nuj√≠ a prov√°dƒõj√≠ sekvence k dosa≈æen√≠ c√≠l≈Ø (pl√°novaƒçe tras, pl√°novaƒçe √∫kol≈Ø)
- **üß† Uƒç√≠c√≠ se agenti**: Adaptivn√≠ syst√©my, kter√© se zlep≈°uj√≠ v pr≈Øbƒõhu ƒçasu (doporuƒçovac√≠ syst√©my, personalizovan√≠ asistenti)

### Kl√≠ƒçov√© v√Ωhody AI agent≈Ø

AI agenti nab√≠zej√≠ nƒõkolik z√°kladn√≠ch v√Ωhod, kter√© je ƒçin√≠ ide√°ln√≠mi pro aplikace edge computingu:

**Operaƒçn√≠ autonomie**: Agenti poskytuj√≠ nez√°visl√© prov√°dƒõn√≠ √∫kol≈Ø bez neust√°l√©ho lidsk√©ho dohledu, co≈æ je ide√°ln√≠ pro aplikace v re√°ln√©m ƒçase. Vy≈æaduj√≠ minim√°ln√≠ dohled p≈ôi zachov√°n√≠ adaptivn√≠ho chov√°n√≠, co≈æ umo≈æ≈àuje nasazen√≠ na za≈ô√≠zen√≠ch s omezen√Ωmi zdroji a sni≈æuje provozn√≠ n√°klady.

**Flexibilita nasazen√≠**: Tyto syst√©my umo≈æ≈àuj√≠ AI schopnosti na za≈ô√≠zen√≠ bez po≈æadavk≈Ø na p≈ôipojen√≠ k internetu, zvy≈°uj√≠ soukrom√≠ a bezpeƒçnost prost≈ôednictv√≠m lok√°ln√≠ho zpracov√°n√≠, mohou b√Ωt p≈ôizp≈Øsobeny pro aplikace specifick√© pro danou oblast a jsou vhodn√© pro r≈Øzn√° prost≈ôed√≠ edge computingu.

**N√°kladov√° efektivita**: Agentn√≠ syst√©my nab√≠zej√≠ n√°kladovƒõ efektivn√≠ nasazen√≠ ve srovn√°n√≠ s cloudov√Ωmi ≈ôe≈°en√≠mi, s ni≈æ≈°√≠mi provozn√≠mi n√°klady a ni≈æ≈°√≠mi po≈æadavky na ≈°√≠≈ôku p√°sma pro aplikace na edge.

## Pokroƒçil√© strategie mal√Ωch jazykov√Ωch model≈Ø

### Z√°klady SLM (Small Language Model)

Mal√Ω jazykov√Ω model (SLM) je jazykov√Ω model, kter√Ω se vejde na bƒõ≈æn√© spot≈ôebitelsk√© elektronick√© za≈ô√≠zen√≠ a prov√°d√≠ inference s latenc√≠ dostateƒçnƒõ n√≠zkou, aby byl praktick√Ω p≈ôi obsluze agentn√≠ch po≈æadavk≈Ø jednoho u≈æivatele. Prakticky vzato, SLM jsou obvykle modely s m√©nƒõ ne≈æ 10 miliardami parametr≈Ø.

**Funkce objevov√°n√≠ form√°t≈Ø**: SLM nab√≠zej√≠ pokroƒçilou podporu pro r≈Øzn√© √∫rovnƒõ kvantizace, kompatibilitu nap≈ô√≠ƒç platformami, optimalizaci v√Ωkonu v re√°ln√©m ƒçase a schopnosti nasazen√≠ na edge. U≈æivatel√© mohou vyu≈æ√≠vat zv√Ω≈°en√© soukrom√≠ prost≈ôednictv√≠m lok√°ln√≠ho zpracov√°n√≠ a podporu WebGPU pro nasazen√≠ v prohl√≠≈æeƒçi.

**Sb√≠rky √∫rovn√≠ kvantizace**: Popul√°rn√≠ form√°ty SLM zahrnuj√≠ Q4_K_M pro vyv√°≈æenou kompresi v mobiln√≠ch aplikac√≠ch, Q5_K_S s√©rii pro nasazen√≠ zamƒõ≈ôen√© na kvalitu na edge, Q8_0 pro t√©mƒõ≈ô p≈Øvodn√≠ p≈ôesnost na v√Ωkonn√Ωch edge za≈ô√≠zen√≠ch a experiment√°ln√≠ form√°ty jako Q2_K pro sc√©n√°≈ôe s ultra n√≠zk√Ωmi zdroji.

### GGUF (General GGML Universal Format) pro nasazen√≠ SLM

GGUF slou≈æ√≠ jako prim√°rn√≠ form√°t pro nasazen√≠ kvantizovan√Ωch SLM na CPU a edge za≈ô√≠zen√≠ch, speci√°lnƒõ optimalizovan√Ω pro agentn√≠ aplikace:

**Agentnƒõ optimalizovan√© funkce**: Form√°t poskytuje komplexn√≠ zdroje pro konverzi a nasazen√≠ SLM s roz≈°√≠≈ôenou podporou pro vol√°n√≠ n√°stroj≈Ø, generov√°n√≠ strukturovan√Ωch v√Ωstup≈Ø a v√≠cen√°sobn√© konverzace. Kompatibilita nap≈ô√≠ƒç platformami zaji≈°≈•uje konzistentn√≠ chov√°n√≠ agent≈Ø na r≈Øzn√Ωch edge za≈ô√≠zen√≠ch.

**Optimalizace v√Ωkonu**: GGUF umo≈æ≈àuje efektivn√≠ vyu≈æit√≠ pamƒõti pro pracovn√≠ postupy agent≈Ø, podporuje dynamick√© naƒç√≠t√°n√≠ model≈Ø pro syst√©my s v√≠ce agenty a poskytuje optimalizovanou inference pro interakce agent≈Ø v re√°ln√©m ƒçase.

### Edge-optimalizovan√© r√°mce SLM

#### Optimalizace Llama.cpp pro agenty

Llama.cpp poskytuje ≈°piƒçkov√© kvantizaƒçn√≠ techniky speci√°lnƒõ optimalizovan√© pro nasazen√≠ agentn√≠ch SLM:

**Agentnƒõ specifick√° kvantizace**: R√°mec podporuje Q4_0 (optim√°ln√≠ pro mobiln√≠ nasazen√≠ agent≈Ø s 75% redukc√≠ velikosti), Q5_1 (vyv√°≈æen√° kvalita-komprese pro inference agent≈Ø na edge) a Q8_0 (t√©mƒõ≈ô p≈Øvodn√≠ kvalita pro produkƒçn√≠ agentn√≠ syst√©my). Pokroƒçil√© form√°ty umo≈æ≈àuj√≠ ultra-komprimovan√© agenty pro extr√©mn√≠ edge sc√©n√°≈ôe.

**V√Ωhody implementace**: Inference optimalizovan√° pro CPU s akcelerac√≠ SIMD poskytuje pamƒõ≈•ovƒõ efektivn√≠ prov√°dƒõn√≠ agent≈Ø. Kompatibilita nap≈ô√≠ƒç platformami na architektur√°ch x86, ARM a Apple Silicon umo≈æ≈àuje univerz√°ln√≠ schopnosti nasazen√≠ agent≈Ø.

#### Apple MLX Framework pro SLM agenty

Apple MLX poskytuje nativn√≠ optimalizaci speci√°lnƒõ navr≈æenou pro agenty poh√°nƒõn√© SLM na za≈ô√≠zen√≠ch Apple Silicon:

**Optimalizace agent≈Ø na Apple Silicon**: R√°mec vyu≈æ√≠v√° sjednocenou pamƒõ≈•ovou architekturu s integrac√≠ Metal Performance Shaders, automatickou sm√≠≈°enou p≈ôesnost pro inference agent≈Ø a optimalizovanou ≈°√≠≈ôku pamƒõ≈•ov√©ho p√°sma pro syst√©my s v√≠ce agenty. Agenti SLM vykazuj√≠ v√Ωjimeƒçn√Ω v√Ωkon na ƒçipech ≈ôady M.

**V√Ωvojov√© funkce**: Podpora API pro Python a Swift s optimalizacemi specifick√Ωmi pro agenty, automatick√° diferenciace pro uƒçen√≠ agent≈Ø a bezprobl√©mov√° integrace s v√Ωvojov√Ωmi n√°stroji Apple poskytuj√≠ komplexn√≠ prost≈ôed√≠ pro v√Ωvoj agent≈Ø.

#### ONNX Runtime pro agenty SLM nap≈ô√≠ƒç platformami

ONNX Runtime poskytuje univerz√°ln√≠ inference engine, kter√Ω umo≈æ≈àuje agent≈Øm SLM bƒõ≈æet konzistentnƒõ na r≈Øzn√Ωch hardwarov√Ωch platform√°ch a operaƒçn√≠ch syst√©mech:

**Univerz√°ln√≠ nasazen√≠**: ONNX Runtime zaji≈°≈•uje konzistentn√≠ chov√°n√≠ agent≈Ø SLM nap≈ô√≠ƒç platformami Windows, Linux, macOS, iOS a Android. Tato kompatibilita nap≈ô√≠ƒç platformami umo≈æ≈àuje v√Ωvoj√°≈ô≈Øm ps√°t jednou a nasadit v≈°ude, co≈æ v√Ωraznƒõ sni≈æuje n√°klady na v√Ωvoj a √∫dr≈æbu pro aplikace na v√≠ce platform√°ch.

**Mo≈ænosti hardwarov√© akcelerace**: R√°mec poskytuje optimalizovan√© poskytovatele exekuce pro r≈Øzn√© hardwarov√© konfigurace vƒçetnƒõ CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) a specializovan√Ωch akceler√°tor≈Ø (Intel VPU, Qualcomm NPU). Agenti SLM mohou automaticky vyu≈æ√≠vat nejlep≈°√≠ dostupn√Ω hardware bez zmƒõn k√≥du.

**Funkce p≈ôipraven√© pro produkci**: ONNX Runtime nab√≠z√≠ funkce na podnikov√© √∫rovni nezbytn√© pro nasazen√≠ agent≈Ø v produkci, vƒçetnƒõ optimalizace graf≈Ø pro rychlej≈°√≠ inference, spr√°vy pamƒõti pro prost≈ôed√≠ s omezen√Ωmi zdroji a komplexn√≠ch n√°stroj≈Ø pro profilov√°n√≠ v√Ωkonu. R√°mec podporuje API pro Python i C++ pro flexibiln√≠ integraci.

## SLM vs LLM v agentn√≠ch syst√©mech: Pokroƒçil√© srovn√°n√≠

### V√Ωhody SLM v agentn√≠ch aplikac√≠ch

**Operaƒçn√≠ efektivita**: SLM poskytuj√≠ 10-30√ó sn√≠≈æen√≠ n√°klad≈Ø ve srovn√°n√≠ s LLM pro agentn√≠ √∫koly, umo≈æ≈àuj√≠ agentn√≠ odpovƒõdi v re√°ln√©m ƒçase ve velk√©m mƒõ≈ô√≠tku. Nab√≠zej√≠ rychlej≈°√≠ ƒçasy inference d√≠ky sn√≠≈æen√© v√Ωpoƒçetn√≠ slo≈æitosti, co≈æ je ƒçin√≠ ide√°ln√≠mi pro interaktivn√≠ agentn√≠ aplikace.

**Schopnosti nasazen√≠ na edge**: SLM umo≈æ≈àuj√≠ prov√°dƒõn√≠ agent≈Ø na za≈ô√≠zen√≠ bez z√°vislosti na internetu, zv√Ω≈°en√© soukrom√≠ prost≈ôednictv√≠m lok√°ln√≠ho zpracov√°n√≠ a p≈ôizp≈Øsoben√≠ pro aplikace specifick√© pro danou oblast vhodn√© pro r≈Øzn√° prost≈ôed√≠ edge computingu.

**Optimalizace specifick√° pro agenty**: SLM vynikaj√≠ p≈ôi vol√°n√≠ n√°stroj≈Ø, generov√°n√≠ strukturovan√Ωch v√Ωstup≈Ø a rutinn√≠ch pracovn√≠ch postupech rozhodov√°n√≠, kter√© tvo≈ô√≠ 70-80% typick√Ωch agentn√≠ch √∫kol≈Ø.

### Kdy pou≈æ√≠t SLM vs LLM v agentn√≠ch syst√©mech

**Ide√°ln√≠ pro SLM**:
- **Opakuj√≠c√≠ se agentn√≠ √∫koly**: Zad√°v√°n√≠ dat, vypl≈àov√°n√≠ formul√°≈ô≈Ø, rutinn√≠ API vol√°n√≠
- **Integrace n√°stroj≈Ø**: Dotazy na datab√°ze, operace se soubory, interakce se syst√©mem
- **Strukturovan√© pracovn√≠ postupy**: N√°sledov√°n√≠ p≈ôeddefinovan√Ωch proces≈Ø agent≈Ø
- **Agenti specifick√©ho oboru**: Z√°kaznick√Ω servis, pl√°nov√°n√≠, z√°kladn√≠ anal√Ωza
- **Lok√°ln√≠ zpracov√°n√≠**: Operace agent≈Ø citliv√© na soukrom√≠

**Lep≈°√≠ pro LLM**:
- **Komplexn√≠ uva≈æov√°n√≠**: Nov√© ≈ôe≈°en√≠ probl√©m≈Ø, strategick√© pl√°nov√°n√≠
- **Otev≈ôen√© konverzace**: Obecn√Ω chat, kreativn√≠ diskuse
- **√ökoly s ≈°irok√Ωmi znalostmi**: V√Ωzkum vy≈æaduj√≠c√≠ rozs√°hl√© obecn√© znalosti
- **Nov√© situace**: ≈òe≈°en√≠ zcela nov√Ωch sc√©n√°≈ô≈Ø agent≈Ø

### Hybridn√≠ architektura agent≈Ø

Optim√°ln√≠ p≈ô√≠stup kombinuje SLM a LLM v heterogenn√≠ch agentn√≠ch syst√©mech:

**Chytr√° orchestrace agent≈Ø**:
1. **SLM jako prim√°rn√≠**: Zpracov√°n√≠ 70-80% rutinn√≠ch agentn√≠ch √∫kol≈Ø lok√°lnƒõ
2. **LLM podle pot≈ôeby**: Smƒõrov√°n√≠ slo≈æit√Ωch dotaz≈Ø na cloudov√© vƒõt≈°√≠ modely
3. **Specializovan√© SLM**: R≈Øzn√© mal√© modely pro r≈Øzn√© dom√©ny agent≈Ø
4. **Optimalizace n√°klad≈Ø**: Minimalizace drah√Ωch vol√°n√≠ LLM prost≈ôednictv√≠m inteligentn√≠ho smƒõrov√°n√≠

## Strategie nasazen√≠ produkƒçn√≠ch SLM agent≈Ø

### Foundry Local: Edge AI runtime na podnikov√© √∫rovni

Foundry Local (https://github.com/microsoft/foundry-local) slou≈æ√≠ jako vlajkov√© ≈ôe≈°en√≠ Microsoftu pro nasazen√≠ mal√Ωch jazykov√Ωch model≈Ø v produkƒçn√≠ch edge prost≈ôed√≠ch. Poskytuje kompletn√≠ runtime prost≈ôed√≠ speci√°lnƒõ navr≈æen√© pro agenty poh√°nƒõn√© SLM s funkcemi na podnikov√© √∫rovni a bezprobl√©mov√Ωmi integraƒçn√≠mi schopnostmi.

**Z√°kladn√≠ architektura a funkce**:
- **Kompatibiln√≠ API s OpenAI**: Pln√° kompatibilita s OpenAI SDK a integracemi Agent Framework
- **Automatick√° optimalizace hardwaru**: Inteligentn√≠ v√Ωbƒõr variant model≈Ø na z√°kladƒõ dostupn√©ho hardwaru (CUDA GPU, Qualcomm NPU, CPU)
- **Spr√°va model≈Ø**: Automatick√© stahov√°n√≠, ukl√°d√°n√≠ do mezipamƒõti a spr√°va ≈æivotn√≠ho cyklu model≈Ø SLM
- **Objevov√°n√≠ slu≈æeb**: Detekce slu≈æeb bez konfigurace pro agentn√≠ r√°mce
- **Optimalizace zdroj≈Ø**: Inteligentn√≠ spr√°va pamƒõti a energetick√° √∫ƒçinnost pro nasazen√≠ na edge

#### Instalace a nastaven√≠

**Instalace nap≈ô√≠ƒç platformami**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Rychl√Ω start pro v√Ωvoj agent≈Ø**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Integrace Agent Framework

**Integrace Foundry Local SDK**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Automatick√Ω v√Ωbƒõr model≈Ø a optimalizace hardwaru**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Produkƒçn√≠ vzory nasazen√≠

**Produkƒçn√≠ nastaven√≠ jednoho agenta**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Orchestrace produkce v√≠ce agent≈Ø**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### Funkce na podnikov√© √∫rovni a monitorov√°n√≠

**Monitorov√°n√≠ zdrav√≠ a pozorovatelnost**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Spr√°va zdroj≈Ø a automatick√© ≈°k√°lov√°n√≠**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### Pokroƒçil√° konfigurace a optimalizace

**Vlastn√≠ konfigurace modelu**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Kontroln√≠ seznam pro produkƒçn√≠ nasazen√≠**:

‚úÖ **Konfigurace slu≈æby**:
- Nakonfigurujte vhodn√© aliasy model≈Ø pro p≈ô√≠pady pou≈æit√≠
- Nastavte limity zdroj≈Ø a prahov√© hodnoty monitorov√°n√≠
- Aktivujte kontroly zdrav√≠ a sbƒõr metrik
- Nakonfigurujte automatick√© restartov√°n√≠ a z√°lo≈æn√≠ re≈æim

‚úÖ **Nastaven√≠ zabezpeƒçen√≠**:
- Aktivujte p≈ô√≠stup k API pouze lok√°lnƒõ (bez extern√≠ho p≈ô√≠stupu)
- Nakonfigurujte spr√°vu kl√≠ƒç≈Ø API
- Nastavte auditn√≠ logov√°n√≠ interakc√≠ agent≈Ø
- Implementujte omezen√≠ rychlosti pro produkƒçn√≠ pou≈æit√≠

‚úÖ **Optimalizace v√Ωkonu**:
- Testujte v√Ωkon modelu p≈ôi oƒçek√°van√© z√°tƒõ≈æi
- Nakonfigurujte vhodn√© √∫rovnƒõ kvantizace
- Nastavte strategie ukl√°d√°n√≠ model≈Ø do mezipamƒõti a zah≈ô√≠v√°n√≠
- Monitorujte vzory vyu≈æit√≠ pamƒõti a CPU

‚úÖ **Testov√°n√≠ integrace**:
- Testujte integraci agentn√≠ho r√°mce
- Ovƒõ≈ôte schopnosti offline provozu
- Testujte sc√©n√°≈ôe z√°lo≈æn√≠ho re≈æimu a obnovy
- Validujte pracovn√≠ postupy agent≈Ø od zaƒç√°tku do konce

### Ollama: Zjednodu≈°en√© nasazen√≠ agent≈Ø SLM

### Ollama: Nasazen√≠ agent≈Ø SLM zamƒõ≈ôen√© na komunitu

Ollama poskytuje p≈ô√≠stup zamƒõ≈ôen√Ω na komunitu pro nasazen√≠ agent≈Ø SLM s d≈Ørazem na jednoduchost, rozs√°hl√Ω ekosyst√©m model≈Ø a u≈æivatelsky p≈ô√≠vƒõtiv√© pracovn√≠ postupy. Zat√≠mco Foundry Local se zamƒõ≈ôuje na funkce na podnikov√© √∫rovni, Ollama vynik√° v rychl√©m prototypov√°n√≠, p≈ô√≠stupu k model≈Øm od komunity a zjednodu≈°en√Ωch sc√©n√°≈ô√≠ch nasazen√≠.

**Z√°kladn√≠ architektura a funkce**:
- **Kompatibiln√≠ API s OpenAI**: Pln√° kompatibilita REST API pro bezprobl√©movou integraci agentn√≠ho r√°mce
- **Rozs√°hl√° knihovna model≈Ø**: P≈ô√≠stup ke stovk√°m model≈Ø od komunity i ofici√°ln√≠ch model≈Ø
- **
- Testov√°n√≠ integrace Microsoft Agent Framework
- Ovƒõ≈ôen√≠ schopnost√≠ offline provozu
- Testov√°n√≠ sc√©n√°≈ô≈Ø selh√°n√≠ a zpracov√°n√≠ chyb
- Validace kompletn√≠ch pracovn√≠ch postup≈Ø agent≈Ø

**Srovn√°n√≠ s Foundry Local**:

| Funkce | Foundry Local | Ollama |
|--------|---------------|--------|
| **C√≠lov√© pou≈æit√≠** | Podnikov√° produkce | V√Ωvoj a komunita |
| **Ekosyst√©m model≈Ø** | Kur√°torov√°no Microsoftem | Rozs√°hl√° komunita |
| **Optimalizace hardwaru** | Automatick√° (CUDA/NPU/CPU) | Manu√°ln√≠ konfigurace |
| **Podnikov√© funkce** | Vestavƒõn√© monitorov√°n√≠, bezpeƒçnost | N√°stroje komunity |
| **Slo≈æitost nasazen√≠** | Jednoduch√© (winget install) | Jednoduch√© (curl install) |
| **Kompatibilita API** | OpenAI + roz≈°√≠≈ôen√≠ | Standard OpenAI |
| **Podpora** | Ofici√°ln√≠ podpora Microsoftu | ≈ò√≠zeno komunitou |
| **Nejlep≈°√≠ pro** | Produkƒçn√≠ agenti | Prototypov√°n√≠, v√Ωzkum |

**Kdy zvolit Ollama**:
- **V√Ωvoj a prototypov√°n√≠**: Rychl√© experimentov√°n√≠ s r≈Øzn√Ωmi modely
- **Komunitn√≠ modely**: P≈ô√≠stup k nejnovƒõj≈°√≠m model≈Øm od komunity
- **Vzdƒõl√°vac√≠ vyu≈æit√≠**: Uƒçen√≠ a v√Ωuka v√Ωvoje AI agent≈Ø
- **V√Ωzkumn√© projekty**: Akademick√Ω v√Ωzkum vy≈æaduj√≠c√≠ p≈ô√≠stup k r≈Øzn√Ωm model≈Øm
- **Vlastn√≠ modely**: Vytv√°≈ôen√≠ a testov√°n√≠ vlastn√≠ch model≈Ø s jemn√Ωm doladƒõn√≠m

### VLLM: Vysoce v√Ωkonn√° inference SLM agent≈Ø

VLLM (Inference velmi velk√Ωch jazykov√Ωch model≈Ø) poskytuje vysoce v√Ωkonn√Ω, pamƒõ≈•ovƒõ efektivn√≠ inference engine, optimalizovan√Ω pro produkƒçn√≠ nasazen√≠ SLM ve velk√©m mƒõ≈ô√≠tku. Zat√≠mco Foundry Local se zamƒõ≈ôuje na snadn√© pou≈æit√≠ a Ollama na komunitn√≠ modely, VLLM vynik√° v sc√©n√°≈ô√≠ch vy≈æaduj√≠c√≠ch maxim√°ln√≠ propustnost a efektivn√≠ vyu≈æit√≠ zdroj≈Ø.

**Z√°kladn√≠ architektura a funkce**:
- **PagedAttention**: Revoluƒçn√≠ spr√°va pamƒõti pro efektivn√≠ v√Ωpoƒçet pozornosti
- **Dynamick√© d√°vkov√°n√≠**: Inteligentn√≠ d√°vkov√°n√≠ po≈æadavk≈Ø pro optim√°ln√≠ propustnost
- **Optimalizace GPU**: Pokroƒçil√© CUDA j√°dra a podpora paralelismu tensor≈Ø
- **Kompatibilita s OpenAI**: Pln√° kompatibilita API pro bezprobl√©movou integraci
- **Spekulativn√≠ dek√≥dov√°n√≠**: Pokroƒçil√© techniky zrychlen√≠ inference
- **Podpora kvantizace**: Kvantizace INT4, INT8 a FP16 pro efektivn√≠ vyu≈æit√≠ pamƒõti

#### Instalace a nastaven√≠

**Mo≈ænosti instalace**:
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

**Rychl√Ω start pro v√Ωvoj agent≈Ø**:
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```

#### Integrace Agent Framework

**VLLM s Microsoft Agent Framework**:
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```

**Nastaven√≠ v√≠ce agent≈Ø s vysokou propustnost√≠**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```

#### Produkƒçn√≠ vzory nasazen√≠

**Podnikov√° slu≈æba VLLM**:
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```

#### Podnikov√© funkce a monitorov√°n√≠

**Pokroƒçil√© monitorov√°n√≠ v√Ωkonu VLLM**:
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```

#### Pokroƒçil√° konfigurace a optimalizace

**≈†ablony konfigurace produkƒçn√≠ho VLLM**:
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```

**Kontroln√≠ seznam pro produkƒçn√≠ nasazen√≠ VLLM**:

‚úÖ **Optimalizace hardwaru**:
- Konfigurace paralelismu tensor≈Ø pro nastaven√≠ v√≠ce GPU
- Aktivace kvantizace (AWQ/GPTQ) pro efektivn√≠ vyu≈æit√≠ pamƒõti
- Nastaven√≠ optim√°ln√≠ho vyu≈æit√≠ pamƒõti GPU (85-95 %)
- Konfigurace vhodn√Ωch velikost√≠ d√°vky pro propustnost

‚úÖ **Ladƒõn√≠ v√Ωkonu**:
- Aktivace prefixov√©ho ukl√°d√°n√≠ do mezipamƒõti pro opakovan√© dotazy
- Konfigurace segmentovan√©ho p≈ôedvyplnƒõn√≠ pro dlouh√© sekvence
- Nastaven√≠ spekulativn√≠ho dek√≥dov√°n√≠ pro rychlej≈°√≠ inference
- Optimalizace max_num_seqs na z√°kladƒõ hardwaru

‚úÖ **Produkƒçn√≠ funkce**:
- Nastaven√≠ monitorov√°n√≠ stavu a sbƒõru metrik
- Konfigurace automatick√©ho restartu a p≈ôepnut√≠ p≈ôi selh√°n√≠
- Implementace front po≈æadavk≈Ø a vyva≈æov√°n√≠ z√°tƒõ≈æe
- Nastaven√≠ komplexn√≠ho logov√°n√≠ a upozornƒõn√≠

‚úÖ **Bezpeƒçnost a spolehlivost**:
- Konfigurace pravidel firewallu a kontrol p≈ô√≠stupu
- Nastaven√≠ omezen√≠ rychlosti API a autentizace
- Implementace plynul√©ho vypnut√≠ a vyƒçi≈°tƒõn√≠
- Konfigurace z√°lohov√°n√≠ a obnovy po hav√°rii

‚úÖ **Testov√°n√≠ integrace**:
- Testov√°n√≠ integrace Microsoft Agent Framework
- Validace sc√©n√°≈ô≈Ø s vysokou propustnost√≠
- Testov√°n√≠ postup≈Ø p≈ôi selh√°n√≠ a obnovƒõ
- Benchmarking v√Ωkonu p≈ôi z√°tƒõ≈æi

**Srovn√°n√≠ s jin√Ωmi ≈ôe≈°en√≠mi**:

| Funkce | VLLM | Foundry Local | Ollama |
|--------|------|---------------|--------|
| **C√≠lov√© pou≈æit√≠** | Produkce s vysokou propustnost√≠ | Snadn√© pou≈æit√≠ v podniku | V√Ωvoj a komunita |
| **V√Ωkon** | Maxim√°ln√≠ propustnost | Vyv√°≈æen√Ω | Dobr√Ω |
| **Efektivita pamƒõti** | Optimalizace PagedAttention | Automatick√° optimalizace | Standardn√≠ |
| **Slo≈æitost nastaven√≠** | Vysok√° (mnoho parametr≈Ø) | N√≠zk√° (automatick√°) | N√≠zk√° (jednoduch√°) |
| **≈†k√°lovatelnost** | V√Ωborn√° (tensor/pipeline paralelismus) | Dobr√° | Omezen√° |
| **Kvantizace** | Pokroƒçil√° (AWQ, GPTQ, FP8) | Automatick√° | Standardn√≠ GGUF |
| **Podnikov√© funkce** | Nutn√° vlastn√≠ implementace | Vestavƒõn√© | N√°stroje komunity |
| **Nejlep≈°√≠ pro** | Produkƒçn√≠ agenti ve velk√©m mƒõ≈ô√≠tku | Podnikov√° produkce | V√Ωvoj |

**Kdy zvolit VLLM**:
- **Po≈æadavky na vysokou propustnost**: Zpracov√°n√≠ stovek po≈æadavk≈Ø za sekundu
- **Nasazen√≠ ve velk√©m mƒõ≈ô√≠tku**: Multi-GPU, multi-node nasazen√≠
- **Kritick√Ω v√Ωkon**: Odezvy pod sekundu ve velk√©m mƒõ≈ô√≠tku
- **Pokroƒçil√° optimalizace**: Pot≈ôeba vlastn√≠ kvantizace a d√°vkov√°n√≠
- **Efektivita zdroj≈Ø**: Maxim√°ln√≠ vyu≈æit√≠ drah√©ho GPU hardwaru

## Re√°ln√© aplikace SLM agent≈Ø

### SLM agenti pro z√°kaznick√Ω servis
- **Schopnosti SLM**: Vyhled√°v√°n√≠ √∫ƒçt≈Ø, resetov√°n√≠ hesel, kontrola stavu objedn√°vek
- **√öspory n√°klad≈Ø**: 10x sn√≠≈æen√≠ n√°klad≈Ø na inference ve srovn√°n√≠ s LLM agenty
- **V√Ωkon**: Rychlej≈°√≠ odezvy s konzistentn√≠ kvalitou pro rutinn√≠ dotazy

### SLM agenti pro obchodn√≠ procesy
- **Agenti pro zpracov√°n√≠ faktur**: Extrakce dat, validace informac√≠, smƒõrov√°n√≠ k schv√°len√≠
- **Agenti pro spr√°vu e-mail≈Ø**: Kategorizace, prioritizace, automatick√© n√°vrhy odpovƒõd√≠
- **Agenti pro pl√°nov√°n√≠**: Koordinace sch≈Øzek, spr√°va kalend√°≈ô≈Ø, zas√≠l√°n√≠ p≈ôipom√≠nek

### Osobn√≠ digit√°ln√≠ asistenti SLM
- **Agenti pro spr√°vu √∫kol≈Ø**: Vytv√°≈ôen√≠, aktualizace, organizace seznam≈Ø √∫kol≈Ø
- **Agenti pro sbƒõr informac√≠**: V√Ωzkum t√©mat, lok√°ln√≠ shrnut√≠ zji≈°tƒõn√≠
- **Agenti pro komunikaci**: N√°vrhy e-mail≈Ø, zpr√°v, p≈ô√≠spƒõvk≈Ø na soci√°ln√≠ s√≠tƒõ

### SLM agenti pro obchodov√°n√≠ a finance
- **Agenti pro sledov√°n√≠ trhu**: Sledov√°n√≠ cen, identifikace trend≈Ø v re√°ln√©m ƒçase
- **Agenti pro generov√°n√≠ zpr√°v**: Automatick√© vytv√°≈ôen√≠ denn√≠ch/t√Ωdenn√≠ch p≈ôehled≈Ø
- **Agenti pro hodnocen√≠ rizik**: Posouzen√≠ portfoliov√Ωch pozic pomoc√≠ lok√°ln√≠ch dat

### SLM agenti pro podporu zdravotnictv√≠
- **Agenti pro pl√°nov√°n√≠ pacient≈Ø**: Koordinace sch≈Øzek, automatick√© p≈ôipom√≠nky
- **Agenti pro dokumentaci**: Lok√°ln√≠ generov√°n√≠ l√©ka≈ôsk√Ωch souhrn≈Ø, zpr√°v
- **Agenti pro spr√°vu recept≈Ø**: Sledov√°n√≠ doplnƒõn√≠, kontrola interakc√≠

## Microsoft Agent Framework: V√Ωvoj produkƒçn√≠ch agent≈Ø

### P≈ôehled a architektura

Microsoft Agent Framework poskytuje komplexn√≠, podnikov√© ≈ôe≈°en√≠ pro vytv√°≈ôen√≠, nasazen√≠ a spr√°vu AI agent≈Ø, kte≈ô√≠ mohou fungovat jak v cloudu, tak v offline prost≈ôed√≠ na okraji s√≠tƒõ. Framework je navr≈æen tak, aby bezprobl√©movƒõ pracoval s mal√Ωmi jazykov√Ωmi modely a sc√©n√°≈ôi edge computingu, co≈æ ho ƒçin√≠ ide√°ln√≠m pro nasazen√≠ citliv√° na soukrom√≠ a omezen√© zdroje.

**Z√°kladn√≠ komponenty frameworku**:
- **Agent Runtime**: Lehk√Ω v√Ωkonn√Ω prost≈ôed√≠ optimalizovan√© pro edge za≈ô√≠zen√≠
- **Syst√©m integrace n√°stroj≈Ø**: Roz≈°i≈ôiteln√° architektura plugin≈Ø pro p≈ôipojen√≠ extern√≠ch slu≈æeb a API
- **Spr√°va stavu**: Trval√° pamƒõ≈• agenta a spr√°va kontextu mezi relacemi
- **Bezpeƒçnostn√≠ vrstva**: Vestavƒõn√© bezpeƒçnostn√≠ kontroly pro podnikov√© nasazen√≠
- **Orchestraƒçn√≠ engine**: Koordinace v√≠ce agent≈Ø a spr√°va pracovn√≠ch postup≈Ø

### Kl√≠ƒçov√© funkce pro nasazen√≠ na okraji

**Offline-First Architektura**: Microsoft Agent Framework je navr≈æen s principy offline-first, co≈æ umo≈æ≈àuje agent≈Øm efektivnƒõ fungovat bez neust√°l√©ho p≈ôipojen√≠ k internetu. To zahrnuje lok√°ln√≠ inference model≈Ø, ulo≈æen√© znalostn√≠ b√°ze, offline prov√°dƒõn√≠ n√°stroj≈Ø a plynul√© degradace p≈ôi nedostupnosti cloudov√Ωch slu≈æeb.

**Optimalizace zdroj≈Ø**: Framework poskytuje inteligentn√≠ spr√°vu zdroj≈Ø s automatickou optimalizac√≠ pamƒõti pro SLM, vyva≈æov√°n√≠ z√°tƒõ≈æe CPU/GPU pro edge za≈ô√≠zen√≠, adaptivn√≠ v√Ωbƒõr model≈Ø na z√°kladƒõ dostupn√Ωch zdroj≈Ø a energeticky efektivn√≠ vzory inference pro mobiln√≠ nasazen√≠.

**Bezpeƒçnost a soukrom√≠**: Podnikov√© bezpeƒçnostn√≠ funkce zahrnuj√≠ lok√°ln√≠ zpracov√°n√≠ dat pro zachov√°n√≠ soukrom√≠, ≈°ifrovan√© komunikaƒçn√≠ kan√°ly agent≈Ø, ≈ô√≠zen√≠ p≈ô√≠stupu na z√°kladƒõ rol√≠ pro schopnosti agent≈Ø a auditn√≠ logov√°n√≠ pro po≈æadavky na shodu.

### Integrace s Foundry Local

Microsoft Agent Framework se bezprobl√©movƒõ integruje s Foundry Local a poskytuje kompletn√≠ ≈ôe≈°en√≠ pro edge AI:

**Automatick√© vyhled√°v√°n√≠ model≈Ø**: Framework automaticky detekuje a p≈ôipojuje se k instanc√≠m Foundry Local, vyhled√°v√° dostupn√© SLM modely a vyb√≠r√° optim√°ln√≠ modely na z√°kladƒõ po≈æadavk≈Ø agent≈Ø a schopnost√≠ hardwaru.

**Dynamick√© naƒç√≠t√°n√≠ model≈Ø**: Agenti mohou dynamicky naƒç√≠tat r≈Øzn√© SLM modely pro specifick√© √∫koly, co≈æ umo≈æ≈àuje syst√©my v√≠ce model≈Ø, kde r≈Øzn√© modely zpracov√°vaj√≠ r≈Øzn√© typy po≈æadavk≈Ø, a automatick√© p≈ôepnut√≠ mezi modely na z√°kladƒõ dostupnosti a v√Ωkonu.

**Optimalizace v√Ωkonu**: Integrovan√© mechanismy ukl√°d√°n√≠ do mezipamƒõti sni≈æuj√≠ ƒçasy naƒç√≠t√°n√≠ model≈Ø, pooling p≈ôipojen√≠ optimalizuje API vol√°n√≠ na Foundry Local a inteligentn√≠ d√°vkov√°n√≠ zlep≈°uje propustnost pro v√≠ce po≈æadavk≈Ø agent≈Ø.

### Vytv√°≈ôen√≠ agent≈Ø s Microsoft Agent Framework

#### Definice a konfigurace agenta

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```

#### Integrace n√°stroj≈Ø pro sc√©n√°≈ôe na okraji

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```

#### Orchestrace v√≠ce agent≈Ø

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```

### Pokroƒçil√© vzory nasazen√≠ na okraji

#### Hierarchick√° architektura agent≈Ø

**Lok√°ln√≠ klastery agent≈Ø**: Nasazen√≠ v√≠ce specializovan√Ωch SLM agent≈Ø na edge za≈ô√≠zen√≠, ka≈æd√Ω optimalizovan√Ω pro specifick√© √∫koly. Pou≈æit√≠ lehk√Ωch model≈Ø jako Qwen2.5-0.5B pro jednoduch√© smƒõrov√°n√≠ a pl√°nov√°n√≠, st≈ôedn√≠ch model≈Ø jako Phi-4-Mini pro z√°kaznick√Ω servis a dokumentaci, a vƒõt≈°√≠ch model≈Ø pro slo≈æit√© uva≈æov√°n√≠, pokud to zdroje umo≈æ≈àuj√≠.

**Koordinace edge-to-cloud**: Implementace inteligentn√≠ch eskalaƒçn√≠ch vzor≈Ø, kde lok√°ln√≠ agenti zpracov√°vaj√≠ rutinn√≠ √∫koly, cloudov√≠ agenti poskytuj√≠ slo≈æit√© uva≈æov√°n√≠, pokud to p≈ôipojen√≠ umo≈æ≈àuje, a plynul√© p≈ôed√°v√°n√≠ mezi edge a cloudov√Ωm zpracov√°n√≠m zachov√°v√° kontinuitu.

#### Konfigurace nasazen√≠

**Nasazen√≠ na jednom za≈ô√≠zen√≠**:
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```

**Distribuovan√© nasazen√≠ na okraji**:
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```

### Optimalizace v√Ωkonu pro agenty na okraji

#### Strategie v√Ωbƒõru model≈Ø

**P≈ôi≈ôazen√≠ model≈Ø na z√°kladƒõ √∫kol≈Ø**: Microsoft Agent Framework umo≈æ≈àuje inteligentn√≠ v√Ωbƒõr model≈Ø na z√°kladƒõ slo≈æitosti √∫kolu a po≈æadavk≈Ø:

- **Jednoduch√© √∫koly** (Q&A, smƒõrov√°n√≠): Qwen2.5-0.5B (500MB, <100ms odezva)
- **St≈ôednƒõ slo≈æit√© √∫koly** (z√°kaznick√Ω servis, pl√°nov√°n√≠): Phi-4-Mini (2.4GB, 200-500ms odezva)
- **Slo≈æit√© √∫koly** (technick√° anal√Ωza, pl√°nov√°n√≠): Phi-4 (7GB, 1-3s odezva, pokud to zdroje umo≈æ≈àuj√≠)

**Dynamick√© p≈ôep√≠n√°n√≠ model≈Ø**: Agenti mohou p≈ôep√≠nat mezi modely na z√°kladƒõ aktu√°ln√≠ho zat√≠≈æen√≠ syst√©mu, hodnocen√≠ slo≈æitosti √∫kolu, priorit u≈æivatele a dostupn√Ωch hardwarov√Ωch zdroj≈Ø.

#### Spr√°va pamƒõti a zdroj≈Ø

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

### Podnikov√© vzory integrace

#### Bezpeƒçnost a shoda

**Lok√°ln√≠ zpracov√°n√≠ dat**: Ve≈°ker√© zpracov√°n√≠ agent≈Ø prob√≠h√° lok√°lnƒõ, co≈æ zaji≈°≈•uje, ≈æe citliv√° data nikdy neopust√≠ edge za≈ô√≠zen√≠. To zahrnuje ochranu informac√≠ o z√°kazn√≠c√≠ch, shodu s HIPAA pro zdravotnick√© agenty, bezpeƒçnost finanƒçn√≠ch dat pro bankovn√≠ agenty a shodu s GDPR pro evropsk√° nasazen√≠.

**Kontrola p≈ô√≠stupu**: Opr√°vnƒõn√≠ na z√°kladƒõ rol√≠ kontroluj√≠, ke kter√Ωm n√°stroj≈Øm maj√≠ agenti p≈ô√≠stup, autentizace u≈æivatel≈Ø pro interakce s agenty a auditn√≠ stopy pro v≈°echny akce a rozhodnut√≠ agent≈Ø.

#### Monitorov√°n√≠ a pozorovatelnost

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```

### Re√°ln√© p≈ô√≠klady implementace

#### Syst√©m agent≈Ø pro maloobchod na okraji

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```

#### Agent pro podporu zdravotnictv√≠

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```

### Nejlep≈°√≠ postupy pro Microsoft Agent Framework

#### Pokyny pro v√Ωvoj

1. **Zaƒçnƒõte jednodu≈°e**: Zaƒçnƒõte sc√©n√°≈ôi s jedn√≠m agentem p≈ôed budov√°n√≠m komplexn√≠ch syst√©m≈Ø v√≠ce agent≈Ø
2. **Spr√°vn√° velikost modelu**: Vyberte nejmen≈°√≠ model, kter√Ω spl≈àuje va≈°e po≈æadavky na p≈ôesnost
3. **N√°vrh n√°stroj≈Ø**: Vytv√°≈ôejte zamƒõ≈ôen√©, jedno√∫ƒçelov√© n√°stroje m√≠sto slo≈æit√Ωch multifunkƒçn√≠ch n√°stroj≈Ø
4. **Zpracov√°n√≠ chyb**: Implementujte plynulou degradaci pro offline sc√©n√°≈ôe a selh√°n√≠ model≈Ø
5. **Testov√°n√≠**: Testujte agenty d≈Økladnƒõ v offline podm√≠nk√°ch a prost≈ôed√≠ch s omezen√Ωmi zdroji

#### Nejlep≈°√≠ postupy pro nasazen√≠

1. **Postupn√© zav√°dƒõn√≠**: Nasazujte nejprve mal√Ωm skupin√°m u≈æivatel≈Ø, peƒçlivƒõ sledujte metriky v√Ωkonu
2. **Monitorov√°n√≠ zdroj≈Ø**: Nastavte upozornƒõn√≠ na limity pamƒõti, CPU a doby odezvy
3. **Strategie z√°lohov√°n√≠**: V≈ædy mƒõjte z√°lo≈æn√≠ pl√°ny pro selh√°n√≠ model≈Ø nebo vyƒçerp√°n√≠ zdroj≈Ø
4. **Bezpeƒçnost na prvn√≠m m√≠stƒõ**: Implementujte bezpeƒçnostn√≠ kontroly od zaƒç√°tku, ne jako dodateƒçn√© opat≈ôen√≠
5. **Dokumentace**: Udr≈æujte jasnou dokumentaci schopnost√≠ a omezen√≠ agent≈Ø

### Budouc√≠ pl√°ny a integrace
**V√Ωbƒõr frameworku pro nasazen√≠ agent≈Ø**: Vyberte optimalizaƒçn√≠ frameworky podle c√≠lov√©ho hardwaru a po≈æadavk≈Ø agent≈Ø. Pou≈æijte Llama.cpp pro nasazen√≠ agent≈Ø optimalizovan√Ωch pro CPU, Apple MLX pro aplikace agent≈Ø na Apple Silicon a ONNX pro kompatibilitu agent≈Ø nap≈ô√≠ƒç platformami.

## Praktick√° konverze SLM agent≈Ø a p≈ô√≠klady pou≈æit√≠

### Sc√©n√°≈ôe nasazen√≠ agent≈Ø v re√°ln√©m svƒõtƒõ

**Mobiln√≠ aplikace agent≈Ø**: Form√°ty Q4_K vynikaj√≠ v aplikac√≠ch pro chytr√© telefony d√≠ky minim√°ln√≠mu vyu≈æit√≠ pamƒõti, zat√≠mco Q8_0 poskytuje vyv√°≈æen√Ω v√Ωkon pro syst√©my agent≈Ø na tabletech. Form√°ty Q5_K nab√≠zej√≠ ≈°piƒçkovou kvalitu pro mobiln√≠ produktivn√≠ agenty.

**Agentov√© v√Ωpoƒçty na desktopu a na okraji s√≠tƒõ**: Q5_K zaji≈°≈•uje optim√°ln√≠ v√Ωkon pro aplikace agent≈Ø na desktopu, Q8_0 poskytuje vysoce kvalitn√≠ inferenci pro pracovn√≠ stanice a Q4_K umo≈æ≈àuje efektivn√≠ zpracov√°n√≠ na za≈ô√≠zen√≠ch na okraji s√≠tƒõ.

**V√Ωzkumn√≠ a experiment√°ln√≠ agenti**: Pokroƒçil√© kvantizaƒçn√≠ form√°ty umo≈æ≈àuj√≠ zkoum√°n√≠ ultra-n√≠zk√© p≈ôesnosti inferenc√≠ agent≈Ø pro akademick√Ω v√Ωzkum a aplikace proof-of-concept vy≈æaduj√≠c√≠ extr√©mn√≠ omezen√≠ zdroj≈Ø.

### V√Ωkonnostn√≠ benchmarky SLM agent≈Ø

**Rychlost inferenc√≠ agent≈Ø**: Q4_K dosahuje nejrychlej≈°√≠ch odezev agent≈Ø na mobiln√≠ch CPU, Q5_K poskytuje vyv√°≈æen√Ω pomƒõr rychlosti a kvality pro obecn√© aplikace agent≈Ø, Q8_0 nab√≠z√≠ ≈°piƒçkovou kvalitu pro slo≈æit√© √∫koly agent≈Ø a experiment√°ln√≠ form√°ty zaji≈°≈•uj√≠ maxim√°ln√≠ propustnost pro specializovan√Ω hardware agent≈Ø.

**Po≈æadavky na pamƒõ≈• agent≈Ø**: √örovnƒõ kvantizace pro agenty se pohybuj√≠ od Q2_K (m√©nƒõ ne≈æ 500 MB pro mal√© modely agent≈Ø) po Q8_0 (p≈ôibli≈ænƒõ 50 % p≈Øvodn√≠ velikosti), p≈ôiƒçem≈æ experiment√°ln√≠ konfigurace dosahuj√≠ maxim√°ln√≠ komprese pro prost≈ôed√≠ s omezen√Ωmi zdroji.

## V√Ωzvy a √∫vahy pro SLM agenty

### Kompromisy v√Ωkonu v syst√©mech agent≈Ø

Nasazen√≠ SLM agent≈Ø vy≈æaduje peƒçliv√© zv√°≈æen√≠ kompromis≈Ø mezi velikost√≠ modelu, rychlost√≠ odezvy agent≈Ø a kvalitou v√Ωstupu. Zat√≠mco Q4_K nab√≠z√≠ v√Ωjimeƒçnou rychlost a efektivitu pro mobiln√≠ agenty, Q8_0 poskytuje ≈°piƒçkovou kvalitu pro slo≈æit√© √∫koly agent≈Ø. Q5_K p≈ôedstavuje st≈ôedn√≠ cestu vhodnou pro vƒõt≈°inu obecn√Ωch aplikac√≠ agent≈Ø.

### Kompatibilita hardwaru pro SLM agenty

R≈Øzn√° za≈ô√≠zen√≠ na okraji s√≠tƒõ maj√≠ r≈Øzn√© schopnosti pro nasazen√≠ SLM agent≈Ø. Q4_K bƒõ≈æ√≠ efektivnƒõ na z√°kladn√≠ch procesorech pro jednoduch√© agenty, Q5_K vy≈æaduje st≈ôedn√≠ v√Ωpoƒçetn√≠ zdroje pro vyv√°≈æen√Ω v√Ωkon agent≈Ø a Q8_0 tƒõ≈æ√≠ z vysoce v√Ωkonn√©ho hardwaru pro pokroƒçil√© schopnosti agent≈Ø.

### Bezpeƒçnost a ochrana soukrom√≠ v syst√©mech SLM agent≈Ø

Zat√≠mco SLM agenti umo≈æ≈àuj√≠ lok√°ln√≠ zpracov√°n√≠ pro zv√Ω≈°enou ochranu soukrom√≠, je nutn√© implementovat spr√°vn√° bezpeƒçnostn√≠ opat≈ôen√≠ k ochranƒõ model≈Ø agent≈Ø a dat v prost≈ôed√≠ch na okraji s√≠tƒõ. To je obzvl√°≈°tƒõ d≈Øle≈æit√© p≈ôi nasazen√≠ vysoce p≈ôesn√Ωch form√°t≈Ø agent≈Ø v podnikov√Ωch prost≈ôed√≠ch nebo komprimovan√Ωch form√°t≈Ø agent≈Ø v aplikac√≠ch, kter√© zpracov√°vaj√≠ citliv√° data.

## Budouc√≠ trendy ve v√Ωvoji SLM agent≈Ø

Prost≈ôed√≠ SLM agent≈Ø se neust√°le vyv√≠j√≠ d√≠ky pokrok≈Øm v kompresn√≠ch technik√°ch, metod√°ch optimalizace a strategi√≠ch nasazen√≠ na okraji s√≠tƒõ. Budouc√≠ v√Ωvoj zahrnuje efektivnƒõj≈°√≠ algoritmy kvantizace pro modely agent≈Ø, vylep≈°en√© metody komprese pro pracovn√≠ postupy agent≈Ø a lep≈°√≠ integraci s hardwarov√Ωmi akceler√°tory na okraji s√≠tƒõ pro zpracov√°n√≠ agent≈Ø.

**Predikce trhu pro SLM agenty**: Podle ned√°vn√©ho v√Ωzkumu by automatizace poh√°nƒõn√° agenty mohla do roku 2027 eliminovat 40‚Äì60 % opakuj√≠c√≠ch se kognitivn√≠ch √∫kol≈Ø v podnikov√Ωch pracovn√≠ch postupech, p≈ôiƒçem≈æ SLM budou v√©st tuto transformaci d√≠ky sv√© n√°kladov√© efektivitƒõ a flexibilitƒõ nasazen√≠.

**Technologick√© trendy v SLM agentech**:
- **Specializovan√≠ SLM agenti**: Modely zamƒõ≈ôen√© na konkr√©tn√≠ √∫koly agent≈Ø a odvƒõtv√≠
- **V√Ωpoƒçty agent≈Ø na okraji s√≠tƒõ**: Vylep≈°en√© schopnosti agent≈Ø na za≈ô√≠zen√≠ s lep≈°√≠ ochranou soukrom√≠ a sn√≠≈æenou latenc√≠
- **Orchestrace agent≈Ø**: Lep≈°√≠ koordinace mezi v√≠ce SLM agenty s dynamick√Ωm smƒõrov√°n√≠m a vyva≈æov√°n√≠m z√°tƒõ≈æe
- **Demokratizace**: Flexibilita SLM umo≈æ≈àuje ≈°ir≈°√≠ zapojen√≠ do v√Ωvoje agent≈Ø nap≈ô√≠ƒç organizacemi

## Zaƒç√≠n√°me s SLM agenty

### Krok 1: Nastaven√≠ prost≈ôed√≠ Microsoft Agent Framework

**Instalace z√°vislost√≠**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inicializace Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Krok 2: V√Ωbƒõr SLM pro aplikace agent≈Ø
Obl√≠ben√© mo≈ænosti pro Microsoft Agent Framework:
- **Microsoft Phi-4 Mini (3.8B)**: Skvƒõl√Ω pro obecn√© √∫koly agent≈Ø s vyv√°≈æen√Ωm v√Ωkonem
- **Qwen2.5-0.5B (0.5B)**: Ultra-efektivn√≠ pro jednoduch√© agenty zamƒõ≈ôen√© na smƒõrov√°n√≠ a klasifikaci
- **Qwen2.5-Coder-0.5B (0.5B)**: Specializovan√Ω na √∫koly agent≈Ø souvisej√≠c√≠ s k√≥dem
- **Phi-4 (7B)**: Pokroƒçil√© uva≈æov√°n√≠ pro slo≈æit√© sc√©n√°≈ôe na okraji s√≠tƒõ, pokud to zdroje umo≈æ≈àuj√≠

### Krok 3: Vytvo≈ôen√≠ prvn√≠ho agenta s Microsoft Agent Framework

**Z√°kladn√≠ nastaven√≠ agenta**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Krok 4: Definov√°n√≠ rozsahu a po≈æadavk≈Ø agenta
Zaƒçnƒõte s c√≠len√Ωmi, dob≈ôe definovan√Ωmi aplikacemi agent≈Ø pomoc√≠ Microsoft Agent Framework:
- **Agenti pro jeden obor**: Z√°kaznick√Ω servis NEBO pl√°nov√°n√≠ NEBO v√Ωzkum
- **Jasn√© c√≠le agenta**: Specifick√©, mƒõ≈ôiteln√© c√≠le pro v√Ωkon agenta
- **Omezen√° integrace n√°stroj≈Ø**: Maxim√°lnƒõ 3‚Äì5 n√°stroj≈Ø pro poƒç√°teƒçn√≠ nasazen√≠ agenta
- **Definovan√© hranice agenta**: Jasn√© cesty eskalace pro slo≈æit√© sc√©n√°≈ôe
- **Design zamƒõ≈ôen√Ω na okraj s√≠tƒõ**: Prioritizace offline funkcionality a lok√°ln√≠ho zpracov√°n√≠

### Krok 5: Implementace nasazen√≠ na okraji s√≠tƒõ s Microsoft Agent Framework

**Konfigurace zdroj≈Ø**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Nasazen√≠ bezpeƒçnostn√≠ch opat≈ôen√≠ pro agenty na okraji s√≠tƒõ**:
- **Lok√°ln√≠ validace vstup≈Ø**: Kontrola po≈æadavk≈Ø bez z√°vislosti na cloudu
- **Offline filtrov√°n√≠ v√Ωstup≈Ø**: Zaji≈°tƒõn√≠ kvality odpovƒõd√≠ lok√°lnƒõ
- **Bezpeƒçnostn√≠ kontroly na okraji s√≠tƒõ**: Implementace bezpeƒçnostn√≠ch opat≈ôen√≠ bez nutnosti p≈ôipojen√≠ k internetu
- **Lok√°ln√≠ monitorov√°n√≠**: Sledov√°n√≠ v√Ωkonu a oznaƒçov√°n√≠ probl√©m≈Ø pomoc√≠ telemetrie na okraji s√≠tƒõ

### Krok 6: Mƒõ≈ôen√≠ a optimalizace v√Ωkonu agent≈Ø na okraji s√≠tƒõ
- **M√≠ra dokonƒçen√≠ √∫kol≈Ø agenta**: Sledov√°n√≠ √∫spƒõ≈°nosti v offline sc√©n√°≈ô√≠ch
- **ƒåasy odezvy agenta**: Zaji≈°tƒõn√≠ odezvy pod jednu sekundu pro nasazen√≠ na okraji s√≠tƒõ
- **Vyu≈æit√≠ zdroj≈Ø**: Sledov√°n√≠ pamƒõti, CPU a spot≈ôeby baterie na za≈ô√≠zen√≠ch na okraji s√≠tƒõ
- **N√°kladov√° efektivita**: Porovn√°n√≠ n√°klad≈Ø na nasazen√≠ na okraji s√≠tƒõ s alternativami zalo≈æen√Ωmi na cloudu
- **Spolehlivost offline**: Mƒõ≈ôen√≠ v√Ωkonu agenta bƒõhem v√Ωpadk≈Ø s√≠tƒõ

## Kl√≠ƒçov√© poznatky pro implementaci SLM agent≈Ø

1. **SLM jsou dostateƒçn√© pro agenty**: Pro vƒõt≈°inu √∫kol≈Ø agent≈Ø mal√© modely funguj√≠ stejnƒõ dob≈ôe jako velk√©, p≈ôiƒçem≈æ nab√≠zej√≠ v√Ωznamn√© v√Ωhody
2. **N√°kladov√° efektivita agent≈Ø**: 10‚Äì30x levnƒõj≈°√≠ provoz SLM agent≈Ø, co≈æ je ƒçin√≠ ekonomicky ≈æivotaschopn√Ωmi pro ≈°irok√© nasazen√≠
3. **Specializace funguje pro agenty**: Jemnƒõ doladƒõn√© SLM ƒçasto p≈ôekon√°vaj√≠ obecn√© LLM v konkr√©tn√≠ch aplikac√≠ch agent≈Ø
4. **Hybridn√≠ architektura agent≈Ø**: Pou≈æ√≠vejte SLM pro rutinn√≠ √∫koly agent≈Ø, LLM pro slo≈æit√© uva≈æov√°n√≠, kdy≈æ je to nutn√©
5. **Microsoft Agent Framework umo≈æ≈àuje produkƒçn√≠ nasazen√≠**: Poskytuje n√°stroje na podnikov√© √∫rovni pro vytv√°≈ôen√≠, nasazen√≠ a spr√°vu agent≈Ø na okraji s√≠tƒõ
6. **Principy designu zamƒõ≈ôen√© na okraj s√≠tƒõ**: Agenti schopn√≠ offline zpracov√°n√≠ s lok√°ln√≠m zpracov√°n√≠m zaji≈°≈•uj√≠ ochranu soukrom√≠ a spolehlivost
7. **Integrace Foundry Local**: Bezprobl√©mov√© propojen√≠ mezi Microsoft Agent Framework a lok√°ln√≠ inferenc√≠ model≈Ø
8. **Budoucnost pat≈ô√≠ SLM agent≈Øm**: Mal√© jazykov√© modely s produkƒçn√≠mi frameworky jsou budoucnost√≠ agentick√© AI, umo≈æ≈àuj√≠c√≠ demokratizovan√© a efektivn√≠ nasazen√≠ agent≈Ø

## Odkazy a dal≈°√≠ ƒçten√≠

### Z√°kladn√≠ v√Ωzkumn√© pr√°ce a publikace

#### AI agenti a agentick√© syst√©my
- **"Language Agents as Optimizable Graphs"** (2024) - Z√°kladn√≠ v√Ωzkum architektury agent≈Ø a optimalizaƒçn√≠ch strategi√≠
  - Auto≈ôi: Wenyue Hua, Lishan Yang, et al.
  - Odkaz: https://arxiv.org/abs/2402.16823
  - Kl√≠ƒçov√© poznatky: N√°vrh agent≈Ø zalo≈æen√Ω na grafech a optimalizaƒçn√≠ strategie

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Auto≈ôi: Zhiheng Xi, Wenxiang Chen, et al.
  - Odkaz: https://arxiv.org/abs/2309.07864
  - Kl√≠ƒçov√© poznatky: Komplexn√≠ p≈ôehled schopnost√≠ a aplikac√≠ agent≈Ø zalo≈æen√Ωch na LLM

- **"Cognitive Architectures for Language Agents"** (2024)
  - Auto≈ôi: Theodore Sumers, Shunyu Yao, et al.
  - Odkaz: https://arxiv.org/abs/2309.02427
  - Kl√≠ƒçov√© poznatky: Kognitivn√≠ r√°mce pro n√°vrh inteligentn√≠ch agent≈Ø

#### Mal√© jazykov√© modely a optimalizace
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Auto≈ôi: Microsoft Research Team
  - Odkaz: https://arxiv.org/abs/2404.14219
  - Kl√≠ƒçov√© poznatky: Principy n√°vrhu SLM a strategie mobiln√≠ho nasazen√≠

- **"Qwen2.5 Technical Report"** (2024)
  - Auto≈ôi: Alibaba Cloud Team
  - Odkaz: https://arxiv.org/abs/2407.10671
  - Kl√≠ƒçov√© poznatky: Pokroƒçil√© techniky tr√©ninku SLM a optimalizace v√Ωkonu

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Auto≈ôi: Peiyuan Zhang, Guangtao Zeng, et al.
  - Odkaz: https://arxiv.org/abs/2401.02385
  - Kl√≠ƒçov√© poznatky: Ultra-kompaktn√≠ n√°vrh modelu a efektivita tr√©ninku

### Ofici√°ln√≠ dokumentace a frameworky

#### Microsoft Agent Framework
- **Ofici√°ln√≠ dokumentace**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub Repository**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Prim√°rn√≠ repozit√°≈ô**: https://github.com/microsoft/foundry-local
- **Dokumentace**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Hlavn√≠ repozit√°≈ô**: https://github.com/vllm-project/vllm
- **Dokumentace**: https://docs.vllm.ai/


#### Ollama
- **Ofici√°ln√≠ web**: https://ollama.ai/
- **GitHub Repository**: https://github.com/ollama/ollama

### Frameworky pro optimalizaci model≈Ø

#### Llama.cpp
- **Repozit√°≈ô**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentace**: https://microsoft.github.io/Olive/
- **GitHub Repository**: https://github.com/microsoft/Olive

#### OpenVINO
- **Ofici√°ln√≠ str√°nka**: https://docs.openvino.ai/

#### Apple MLX
- **Repozit√°≈ô**: https://github.com/ml-explore/mlx

### Pr≈Ømyslov√© zpr√°vy a anal√Ωzy trhu

#### V√Ωzkum trhu AI agent≈Ø
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Odkaz: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - Kl√≠ƒçov√© poznatky: Trendy trhu a vzorce adopce v podnic√≠ch

#### Technick√© benchmarky

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Odkaz: https://mlcommons.org/en/inference-edge/
  - Kl√≠ƒçov√© poznatky: Standardizovan√© metriky v√Ωkonu pro nasazen√≠ na okraji s√≠tƒõ

### Standardy a specifikace

#### Form√°ty model≈Ø a standardy
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Form√°t modelu nap≈ô√≠ƒç platformami pro interoperabilitu
- **GGUF Specification**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Kvantizovan√Ω form√°t modelu pro inferenci na CPU
- **OpenAI API Specification**: https://platform.openai.com/docs/api-reference
  - Standardn√≠ form√°t API pro integraci jazykov√Ωch model≈Ø

#### Bezpeƒçnost a shoda
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI Systems**: R√°mec pro AI syst√©my a bezpeƒçnost
- **IEEE Standards for AI**: https://standards.ieee.org/industry-connections/ai/

Posun smƒõrem k agent≈Øm poh√°nƒõn√Ωm SLM p≈ôedstavuje z√°sadn√≠ zmƒõnu v p≈ô√≠stupu k nasazen√≠ AI. Microsoft Agent Framework, kombinovan√Ω s lok√°ln√≠mi platformami a efektivn√≠mi mal√Ωmi jazykov√Ωmi modely, poskytuje kompletn√≠ ≈ôe≈°en√≠ pro vytv√°≈ôen√≠ produkƒçnƒõ p≈ôipraven√Ωch agent≈Ø, kte≈ô√≠ efektivnƒõ funguj√≠ v prost≈ôed√≠ch na okraji s√≠tƒõ. Zamƒõ≈ôen√≠m na efektivitu, specializaci a praktickou u≈æiteƒçnost ƒçin√≠ tento technologick√Ω stack AI agenty dostupnƒõj≈°√≠mi, cenovƒõ v√Ωhodnƒõj≈°√≠mi a efektivnƒõj≈°√≠mi pro aplikace v re√°ln√©m svƒõtƒõ nap≈ô√≠ƒç v≈°emi odvƒõtv√≠mi a prost≈ôed√≠mi v√Ωpoƒçt≈Ø na okraji s√≠tƒõ.

Jak postupujeme do roku 2025, kombinace st√°le schopnƒõj≈°√≠ch mal√Ωch model≈Ø, sofistikovan√Ωch framework≈Ø pro agenty, jako je Microsoft Agent Framework, a robustn√≠ch platforem pro nasazen√≠ na okraji s√≠tƒõ odemkne nov√© mo≈ænosti pro autonomn√≠ syst√©my, kter√© mohou efektivnƒõ fungovat na za≈ô√≠zen√≠ch na okraji s√≠tƒõ p≈ôi zachov√°n√≠ soukrom√≠, sn√≠≈æen√≠ n√°klad≈Ø a poskytov√°n√≠ v√Ωjimeƒçn√Ωch u≈æivatelsk√Ωch zku≈°enost√≠.

**Dal≈°√≠ kroky pro implementaci**:
1. **Prozkoumejte vol√°n√≠ funkc√≠**: Nauƒçte se, jak SLM zpracov√°vaj√≠ integraci n√°stroj≈Ø a strukturovan√© v√Ωstupy
2. **Ovl√°dnƒõte Model Context Protocol (MCP)**: Pochopte pokroƒçil√© komunikaƒçn√≠ vzory agent≈Ø
3. **Vytvo≈ôte produkƒçn√≠ agenty**: Pou≈æij

---

**Prohl√°≈°en√≠**:  
Tento dokument byl p≈ôelo≈æen pomoc√≠ slu≈æby AI pro p≈ôeklady [Co-op Translator](https://github.com/Azure/co-op-translator). I kdy≈æ se sna≈æ√≠me o p≈ôesnost, mƒõjte pros√≠m na pamƒõti, ≈æe automatick√© p≈ôeklady mohou obsahovat chyby nebo nep≈ôesnosti. P≈Øvodn√≠ dokument v jeho rodn√©m jazyce by mƒõl b√Ωt pova≈æov√°n za autoritativn√≠ zdroj. Pro d≈Øle≈æit√© informace se doporuƒçuje profesion√°ln√≠ lidsk√Ω p≈ôeklad. Neodpov√≠d√°me za ≈æ√°dn√° nedorozumƒõn√≠ nebo nespr√°vn√© interpretace vypl√Ωvaj√≠c√≠ z pou≈æit√≠ tohoto p≈ôekladu.