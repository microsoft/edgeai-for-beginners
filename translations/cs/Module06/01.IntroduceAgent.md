<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T14:14:21+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "cs"
}
-->
# AI agenti a malÃ© jazykovÃ© modely: KomplexnÃ­ prÅ¯vodce

## Ãšvod

V tomto tutoriÃ¡lu se podÃ­vÃ¡me na AI agenty a malÃ© jazykovÃ© modely (SLM) a jejich pokroÄilÃ© implementaÄnÃ­ strategie pro prostÅ™edÃ­ edge computingu. Probereme zÃ¡kladnÃ­ koncepty agentnÃ­ AI, optimalizaÄnÃ­ techniky SLM, praktickÃ© strategie nasazenÃ­ pro zaÅ™Ã­zenÃ­ s omezenÃ½mi zdroji a Microsoft Agent Framework pro vytvÃ¡Å™enÃ­ produkÄnÄ› pÅ™ipravenÃ½ch agentnÃ­ch systÃ©mÅ¯.

Oblast umÄ›lÃ© inteligence zaÅ¾Ã­vÃ¡ v roce 2025 paradigmatickÃ½ posun. ZatÃ­mco rok 2023 byl rokem chatbotÅ¯ a rok 2024 pÅ™inesl boom kopilotÅ¯, rok 2025 patÅ™Ã­ AI agentÅ¯m â€” inteligentnÃ­m systÃ©mÅ¯m, kterÃ© myslÃ­, plÃ¡nujÃ­, pouÅ¾Ã­vajÃ­ nÃ¡stroje a vykonÃ¡vajÃ­ Ãºkoly s minimÃ¡lnÃ­m lidskÃ½m vstupem, stÃ¡le vÃ­ce pohÃ¡nÄ›nÃ© efektivnÃ­mi malÃ½mi jazykovÃ½mi modely. Microsoft Agent Framework se stÃ¡vÃ¡ pÅ™ednÃ­m Å™eÅ¡enÃ­m pro vytvÃ¡Å™enÃ­ tÄ›chto inteligentnÃ­ch systÃ©mÅ¯ s offline schopnostmi na edge zaÅ™Ã­zenÃ­ch.

## CÃ­le uÄenÃ­

Na konci tohoto tutoriÃ¡lu budete schopni:

- ğŸ¤– PorozumÄ›t zÃ¡kladnÃ­m konceptÅ¯m AI agentÅ¯ a agentnÃ­ch systÃ©mÅ¯
- ğŸ”¬ Identifikovat vÃ½hody malÃ½ch jazykovÃ½ch modelÅ¯ oproti velkÃ½m jazykovÃ½m modelÅ¯m v agentnÃ­ch aplikacÃ­ch
- ğŸš€ NauÄit se pokroÄilÃ© strategie nasazenÃ­ SLM pro prostÅ™edÃ­ edge computingu
- ğŸ“± Implementovat praktickÃ© agenty pohÃ¡nÄ›nÃ© SLM pro reÃ¡lnÃ© aplikace
- ğŸ—ï¸ VytvoÅ™it produkÄnÄ› pÅ™ipravenÃ© agenty pomocÃ­ Microsoft Agent Framework
- ğŸŒ Nasadit offline agenty na edge zaÅ™Ã­zenÃ­ch s integracÃ­ lokÃ¡lnÃ­ch LLM a SLM
- ğŸ”§ Integrovat Microsoft Agent Framework s Foundry Local pro nasazenÃ­ na edge

## PorozumÄ›nÃ­ AI agentÅ¯m: ZÃ¡klady a klasifikace

### Definice a zÃ¡kladnÃ­ koncepty

UmÄ›lÃ½ inteligentnÃ­ agent oznaÄuje systÃ©m nebo program, kterÃ½ je schopen autonomnÄ› vykonÃ¡vat Ãºkoly jmÃ©nem uÅ¾ivatele nebo jinÃ©ho systÃ©mu tÃ­m, Å¾e navrhuje svÅ¯j pracovnÃ­ postup a vyuÅ¾Ã­vÃ¡ dostupnÃ© nÃ¡stroje. Na rozdÃ­l od tradiÄnÃ­ AI, kterÃ¡ pouze odpovÃ­dÃ¡ na vaÅ¡e otÃ¡zky, agent mÅ¯Å¾e jednat nezÃ¡visle na dosaÅ¾enÃ­ cÃ­lÅ¯.

### RÃ¡mec klasifikace agentÅ¯

PorozumÄ›nÃ­ hranicÃ­m agentÅ¯ pomÃ¡hÃ¡ pÅ™i vÃ½bÄ›ru vhodnÃ½ch typÅ¯ agentÅ¯ pro rÅ¯znÃ© scÃ©nÃ¡Å™e vÃ½poÄetnÃ­ techniky:

- **ğŸ”¬ JednoduchÃ© reflexnÃ­ agenty**: SystÃ©my zaloÅ¾enÃ© na pravidlech, kterÃ© reagujÃ­ na okamÅ¾itÃ© vnÃ­mÃ¡nÃ­ (termostaty, zÃ¡kladnÃ­ automatizace)
- **ğŸ“± Agenti zaloÅ¾enÃ­ na modelu**: SystÃ©my, kterÃ© udrÅ¾ujÃ­ vnitÅ™nÃ­ stav a pamÄ›Å¥ (robotickÃ© vysavaÄe, navigaÄnÃ­ systÃ©my)
- **âš–ï¸ Agenti zaloÅ¾enÃ­ na cÃ­lech**: SystÃ©my, kterÃ© plÃ¡nujÃ­ a provÃ¡dÄ›jÃ­ sekvence k dosaÅ¾enÃ­ cÃ­lÅ¯ (plÃ¡novaÄe tras, plÃ¡novaÄe ÃºkolÅ¯)
- **ğŸ§  UÄÃ­cÃ­ se agenti**: AdaptivnÃ­ systÃ©my, kterÃ© se zlepÅ¡ujÃ­ v prÅ¯bÄ›hu Äasu (doporuÄovacÃ­ systÃ©my, personalizovanÃ­ asistenti)

### KlÃ­ÄovÃ© vÃ½hody AI agentÅ¯

AI agenti nabÃ­zejÃ­ nÄ›kolik zÃ¡kladnÃ­ch vÃ½hod, kterÃ© je ÄinÃ­ ideÃ¡lnÃ­mi pro aplikace edge computingu:

**OperaÄnÃ­ autonomie**: Agenti poskytujÃ­ nezÃ¡vislÃ© provÃ¡dÄ›nÃ­ ÃºkolÅ¯ bez neustÃ¡lÃ©ho lidskÃ©ho dohledu, coÅ¾ je ideÃ¡lnÃ­ pro aplikace v reÃ¡lnÃ©m Äase. VyÅ¾adujÃ­ minimÃ¡lnÃ­ dohled pÅ™i zachovÃ¡nÃ­ adaptivnÃ­ho chovÃ¡nÃ­, coÅ¾ umoÅ¾Åˆuje nasazenÃ­ na zaÅ™Ã­zenÃ­ch s omezenÃ½mi zdroji a sniÅ¾uje provoznÃ­ nÃ¡klady.

**Flexibilita nasazenÃ­**: Tyto systÃ©my umoÅ¾ÅˆujÃ­ AI schopnosti na zaÅ™Ã­zenÃ­ bez poÅ¾adavkÅ¯ na pÅ™ipojenÃ­ k internetu, zvyÅ¡ujÃ­ soukromÃ­ a bezpeÄnost prostÅ™ednictvÃ­m lokÃ¡lnÃ­ho zpracovÃ¡nÃ­, mohou bÃ½t pÅ™izpÅ¯sobeny pro aplikace specifickÃ© pro danou oblast a jsou vhodnÃ© pro rÅ¯znÃ¡ prostÅ™edÃ­ edge computingu.

**NÃ¡kladovÃ¡ efektivita**: AgentnÃ­ systÃ©my nabÃ­zejÃ­ nÃ¡kladovÄ› efektivnÃ­ nasazenÃ­ ve srovnÃ¡nÃ­ s cloudovÃ½mi Å™eÅ¡enÃ­mi, s niÅ¾Å¡Ã­mi provoznÃ­mi nÃ¡klady a niÅ¾Å¡Ã­mi poÅ¾adavky na Å¡Ã­Å™ku pÃ¡sma pro aplikace na edge.

## PokroÄilÃ© strategie malÃ½ch jazykovÃ½ch modelÅ¯

### ZÃ¡klady SLM (Small Language Model)

MalÃ½ jazykovÃ½ model (SLM) je jazykovÃ½ model, kterÃ½ se vejde na bÄ›Å¾nÃ© spotÅ™ebitelskÃ© elektronickÃ© zaÅ™Ã­zenÃ­ a provÃ¡dÃ­ inference s latencÃ­ dostateÄnÄ› nÃ­zkou, aby byl praktickÃ½ pÅ™i obsluze agentnÃ­ch poÅ¾adavkÅ¯ jednoho uÅ¾ivatele. Prakticky vzato, SLM jsou obvykle modely s mÃ©nÄ› neÅ¾ 10 miliardami parametrÅ¯.

**Funkce objevovÃ¡nÃ­ formÃ¡tÅ¯**: SLM nabÃ­zejÃ­ pokroÄilou podporu pro rÅ¯znÃ© ÃºrovnÄ› kvantizace, kompatibilitu napÅ™Ã­Ä platformami, optimalizaci vÃ½konu v reÃ¡lnÃ©m Äase a schopnosti nasazenÃ­ na edge. UÅ¾ivatelÃ© mohou vyuÅ¾Ã­vat zvÃ½Å¡enÃ© soukromÃ­ prostÅ™ednictvÃ­m lokÃ¡lnÃ­ho zpracovÃ¡nÃ­ a podporu WebGPU pro nasazenÃ­ v prohlÃ­Å¾eÄi.

**SbÃ­rky ÃºrovnÃ­ kvantizace**: PopulÃ¡rnÃ­ formÃ¡ty SLM zahrnujÃ­ Q4_K_M pro vyvÃ¡Å¾enou kompresi v mobilnÃ­ch aplikacÃ­ch, Q5_K_S sÃ©rii pro nasazenÃ­ zamÄ›Å™enÃ© na kvalitu na edge, Q8_0 pro tÃ©mÄ›Å™ pÅ¯vodnÃ­ pÅ™esnost na vÃ½konnÃ½ch edge zaÅ™Ã­zenÃ­ch a experimentÃ¡lnÃ­ formÃ¡ty jako Q2_K pro scÃ©nÃ¡Å™e s ultra nÃ­zkÃ½mi zdroji.

### GGUF (General GGML Universal Format) pro nasazenÃ­ SLM

GGUF slouÅ¾Ã­ jako primÃ¡rnÃ­ formÃ¡t pro nasazenÃ­ kvantizovanÃ½ch SLM na CPU a edge zaÅ™Ã­zenÃ­ch, speciÃ¡lnÄ› optimalizovanÃ½ pro agentnÃ­ aplikace:

**AgentnÄ› optimalizovanÃ© funkce**: FormÃ¡t poskytuje komplexnÃ­ zdroje pro konverzi a nasazenÃ­ SLM s rozÅ¡Ã­Å™enou podporou pro volÃ¡nÃ­ nÃ¡strojÅ¯, generovÃ¡nÃ­ strukturovanÃ½ch vÃ½stupÅ¯ a vÃ­cenÃ¡sobnÃ© konverzace. Kompatibilita napÅ™Ã­Ä platformami zajiÅ¡Å¥uje konzistentnÃ­ chovÃ¡nÃ­ agentÅ¯ na rÅ¯znÃ½ch edge zaÅ™Ã­zenÃ­ch.

**Optimalizace vÃ½konu**: GGUF umoÅ¾Åˆuje efektivnÃ­ vyuÅ¾itÃ­ pamÄ›ti pro pracovnÃ­ postupy agentÅ¯, podporuje dynamickÃ© naÄÃ­tÃ¡nÃ­ modelÅ¯ pro systÃ©my s vÃ­ce agenty a poskytuje optimalizovanou inference pro interakce agentÅ¯ v reÃ¡lnÃ©m Äase.

### Edge-optimalizovanÃ© rÃ¡mce SLM

#### Optimalizace Llama.cpp pro agenty

Llama.cpp poskytuje Å¡piÄkovÃ© kvantizaÄnÃ­ techniky speciÃ¡lnÄ› optimalizovanÃ© pro nasazenÃ­ agentnÃ­ch SLM:

**AgentnÄ› specifickÃ¡ kvantizace**: RÃ¡mec podporuje Q4_0 (optimÃ¡lnÃ­ pro mobilnÃ­ nasazenÃ­ agentÅ¯ s 75% redukcÃ­ velikosti), Q5_1 (vyvÃ¡Å¾enÃ¡ kvalita-komprese pro inference agentÅ¯ na edge) a Q8_0 (tÃ©mÄ›Å™ pÅ¯vodnÃ­ kvalita pro produkÄnÃ­ agentnÃ­ systÃ©my). PokroÄilÃ© formÃ¡ty umoÅ¾ÅˆujÃ­ ultra-komprimovanÃ© agenty pro extrÃ©mnÃ­ edge scÃ©nÃ¡Å™e.

**VÃ½hody implementace**: Inference optimalizovanÃ¡ pro CPU s akceleracÃ­ SIMD poskytuje pamÄ›Å¥ovÄ› efektivnÃ­ provÃ¡dÄ›nÃ­ agentÅ¯. Kompatibilita napÅ™Ã­Ä platformami na architekturÃ¡ch x86, ARM a Apple Silicon umoÅ¾Åˆuje univerzÃ¡lnÃ­ schopnosti nasazenÃ­ agentÅ¯.

#### Apple MLX Framework pro SLM agenty

Apple MLX poskytuje nativnÃ­ optimalizaci speciÃ¡lnÄ› navrÅ¾enou pro agenty pohÃ¡nÄ›nÃ© SLM na zaÅ™Ã­zenÃ­ch Apple Silicon:

**Optimalizace agentÅ¯ na Apple Silicon**: RÃ¡mec vyuÅ¾Ã­vÃ¡ sjednocenou pamÄ›Å¥ovou architekturu s integracÃ­ Metal Performance Shaders, automatickou smÃ­Å¡enou pÅ™esnost pro inference agentÅ¯ a optimalizovanou Å¡Ã­Å™ku pamÄ›Å¥ovÃ©ho pÃ¡sma pro systÃ©my s vÃ­ce agenty. Agenti SLM vykazujÃ­ vÃ½jimeÄnÃ½ vÃ½kon na Äipech Å™ady M.

**VÃ½vojovÃ© funkce**: Podpora API pro Python a Swift s optimalizacemi specifickÃ½mi pro agenty, automatickÃ¡ diferenciace pro uÄenÃ­ agentÅ¯ a bezproblÃ©movÃ¡ integrace s vÃ½vojovÃ½mi nÃ¡stroji Apple poskytujÃ­ komplexnÃ­ prostÅ™edÃ­ pro vÃ½voj agentÅ¯.

#### ONNX Runtime pro agenty SLM napÅ™Ã­Ä platformami

ONNX Runtime poskytuje univerzÃ¡lnÃ­ inference engine, kterÃ½ umoÅ¾Åˆuje agentÅ¯m SLM bÄ›Å¾et konzistentnÄ› na rÅ¯znÃ½ch hardwarovÃ½ch platformÃ¡ch a operaÄnÃ­ch systÃ©mech:

**UniverzÃ¡lnÃ­ nasazenÃ­**: ONNX Runtime zajiÅ¡Å¥uje konzistentnÃ­ chovÃ¡nÃ­ agentÅ¯ SLM napÅ™Ã­Ä platformami Windows, Linux, macOS, iOS a Android. Tato kompatibilita napÅ™Ã­Ä platformami umoÅ¾Åˆuje vÃ½vojÃ¡Å™Å¯m psÃ¡t jednou a nasadit vÅ¡ude, coÅ¾ vÃ½raznÄ› sniÅ¾uje nÃ¡klady na vÃ½voj a ÃºdrÅ¾bu pro aplikace na vÃ­ce platformÃ¡ch.

**MoÅ¾nosti hardwarovÃ© akcelerace**: RÃ¡mec poskytuje optimalizovanÃ© poskytovatele exekuce pro rÅ¯znÃ© hardwarovÃ© konfigurace vÄetnÄ› CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) a specializovanÃ½ch akcelerÃ¡torÅ¯ (Intel VPU, Qualcomm NPU). Agenti SLM mohou automaticky vyuÅ¾Ã­vat nejlepÅ¡Ã­ dostupnÃ½ hardware bez zmÄ›n kÃ³du.

**Funkce pÅ™ipravenÃ© pro produkci**: ONNX Runtime nabÃ­zÃ­ funkce na podnikovÃ© Ãºrovni nezbytnÃ© pro nasazenÃ­ agentÅ¯ v produkci, vÄetnÄ› optimalizace grafÅ¯ pro rychlejÅ¡Ã­ inference, sprÃ¡vy pamÄ›ti pro prostÅ™edÃ­ s omezenÃ½mi zdroji a komplexnÃ­ch nÃ¡strojÅ¯ pro profilovÃ¡nÃ­ vÃ½konu. RÃ¡mec podporuje API pro Python i C++ pro flexibilnÃ­ integraci.

## SLM vs LLM v agentnÃ­ch systÃ©mech: PokroÄilÃ© srovnÃ¡nÃ­

### VÃ½hody SLM v agentnÃ­ch aplikacÃ­ch

**OperaÄnÃ­ efektivita**: SLM poskytujÃ­ 10-30Ã— snÃ­Å¾enÃ­ nÃ¡kladÅ¯ ve srovnÃ¡nÃ­ s LLM pro agentnÃ­ Ãºkoly, umoÅ¾ÅˆujÃ­ agentnÃ­ odpovÄ›di v reÃ¡lnÃ©m Äase ve velkÃ©m mÄ›Å™Ã­tku. NabÃ­zejÃ­ rychlejÅ¡Ã­ Äasy inference dÃ­ky snÃ­Å¾enÃ© vÃ½poÄetnÃ­ sloÅ¾itosti, coÅ¾ je ÄinÃ­ ideÃ¡lnÃ­mi pro interaktivnÃ­ agentnÃ­ aplikace.

**Schopnosti nasazenÃ­ na edge**: SLM umoÅ¾ÅˆujÃ­ provÃ¡dÄ›nÃ­ agentÅ¯ na zaÅ™Ã­zenÃ­ bez zÃ¡vislosti na internetu, zvÃ½Å¡enÃ© soukromÃ­ prostÅ™ednictvÃ­m lokÃ¡lnÃ­ho zpracovÃ¡nÃ­ a pÅ™izpÅ¯sobenÃ­ pro aplikace specifickÃ© pro danou oblast vhodnÃ© pro rÅ¯znÃ¡ prostÅ™edÃ­ edge computingu.

**Optimalizace specifickÃ¡ pro agenty**: SLM vynikajÃ­ pÅ™i volÃ¡nÃ­ nÃ¡strojÅ¯, generovÃ¡nÃ­ strukturovanÃ½ch vÃ½stupÅ¯ a rutinnÃ­ch pracovnÃ­ch postupech rozhodovÃ¡nÃ­, kterÃ© tvoÅ™Ã­ 70-80% typickÃ½ch agentnÃ­ch ÃºkolÅ¯.

### Kdy pouÅ¾Ã­t SLM vs LLM v agentnÃ­ch systÃ©mech

**IdeÃ¡lnÃ­ pro SLM**:
- **OpakujÃ­cÃ­ se agentnÃ­ Ãºkoly**: ZadÃ¡vÃ¡nÃ­ dat, vyplÅˆovÃ¡nÃ­ formulÃ¡Å™Å¯, rutinnÃ­ API volÃ¡nÃ­
- **Integrace nÃ¡strojÅ¯**: Dotazy na databÃ¡ze, operace se soubory, interakce se systÃ©mem
- **StrukturovanÃ© pracovnÃ­ postupy**: NÃ¡sledovÃ¡nÃ­ pÅ™eddefinovanÃ½ch procesÅ¯ agentÅ¯
- **Agenti specifickÃ©ho oboru**: ZÃ¡kaznickÃ½ servis, plÃ¡novÃ¡nÃ­, zÃ¡kladnÃ­ analÃ½za
- **LokÃ¡lnÃ­ zpracovÃ¡nÃ­**: Operace agentÅ¯ citlivÃ© na soukromÃ­

**LepÅ¡Ã­ pro LLM**:
- **KomplexnÃ­ uvaÅ¾ovÃ¡nÃ­**: NovÃ© Å™eÅ¡enÃ­ problÃ©mÅ¯, strategickÃ© plÃ¡novÃ¡nÃ­
- **OtevÅ™enÃ© konverzace**: ObecnÃ½ chat, kreativnÃ­ diskuse
- **Ãškoly s Å¡irokÃ½mi znalostmi**: VÃ½zkum vyÅ¾adujÃ­cÃ­ rozsÃ¡hlÃ© obecnÃ© znalosti
- **NovÃ© situace**: Å˜eÅ¡enÃ­ zcela novÃ½ch scÃ©nÃ¡Å™Å¯ agentÅ¯

### HybridnÃ­ architektura agentÅ¯

OptimÃ¡lnÃ­ pÅ™Ã­stup kombinuje SLM a LLM v heterogennÃ­ch agentnÃ­ch systÃ©mech:

**ChytrÃ¡ orchestrace agentÅ¯**:
1. **SLM jako primÃ¡rnÃ­**: ZpracovÃ¡nÃ­ 70-80% rutinnÃ­ch agentnÃ­ch ÃºkolÅ¯ lokÃ¡lnÄ›
2. **LLM podle potÅ™eby**: SmÄ›rovÃ¡nÃ­ sloÅ¾itÃ½ch dotazÅ¯ na cloudovÃ© vÄ›tÅ¡Ã­ modely
3. **SpecializovanÃ© SLM**: RÅ¯znÃ© malÃ© modely pro rÅ¯znÃ© domÃ©ny agentÅ¯
4. **Optimalizace nÃ¡kladÅ¯**: Minimalizace drahÃ½ch volÃ¡nÃ­ LLM prostÅ™ednictvÃ­m inteligentnÃ­ho smÄ›rovÃ¡nÃ­

## Strategie nasazenÃ­ produkÄnÃ­ch SLM agentÅ¯

### Foundry Local: Edge AI runtime na podnikovÃ© Ãºrovni

Foundry Local (https://github.com/microsoft/foundry-local) slouÅ¾Ã­ jako vlajkovÃ© Å™eÅ¡enÃ­ Microsoftu pro nasazenÃ­ malÃ½ch jazykovÃ½ch modelÅ¯ v produkÄnÃ­ch edge prostÅ™edÃ­ch. Poskytuje kompletnÃ­ runtime prostÅ™edÃ­ speciÃ¡lnÄ› navrÅ¾enÃ© pro agenty pohÃ¡nÄ›nÃ© SLM s funkcemi na podnikovÃ© Ãºrovni a bezproblÃ©movÃ½mi integraÄnÃ­mi schopnostmi.

**ZÃ¡kladnÃ­ architektura a funkce**:
- **KompatibilnÃ­ API s OpenAI**: PlnÃ¡ kompatibilita s OpenAI SDK a integracemi Agent Framework
- **AutomatickÃ¡ optimalizace hardwaru**: InteligentnÃ­ vÃ½bÄ›r variant modelÅ¯ na zÃ¡kladÄ› dostupnÃ©ho hardwaru (CUDA GPU, Qualcomm NPU, CPU)
- **SprÃ¡va modelÅ¯**: AutomatickÃ© stahovÃ¡nÃ­, uklÃ¡dÃ¡nÃ­ do mezipamÄ›ti a sprÃ¡va Å¾ivotnÃ­ho cyklu modelÅ¯ SLM
- **ObjevovÃ¡nÃ­ sluÅ¾eb**: Detekce sluÅ¾eb bez konfigurace pro agentnÃ­ rÃ¡mce
- **Optimalizace zdrojÅ¯**: InteligentnÃ­ sprÃ¡va pamÄ›ti a energetickÃ¡ ÃºÄinnost pro nasazenÃ­ na edge

#### Instalace a nastavenÃ­

**Instalace napÅ™Ã­Ä platformami**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**RychlÃ½ start pro vÃ½voj agentÅ¯**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Integrace Agent Framework

**Integrace Foundry Local SDK**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**AutomatickÃ½ vÃ½bÄ›r modelÅ¯ a optimalizace hardwaru**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### ProdukÄnÃ­ vzory nasazenÃ­

**ProdukÄnÃ­ nastavenÃ­ jednoho agenta**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Orchestrace produkce vÃ­ce agentÅ¯**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### Funkce na podnikovÃ© Ãºrovni a monitorovÃ¡nÃ­

**MonitorovÃ¡nÃ­ zdravÃ­ a pozorovatelnost**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**SprÃ¡va zdrojÅ¯ a automatickÃ© Å¡kÃ¡lovÃ¡nÃ­**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### PokroÄilÃ¡ konfigurace a optimalizace

**VlastnÃ­ konfigurace modelu**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**KontrolnÃ­ seznam pro produkÄnÃ­ nasazenÃ­**:

âœ… **Konfigurace sluÅ¾by**:
- Nakonfigurujte vhodnÃ© aliasy modelÅ¯ pro pÅ™Ã­pady pouÅ¾itÃ­
- Nastavte limity zdrojÅ¯ a prahovÃ© hodnoty monitorovÃ¡nÃ­
- Aktivujte kontroly zdravÃ­ a sbÄ›r metrik
- Nakonfigurujte automatickÃ© restartovÃ¡nÃ­ a zÃ¡loÅ¾nÃ­ reÅ¾im

âœ… **NastavenÃ­ zabezpeÄenÃ­**:
- Aktivujte pÅ™Ã­stup k API pouze lokÃ¡lnÄ› (bez externÃ­ho pÅ™Ã­stupu)
- Nakonfigurujte sprÃ¡vu klÃ­ÄÅ¯ API
- Nastavte auditnÃ­ logovÃ¡nÃ­ interakcÃ­ agentÅ¯
- Implementujte omezenÃ­ rychlosti pro produkÄnÃ­ pouÅ¾itÃ­

âœ… **Optimalizace vÃ½konu**:
- Testujte vÃ½kon modelu pÅ™i oÄekÃ¡vanÃ© zÃ¡tÄ›Å¾i
- Nakonfigurujte vhodnÃ© ÃºrovnÄ› kvantizace
- Nastavte strategie uklÃ¡dÃ¡nÃ­ modelÅ¯ do mezipamÄ›ti a zahÅ™Ã­vÃ¡nÃ­
- Monitorujte vzory vyuÅ¾itÃ­ pamÄ›ti a CPU

âœ… **TestovÃ¡nÃ­ integrace**:
- Testujte integraci agentnÃ­ho rÃ¡mce
- OvÄ›Å™te schopnosti offline provozu
- Testujte scÃ©nÃ¡Å™e zÃ¡loÅ¾nÃ­ho reÅ¾imu a obnovy
- Validujte pracovnÃ­ postupy agentÅ¯ od zaÄÃ¡tku do konce

### Ollama: ZjednoduÅ¡enÃ© nasazenÃ­ agentÅ¯ SLM

### Ollama: NasazenÃ­ agentÅ¯ SLM zamÄ›Å™enÃ© na komunitu

Ollama poskytuje pÅ™Ã­stup zamÄ›Å™enÃ½ na komunitu pro nasazenÃ­ agentÅ¯ SLM s dÅ¯razem na jednoduchost, rozsÃ¡hlÃ½ ekosystÃ©m modelÅ¯ a uÅ¾ivatelsky pÅ™Ã­vÄ›tivÃ© pracovnÃ­ postupy. ZatÃ­mco Foundry Local se zamÄ›Å™uje na funkce na podnikovÃ© Ãºrovni, Ollama vynikÃ¡ v rychlÃ©m prototypovÃ¡nÃ­, pÅ™Ã­stupu k modelÅ¯m od komunity a zjednoduÅ¡enÃ½ch scÃ©nÃ¡Å™Ã­ch nasazenÃ­.

**ZÃ¡kladnÃ­ architektura a funkce**:
- **KompatibilnÃ­ API s OpenAI**: PlnÃ¡ kompatibilita REST API pro bezproblÃ©movou integraci agentnÃ­ho rÃ¡mce
- **RozsÃ¡hlÃ¡ knihovna modelÅ¯**: PÅ™Ã­stup ke stovkÃ¡m modelÅ¯ od komunity i oficiÃ¡lnÃ­ch modelÅ¯
- **
- TestovÃ¡nÃ­ integrace Microsoft Agent Framework
- OvÄ›Å™enÃ­ schopnostÃ­ offline provozu
- TestovÃ¡nÃ­ scÃ©nÃ¡Å™Å¯ selhÃ¡nÃ­ a zpracovÃ¡nÃ­ chyb
- Validace kompletnÃ­ch pracovnÃ­ch postupÅ¯ agentÅ¯

**SrovnÃ¡nÃ­ s Foundry Local**:

| Funkce | Foundry Local | Ollama |
|--------|---------------|--------|
| **CÃ­lovÃ© pouÅ¾itÃ­** | PodnikovÃ¡ produkce | VÃ½voj a komunita |
| **EkosystÃ©m modelÅ¯** | KurÃ¡torovÃ¡no Microsoftem | RozsÃ¡hlÃ¡ komunita |
| **Optimalizace hardwaru** | AutomatickÃ¡ (CUDA/NPU/CPU) | ManuÃ¡lnÃ­ konfigurace |
| **PodnikovÃ© funkce** | VestavÄ›nÃ© monitorovÃ¡nÃ­, bezpeÄnost | NÃ¡stroje komunity |
| **SloÅ¾itost nasazenÃ­** | JednoduchÃ© (winget install) | JednoduchÃ© (curl install) |
| **Kompatibilita API** | OpenAI + rozÅ¡Ã­Å™enÃ­ | Standard OpenAI |
| **Podpora** | OficiÃ¡lnÃ­ podpora Microsoftu | Å˜Ã­zeno komunitou |
| **NejlepÅ¡Ã­ pro** | ProdukÄnÃ­ agenti | PrototypovÃ¡nÃ­, vÃ½zkum |

**Kdy zvolit Ollama**:
- **VÃ½voj a prototypovÃ¡nÃ­**: RychlÃ© experimentovÃ¡nÃ­ s rÅ¯znÃ½mi modely
- **KomunitnÃ­ modely**: PÅ™Ã­stup k nejnovÄ›jÅ¡Ã­m modelÅ¯m od komunity
- **VzdÄ›lÃ¡vacÃ­ vyuÅ¾itÃ­**: UÄenÃ­ a vÃ½uka vÃ½voje AI agentÅ¯
- **VÃ½zkumnÃ© projekty**: AkademickÃ½ vÃ½zkum vyÅ¾adujÃ­cÃ­ pÅ™Ã­stup k rÅ¯znÃ½m modelÅ¯m
- **VlastnÃ­ modely**: VytvÃ¡Å™enÃ­ a testovÃ¡nÃ­ vlastnÃ­ch modelÅ¯ s jemnÃ½m doladÄ›nÃ­m

### VLLM: Vysoce vÃ½konnÃ¡ inference SLM agentÅ¯

VLLM (Inference velmi velkÃ½ch jazykovÃ½ch modelÅ¯) poskytuje vysoce vÃ½konnÃ½, pamÄ›Å¥ovÄ› efektivnÃ­ inference engine, optimalizovanÃ½ pro produkÄnÃ­ nasazenÃ­ SLM ve velkÃ©m mÄ›Å™Ã­tku. ZatÃ­mco Foundry Local se zamÄ›Å™uje na snadnÃ© pouÅ¾itÃ­ a Ollama na komunitnÃ­ modely, VLLM vynikÃ¡ v scÃ©nÃ¡Å™Ã­ch vyÅ¾adujÃ­cÃ­ch maximÃ¡lnÃ­ propustnost a efektivnÃ­ vyuÅ¾itÃ­ zdrojÅ¯.

**ZÃ¡kladnÃ­ architektura a funkce**:
- **PagedAttention**: RevoluÄnÃ­ sprÃ¡va pamÄ›ti pro efektivnÃ­ vÃ½poÄet pozornosti
- **DynamickÃ© dÃ¡vkovÃ¡nÃ­**: InteligentnÃ­ dÃ¡vkovÃ¡nÃ­ poÅ¾adavkÅ¯ pro optimÃ¡lnÃ­ propustnost
- **Optimalizace GPU**: PokroÄilÃ© CUDA jÃ¡dra a podpora paralelismu tensorÅ¯
- **Kompatibilita s OpenAI**: PlnÃ¡ kompatibilita API pro bezproblÃ©movou integraci
- **SpekulativnÃ­ dekÃ³dovÃ¡nÃ­**: PokroÄilÃ© techniky zrychlenÃ­ inference
- **Podpora kvantizace**: Kvantizace INT4, INT8 a FP16 pro efektivnÃ­ vyuÅ¾itÃ­ pamÄ›ti

#### Instalace a nastavenÃ­

**MoÅ¾nosti instalace**:
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

**RychlÃ½ start pro vÃ½voj agentÅ¯**:
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```

#### Integrace Agent Framework

**VLLM s Microsoft Agent Framework**:
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```

**NastavenÃ­ vÃ­ce agentÅ¯ s vysokou propustnostÃ­**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```

#### ProdukÄnÃ­ vzory nasazenÃ­

**PodnikovÃ¡ sluÅ¾ba VLLM**:
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```

#### PodnikovÃ© funkce a monitorovÃ¡nÃ­

**PokroÄilÃ© monitorovÃ¡nÃ­ vÃ½konu VLLM**:
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```

#### PokroÄilÃ¡ konfigurace a optimalizace

**Å ablony konfigurace produkÄnÃ­ho VLLM**:
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```

**KontrolnÃ­ seznam pro produkÄnÃ­ nasazenÃ­ VLLM**:

âœ… **Optimalizace hardwaru**:
- Konfigurace paralelismu tensorÅ¯ pro nastavenÃ­ vÃ­ce GPU
- Aktivace kvantizace (AWQ/GPTQ) pro efektivnÃ­ vyuÅ¾itÃ­ pamÄ›ti
- NastavenÃ­ optimÃ¡lnÃ­ho vyuÅ¾itÃ­ pamÄ›ti GPU (85-95 %)
- Konfigurace vhodnÃ½ch velikostÃ­ dÃ¡vky pro propustnost

âœ… **LadÄ›nÃ­ vÃ½konu**:
- Aktivace prefixovÃ©ho uklÃ¡dÃ¡nÃ­ do mezipamÄ›ti pro opakovanÃ© dotazy
- Konfigurace segmentovanÃ©ho pÅ™edvyplnÄ›nÃ­ pro dlouhÃ© sekvence
- NastavenÃ­ spekulativnÃ­ho dekÃ³dovÃ¡nÃ­ pro rychlejÅ¡Ã­ inference
- Optimalizace max_num_seqs na zÃ¡kladÄ› hardwaru

âœ… **ProdukÄnÃ­ funkce**:
- NastavenÃ­ monitorovÃ¡nÃ­ stavu a sbÄ›ru metrik
- Konfigurace automatickÃ©ho restartu a pÅ™epnutÃ­ pÅ™i selhÃ¡nÃ­
- Implementace front poÅ¾adavkÅ¯ a vyvaÅ¾ovÃ¡nÃ­ zÃ¡tÄ›Å¾e
- NastavenÃ­ komplexnÃ­ho logovÃ¡nÃ­ a upozornÄ›nÃ­

âœ… **BezpeÄnost a spolehlivost**:
- Konfigurace pravidel firewallu a kontrol pÅ™Ã­stupu
- NastavenÃ­ omezenÃ­ rychlosti API a autentizace
- Implementace plynulÃ©ho vypnutÃ­ a vyÄiÅ¡tÄ›nÃ­
- Konfigurace zÃ¡lohovÃ¡nÃ­ a obnovy po havÃ¡rii

âœ… **TestovÃ¡nÃ­ integrace**:
- TestovÃ¡nÃ­ integrace Microsoft Agent Framework
- Validace scÃ©nÃ¡Å™Å¯ s vysokou propustnostÃ­
- TestovÃ¡nÃ­ postupÅ¯ pÅ™i selhÃ¡nÃ­ a obnovÄ›
- Benchmarking vÃ½konu pÅ™i zÃ¡tÄ›Å¾i

**SrovnÃ¡nÃ­ s jinÃ½mi Å™eÅ¡enÃ­mi**:

| Funkce | VLLM | Foundry Local | Ollama |
|--------|------|---------------|--------|
| **CÃ­lovÃ© pouÅ¾itÃ­** | Produkce s vysokou propustnostÃ­ | SnadnÃ© pouÅ¾itÃ­ v podniku | VÃ½voj a komunita |
| **VÃ½kon** | MaximÃ¡lnÃ­ propustnost | VyvÃ¡Å¾enÃ½ | DobrÃ½ |
| **Efektivita pamÄ›ti** | Optimalizace PagedAttention | AutomatickÃ¡ optimalizace | StandardnÃ­ |
| **SloÅ¾itost nastavenÃ­** | VysokÃ¡ (mnoho parametrÅ¯) | NÃ­zkÃ¡ (automatickÃ¡) | NÃ­zkÃ¡ (jednoduchÃ¡) |
| **Å kÃ¡lovatelnost** | VÃ½bornÃ¡ (tensor/pipeline paralelismus) | DobrÃ¡ | OmezenÃ¡ |
| **Kvantizace** | PokroÄilÃ¡ (AWQ, GPTQ, FP8) | AutomatickÃ¡ | StandardnÃ­ GGUF |
| **PodnikovÃ© funkce** | NutnÃ¡ vlastnÃ­ implementace | VestavÄ›nÃ© | NÃ¡stroje komunity |
| **NejlepÅ¡Ã­ pro** | ProdukÄnÃ­ agenti ve velkÃ©m mÄ›Å™Ã­tku | PodnikovÃ¡ produkce | VÃ½voj |

**Kdy zvolit VLLM**:
- **PoÅ¾adavky na vysokou propustnost**: ZpracovÃ¡nÃ­ stovek poÅ¾adavkÅ¯ za sekundu
- **NasazenÃ­ ve velkÃ©m mÄ›Å™Ã­tku**: Multi-GPU, multi-node nasazenÃ­
- **KritickÃ½ vÃ½kon**: Odezvy pod sekundu ve velkÃ©m mÄ›Å™Ã­tku
- **PokroÄilÃ¡ optimalizace**: PotÅ™eba vlastnÃ­ kvantizace a dÃ¡vkovÃ¡nÃ­
- **Efektivita zdrojÅ¯**: MaximÃ¡lnÃ­ vyuÅ¾itÃ­ drahÃ©ho GPU hardwaru

## ReÃ¡lnÃ© aplikace SLM agentÅ¯

### SLM agenti pro zÃ¡kaznickÃ½ servis
- **Schopnosti SLM**: VyhledÃ¡vÃ¡nÃ­ ÃºÄtÅ¯, resetovÃ¡nÃ­ hesel, kontrola stavu objednÃ¡vek
- **Ãšspory nÃ¡kladÅ¯**: 10x snÃ­Å¾enÃ­ nÃ¡kladÅ¯ na inference ve srovnÃ¡nÃ­ s LLM agenty
- **VÃ½kon**: RychlejÅ¡Ã­ odezvy s konzistentnÃ­ kvalitou pro rutinnÃ­ dotazy

### SLM agenti pro obchodnÃ­ procesy
- **Agenti pro zpracovÃ¡nÃ­ faktur**: Extrakce dat, validace informacÃ­, smÄ›rovÃ¡nÃ­ k schvÃ¡lenÃ­
- **Agenti pro sprÃ¡vu e-mailÅ¯**: Kategorizace, prioritizace, automatickÃ© nÃ¡vrhy odpovÄ›dÃ­
- **Agenti pro plÃ¡novÃ¡nÃ­**: Koordinace schÅ¯zek, sprÃ¡va kalendÃ¡Å™Å¯, zasÃ­lÃ¡nÃ­ pÅ™ipomÃ­nek

### OsobnÃ­ digitÃ¡lnÃ­ asistenti SLM
- **Agenti pro sprÃ¡vu ÃºkolÅ¯**: VytvÃ¡Å™enÃ­, aktualizace, organizace seznamÅ¯ ÃºkolÅ¯
- **Agenti pro sbÄ›r informacÃ­**: VÃ½zkum tÃ©mat, lokÃ¡lnÃ­ shrnutÃ­ zjiÅ¡tÄ›nÃ­
- **Agenti pro komunikaci**: NÃ¡vrhy e-mailÅ¯, zprÃ¡v, pÅ™Ã­spÄ›vkÅ¯ na sociÃ¡lnÃ­ sÃ­tÄ›

### SLM agenti pro obchodovÃ¡nÃ­ a finance
- **Agenti pro sledovÃ¡nÃ­ trhu**: SledovÃ¡nÃ­ cen, identifikace trendÅ¯ v reÃ¡lnÃ©m Äase
- **Agenti pro generovÃ¡nÃ­ zprÃ¡v**: AutomatickÃ© vytvÃ¡Å™enÃ­ dennÃ­ch/tÃ½dennÃ­ch pÅ™ehledÅ¯
- **Agenti pro hodnocenÃ­ rizik**: PosouzenÃ­ portfoliovÃ½ch pozic pomocÃ­ lokÃ¡lnÃ­ch dat

### SLM agenti pro podporu zdravotnictvÃ­
- **Agenti pro plÃ¡novÃ¡nÃ­ pacientÅ¯**: Koordinace schÅ¯zek, automatickÃ© pÅ™ipomÃ­nky
- **Agenti pro dokumentaci**: LokÃ¡lnÃ­ generovÃ¡nÃ­ lÃ©kaÅ™skÃ½ch souhrnÅ¯, zprÃ¡v
- **Agenti pro sprÃ¡vu receptÅ¯**: SledovÃ¡nÃ­ doplnÄ›nÃ­, kontrola interakcÃ­

## Microsoft Agent Framework: VÃ½voj produkÄnÃ­ch agentÅ¯

### PÅ™ehled a architektura

Microsoft Agent Framework poskytuje komplexnÃ­, podnikovÃ© Å™eÅ¡enÃ­ pro vytvÃ¡Å™enÃ­, nasazenÃ­ a sprÃ¡vu AI agentÅ¯, kteÅ™Ã­ mohou fungovat jak v cloudu, tak v offline prostÅ™edÃ­ na okraji sÃ­tÄ›. Framework je navrÅ¾en tak, aby bezproblÃ©movÄ› pracoval s malÃ½mi jazykovÃ½mi modely a scÃ©nÃ¡Å™i edge computingu, coÅ¾ ho ÄinÃ­ ideÃ¡lnÃ­m pro nasazenÃ­ citlivÃ¡ na soukromÃ­ a omezenÃ© zdroje.

**ZÃ¡kladnÃ­ komponenty frameworku**:
- **Agent Runtime**: LehkÃ½ vÃ½konnÃ½ prostÅ™edÃ­ optimalizovanÃ© pro edge zaÅ™Ã­zenÃ­
- **SystÃ©m integrace nÃ¡strojÅ¯**: RozÅ¡iÅ™itelnÃ¡ architektura pluginÅ¯ pro pÅ™ipojenÃ­ externÃ­ch sluÅ¾eb a API
- **SprÃ¡va stavu**: TrvalÃ¡ pamÄ›Å¥ agenta a sprÃ¡va kontextu mezi relacemi
- **BezpeÄnostnÃ­ vrstva**: VestavÄ›nÃ© bezpeÄnostnÃ­ kontroly pro podnikovÃ© nasazenÃ­
- **OrchestraÄnÃ­ engine**: Koordinace vÃ­ce agentÅ¯ a sprÃ¡va pracovnÃ­ch postupÅ¯

### KlÃ­ÄovÃ© funkce pro nasazenÃ­ na okraji

**Offline-First Architektura**: Microsoft Agent Framework je navrÅ¾en s principy offline-first, coÅ¾ umoÅ¾Åˆuje agentÅ¯m efektivnÄ› fungovat bez neustÃ¡lÃ©ho pÅ™ipojenÃ­ k internetu. To zahrnuje lokÃ¡lnÃ­ inference modelÅ¯, uloÅ¾enÃ© znalostnÃ­ bÃ¡ze, offline provÃ¡dÄ›nÃ­ nÃ¡strojÅ¯ a plynulÃ© degradace pÅ™i nedostupnosti cloudovÃ½ch sluÅ¾eb.

**Optimalizace zdrojÅ¯**: Framework poskytuje inteligentnÃ­ sprÃ¡vu zdrojÅ¯ s automatickou optimalizacÃ­ pamÄ›ti pro SLM, vyvaÅ¾ovÃ¡nÃ­ zÃ¡tÄ›Å¾e CPU/GPU pro edge zaÅ™Ã­zenÃ­, adaptivnÃ­ vÃ½bÄ›r modelÅ¯ na zÃ¡kladÄ› dostupnÃ½ch zdrojÅ¯ a energeticky efektivnÃ­ vzory inference pro mobilnÃ­ nasazenÃ­.

**BezpeÄnost a soukromÃ­**: PodnikovÃ© bezpeÄnostnÃ­ funkce zahrnujÃ­ lokÃ¡lnÃ­ zpracovÃ¡nÃ­ dat pro zachovÃ¡nÃ­ soukromÃ­, Å¡ifrovanÃ© komunikaÄnÃ­ kanÃ¡ly agentÅ¯, Å™Ã­zenÃ­ pÅ™Ã­stupu na zÃ¡kladÄ› rolÃ­ pro schopnosti agentÅ¯ a auditnÃ­ logovÃ¡nÃ­ pro poÅ¾adavky na shodu.

### Integrace s Foundry Local

Microsoft Agent Framework se bezproblÃ©movÄ› integruje s Foundry Local a poskytuje kompletnÃ­ Å™eÅ¡enÃ­ pro edge AI:

**AutomatickÃ© vyhledÃ¡vÃ¡nÃ­ modelÅ¯**: Framework automaticky detekuje a pÅ™ipojuje se k instancÃ­m Foundry Local, vyhledÃ¡vÃ¡ dostupnÃ© SLM modely a vybÃ­rÃ¡ optimÃ¡lnÃ­ modely na zÃ¡kladÄ› poÅ¾adavkÅ¯ agentÅ¯ a schopnostÃ­ hardwaru.

**DynamickÃ© naÄÃ­tÃ¡nÃ­ modelÅ¯**: Agenti mohou dynamicky naÄÃ­tat rÅ¯znÃ© SLM modely pro specifickÃ© Ãºkoly, coÅ¾ umoÅ¾Åˆuje systÃ©my vÃ­ce modelÅ¯, kde rÅ¯znÃ© modely zpracovÃ¡vajÃ­ rÅ¯znÃ© typy poÅ¾adavkÅ¯, a automatickÃ© pÅ™epnutÃ­ mezi modely na zÃ¡kladÄ› dostupnosti a vÃ½konu.

**Optimalizace vÃ½konu**: IntegrovanÃ© mechanismy uklÃ¡dÃ¡nÃ­ do mezipamÄ›ti sniÅ¾ujÃ­ Äasy naÄÃ­tÃ¡nÃ­ modelÅ¯, pooling pÅ™ipojenÃ­ optimalizuje API volÃ¡nÃ­ na Foundry Local a inteligentnÃ­ dÃ¡vkovÃ¡nÃ­ zlepÅ¡uje propustnost pro vÃ­ce poÅ¾adavkÅ¯ agentÅ¯.

### VytvÃ¡Å™enÃ­ agentÅ¯ s Microsoft Agent Framework

#### Definice a konfigurace agenta

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```

#### Integrace nÃ¡strojÅ¯ pro scÃ©nÃ¡Å™e na okraji

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```

#### Orchestrace vÃ­ce agentÅ¯

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```

### PokroÄilÃ© vzory nasazenÃ­ na okraji

#### HierarchickÃ¡ architektura agentÅ¯

**LokÃ¡lnÃ­ klastery agentÅ¯**: NasazenÃ­ vÃ­ce specializovanÃ½ch SLM agentÅ¯ na edge zaÅ™Ã­zenÃ­, kaÅ¾dÃ½ optimalizovanÃ½ pro specifickÃ© Ãºkoly. PouÅ¾itÃ­ lehkÃ½ch modelÅ¯ jako Qwen2.5-0.5B pro jednoduchÃ© smÄ›rovÃ¡nÃ­ a plÃ¡novÃ¡nÃ­, stÅ™ednÃ­ch modelÅ¯ jako Phi-4-Mini pro zÃ¡kaznickÃ½ servis a dokumentaci, a vÄ›tÅ¡Ã­ch modelÅ¯ pro sloÅ¾itÃ© uvaÅ¾ovÃ¡nÃ­, pokud to zdroje umoÅ¾ÅˆujÃ­.

**Koordinace edge-to-cloud**: Implementace inteligentnÃ­ch eskalaÄnÃ­ch vzorÅ¯, kde lokÃ¡lnÃ­ agenti zpracovÃ¡vajÃ­ rutinnÃ­ Ãºkoly, cloudovÃ­ agenti poskytujÃ­ sloÅ¾itÃ© uvaÅ¾ovÃ¡nÃ­, pokud to pÅ™ipojenÃ­ umoÅ¾Åˆuje, a plynulÃ© pÅ™edÃ¡vÃ¡nÃ­ mezi edge a cloudovÃ½m zpracovÃ¡nÃ­m zachovÃ¡vÃ¡ kontinuitu.

#### Konfigurace nasazenÃ­

**NasazenÃ­ na jednom zaÅ™Ã­zenÃ­**:
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```

**DistribuovanÃ© nasazenÃ­ na okraji**:
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```

### Optimalizace vÃ½konu pro agenty na okraji

#### Strategie vÃ½bÄ›ru modelÅ¯

**PÅ™iÅ™azenÃ­ modelÅ¯ na zÃ¡kladÄ› ÃºkolÅ¯**: Microsoft Agent Framework umoÅ¾Åˆuje inteligentnÃ­ vÃ½bÄ›r modelÅ¯ na zÃ¡kladÄ› sloÅ¾itosti Ãºkolu a poÅ¾adavkÅ¯:

- **JednoduchÃ© Ãºkoly** (Q&A, smÄ›rovÃ¡nÃ­): Qwen2.5-0.5B (500MB, <100ms odezva)
- **StÅ™ednÄ› sloÅ¾itÃ© Ãºkoly** (zÃ¡kaznickÃ½ servis, plÃ¡novÃ¡nÃ­): Phi-4-Mini (2.4GB, 200-500ms odezva)
- **SloÅ¾itÃ© Ãºkoly** (technickÃ¡ analÃ½za, plÃ¡novÃ¡nÃ­): Phi-4 (7GB, 1-3s odezva, pokud to zdroje umoÅ¾ÅˆujÃ­)

**DynamickÃ© pÅ™epÃ­nÃ¡nÃ­ modelÅ¯**: Agenti mohou pÅ™epÃ­nat mezi modely na zÃ¡kladÄ› aktuÃ¡lnÃ­ho zatÃ­Å¾enÃ­ systÃ©mu, hodnocenÃ­ sloÅ¾itosti Ãºkolu, priorit uÅ¾ivatele a dostupnÃ½ch hardwarovÃ½ch zdrojÅ¯.

#### SprÃ¡va pamÄ›ti a zdrojÅ¯

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

### PodnikovÃ© vzory integrace

#### BezpeÄnost a shoda

**LokÃ¡lnÃ­ zpracovÃ¡nÃ­ dat**: VeÅ¡kerÃ© zpracovÃ¡nÃ­ agentÅ¯ probÃ­hÃ¡ lokÃ¡lnÄ›, coÅ¾ zajiÅ¡Å¥uje, Å¾e citlivÃ¡ data nikdy neopustÃ­ edge zaÅ™Ã­zenÃ­. To zahrnuje ochranu informacÃ­ o zÃ¡kaznÃ­cÃ­ch, shodu s HIPAA pro zdravotnickÃ© agenty, bezpeÄnost finanÄnÃ­ch dat pro bankovnÃ­ agenty a shodu s GDPR pro evropskÃ¡ nasazenÃ­.

**Kontrola pÅ™Ã­stupu**: OprÃ¡vnÄ›nÃ­ na zÃ¡kladÄ› rolÃ­ kontrolujÃ­, ke kterÃ½m nÃ¡strojÅ¯m majÃ­ agenti pÅ™Ã­stup, autentizace uÅ¾ivatelÅ¯ pro interakce s agenty a auditnÃ­ stopy pro vÅ¡echny akce a rozhodnutÃ­ agentÅ¯.

#### MonitorovÃ¡nÃ­ a pozorovatelnost

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```

### ReÃ¡lnÃ© pÅ™Ã­klady implementace

#### SystÃ©m agentÅ¯ pro maloobchod na okraji

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```

#### Agent pro podporu zdravotnictvÃ­

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```

### NejlepÅ¡Ã­ postupy pro Microsoft Agent Framework

#### Pokyny pro vÃ½voj

1. **ZaÄnÄ›te jednoduÅ¡e**: ZaÄnÄ›te scÃ©nÃ¡Å™i s jednÃ­m agentem pÅ™ed budovÃ¡nÃ­m komplexnÃ­ch systÃ©mÅ¯ vÃ­ce agentÅ¯
2. **SprÃ¡vnÃ¡ velikost modelu**: Vyberte nejmenÅ¡Ã­ model, kterÃ½ splÅˆuje vaÅ¡e poÅ¾adavky na pÅ™esnost
3. **NÃ¡vrh nÃ¡strojÅ¯**: VytvÃ¡Å™ejte zamÄ›Å™enÃ©, jednoÃºÄelovÃ© nÃ¡stroje mÃ­sto sloÅ¾itÃ½ch multifunkÄnÃ­ch nÃ¡strojÅ¯
4. **ZpracovÃ¡nÃ­ chyb**: Implementujte plynulou degradaci pro offline scÃ©nÃ¡Å™e a selhÃ¡nÃ­ modelÅ¯
5. **TestovÃ¡nÃ­**: Testujte agenty dÅ¯kladnÄ› v offline podmÃ­nkÃ¡ch a prostÅ™edÃ­ch s omezenÃ½mi zdroji

#### NejlepÅ¡Ã­ postupy pro nasazenÃ­

1. **PostupnÃ© zavÃ¡dÄ›nÃ­**: Nasazujte nejprve malÃ½m skupinÃ¡m uÅ¾ivatelÅ¯, peÄlivÄ› sledujte metriky vÃ½konu
2. **MonitorovÃ¡nÃ­ zdrojÅ¯**: Nastavte upozornÄ›nÃ­ na limity pamÄ›ti, CPU a doby odezvy
3. **Strategie zÃ¡lohovÃ¡nÃ­**: VÅ¾dy mÄ›jte zÃ¡loÅ¾nÃ­ plÃ¡ny pro selhÃ¡nÃ­ modelÅ¯ nebo vyÄerpÃ¡nÃ­ zdrojÅ¯
4. **BezpeÄnost na prvnÃ­m mÃ­stÄ›**: Implementujte bezpeÄnostnÃ­ kontroly od zaÄÃ¡tku, ne jako dodateÄnÃ© opatÅ™enÃ­
5. **Dokumentace**: UdrÅ¾ujte jasnou dokumentaci schopnostÃ­ a omezenÃ­ agentÅ¯

### BudoucÃ­ plÃ¡ny a integrace
**VÃ½bÄ›r frameworku pro nasazenÃ­ agentÅ¯**: Vyberte optimalizaÄnÃ­ frameworky podle cÃ­lovÃ©ho hardwaru a poÅ¾adavkÅ¯ agentÅ¯. PouÅ¾ijte Llama.cpp pro nasazenÃ­ agentÅ¯ optimalizovanÃ½ch pro CPU, Apple MLX pro aplikace agentÅ¯ na Apple Silicon a ONNX pro kompatibilitu agentÅ¯ napÅ™Ã­Ä platformami.

## PraktickÃ¡ konverze SLM agentÅ¯ a pÅ™Ã­klady pouÅ¾itÃ­

### ScÃ©nÃ¡Å™e nasazenÃ­ agentÅ¯ v reÃ¡lnÃ©m svÄ›tÄ›

**MobilnÃ­ aplikace agentÅ¯**: FormÃ¡ty Q4_K vynikajÃ­ v aplikacÃ­ch pro chytrÃ© telefony dÃ­ky minimÃ¡lnÃ­mu vyuÅ¾itÃ­ pamÄ›ti, zatÃ­mco Q8_0 poskytuje vyvÃ¡Å¾enÃ½ vÃ½kon pro systÃ©my agentÅ¯ na tabletech. FormÃ¡ty Q5_K nabÃ­zejÃ­ Å¡piÄkovou kvalitu pro mobilnÃ­ produktivnÃ­ agenty.

**AgentovÃ© vÃ½poÄty na desktopu a na okraji sÃ­tÄ›**: Q5_K zajiÅ¡Å¥uje optimÃ¡lnÃ­ vÃ½kon pro aplikace agentÅ¯ na desktopu, Q8_0 poskytuje vysoce kvalitnÃ­ inferenci pro pracovnÃ­ stanice a Q4_K umoÅ¾Åˆuje efektivnÃ­ zpracovÃ¡nÃ­ na zaÅ™Ã­zenÃ­ch na okraji sÃ­tÄ›.

**VÃ½zkumnÃ­ a experimentÃ¡lnÃ­ agenti**: PokroÄilÃ© kvantizaÄnÃ­ formÃ¡ty umoÅ¾ÅˆujÃ­ zkoumÃ¡nÃ­ ultra-nÃ­zkÃ© pÅ™esnosti inferencÃ­ agentÅ¯ pro akademickÃ½ vÃ½zkum a aplikace proof-of-concept vyÅ¾adujÃ­cÃ­ extrÃ©mnÃ­ omezenÃ­ zdrojÅ¯.

### VÃ½konnostnÃ­ benchmarky SLM agentÅ¯

**Rychlost inferencÃ­ agentÅ¯**: Q4_K dosahuje nejrychlejÅ¡Ã­ch odezev agentÅ¯ na mobilnÃ­ch CPU, Q5_K poskytuje vyvÃ¡Å¾enÃ½ pomÄ›r rychlosti a kvality pro obecnÃ© aplikace agentÅ¯, Q8_0 nabÃ­zÃ­ Å¡piÄkovou kvalitu pro sloÅ¾itÃ© Ãºkoly agentÅ¯ a experimentÃ¡lnÃ­ formÃ¡ty zajiÅ¡Å¥ujÃ­ maximÃ¡lnÃ­ propustnost pro specializovanÃ½ hardware agentÅ¯.

**PoÅ¾adavky na pamÄ›Å¥ agentÅ¯**: ÃšrovnÄ› kvantizace pro agenty se pohybujÃ­ od Q2_K (mÃ©nÄ› neÅ¾ 500 MB pro malÃ© modely agentÅ¯) po Q8_0 (pÅ™ibliÅ¾nÄ› 50 % pÅ¯vodnÃ­ velikosti), pÅ™iÄemÅ¾ experimentÃ¡lnÃ­ konfigurace dosahujÃ­ maximÃ¡lnÃ­ komprese pro prostÅ™edÃ­ s omezenÃ½mi zdroji.

## VÃ½zvy a Ãºvahy pro SLM agenty

### Kompromisy vÃ½konu v systÃ©mech agentÅ¯

NasazenÃ­ SLM agentÅ¯ vyÅ¾aduje peÄlivÃ© zvÃ¡Å¾enÃ­ kompromisÅ¯ mezi velikostÃ­ modelu, rychlostÃ­ odezvy agentÅ¯ a kvalitou vÃ½stupu. ZatÃ­mco Q4_K nabÃ­zÃ­ vÃ½jimeÄnou rychlost a efektivitu pro mobilnÃ­ agenty, Q8_0 poskytuje Å¡piÄkovou kvalitu pro sloÅ¾itÃ© Ãºkoly agentÅ¯. Q5_K pÅ™edstavuje stÅ™ednÃ­ cestu vhodnou pro vÄ›tÅ¡inu obecnÃ½ch aplikacÃ­ agentÅ¯.

### Kompatibilita hardwaru pro SLM agenty

RÅ¯znÃ¡ zaÅ™Ã­zenÃ­ na okraji sÃ­tÄ› majÃ­ rÅ¯znÃ© schopnosti pro nasazenÃ­ SLM agentÅ¯. Q4_K bÄ›Å¾Ã­ efektivnÄ› na zÃ¡kladnÃ­ch procesorech pro jednoduchÃ© agenty, Q5_K vyÅ¾aduje stÅ™ednÃ­ vÃ½poÄetnÃ­ zdroje pro vyvÃ¡Å¾enÃ½ vÃ½kon agentÅ¯ a Q8_0 tÄ›Å¾Ã­ z vysoce vÃ½konnÃ©ho hardwaru pro pokroÄilÃ© schopnosti agentÅ¯.

### BezpeÄnost a ochrana soukromÃ­ v systÃ©mech SLM agentÅ¯

ZatÃ­mco SLM agenti umoÅ¾ÅˆujÃ­ lokÃ¡lnÃ­ zpracovÃ¡nÃ­ pro zvÃ½Å¡enou ochranu soukromÃ­, je nutnÃ© implementovat sprÃ¡vnÃ¡ bezpeÄnostnÃ­ opatÅ™enÃ­ k ochranÄ› modelÅ¯ agentÅ¯ a dat v prostÅ™edÃ­ch na okraji sÃ­tÄ›. To je obzvlÃ¡Å¡tÄ› dÅ¯leÅ¾itÃ© pÅ™i nasazenÃ­ vysoce pÅ™esnÃ½ch formÃ¡tÅ¯ agentÅ¯ v podnikovÃ½ch prostÅ™edÃ­ch nebo komprimovanÃ½ch formÃ¡tÅ¯ agentÅ¯ v aplikacÃ­ch, kterÃ© zpracovÃ¡vajÃ­ citlivÃ¡ data.

## BudoucÃ­ trendy ve vÃ½voji SLM agentÅ¯

ProstÅ™edÃ­ SLM agentÅ¯ se neustÃ¡le vyvÃ­jÃ­ dÃ­ky pokrokÅ¯m v kompresnÃ­ch technikÃ¡ch, metodÃ¡ch optimalizace a strategiÃ­ch nasazenÃ­ na okraji sÃ­tÄ›. BudoucÃ­ vÃ½voj zahrnuje efektivnÄ›jÅ¡Ã­ algoritmy kvantizace pro modely agentÅ¯, vylepÅ¡enÃ© metody komprese pro pracovnÃ­ postupy agentÅ¯ a lepÅ¡Ã­ integraci s hardwarovÃ½mi akcelerÃ¡tory na okraji sÃ­tÄ› pro zpracovÃ¡nÃ­ agentÅ¯.

**Predikce trhu pro SLM agenty**: Podle nedÃ¡vnÃ©ho vÃ½zkumu by automatizace pohÃ¡nÄ›nÃ¡ agenty mohla do roku 2027 eliminovat 40â€“60 % opakujÃ­cÃ­ch se kognitivnÃ­ch ÃºkolÅ¯ v podnikovÃ½ch pracovnÃ­ch postupech, pÅ™iÄemÅ¾ SLM budou vÃ©st tuto transformaci dÃ­ky svÃ© nÃ¡kladovÃ© efektivitÄ› a flexibilitÄ› nasazenÃ­.

**TechnologickÃ© trendy v SLM agentech**:
- **SpecializovanÃ­ SLM agenti**: Modely zamÄ›Å™enÃ© na konkrÃ©tnÃ­ Ãºkoly agentÅ¯ a odvÄ›tvÃ­
- **VÃ½poÄty agentÅ¯ na okraji sÃ­tÄ›**: VylepÅ¡enÃ© schopnosti agentÅ¯ na zaÅ™Ã­zenÃ­ s lepÅ¡Ã­ ochranou soukromÃ­ a snÃ­Å¾enou latencÃ­
- **Orchestrace agentÅ¯**: LepÅ¡Ã­ koordinace mezi vÃ­ce SLM agenty s dynamickÃ½m smÄ›rovÃ¡nÃ­m a vyvaÅ¾ovÃ¡nÃ­m zÃ¡tÄ›Å¾e
- **Demokratizace**: Flexibilita SLM umoÅ¾Åˆuje Å¡irÅ¡Ã­ zapojenÃ­ do vÃ½voje agentÅ¯ napÅ™Ã­Ä organizacemi

## ZaÄÃ­nÃ¡me s SLM agenty

### Krok 1: NastavenÃ­ prostÅ™edÃ­ Microsoft Agent Framework

**Instalace zÃ¡vislostÃ­**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inicializace Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Krok 2: VÃ½bÄ›r SLM pro aplikace agentÅ¯
OblÃ­benÃ© moÅ¾nosti pro Microsoft Agent Framework:
- **Microsoft Phi-4 Mini (3.8B)**: SkvÄ›lÃ½ pro obecnÃ© Ãºkoly agentÅ¯ s vyvÃ¡Å¾enÃ½m vÃ½konem
- **Qwen2.5-0.5B (0.5B)**: Ultra-efektivnÃ­ pro jednoduchÃ© agenty zamÄ›Å™enÃ© na smÄ›rovÃ¡nÃ­ a klasifikaci
- **Qwen2.5-Coder-0.5B (0.5B)**: SpecializovanÃ½ na Ãºkoly agentÅ¯ souvisejÃ­cÃ­ s kÃ³dem
- **Phi-4 (7B)**: PokroÄilÃ© uvaÅ¾ovÃ¡nÃ­ pro sloÅ¾itÃ© scÃ©nÃ¡Å™e na okraji sÃ­tÄ›, pokud to zdroje umoÅ¾ÅˆujÃ­

### Krok 3: VytvoÅ™enÃ­ prvnÃ­ho agenta s Microsoft Agent Framework

**ZÃ¡kladnÃ­ nastavenÃ­ agenta**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Krok 4: DefinovÃ¡nÃ­ rozsahu a poÅ¾adavkÅ¯ agenta
ZaÄnÄ›te s cÃ­lenÃ½mi, dobÅ™e definovanÃ½mi aplikacemi agentÅ¯ pomocÃ­ Microsoft Agent Framework:
- **Agenti pro jeden obor**: ZÃ¡kaznickÃ½ servis NEBO plÃ¡novÃ¡nÃ­ NEBO vÃ½zkum
- **JasnÃ© cÃ­le agenta**: SpecifickÃ©, mÄ›Å™itelnÃ© cÃ­le pro vÃ½kon agenta
- **OmezenÃ¡ integrace nÃ¡strojÅ¯**: MaximÃ¡lnÄ› 3â€“5 nÃ¡strojÅ¯ pro poÄÃ¡teÄnÃ­ nasazenÃ­ agenta
- **DefinovanÃ© hranice agenta**: JasnÃ© cesty eskalace pro sloÅ¾itÃ© scÃ©nÃ¡Å™e
- **Design zamÄ›Å™enÃ½ na okraj sÃ­tÄ›**: Prioritizace offline funkcionality a lokÃ¡lnÃ­ho zpracovÃ¡nÃ­

### Krok 5: Implementace nasazenÃ­ na okraji sÃ­tÄ› s Microsoft Agent Framework

**Konfigurace zdrojÅ¯**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**NasazenÃ­ bezpeÄnostnÃ­ch opatÅ™enÃ­ pro agenty na okraji sÃ­tÄ›**:
- **LokÃ¡lnÃ­ validace vstupÅ¯**: Kontrola poÅ¾adavkÅ¯ bez zÃ¡vislosti na cloudu
- **Offline filtrovÃ¡nÃ­ vÃ½stupÅ¯**: ZajiÅ¡tÄ›nÃ­ kvality odpovÄ›dÃ­ lokÃ¡lnÄ›
- **BezpeÄnostnÃ­ kontroly na okraji sÃ­tÄ›**: Implementace bezpeÄnostnÃ­ch opatÅ™enÃ­ bez nutnosti pÅ™ipojenÃ­ k internetu
- **LokÃ¡lnÃ­ monitorovÃ¡nÃ­**: SledovÃ¡nÃ­ vÃ½konu a oznaÄovÃ¡nÃ­ problÃ©mÅ¯ pomocÃ­ telemetrie na okraji sÃ­tÄ›

### Krok 6: MÄ›Å™enÃ­ a optimalizace vÃ½konu agentÅ¯ na okraji sÃ­tÄ›
- **MÃ­ra dokonÄenÃ­ ÃºkolÅ¯ agenta**: SledovÃ¡nÃ­ ÃºspÄ›Å¡nosti v offline scÃ©nÃ¡Å™Ã­ch
- **ÄŒasy odezvy agenta**: ZajiÅ¡tÄ›nÃ­ odezvy pod jednu sekundu pro nasazenÃ­ na okraji sÃ­tÄ›
- **VyuÅ¾itÃ­ zdrojÅ¯**: SledovÃ¡nÃ­ pamÄ›ti, CPU a spotÅ™eby baterie na zaÅ™Ã­zenÃ­ch na okraji sÃ­tÄ›
- **NÃ¡kladovÃ¡ efektivita**: PorovnÃ¡nÃ­ nÃ¡kladÅ¯ na nasazenÃ­ na okraji sÃ­tÄ› s alternativami zaloÅ¾enÃ½mi na cloudu
- **Spolehlivost offline**: MÄ›Å™enÃ­ vÃ½konu agenta bÄ›hem vÃ½padkÅ¯ sÃ­tÄ›

## KlÃ­ÄovÃ© poznatky pro implementaci SLM agentÅ¯

1. **SLM jsou dostateÄnÃ© pro agenty**: Pro vÄ›tÅ¡inu ÃºkolÅ¯ agentÅ¯ malÃ© modely fungujÃ­ stejnÄ› dobÅ™e jako velkÃ©, pÅ™iÄemÅ¾ nabÃ­zejÃ­ vÃ½znamnÃ© vÃ½hody
2. **NÃ¡kladovÃ¡ efektivita agentÅ¯**: 10â€“30x levnÄ›jÅ¡Ã­ provoz SLM agentÅ¯, coÅ¾ je ÄinÃ­ ekonomicky Å¾ivotaschopnÃ½mi pro Å¡irokÃ© nasazenÃ­
3. **Specializace funguje pro agenty**: JemnÄ› doladÄ›nÃ© SLM Äasto pÅ™ekonÃ¡vajÃ­ obecnÃ© LLM v konkrÃ©tnÃ­ch aplikacÃ­ch agentÅ¯
4. **HybridnÃ­ architektura agentÅ¯**: PouÅ¾Ã­vejte SLM pro rutinnÃ­ Ãºkoly agentÅ¯, LLM pro sloÅ¾itÃ© uvaÅ¾ovÃ¡nÃ­, kdyÅ¾ je to nutnÃ©
5. **Microsoft Agent Framework umoÅ¾Åˆuje produkÄnÃ­ nasazenÃ­**: Poskytuje nÃ¡stroje na podnikovÃ© Ãºrovni pro vytvÃ¡Å™enÃ­, nasazenÃ­ a sprÃ¡vu agentÅ¯ na okraji sÃ­tÄ›
6. **Principy designu zamÄ›Å™enÃ© na okraj sÃ­tÄ›**: Agenti schopnÃ­ offline zpracovÃ¡nÃ­ s lokÃ¡lnÃ­m zpracovÃ¡nÃ­m zajiÅ¡Å¥ujÃ­ ochranu soukromÃ­ a spolehlivost
7. **Integrace Foundry Local**: BezproblÃ©movÃ© propojenÃ­ mezi Microsoft Agent Framework a lokÃ¡lnÃ­ inferencÃ­ modelÅ¯
8. **Budoucnost patÅ™Ã­ SLM agentÅ¯m**: MalÃ© jazykovÃ© modely s produkÄnÃ­mi frameworky jsou budoucnostÃ­ agentickÃ© AI, umoÅ¾ÅˆujÃ­cÃ­ demokratizovanÃ© a efektivnÃ­ nasazenÃ­ agentÅ¯

## Odkazy a dalÅ¡Ã­ ÄtenÃ­

### ZÃ¡kladnÃ­ vÃ½zkumnÃ© prÃ¡ce a publikace

#### AI agenti a agentickÃ© systÃ©my
- **"Language Agents as Optimizable Graphs"** (2024) - ZÃ¡kladnÃ­ vÃ½zkum architektury agentÅ¯ a optimalizaÄnÃ­ch strategiÃ­
  - AutoÅ™i: Wenyue Hua, Lishan Yang, et al.
  - Odkaz: https://arxiv.org/abs/2402.16823
  - KlÃ­ÄovÃ© poznatky: NÃ¡vrh agentÅ¯ zaloÅ¾enÃ½ na grafech a optimalizaÄnÃ­ strategie

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - AutoÅ™i: Zhiheng Xi, Wenxiang Chen, et al.
  - Odkaz: https://arxiv.org/abs/2309.07864
  - KlÃ­ÄovÃ© poznatky: KomplexnÃ­ pÅ™ehled schopnostÃ­ a aplikacÃ­ agentÅ¯ zaloÅ¾enÃ½ch na LLM

- **"Cognitive Architectures for Language Agents"** (2024)
  - AutoÅ™i: Theodore Sumers, Shunyu Yao, et al.
  - Odkaz: https://arxiv.org/abs/2309.02427
  - KlÃ­ÄovÃ© poznatky: KognitivnÃ­ rÃ¡mce pro nÃ¡vrh inteligentnÃ­ch agentÅ¯

#### MalÃ© jazykovÃ© modely a optimalizace
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - AutoÅ™i: Microsoft Research Team
  - Odkaz: https://arxiv.org/abs/2404.14219
  - KlÃ­ÄovÃ© poznatky: Principy nÃ¡vrhu SLM a strategie mobilnÃ­ho nasazenÃ­

- **"Qwen2.5 Technical Report"** (2024)
  - AutoÅ™i: Alibaba Cloud Team
  - Odkaz: https://arxiv.org/abs/2407.10671
  - KlÃ­ÄovÃ© poznatky: PokroÄilÃ© techniky trÃ©ninku SLM a optimalizace vÃ½konu

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - AutoÅ™i: Peiyuan Zhang, Guangtao Zeng, et al.
  - Odkaz: https://arxiv.org/abs/2401.02385
  - KlÃ­ÄovÃ© poznatky: Ultra-kompaktnÃ­ nÃ¡vrh modelu a efektivita trÃ©ninku

### OficiÃ¡lnÃ­ dokumentace a frameworky

#### Microsoft Agent Framework
- **OficiÃ¡lnÃ­ dokumentace**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub Repository**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **PrimÃ¡rnÃ­ repozitÃ¡Å™**: https://github.com/microsoft/foundry-local
- **Dokumentace**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **HlavnÃ­ repozitÃ¡Å™**: https://github.com/vllm-project/vllm
- **Dokumentace**: https://docs.vllm.ai/


#### Ollama
- **OficiÃ¡lnÃ­ web**: https://ollama.ai/
- **GitHub Repository**: https://github.com/ollama/ollama

### Frameworky pro optimalizaci modelÅ¯

#### Llama.cpp
- **RepozitÃ¡Å™**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentace**: https://microsoft.github.io/Olive/
- **GitHub Repository**: https://github.com/microsoft/Olive

#### OpenVINO
- **OficiÃ¡lnÃ­ strÃ¡nka**: https://docs.openvino.ai/

#### Apple MLX
- **RepozitÃ¡Å™**: https://github.com/ml-explore/mlx

### PrÅ¯myslovÃ© zprÃ¡vy a analÃ½zy trhu

#### VÃ½zkum trhu AI agentÅ¯
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Odkaz: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - KlÃ­ÄovÃ© poznatky: Trendy trhu a vzorce adopce v podnicÃ­ch

#### TechnickÃ© benchmarky

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Odkaz: https://mlcommons.org/en/inference-edge/
  - KlÃ­ÄovÃ© poznatky: StandardizovanÃ© metriky vÃ½konu pro nasazenÃ­ na okraji sÃ­tÄ›

### Standardy a specifikace

#### FormÃ¡ty modelÅ¯ a standardy
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - FormÃ¡t modelu napÅ™Ã­Ä platformami pro interoperabilitu
- **GGUF Specification**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - KvantizovanÃ½ formÃ¡t modelu pro inferenci na CPU
- **OpenAI API Specification**: https://platform.openai.com/docs/api-reference
  - StandardnÃ­ formÃ¡t API pro integraci jazykovÃ½ch modelÅ¯

#### BezpeÄnost a shoda
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI Systems**: RÃ¡mec pro AI systÃ©my a bezpeÄnost
- **IEEE Standards for AI**: https://standards.ieee.org/industry-connections/ai/

Posun smÄ›rem k agentÅ¯m pohÃ¡nÄ›nÃ½m SLM pÅ™edstavuje zÃ¡sadnÃ­ zmÄ›nu v pÅ™Ã­stupu k nasazenÃ­ AI. Microsoft Agent Framework, kombinovanÃ½ s lokÃ¡lnÃ­mi platformami a efektivnÃ­mi malÃ½mi jazykovÃ½mi modely, poskytuje kompletnÃ­ Å™eÅ¡enÃ­ pro vytvÃ¡Å™enÃ­ produkÄnÄ› pÅ™ipravenÃ½ch agentÅ¯, kteÅ™Ã­ efektivnÄ› fungujÃ­ v prostÅ™edÃ­ch na okraji sÃ­tÄ›. ZamÄ›Å™enÃ­m na efektivitu, specializaci a praktickou uÅ¾iteÄnost ÄinÃ­ tento technologickÃ½ stack AI agenty dostupnÄ›jÅ¡Ã­mi, cenovÄ› vÃ½hodnÄ›jÅ¡Ã­mi a efektivnÄ›jÅ¡Ã­mi pro aplikace v reÃ¡lnÃ©m svÄ›tÄ› napÅ™Ã­Ä vÅ¡emi odvÄ›tvÃ­mi a prostÅ™edÃ­mi vÃ½poÄtÅ¯ na okraji sÃ­tÄ›.

Jak postupujeme do roku 2025, kombinace stÃ¡le schopnÄ›jÅ¡Ã­ch malÃ½ch modelÅ¯, sofistikovanÃ½ch frameworkÅ¯ pro agenty, jako je Microsoft Agent Framework, a robustnÃ­ch platforem pro nasazenÃ­ na okraji sÃ­tÄ› odemkne novÃ© moÅ¾nosti pro autonomnÃ­ systÃ©my, kterÃ© mohou efektivnÄ› fungovat na zaÅ™Ã­zenÃ­ch na okraji sÃ­tÄ› pÅ™i zachovÃ¡nÃ­ soukromÃ­, snÃ­Å¾enÃ­ nÃ¡kladÅ¯ a poskytovÃ¡nÃ­ vÃ½jimeÄnÃ½ch uÅ¾ivatelskÃ½ch zkuÅ¡enostÃ­.

**DalÅ¡Ã­ kroky pro implementaci**:
1. **Prozkoumejte volÃ¡nÃ­ funkcÃ­**: NauÄte se, jak SLM zpracovÃ¡vajÃ­ integraci nÃ¡strojÅ¯ a strukturovanÃ© vÃ½stupy
2. **OvlÃ¡dnÄ›te Model Context Protocol (MCP)**: Pochopte pokroÄilÃ© komunikaÄnÃ­ vzory agentÅ¯
3. **VytvoÅ™te produkÄnÃ­ agenty**: PouÅ¾ij

---

**ProhlÃ¡Å¡enÃ­**:  
Tento dokument byl pÅ™eloÅ¾en pomocÃ­ sluÅ¾by AI pro pÅ™eklady [Co-op Translator](https://github.com/Azure/co-op-translator). I kdyÅ¾ se snaÅ¾Ã­me o pÅ™esnost, mÄ›jte prosÃ­m na pamÄ›ti, Å¾e automatickÃ© pÅ™eklady mohou obsahovat chyby nebo nepÅ™esnosti. PÅ¯vodnÃ­ dokument v jeho rodnÃ©m jazyce by mÄ›l bÃ½t povaÅ¾ovÃ¡n za autoritativnÃ­ zdroj. Pro dÅ¯leÅ¾itÃ© informace se doporuÄuje profesionÃ¡lnÃ­ lidskÃ½ pÅ™eklad. NeodpovÃ­dÃ¡me za Å¾Ã¡dnÃ¡ nedorozumÄ›nÃ­ nebo nesprÃ¡vnÃ© interpretace vyplÃ½vajÃ­cÃ­ z pouÅ¾itÃ­ tohoto pÅ™ekladu.