# Mục 7: Bộ công cụ tối ưu hóa Qualcomm QNN (Qualcomm Neural Network)

## Mục lục
1. [Giới thiệu](../../../Module04)
2. [Qualcomm QNN là gì?](../../../Module04)
3. [Cài đặt](../../../Module04)
4. [Hướng dẫn bắt đầu nhanh](../../../Module04)
5. [Ví dụ: Chuyển đổi và tối ưu hóa mô hình với QNN](../../../Module04)
6. [Sử dụng nâng cao](../../../Module04)
7. [Thực hành tốt nhất](../../../Module04)
8. [Khắc phục sự cố](../../../Module04)
9. [Tài nguyên bổ sung](../../../Module04)

## Giới thiệu

Qualcomm QNN (Qualcomm Neural Network) là một khung làm việc suy luận AI toàn diện được thiết kế để khai thác tối đa tiềm năng của các bộ tăng tốc phần cứng AI của Qualcomm, bao gồm Hexagon NPU, Adreno GPU và Kryo CPU. Dù bạn đang nhắm đến các thiết bị di động, nền tảng tính toán biên hay hệ thống ô tô, QNN cung cấp khả năng suy luận tối ưu hóa tận dụng các đơn vị xử lý AI chuyên biệt của Qualcomm để đạt hiệu suất và hiệu quả năng lượng tối đa.

## Qualcomm QNN là gì?

Qualcomm QNN là một khung làm việc suy luận AI thống nhất, cho phép các nhà phát triển triển khai mô hình AI một cách hiệu quả trên kiến trúc tính toán dị thể của Qualcomm. Nó cung cấp giao diện lập trình thống nhất để truy cập Hexagon NPU (Neural Processing Unit), Adreno GPU và Kryo CPU, tự động chọn đơn vị xử lý tối ưu cho các lớp và hoạt động khác nhau của mô hình.

### Các tính năng chính

- **Tính toán dị thể**: Truy cập thống nhất vào NPU, GPU và CPU với phân phối khối lượng công việc tự động
- **Tối ưu hóa theo phần cứng**: Các tối ưu hóa chuyên biệt cho nền tảng Snapdragon của Qualcomm
- **Hỗ trợ lượng tử hóa**: Kỹ thuật lượng tử hóa tiên tiến INT8, INT16 và hỗn hợp độ chính xác
- **Công cụ chuyển đổi mô hình**: Hỗ trợ trực tiếp cho các mô hình TensorFlow, PyTorch, ONNX và Caffe
- **Tối ưu hóa AI biên**: Được thiết kế đặc biệt cho các kịch bản triển khai di động và biên với trọng tâm là hiệu quả năng lượng

### Lợi ích

- **Hiệu suất tối đa**: Tận dụng phần cứng AI chuyên biệt để cải thiện hiệu suất lên đến 15 lần
- **Hiệu quả năng lượng**: Tối ưu hóa cho các thiết bị di động và chạy bằng pin với quản lý năng lượng thông minh
- **Độ trễ thấp**: Suy luận tăng tốc phần cứng với chi phí tối thiểu cho các ứng dụng thời gian thực
- **Triển khai mở rộng**: Từ điện thoại thông minh đến nền tảng ô tô trong hệ sinh thái của Qualcomm
- **Sẵn sàng sản xuất**: Khung làm việc đã được kiểm chứng, sử dụng trong hàng triệu thiết bị đã triển khai

## Cài đặt

### Yêu cầu trước

- Qualcomm QNN SDK (yêu cầu đăng ký với Qualcomm)
- Python 3.7 hoặc cao hơn
- Phần cứng Qualcomm tương thích hoặc trình mô phỏng
- Android NDK (cho triển khai di động)
- Môi trường phát triển Linux hoặc Windows

### Thiết lập QNN SDK

1. **Đăng ký và tải xuống**: Truy cập Qualcomm Developer Network để đăng ký và tải xuống QNN SDK
2. **Giải nén SDK**: Giải nén QNN SDK vào thư mục phát triển của bạn
3. **Đặt biến môi trường**: Cấu hình đường dẫn cho các công cụ và thư viện QNN

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Thiết lập môi trường Python

Tạo và kích hoạt môi trường ảo:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Cài đặt các gói Python cần thiết:

```bash
pip install numpy tensorflow torch onnx
```

### Xác minh cài đặt

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Nếu thành công, bạn sẽ thấy thông tin trợ giúp cho từng công cụ QNN.

## Hướng dẫn bắt đầu nhanh

### Chuyển đổi mô hình đầu tiên của bạn

Hãy chuyển đổi một mô hình PyTorch đơn giản để chạy trên phần cứng Qualcomm:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Chuyển đổi ONNX sang định dạng QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Tạo thư viện mô hình QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Quy trình này làm gì

Quy trình tối ưu hóa bao gồm: chuyển đổi mô hình gốc sang định dạng ONNX, dịch ONNX sang biểu diễn trung gian QNN, áp dụng các tối ưu hóa cụ thể cho phần cứng, và tạo thư viện mô hình đã biên dịch để triển khai.

### Giải thích các tham số chính

- `--input_network`: Tệp mô hình ONNX nguồn
- `--output_path`: Tệp nguồn C++ được tạo
- `--input_dim`: Kích thước tensor đầu vào để tối ưu hóa
- `--quantization_overrides`: Cấu hình lượng tử hóa tùy chỉnh
- `-t x86_64-linux-clang`: Kiến trúc mục tiêu và trình biên dịch

## Ví dụ: Chuyển đổi và tối ưu hóa mô hình với QNN

### Bước 1: Chuyển đổi mô hình nâng cao với lượng tử hóa

Dưới đây là cách áp dụng lượng tử hóa tùy chỉnh trong quá trình chuyển đổi:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Chuyển đổi với lượng tử hóa tùy chỉnh:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Bước 2: Tối ưu hóa đa backend

Cấu hình để thực thi dị thể trên NPU, GPU và CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Bước 3: Tạo binary ngữ cảnh để triển khai

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Bước 4: Suy luận với QNN Runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Cấu trúc đầu ra

Sau khi tối ưu hóa, thư mục triển khai của bạn sẽ chứa:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Sử dụng nâng cao

### Cấu hình backend tùy chỉnh

Cấu hình các tối ưu hóa backend cụ thể:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Lượng tử hóa động

Áp dụng lượng tử hóa trong thời gian chạy để cải thiện độ chính xác:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Phân tích hiệu suất

Theo dõi hiệu suất trên các backend khác nhau:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Lựa chọn backend tự động

Triển khai lựa chọn backend thông minh dựa trên đặc điểm mô hình:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Thực hành tốt nhất

### 1. Tối ưu hóa kiến trúc mô hình
- **Hợp nhất lớp**: Kết hợp các hoạt động như Conv+BatchNorm+ReLU để sử dụng NPU tốt hơn
- **Convolution phân tách theo chiều sâu**: Ưu tiên loại này thay vì convolution tiêu chuẩn cho triển khai di động
- **Thiết kế thân thiện với lượng tử hóa**: Sử dụng các kích hoạt ReLU và tránh các hoạt động không lượng tử hóa tốt

### 2. Chiến lược lượng tử hóa
- **Lượng tử hóa sau huấn luyện**: Bắt đầu với phương pháp này để triển khai nhanh
- **Tập dữ liệu hiệu chuẩn**: Sử dụng dữ liệu đại diện bao phủ tất cả các biến thể đầu vào
- **Độ chính xác hỗn hợp**: Sử dụng INT8 cho hầu hết các lớp, giữ các lớp quan trọng ở độ chính xác cao hơn

### 3. Hướng dẫn lựa chọn backend
- **NPU (HTP)**: Tốt nhất cho khối lượng công việc CNN, mô hình lượng tử hóa và ứng dụng nhạy cảm với năng lượng
- **GPU**: Tối ưu cho các hoạt động tính toán nặng, mô hình lớn và độ chính xác FP16
- **CPU**: Dự phòng cho các hoạt động không được hỗ trợ và gỡ lỗi

### 4. Tối ưu hóa hiệu suất
- **Kích thước batch**: Sử dụng kích thước batch 1 cho các ứng dụng thời gian thực, batch lớn hơn cho thông lượng
- **Tiền xử lý đầu vào**: Giảm thiểu chi phí sao chép và chuyển đổi dữ liệu
- **Tái sử dụng ngữ cảnh**: Biên dịch trước các ngữ cảnh để tránh chi phí biên dịch trong thời gian chạy

### 5. Quản lý bộ nhớ
- **Phân bổ tensor**: Sử dụng phân bổ tĩnh khi có thể để tránh chi phí trong thời gian chạy
- **Bể nhớ**: Triển khai các bể nhớ tùy chỉnh cho các tensor được phân bổ thường xuyên
- **Tái sử dụng bộ đệm**: Tái sử dụng các bộ đệm đầu vào/đầu ra qua các lần gọi suy luận

### 6. Tối ưu hóa năng lượng
- **Chế độ hiệu suất**: Sử dụng chế độ hiệu suất phù hợp dựa trên các ràng buộc nhiệt
- **Thay đổi tần số động**: Cho phép hệ thống thay đổi tần số dựa trên khối lượng công việc
- **Quản lý trạng thái nhàn rỗi**: Giải phóng tài nguyên đúng cách khi không sử dụng

## Khắc phục sự cố

### Các vấn đề thường gặp

#### 1. Vấn đề cài đặt SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Lỗi chuyển đổi mô hình
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Vấn đề lượng tử hóa
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Vấn đề hiệu suất
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Vấn đề bộ nhớ
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Tương thích backend
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Gỡ lỗi hiệu suất

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Nhận trợ giúp

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **Tài liệu QNN**: Có trong gói SDK
- **Diễn đàn cộng đồng**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Hỗ trợ kỹ thuật**: Qua cổng thông tin nhà phát triển Qualcomm

## Tài nguyên bổ sung

### Liên kết chính thức
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Nền tảng Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Cổng nhà phát triển**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Tài liệu học tập
- **Hướng dẫn bắt đầu**: Có trong tài liệu QNN SDK
- **Model Zoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Hướng dẫn tối ưu hóa**: Tài liệu SDK bao gồm hướng dẫn tối ưu hóa toàn diện
- **Video hướng dẫn**: [Kênh YouTube Qualcomm Developer](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Công cụ tích hợp
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Các mô hình được tối ưu hóa trước cho phần cứng Qualcomm
- **API mạng thần kinh Android**: Tích hợp với Android NNAPI
- **TensorFlow Lite Delegate**: Đại diện Qualcomm cho TFLite

### Đánh giá hiệu suất
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Nghiên cứu AI Qualcomm**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Ví dụ cộng đồng
- **Ứng dụng mẫu**: Có trong thư mục ví dụ QNN SDK
- **Kho GitHub**: Các ví dụ và công cụ do cộng đồng đóng góp
- **Blog kỹ thuật**: [Blog nhà phát triển Qualcomm](https://developer.qualcomm.com/blog)

### Công cụ liên quan
- **Bộ công cụ hiệu quả mô hình AI Qualcomm (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Kỹ thuật lượng tử hóa và nén tiên tiến
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - Để so sánh và triển khai dự phòng
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Công cụ suy luận đa nền tảng

### Thông số kỹ thuật phần cứng
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Nền tảng Snapdragon**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Tiếp theo

Tiếp tục hành trình AI biên của bạn bằng cách khám phá [Module 5: SLMOps và triển khai sản xuất](../Module05/README.md) để tìm hiểu về các khía cạnh vận hành của quản lý vòng đời mô hình ngôn ngữ nhỏ.

---

**Tuyên bố miễn trừ trách nhiệm**:  
Tài liệu này đã được dịch bằng dịch vụ dịch thuật AI [Co-op Translator](https://github.com/Azure/co-op-translator). Mặc dù chúng tôi cố gắng đảm bảo độ chính xác, xin lưu ý rằng các bản dịch tự động có thể chứa lỗi hoặc không chính xác. Tài liệu gốc bằng ngôn ngữ bản địa nên được coi là nguồn thông tin chính thức. Đối với thông tin quan trọng, nên sử dụng dịch vụ dịch thuật chuyên nghiệp bởi con người. Chúng tôi không chịu trách nhiệm cho bất kỳ sự hiểu lầm hoặc diễn giải sai nào phát sinh từ việc sử dụng bản dịch này.