# Sezione 03 - Integrazione del Protocollo di Contesto Modello (MCP)

## Introduzione al MCP (Protocollo di Contesto Modello)

Il Protocollo di Contesto Modello (MCP) è uno standard open-source per collegare applicazioni AI a sistemi esterni. Utilizzando MCP, applicazioni AI come Claude o ChatGPT possono connettersi a fonti di dati (ad esempio, file locali, database), strumenti (ad esempio, motori di ricerca, calcolatori) e flussi di lavoro (ad esempio, prompt specializzati), permettendo loro di accedere a informazioni chiave e svolgere compiti.

Pensate al MCP come una **porta USB-C per le applicazioni AI**. Proprio come USB-C offre un modo standardizzato per collegare dispositivi elettronici, MCP offre un modo standardizzato per collegare applicazioni AI a sistemi esterni.

### Cosa può abilitare MCP?

MCP sblocca capacità potenti per le applicazioni AI:

- **Assistenti AI personalizzati**: Gli agenti possono accedere al tuo Google Calendar e Notion, agendo come un assistente AI più personalizzato
- **Generazione avanzata di codice**: Claude Code può generare un'intera app web utilizzando un design Figma
- **Integrazione di dati aziendali**: I chatbot aziendali possono connettersi a più database di un'organizzazione, permettendo agli utenti di analizzare i dati tramite chat
- **Flussi di lavoro creativi**: I modelli AI possono creare design 3D su Blender e stamparli con una stampante 3D
- **Accesso a informazioni in tempo reale**: Collegarsi a fonti di dati esterne per informazioni aggiornate
- **Operazioni complesse multi-step**: Eseguire flussi di lavoro sofisticati combinando più strumenti e sistemi

### Perché MCP è importante?

MCP offre vantaggi in tutto l'ecosistema:

**Per gli sviluppatori**: MCP riduce il tempo e la complessità di sviluppo quando si costruisce o si integra un'applicazione o un agente AI.

**Per le applicazioni AI**: MCP fornisce accesso a un ecosistema di fonti di dati, strumenti e app che migliorano le capacità e l'esperienza dell'utente finale.

**Per gli utenti finali**: MCP porta a applicazioni AI o agenti più capaci che possono accedere ai tuoi dati e agire per tuo conto quando necessario.

## Modelli di linguaggio piccoli (SLM) nel MCP

I modelli di linguaggio piccoli rappresentano un approccio efficiente alla distribuzione dell'AI, offrendo diversi vantaggi:

### Vantaggi degli SLM
- **Efficienza delle risorse**: Minori requisiti computazionali
- **Tempi di risposta più rapidi**: Riduzione della latenza per applicazioni in tempo reale  
- **Convenienza economica**: Necessità infrastrutturali minime
- **Privacy**: Possono funzionare localmente senza trasmissione di dati
- **Personalizzazione**: Più facile da adattare a domini specifici

### Perché gli SLM funzionano bene con MCP

Gli SLM abbinati a MCP creano una combinazione potente in cui le capacità di ragionamento del modello sono potenziate da strumenti esterni, compensando il minor numero di parametri con funzionalità avanzate.

## Panoramica del Python MCP SDK

Il Python MCP SDK fornisce la base per costruire applicazioni abilitate MCP. L'SDK include:

- **Librerie client**: Per connettersi ai server MCP
- **Framework server**: Per creare server MCP personalizzati
- **Gestori di protocollo**: Per gestire la comunicazione
- **Integrazione degli strumenti**: Per eseguire funzioni esterne

## Implementazione pratica: Client MCP Phi-4

Esploriamo un'implementazione reale utilizzando il mini modello Phi-4 di Microsoft integrato con le capacità MCP.

### Panoramica dell'architettura MCP

MCP segue un'architettura **client-server** in cui un host MCP (un'applicazione AI come Claude Code o Claude Desktop) stabilisce connessioni con uno o più server MCP. L'host MCP realizza questo creando un client MCP per ogni server MCP.

#### Partecipanti chiave

- **Host MCP**: L'applicazione AI che coordina e gestisce uno o più client MCP
- **Client MCP**: Un componente che mantiene una connessione a un server MCP e ottiene contesto dal server MCP per l'host MCP
- **Server MCP**: Un programma che fornisce contesto ai client MCP

#### Architettura a due livelli

MCP consiste in due livelli distinti:

**Livello dati**: Definisce il protocollo basato su JSON-RPC per la comunicazione client-server, inclusi:
- Gestione del ciclo di vita (inizializzazione della connessione, negoziazione delle capacità)
- Primitivi principali (strumenti, risorse, prompt)
- Funzionalità client (campionamento, elicitation, logging)
- Funzionalità utilitarie (notifiche, monitoraggio del progresso)

**Livello trasporto**: Definisce i meccanismi e i canali di comunicazione:
- **Trasporto STDIO**: Utilizza flussi di input/output standard per processi locali (prestazioni ottimali, nessun sovraccarico di rete)
- **Trasporto HTTP Streamable**: Utilizza HTTP POST con eventi inviati dal server opzionali per server remoti (supporta l'autenticazione HTTP standard)

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### Primitivi principali MCP

MCP definisce primitivi che specificano i tipi di informazioni contestuali che possono essere condivise con le applicazioni AI e la gamma di azioni che possono essere eseguite.

#### Primitivi del server

MCP definisce tre primitivi principali che i server possono esporre:

**Strumenti**: Funzioni eseguibili che le applicazioni AI possono invocare per eseguire azioni
- Esempi: operazioni sui file, chiamate API, query sui database
- Metodi: `tools/list`, `tools/call`
- Supportano la scoperta e l'esecuzione dinamica

**Risorse**: Fonti di dati che forniscono informazioni contestuali alle applicazioni AI
- Esempi: contenuti dei file, record di database, risposte API
- Metodi: `resources/list`, `resources/read`
- Consentono l'accesso a dati strutturati

**Prompt**: Modelli riutilizzabili che aiutano a strutturare le interazioni con i modelli di linguaggio
- Esempi: prompt di sistema, esempi few-shot
- Metodi: `prompts/list`, `prompts/get`
- Standardizzano i modelli di interazione AI

#### Primitivi del client

MCP definisce anche primitivi che i client possono esporre per abilitare interazioni più ricche:

**Campionamento**: Consente ai server di richiedere completamenti del modello di linguaggio dall'applicazione AI del client
- Metodo: `sampling/complete`
- Abilita lo sviluppo indipendente dal modello per i server
- Fornisce accesso al modello di linguaggio dell'host

**Elicitation**: Consente ai server di richiedere informazioni aggiuntive dagli utenti
- Metodo: `elicitation/request`
- Abilita l'interazione e la conferma dell'utente
- Supporta la raccolta dinamica di informazioni

**Logging**: Consente ai server di inviare messaggi di log ai client
- Utilizzato per scopi di debug e monitoraggio
- Fornisce visibilità sulle operazioni del server

### Ciclo di vita del protocollo MCP

#### Inizializzazione e negoziazione delle capacità

MCP è un protocollo stateful che richiede la gestione del ciclo di vita. Il processo di inizializzazione serve a diversi scopi critici:

1. **Negoziazione della versione del protocollo**: Garantisce che client e server utilizzino versioni del protocollo compatibili (ad esempio, "2025-06-18")
2. **Scoperta delle capacità**: Ogni parte dichiara le funzionalità e i primitivi supportati
3. **Scambio di identità**: Fornisce informazioni di identificazione e versionamento

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### Scoperta ed esecuzione degli strumenti

Dopo l'inizializzazione, i client possono scoprire ed eseguire strumenti:

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### Notifiche in tempo reale

MCP supporta notifiche in tempo reale per aggiornamenti dinamici:

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## Iniziare: Guida passo-passo

### Passo 1: Configurazione dell'ambiente

Installa le dipendenze richieste:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Passo 2: Configurazione di base

Configura le variabili d'ambiente:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Passo 3: Esecuzione del tuo primo client MCP

**Configurazione base di Ollama:**
```bash
python ghmodel_mcp_demo.py
```

**Utilizzo del backend vLLM:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Connessione con eventi inviati dal server:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Server MCP personalizzato:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Passo 4: Utilizzo programmatico

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Funzionalità avanzate

### Supporto multi-backend

L'implementazione supporta sia i backend Ollama che vLLM, permettendo di scegliere in base alle esigenze:

- **Ollama**: Migliore per lo sviluppo e il test locale
- **vLLM**: Ottimizzato per scenari di produzione e throughput elevato

### Protocolli di connessione flessibili

Sono supportate due modalità di connessione:

**Modalità STDIO**: Comunicazione diretta tra processi
- Latenza inferiore
- Adatta per strumenti locali
- Configurazione semplice

**Modalità SSE**: Streaming basato su HTTP
- Compatibile con la rete
- Migliore per sistemi distribuiti
- Aggiornamenti in tempo reale

### Capacità di integrazione degli strumenti

Il sistema può integrarsi con vari strumenti:
- Automazione web (Playwright)
- Operazioni sui file
- Interazioni API
- Comandi di sistema
- Funzioni personalizzate

## Gestione degli errori e migliori pratiche

### Gestione completa degli errori

L'implementazione include una gestione robusta degli errori per:

**Errori di connessione:**
- Fallimenti del server MCP
- Timeout di rete
- Problemi di connettività

**Errori di esecuzione degli strumenti:**
- Strumenti mancanti
- Validazione dei parametri
- Fallimenti di esecuzione

**Errori di elaborazione delle risposte:**
- Problemi di parsing JSON
- Incoerenze di formato
- Anomalie nelle risposte del modello di linguaggio

### Migliori pratiche

1. **Gestione delle risorse**: Utilizza gestori di contesto asincroni
2. **Gestione degli errori**: Implementa blocchi try-catch completi
3. **Logging**: Abilita livelli di logging appropriati
4. **Sicurezza**: Valida gli input e sanifica gli output
5. **Prestazioni**: Utilizza il pooling delle connessioni e la cache

## Applicazioni reali

### Automazione web
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Elaborazione dei dati
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Integrazione API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Ottimizzazione delle prestazioni

### Gestione della memoria
- Gestione efficiente della cronologia dei messaggi
- Pulizia corretta delle risorse
- Pooling delle connessioni

### Ottimizzazione della rete
- Operazioni HTTP asincrone
- Timeout configurabili
- Recupero degli errori in modo graduale

### Elaborazione concorrente
- I/O non bloccante
- Esecuzione parallela degli strumenti
- Modelli asincroni efficienti

## Considerazioni sulla sicurezza

### Protezione dei dati
- Gestione sicura delle chiavi API
- Validazione degli input
- Sanificazione degli output

### Sicurezza della rete
- Supporto HTTPS
- Endpoint locali predefiniti
- Gestione sicura dei token

### Sicurezza dell'esecuzione
- Filtraggio degli strumenti
- Ambienti sandbox
- Logging di audit

## Ecosistema MCP e sviluppo

### Ambito del progetto MCP

L'ecosistema del Protocollo di Contesto Modello include diversi componenti chiave:

- **[Specifiche MCP](https://modelcontextprotocol.io/specification/latest)**: Specifica ufficiale che delinea i requisiti di implementazione per client e server
- **[SDK MCP](https://modelcontextprotocol.io/docs/sdk)**: SDK per diversi linguaggi di programmazione che implementano MCP
- **Strumenti di sviluppo MCP**: Strumenti per sviluppare server e client MCP, incluso il [MCP Inspector](https://github.com/modelcontextprotocol/inspector)
- **[Implementazioni di server di riferimento MCP](https://github.com/modelcontextprotocol/servers)**: Implementazioni di riferimento dei server MCP

### Iniziare con lo sviluppo MCP

Per iniziare a costruire con MCP:

**Costruire server**: [Crea server MCP](https://modelcontextprotocol.io/docs/develop/build-server) per esporre i tuoi dati e strumenti

**Costruire client**: [Sviluppa applicazioni](https://modelcontextprotocol.io/docs/develop/build-client) che si connettono ai server MCP

**Imparare i concetti**: [Comprendi i concetti principali](https://modelcontextprotocol.io/docs/learn/architecture) e l'architettura di MCP

## Conclusione

Gli SLM integrati con MCP rappresentano un cambiamento di paradigma nello sviluppo delle applicazioni AI. Combinando l'efficienza dei modelli piccoli con la potenza degli strumenti esterni, gli sviluppatori possono creare sistemi intelligenti che sono sia efficienti in termini di risorse che altamente capaci.

Il Protocollo di Contesto Modello fornisce un modo standardizzato per collegare le applicazioni AI a sistemi esterni, proprio come USB-C fornisce uno standard universale di connessione per dispositivi elettronici. Questa standardizzazione consente:

- **Integrazione senza soluzione di continuità**: Collegare modelli AI a fonti di dati e strumenti diversi
- **Crescita dell'ecosistema**: Costruire una volta, utilizzare su più applicazioni AI
- **Capacità migliorate**: Potenziare gli SLM con funzionalità esterne
- **Aggiornamenti in tempo reale**: Supportare applicazioni AI dinamiche e reattive

Punti chiave:
- MCP è uno standard aperto che collega applicazioni AI e sistemi esterni
- Il protocollo supporta strumenti, risorse e prompt come primitivi principali
- Le notifiche in tempo reale abilitano applicazioni dinamiche e reattive
- La gestione del ciclo di vita e degli errori è essenziale per l'uso in produzione
- L'ecosistema offre SDK completi e strumenti di sviluppo

## Riferimenti e letture aggiuntive

### Documentazione ufficiale MCP

- **[Sito ufficiale del Protocollo di Contesto Modello](https://modelcontextprotocol.io/)** - Documentazione e specifiche complete
- **[Guida introduttiva MCP](https://modelcontextprotocol.io/docs/getting-started/intro)** - Introduzione e concetti principali
- **[Panoramica dell'architettura MCP](https://modelcontextprotocol.io/docs/learn/architecture)** - Architettura tecnica dettagliata
- **[Specifiche MCP](https://modelcontextprotocol.io/specification/latest)** - Specifica ufficiale del protocollo
- **[Documentazione SDK MCP](https://modelcontextprotocol.io/docs/sdk)** - Guide SDK specifiche per linguaggio

### Risorse di sviluppo

- **[MCP per principianti](https://aka.ms/mcp-for-beginners)** - Guida completa per principianti al Protocollo di Contesto Modello
- **[Organizzazione GitHub MCP](https://github.com/modelcontextprotocol)** - Repository ufficiali ed esempi
- **[Repository server MCP](https://github.com/modelcontextprotocol/servers)** - Implementazioni di server di riferimento
- **[MCP Inspector](https://github.com/modelcontextprotocol/inspector)** - Strumento di sviluppo e debug
- **[Guida alla creazione di server MCP](https://modelcontextprotocol.io/docs/develop/build-server)** - Tutorial per lo sviluppo di server
- **[Guida alla creazione di client MCP](https://modelcontextprotocol.io/docs/develop/build-client)** - Tutorial per lo sviluppo di client

### Modelli di linguaggio piccoli e AI edge

- **[Modelli Phi di Microsoft](https://aka.ms/phicookbook)** - Famiglia di modelli Phi 
- **[Documentazione Foundry Local](https://github.com/microsoft/Foundry-Local)** - Runtime AI edge di Microsoft
- **[Documentazione di Ollama](https://ollama.ai/docs)** - Piattaforma per il deployment locale di LLM
- **[Documentazione di vLLM](https://docs.vllm.ai/)** - Servizio LLM ad alte prestazioni

### Standard tecnici e protocolli

- **[Specifiche JSON-RPC 2.0](https://www.jsonrpc.org/)** - Protocollo RPC sottostante utilizzato da MCP
- **[JSON Schema](https://json-schema.org/)** - Standard di definizione dello schema per gli strumenti MCP
- **[Specifiche OpenAPI](https://swagger.io/specification/)** - Standard per la documentazione API
- **[Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - Standard web per aggiornamenti in tempo reale

### Sviluppo di agenti AI

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - Sviluppo di agenti pronto per la produzione
- **[Documentazione di LangChain](https://docs.langchain.com/)** - Framework per l'integrazione di agenti e strumenti
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - SDK di orchestrazione AI di Microsoft

### Rapporti di settore e ricerca

- **[Annuncio del Model Context Protocol di Anthropic](https://www.anthropic.com/news/model-context-protocol)** - Introduzione originale al MCP
- **[Survey sui piccoli modelli linguistici](https://arxiv.org/abs/2410.20011)** - Indagine accademica sulla ricerca SLM
- **[Analisi del mercato Edge AI](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - Tendenze e previsioni del settore
- **[Best Practices per lo sviluppo di agenti AI](https://arxiv.org/abs/2309.02427)** - Ricerca sulle architetture degli agenti

Questa sezione fornisce le basi per costruire le tue applicazioni MCP alimentate da SLM, aprendo possibilità per automazione, elaborazione dati e integrazione di sistemi intelligenti.

## ➡️ Cosa fare dopo

- [Modulo 7. Esempi di Edge AI](../Module07/README.md)

---

**Disclaimer**:  
Questo documento è stato tradotto utilizzando il servizio di traduzione AI [Co-op Translator](https://github.com/Azure/co-op-translator). Sebbene ci impegniamo per garantire l'accuratezza, si prega di notare che le traduzioni automatiche potrebbero contenere errori o imprecisioni. Il documento originale nella sua lingua nativa dovrebbe essere considerato la fonte autorevole. Per informazioni critiche, si raccomanda una traduzione professionale umana. Non siamo responsabili per eventuali incomprensioni o interpretazioni errate derivanti dall'uso di questa traduzione.