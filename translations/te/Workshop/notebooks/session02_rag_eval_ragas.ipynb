{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5392a8a8",
   "metadata": {},
   "source": [
    "# సెషన్ 2 – రాగాలతో RAG మూల్యాంకనం\n",
    "\n",
    "రాగాస్ మెట్రిక్స్ ఉపయోగించి కనిష్ట RAG పైప్‌లైన్‌ను మూల్యాంకనం చేయండి: answer_relevancy, faithfulness, context_precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34473b",
   "metadata": {},
   "source": [
    "# సన్నివేశం\n",
    "ఈ సన్నివేశం ఒక కనిష్ట Retrieval Augmented Generation (RAG) పైప్‌లైన్‌ను స్థానికంగా మూల్యాంకనం చేస్తుంది. మేము:\n",
    "- ఒక చిన్న సింథటిక్ డాక్యుమెంట్ కార్పస్‌ను నిర్వచిస్తాము.\n",
    "- డాక్స్‌ను ఎంబెడ్ చేసి, ఒక సాధారణ సారూప్యత రిట్రీవర్‌ను అమలు చేస్తాము.\n",
    "- స్థానిక మోడల్ (Foundry Local / OpenAI-అనుకూల) ఉపయోగించి ఆధారిత సమాధానాలను ఉత్పత్తి చేస్తాము.\n",
    "- ragas మెట్రిక్స్ (`answer_relevancy`, `faithfulness`, `context_precision`) ను లెక్కిస్తాము.\n",
    "- వేగవంతమైన పునరావృతానికి (env `RAG_FAST=1`) కేవలం సమాధాన సంబంధితతను మాత్రమే లెక్కించే FAST మోడ్‌ను మద్దతు ఇస్తుంది.\n",
    "\n",
    "మీ స్థానిక మోడల్ + ఎంబెడ్డింగ్స్ స్టాక్ పెద్ద కార్పస్‌లకు విస్తరించే ముందు వాస్తవానికి ఆధారిత సమాధానాలను ఉత్పత్తి చేస్తున్నదని నిర్ధారించడానికి ఈ నోట్‌బుక్‌ను ఉపయోగించండి.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb1aa2",
   "metadata": {},
   "source": [
    "### వివరణ: డిపెండెన్సీ ఇన్‌స్టాలేషన్\n",
    "అవసరమైన లైబ్రరీలను ఇన్‌స్టాల్ చేస్తుంది:\n",
    "- స్థానిక మోడల్ నిర్వహణ కోసం `foundry-local-sdk`.\n",
    "- `openai` క్లయింట్ ఇంటర్‌ఫేస్.\n",
    "- సాంద్ర ఎంబెడ్డింగ్స్ కోసం `sentence-transformers`.\n",
    "- మూల్యాంకనం & మెట్రిక్ లెక్కింపు కోసం `ragas` + `datasets`.\n",
    "- ragas LLM ఇంటర్‌ఫేస్ కోసం `langchain-openai` అడాప్టర్.\n",
    "\n",
    "పునఃప్రారంభించడానికి సురక్షితం; వాతావరణం ఇప్పటికే సిద్ధంగా ఉంటే దాటవేయండి.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff641221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries (ragas pulls datasets, evaluate, etc.)\n",
    "!pip install -q foundry-local-sdk openai sentence-transformers ragas datasets numpy langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e82678",
   "metadata": {},
   "source": [
    "### వివరణ: కోర్ ఇంపోర్ట్స్ & మెట్రిక్స్\n",
    "కోర్ లైబ్రరీలు మరియు రాగాస్ మెట్రిక్స్‌ను లోడ్ చేస్తుంది. ముఖ్య భాగాలు:\n",
    "- ఎంబెడ్డింగ్స్ కోసం SentenceTransformer.\n",
    "- `evaluate` + ఎంపిక చేసిన రాగాస్ మెట్రిక్స్.\n",
    "- మూల్యాంకన కార్పస్ నిర్మాణం కోసం `Dataset`.\n",
    "ఈ ఇంపోర్ట్స్ రిమోట్ కాల్స్‌ను ప్రేరేపించవు (ఎంబెడ్డింగ్స్ కోసం మోడల్ క్యాష్ లోడ్ తప్ప).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519dabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f01938",
   "metadata": {},
   "source": [
    "### వివరణ: టాయ్ కార్పస్ & QA గ్రౌండ్ ట్రూత్\n",
    "ఒక చిన్న ఇన్-మెమరీ కార్పస్ (`DOCS`), యూజర్ ప్రశ్నల సెట్, మరియు ఆశించిన గ్రౌండ్ ట్రూత్ సమాధానాలను నిర్వచిస్తుంది. ఇవి బాహ్య డేటా ఫెచ్‌లను లేకుండా వేగవంతమైన, నిర్ణయాత్మక మెట్రిక్ లెక్కింపును అనుమతిస్తాయి. వాస్తవ పరిస్థితుల్లో మీరు ఉత్పత్తి ప్రశ్నలు + శ్రద్ధగా ఎంపిక చేసిన సమాధానాలను నమూనా తీసుకుంటారు.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27307d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = [\n",
    " 'Foundry Local exposes a local OpenAI-compatible endpoint.',\n",
    " 'RAG retrieves relevant context snippets before generation.',\n",
    " 'Local inference improves privacy and reduces latency.',\n",
    "]\n",
    "QUESTIONS = [\n",
    " 'What advantage does local inference offer?',\n",
    " 'How does RAG improve grounding?',\n",
    "]\n",
    "GROUND_TRUTH = [\n",
    " 'It reduces latency and preserves privacy.',\n",
    " 'It adds retrieved context snippets for factual grounding.',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3b2ec",
   "metadata": {},
   "source": [
    "### వివరణ: సర్వీస్ ఇనిట్, ఎంబెడ్డింగ్స్ & సేఫ్టీ ప్యాచ్\n",
    "Foundry Local మేనేజర్‌ను ప్రారంభిస్తుంది, `promptTemplate` కోసం స్కీమా-డ్రిఫ్ట్ సేఫ్టీ ప్యాచ్‌ను వర్తింపజేస్తుంది, మోడల్ IDని పరిష్కరిస్తుంది, OpenAI-అనుకూల క్లయింట్‌ను సృష్టిస్తుంది, మరియు డాక్యుమెంట్ కార్పస్ కోసం డెన్స్ ఎంబెడ్డింగ్స్‌ను ముందుగా గణిస్తుంది. ఇది రిట్రీవల్ + జనరేషన్ కోసం పునర్వినియోగపరచదగిన స్థితిని సెట్ చేస్తుంది.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156a7bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service running: True | Endpoint: http://127.0.0.1:57127/v1\n",
      "Cached models: [FoundryModelInfo(alias=gpt-oss-20b, id=gpt-oss-20b-cuda-gpu:1, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=9882 MB, license=apache-2.0), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-cuda-gpu:1, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini, id=Phi-4-mini-instruct-cuda-gpu:4, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=3686 MB, license=MIT), FoundryModelInfo(alias=qwen2.5-0.5b, id=qwen2.5-0.5b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=528 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-7b, id=qwen2.5-7b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=4843 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-7b, id=qwen2.5-coder-7b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=4843 MB, license=apache-2.0)]\n",
      "Using model id: Phi-4-mini-instruct-cuda-gpu:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leestott\\AppData\\Local\\miniforge\\envs\\demo\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from foundry_local.models import FoundryModelInfo\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Safe monkeypatch for potential null promptTemplate field (schema drift guard) ---\n",
    "_original_from_list_response = FoundryModelInfo.from_list_response\n",
    "\n",
    "def _safe_from_list_response(response):  # type: ignore\n",
    "    try:\n",
    "        if isinstance(response, dict) and response.get(\"promptTemplate\") is None:\n",
    "            response[\"promptTemplate\"] = {}\n",
    "    except Exception as e:  # pragma: no cover\n",
    "        print(f\"Warning normalizing promptTemplate: {e}\")\n",
    "    return _original_from_list_response(response)\n",
    "\n",
    "if getattr(FoundryModelInfo.from_list_response, \"__name__\", \"\") != \"_safe_from_list_response\":\n",
    "    FoundryModelInfo.from_list_response = staticmethod(_safe_from_list_response)  # type: ignore\n",
    "# --- End monkeypatch ---\n",
    "\n",
    "alias = os.getenv('FOUNDRY_LOCAL_ALIAS','phi-3.5-mini')\n",
    "manager = FoundryLocalManager(alias)\n",
    "print(f\"Service running: {manager.is_service_running()} | Endpoint: {manager.endpoint}\")\n",
    "print('Cached models:', manager.list_cached_models())\n",
    "model_info = manager.get_model_info(alias)\n",
    "model_id = model_info.id\n",
    "print(f\"Using model id: {model_id}\")\n",
    "\n",
    "# OpenAI-compatible client\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or 'not-needed')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "import numpy as np\n",
    "doc_emb = embedder.encode(DOCS, convert_to_numpy=True, normalize_embeddings=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d24a8",
   "metadata": {},
   "source": [
    "### వివరణ: రిట్రీవర్ ఫంక్షన్\n",
    "సాధారణ వెక్టర్ సారూప్యత రిట్రీవర్‌ను సాధారణీకరించిన ఎంబెడ్డింగ్స్ పై డాట్ ప్రొడక్ట్ ఉపయోగించి నిర్వచిస్తుంది. టాప్-k డాక్స్ (k=2 డిఫాల్ట్) ను తిరిగి ఇస్తుంది. ఉత్పత్తిలో స్కేల్ & లేటెన్సీ కోసం ANN సూచిక (FAISS, Chroma, Milvus) తో మార్చండి.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af32d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=2):\n",
    "    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = doc_emb @ q\n",
    "    return [DOCS[i] for i in sims.argsort()[::-1][:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f284e",
   "metadata": {},
   "source": [
    "### వివరణ: జనరేషన్ ఫంక్షన్  \n",
    "`generate` ఒక పరిమిత ప్రాంప్ట్‌ను నిర్మిస్తుంది (సిస్టమ్ కేవలం సందర్భాన్ని ఉపయోగించమని సూచిస్తుంది) మరియు స్థానిక మోడల్‌ను పిలుస్తుంది. తక్కువ ఉష్ణోగ్రత (0.1) సృజనాత్మకత కంటే విశ్వసనీయమైన తీయకాన్ని ప్రాధాన్యం ఇస్తుంది. త్రిమ్మింగ్ చేసిన సమాధాన పాఠ్యాన్ని తిరిగి ఇస్తుంది.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7798ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(query, contexts):\n",
    "    ctx = \"\\n\".join(contexts)\n",
    "    messages = [\n",
    "        {'role':'system','content':'Answer using ONLY the provided context.'},\n",
    "        {'role':'user','content':f\"Context:\\n{ctx}\\n\\nQuestion: {query}\"}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(model=model_id, messages=messages, max_tokens=120, temperature=0.1)\n",
    "    return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbde788",
   "metadata": {},
   "source": [
    "### వివరణ: ఫాల్‌బ్యాక్ క్లయింట్ ప్రారంభం\n",
    "మునుపటి ప్రారంభం సెల్ దాటిపోయినా లేదా విఫలమైతే కూడా `client` ఉనికిలో ఉందని నిర్ధారిస్తుంది—తరువాతి మూల్యాంకన దశల్లో NameError ని నివారిస్తుంది.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e71f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback client initialization (added after patch failure)\n",
    "try:\n",
    "    client  # type: ignore\n",
    "except NameError:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or 'not-needed')\n",
    "    print('Initialized OpenAI-compatible client (late init).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17386ee",
   "metadata": {},
   "source": [
    "### వివరణ: మూల్యాంకన లూప్ & మెట్రిక్స్  \n",
    "మూల్యాంకన డేటాసెట్‌ను నిర్మిస్తుంది (అవసరమైన కాలమ్స్: ప్రశ్న, సమాధానం, సందర్భాలు, గ్రౌండ్_త్రూత్స్, రిఫరెన్స్) తరువాత ఎంపిక చేసిన రాగాల మెట్రిక్స్‌లపై తిరుగుతుంది.  \n",
    "\n",
    "ఆప్టిమైజేషన్:  \n",
    "- FAST_MODE వేగవంతమైన స్మోక్ టెస్టుల కోసం సమాధాన సంబంధితతకు పరిమితం చేస్తుంది.  \n",
    "- ప్రతి మెట్రిక్ లూప్ ఒక మెట్రిక్ విఫలమైనప్పుడు పూర్తి పునఃగణనను నివారిస్తుంది.  \n",
    "\n",
    "ఫలితంగా మెట్రిక్ -> స్కోరు (విఫలం అయితే NaN) డిక్ట్‌ను ఇస్తుంది.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "521a9163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset columns: ['question', 'answer', 'contexts', 'ground_truths', 'reference']\n",
      "Metrics to compute: ['answer_relevancy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy finished in 78.1s -> 0.6975427764759168\n",
      "RAG evaluation results: {'answer_relevancy': 0.6975427764759168}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.6975427764759168}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build evaluation dataset with required columns (including 'reference' for context_precision)\n",
    "records = []\n",
    "for q, gt in zip(QUESTIONS, GROUND_TRUTH):\n",
    "    ctxs = retrieve(q)\n",
    "    ans = generate(q, ctxs)\n",
    "    records.append({\n",
    "        'question': q,\n",
    "        'answer': ans,\n",
    "        'contexts': ctxs,\n",
    "        'ground_truths': [gt],\n",
    "        'reference': gt\n",
    "    })\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.run_config import RunConfig\n",
    "import math, time, os\n",
    "import numpy as np\n",
    "\n",
    "ragas_llm = ChatOpenAI(model=model_id, base_url=manager.endpoint, api_key=manager.api_key or 'not-needed', temperature=0.0, timeout=60)\n",
    "\n",
    "class LocalEmbeddings:\n",
    "    def embed_documents(self, texts):\n",
    "        return embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=True).tolist()\n",
    "    def embed_query(self, text):\n",
    "        return embedder.encode([text], convert_to_numpy=True, normalize_embeddings=True)[0].tolist()\n",
    "\n",
    "# Fast mode: only answer_relevancy unless RAG_FAST=0\n",
    "FAST_MODE = os.getenv('RAG_FAST','1') == '1'\n",
    "metrics = [answer_relevancy] if FAST_MODE else [answer_relevancy, faithfulness, context_precision]\n",
    "\n",
    "base_timeout = 45 if FAST_MODE else 120\n",
    "\n",
    "ds = Dataset.from_list(records)\n",
    "print('Evaluation dataset columns:', ds.column_names)\n",
    "print('Metrics to compute:', [m.name for m in metrics])\n",
    "\n",
    "results_dict = {}\n",
    "for metric in metrics:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        cfg = RunConfig(timeout=base_timeout, max_workers=1)\n",
    "        partial = evaluate(ds, metrics=[metric], llm=ragas_llm, embeddings=LocalEmbeddings(), run_config=cfg, show_progress=False)\n",
    "        raw_val = partial[metric.name]\n",
    "        if isinstance(raw_val, list):\n",
    "            numeric = [v for v in raw_val if isinstance(v, (int, float))]\n",
    "            score = float(np.nanmean(numeric)) if numeric else math.nan\n",
    "        else:\n",
    "            score = float(raw_val)\n",
    "        results_dict[metric.name] = score\n",
    "    except Exception as e:\n",
    "        results_dict[metric.name] = math.nan\n",
    "        print(f\"Metric {metric.name} failed: {e}\")\n",
    "    finally:\n",
    "        print(f\"{metric.name} finished in {time.time()-t0:.1f}s -> {results_dict[metric.name]}\")\n",
    "\n",
    "print('RAG evaluation results:', results_dict)\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**అస్పష్టత**:  \nఈ పత్రాన్ని AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వానికి ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో పొరపాట్లు లేదా తప్పిదాలు ఉండవచ్చు. అసలు పత్రం దాని స్వదేశీ భాషలోనే అధికారిక మూలంగా పరిగణించాలి. ముఖ్యమైన సమాచారానికి, ప్రొఫెషనల్ మానవ అనువాదం సిఫార్సు చేయబడుతుంది. ఈ అనువాదం వాడకంలో ఏర్పడిన ఏవైనా అపార్థాలు లేదా తప్పుదారుల కోసం మేము బాధ్యత వహించము.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "4895a2e01d85b98643a177c89ff7757f",
   "translation_date": "2025-12-16T01:19:51+00:00",
   "source_file": "Workshop/notebooks/session02_rag_eval_ragas.ipynb",
   "language_code": "te"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}