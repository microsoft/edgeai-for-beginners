<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "00583bdd288194f0c4e7d71363e4c48a",
  "translation_date": "2025-12-15T19:33:18+00:00",
  "source_file": "Module05/02.SLMOps-Distillation.md",
  "language_code": "te"
}
-->
# విభాగం 2: మోడల్ డిస్టిలేషన్ - సిద్ధాంతం నుండి ప్రాక్టీస్ వరకు

## విషయ సూచిక
1. [మోడల్ డిస్టిలేషన్ పరిచయం](../../../Module05)
2. [డిస్టిలేషన్ ఎందుకు ముఖ్యం](../../../Module05)
3. [డిస్టిలేషన్ ప్రక్రియ](../../../Module05)
4. [ప్రాక్టికల్ అమలు](../../../Module05)
5. [Azure ML డిస్టిలేషన్ ఉదాహరణ](../../../Module05)
6. [ఉత్తమ ఆచారాలు మరియు ఆప్టిమైజేషన్](../../../Module05)
7. [వాస్తవ ప్రపంచ అనువర్తనాలు](../../../Module05)
8. [సంక్షేపం](../../../Module05)

## మోడల్ డిస్టిలేషన్ పరిచయం {#introduction}

మోడల్ డిస్టిలేషన్ అనేది పెద్ద, క్లిష్టమైన మోడల్స్ పనితీరును ఎక్కువగా నిలుపుకుంటూ చిన్న, మరింత సమర్థవంతమైన మోడల్స్ సృష్టించడానికి ఉపయోగించే శక్తివంతమైన సాంకేతికత. ఈ ప్రక్రియలో పెద్ద "టీచర్" మోడల్ ప్రవర్తనను అనుకరించడానికి ఒక సంక్షిప్త "స్టూడెంట్" మోడల్‌ను శిక్షణ ఇస్తారు.

**ప్రధాన లాభాలు:**
- **ఇన్ఫరెన్స్ కోసం తగ్గిన గణనాత్మక అవసరాలు**
- **తగ్గిన మెమరీ వినియోగం** మరియు నిల్వ అవసరాలు
- **సమంజసమైన ఖచ్చితత్వం while వేగవంతమైన ఇన్ఫరెన్స్ సమయాలు**
- **సంప్రదాయ పరిమిత వనరులలో ఖర్చు-సమర్థవంతమైన అమలు**

## డిస్టిలేషన్ ఎందుకు ముఖ్యం {#why-distillation-matters}

పెద్ద భాషా మోడల్స్ (LLMs) శక్తివంతంగా మారుతున్నప్పటికీ, అవి వనరుల పరంగా ఎక్కువగా అవసరమవుతున్నాయి. బిలియన్ల పరిమాణాల మోడల్ అద్భుత ఫలితాలు ఇస్తే కూడా, అనేక వాస్తవ ప్రపంచ అనువర్తనాలకు ఇది ప్రాక్టికల్ కాకపోవచ్చు:

### వనరు పరిమితులు
- **గణనాత్మక భారము**: పెద్ద మోడల్స్‌కు గణన శక్తి మరియు GPU మెమరీ ఎక్కువ అవసరం
- **ఇన్ఫరెన్స్ ఆలస్యం**: క్లిష్టమైన మోడల్స్ సమాధానాలు ఇవ్వడానికి ఎక్కువ సమయం తీసుకుంటాయి
- **శక్తి వినియోగం**: పెద్ద మోడల్స్ ఎక్కువ విద్యుత్ వినియోగిస్తాయి, ఆపరేషనల్ ఖర్చులు పెరుగుతాయి
- **ఇన్‌ఫ్రాస్ట్రక్చర్ ఖర్చులు**: పెద్ద మోడల్స్ హోస్టింగ్ కోసం ఖరీదైన హార్డ్వేర్ అవసరం

### ప్రాక్టికల్ పరిమితులు
- **మొబైల్ అమలు**: పెద్ద మోడల్స్ మొబైల్ పరికరాల్లో సమర్థవంతంగా నడవలేవు
- **రియల్-టైమ్ అనువర్తనాలు**: తక్కువ ఆలస్యం అవసరమయ్యే అనువర్తనాలు నెమ్మదిగా ఇన్ఫరెన్స్ చేయలేవు
- **ఎడ్జ్ కంప్యూటింగ్**: IoT మరియు ఎడ్జ్ పరికరాలకు పరిమిత గణన వనరులు ఉంటాయి
- **ఖర్చు పరిమితులు**: అనేక సంస్థలు పెద్ద మోడల్ అమలుకు అవసరమైన వనరులు కలిగి ఉండవు

## డిస్టిలేషన్ ప్రక్రియ {#the-distillation-process}

మోడల్ డిస్టిలేషన్ టీచర్ మోడల్ నుండి స్టూడెంట్ మోడల్‌కు జ్ఞానాన్ని బదిలీ చేసే రెండు దశల ప్రక్రియను అనుసరిస్తుంది:

### దశ 1: సింథటిక్ డేటా ఉత్పత్తి

టీచర్ మోడల్ మీ శిక్షణ డేటాసెట్ కోసం సమాధానాలు ఉత్పత్తి చేస్తుంది, టీచర్ జ్ఞానం మరియు తర్కం నమూనాలను పట్టుకునే ఉన్నత-నాణ్యత సింథటిక్ డేటాను సృష్టిస్తుంది.

```python
# సాంకేతిక డేటా ఉత్పత్తి యొక్క భావనాత్మక ఉదాహరణ
def generate_synthetic_data(teacher_model, training_dataset):
    synthetic_data = []
    for input_sample in training_dataset:
        teacher_response = teacher_model.generate(input_sample)
        synthetic_data.append({
            'input': input_sample,
            'teacher_output': teacher_response
        })
    return synthetic_data
```

**ఈ దశ ముఖ్యాంశాలు:**
- టీచర్ మోడల్ ప్రతి శిక్షణ ఉదాహరణను ప్రాసెస్ చేస్తుంది
- ఉత్పత్తి చేసిన సమాధానాలు స్టూడెంట్ శిక్షణకు "గ్రౌండ్ ట్రూత్" అవుతాయి
- ఈ ప్రక్రియ టీచర్ నిర్ణయాల నమూనాలను పట్టుకుంటుంది
- సింథటిక్ డేటా నాణ్యత స్టూడెంట్ మోడల్ పనితీరును నేరుగా ప్రభావితం చేస్తుంది

### దశ 2: స్టూడెంట్ మోడల్ ఫైన్-ట్యూనింగ్

స్టూడెంట్ మోడల్ సింథటిక్ డేటాసెట్‌పై శిక్షణ పొందుతుంది, టీచర్ ప్రవర్తన మరియు సమాధానాలను అనుకరించడం నేర్చుకుంటుంది.

```python
# విద్యార్థి శిక్షణ యొక్క భావనాత్మక ఉదాహరణ
def train_student_model(student_model, synthetic_data):
    for epoch in range(num_epochs):
        for batch in synthetic_data:
            student_output = student_model(batch['input'])
            loss = compute_loss(student_output, batch['teacher_output'])
            optimizer.step(loss.backward())
    return student_model
```

**శిక్షణ లక్ష్యాలు:**
- స్టూడెంట్ మరియు టీచర్ అవుట్పుట్ల మధ్య తేడాను తగ్గించడం
- టీచర్ జ్ఞానాన్ని చిన్న పరిమాణంలో నిలుపుకోవడం
- మోడల్ క్లిష్టతను తగ్గిస్తూ పనితీరును నిలుపుకోవడం

## ప్రాక్టికల్ అమలు {#practical-implementation}

### టీచర్ మరియు స్టూడెంట్ మోడల్స్ ఎంపిక

**టీచర్ మోడల్ ఎంపిక:**
- మీ నిర్దిష్ట పనిపై నిరూపిత పనితీరు కలిగిన పెద్ద LLMs (100B+ పరిమాణాలు) ఎంచుకోండి
- ప్రముఖ టీచర్ మోడల్స్:
  - **DeepSeek V3** (671B పరిమాణాలు) - తర్కం మరియు కోడ్ ఉత్పత్తికి అద్భుతం
  - **Meta Llama 3.1 405B Instruct** - సమగ్ర సాధారణ-ఉద్దేశ్య సామర్థ్యాలు
  - **GPT-4** - విభిన్న పనులపై బలమైన పనితీరు
  - **Claude 3.5 Sonnet** - క్లిష్టమైన తర్కం పనులకు అద్భుతం
- మీ డొమైన్-స్పెసిఫిక్ డేటాపై టీచర్ మోడల్ బాగా పనిచేస్తుందో నిర్ధారించుకోండి

**స్టూడెంట్ మోడల్ ఎంపిక:**
- మోడల్ పరిమాణం మరియు పనితీరు అవసరాల మధ్య సమతుల్యం
- సమర్థవంతమైన, చిన్న మోడల్స్‌పై దృష్టి పెట్టండి:
  - **Microsoft Phi-4-mini** - తాజా సమర్థవంతమైన మోడల్, బలమైన తర్క సామర్థ్యాలతో
  - Meta Llama 3.1 8B Instruct
  - Microsoft Phi-3 Mini (4K మరియు 128K వేరియంట్లు)
  - Microsoft Phi-3.5 Mini Instruct

### అమలు దశలు

1. **డేటా సిద్ధం**
   ```python
   # మీ శిక్షణ డేటాసెట్‌ను సిద్ధం చేయండి
   training_data = load_dataset("your_training_data.jsonl")
   validation_data = load_dataset("your_validation_data.jsonl")
   ```

2. **టీచర్ మోడల్ సెటప్**
   ```python
   # పెద్ద స్థాయి ఉపాధ్యాయ మోడల్‌ను ప్రారంభించండి (100 బిలియన్+ పరిమాణాలు)
   teacher_model = load_model("deepseek-ai/DeepSeek-V3")
   # ప్రత్యామ్నాయం: teacher_model = load_model("meta-llama/Llama-3.1-405B-Instruct")
   ```

3. **సింథటిక్ డేటా ఉత్పత్తి**
   ```python
   # ఉపాధ్యాయ మోడల్ నుండి ప్రతిస్పందనలు ఉత్పత్తి చేయండి
   synthetic_training_data = generate_teacher_responses(
       teacher_model, 
       training_data
   )
   synthetic_validation_data = generate_teacher_responses(
       teacher_model, 
       validation_data
   )
   ```

4. **స్టూడెంట్ మోడల్ శిక్షణ**
   ```python
   # Phi-4-mini ను విద్యార్థి మోడల్‌గా సరిగ్గా సర్దుబాటు చేయండి
   student_model = load_model("microsoft/Phi-4-mini")
   trained_student = fine_tune_student(
       student_model,
       synthetic_training_data,
       synthetic_validation_data
   )
   ```

## Azure ML డిస్టిలేషన్ ఉదాహరణ {#azure-ml-example}

Azure Machine Learning మోడల్ డిస్టిలేషన్ అమలుకు సమగ్ర వేదికను అందిస్తుంది. మీ డిస్టిలేషన్ వర్క్‌ఫ్లో కోసం Azure ML ను ఎలా ఉపయోగించాలో ఇక్కడ ఉంది:

### ముందస్తు అవసరాలు

1. **Azure ML వర్క్‌స్పేస్**: మీ వర్క్‌స్పేస్‌ను సరైన ప్రాంతంలో సెట్ చేయండి
   - పెద్ద టీచర్ మోడల్స్ (DeepSeek V3, Llama 405B) యాక్సెస్ నిర్ధారించుకోండి
   - మోడల్ అందుబాటును బట్టి ప్రాంతాలను కాన్ఫిగర్ చేయండి

2. **కంప్యూట్ వనరులు**: శిక్షణ కోసం సరైన కంప్యూట్ ఇన్స్టాన్సులను కాన్ఫిగర్ చేయండి
   - టీచర్ మోడల్ ఇన్ఫరెన్స్ కోసం హై-మెమరీ ఇన్స్టాన్సులు
   - స్టూడెంట్ మోడల్ ఫైన్-ట్యూనింగ్ కోసం GPU-సహాయంతో కంప్యూట్

### మద్దతు పొందిన పనుల రకాలు

Azure ML వివిధ పనుల కోసం డిస్టిలేషన్ మద్దతు ఇస్తుంది:

- **నేచురల్ లాంగ్వేజ్ ఇంటర్ప్రిటేషన్ (NLI)**
- **కన్వర్సేషనల్ AI**
- **ప్రశ్న మరియు సమాధానం (QA)**
- **గణిత తర్కం**
- **టెక్స్ట్ సమ్మరీకరణ**

### నమూనా అమలు

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import DistillationJob

# Azure ML క్లయింట్‌ను ప్రారంభించండి
ml_client = MLClient.from_config()

# DeepSeek V3 ను టీచర్‌గా మరియు Phi-4-mini ను స్టూడెంట్‌గా డిస్టిలేషన్ జాబ్‌ను నిర్వచించండి
distillation_job = DistillationJob(
    teacher_model="deepseek-v3",  # పెద్ద స్థాయి టీచర్ మోడల్ (671B పరామితులు)
    student_model="phi-4-mini",   # సమర్థవంతమైన స్టూడెంట్ మోడల్
    training_data="./training_data.jsonl",
    validation_data="./validation_data.jsonl",
    task_type="conversation",
    hyperparameters={
        "learning_rate": 2e-5,    # ఫైన్-ట్యూనింగ్ కోసం తక్కువ లెర్నింగ్ రేట్
        "batch_size": 2,          # మెమరీ సామర్థ్యం కోసం చిన్న బ్యాచ్ సైజ్
        "num_epochs": 3,
        "temperature": 0.7        # టీచర్ అవుట్పుట్ సాఫ్ట్‌నెస్
    }
)

# డిస్టిలేషన్ జాబ్‌ను సమర్పించండి
job = ml_client.jobs.create_or_update(distillation_job)
```

### మానిటరింగ్ మరియు మూల్యాంకనం

```python
# శిక్షణ పురోగతిని పర్యవేక్షించండి
job_status = ml_client.jobs.get(job.name)
print(f"Job status: {job_status.status}")

# డిస్టిల్డ్ Phi-4-mini మోడల్‌ను అంచనా వేయండి
evaluation_results = ml_client.models.evaluate(
    model_name="phi-4-mini-distilled",
    test_data="./test_data.jsonl",
    metrics=["accuracy", "bleu_score", "inference_time"]
)

# అసలు Phi-4-mini బేస్‌లైన్‌తో పోల్చండి
baseline_results = ml_client.models.evaluate(
    model_name="phi-4-mini-baseline",
    test_data="./test_data.jsonl"
)

print(f"Distilled model accuracy: {evaluation_results['accuracy']}")
print(f"Baseline model accuracy: {baseline_results['accuracy']}")
print(f"Performance improvement: {evaluation_results['accuracy'] - baseline_results['accuracy']}")
```

## ఉత్తమ ఆచారాలు మరియు ఆప్టిమైజేషన్ {#best-practices}

### డేటా నాణ్యత

**అత్యున్నత నాణ్యత శిక్షణ డేటా కీలకం:**
- విభిన్న మరియు ప్రాతినిధ్య శిక్షణ ఉదాహరణలు ఉండేలా చూసుకోండి
- సాధ్యమైనంత వరకు డొమైన్-స్పెసిఫిక్ డేటాను ఉపయోగించండి
- స్టూడెంట్ శిక్షణకు ముందు టీచర్ అవుట్పుట్లను ధృవీకరించండి
- స్టూడెంట్ మోడల్ నేర్చుకునే సమయంలో పక్షపాతం నివారించడానికి డేటాసెట్ సమతుల్యం చేయండి

### హైపర్‌పారామీటర్ ట్యూనింగ్

**ఆప్టిమైజ్ చేయవలసిన ముఖ్య పారామీటర్లు:**
- **లెర్నింగ్ రేట్**: ఫైన్-ట్యూనింగ్ కోసం చిన్న రేట్లతో (1e-5 నుండి 5e-5) ప్రారంభించండి
- **బ్యాచ్ సైజ్**: మెమరీ పరిమితులు మరియు శిక్షణ స్థిరత్వం మధ్య సమతుల్యం
- **ఎపోక్స్ సంఖ్య**: ఓవర్‌ఫిట్టింగ్ కోసం పర్యవేక్షించండి; సాధారణంగా 2-5 ఎపోక్స్ సరిపోతాయి
- **టెంపరేచర్ స్కేలింగ్**: జ్ఞాన బదిలీ కోసం టీచర్ అవుట్పుట్ సాఫ్ట్‌నెస్‌ను సర్దుబాటు చేయండి

### మోడల్ ఆర్కిటెక్చర్ పరిగణనలు

**టీచర్-స్టూడెంట్ అనుకూలత:**
- టీచర్ మరియు స్టూడెంట్ మోడల్స్ మధ్య ఆర్కిటెక్చరల్ అనుకూలత నిర్ధారించండి
- మెరుగైన జ్ఞాన బదిలీ కోసం మధ్యంతర లేయర్ మ్యాచ్ చేయడం పరిగణించండి
- అవసరమైతే అటెన్షన్ బదిలీ సాంకేతికతలను ఉపయోగించండి

### మూల్యాంకన వ్యూహాలు

**సమగ్ర మూల్యాంకన దృష్టికోణం:**
```python
# బహుముఖ ప్రమాణాల మూల్యాంకనం
evaluation_metrics = {
    'accuracy': evaluate_accuracy(student_model, test_data),
    'latency': measure_inference_time(student_model),
    'memory_usage': profile_memory_consumption(student_model),
    'task_specific_metrics': evaluate_task_performance(student_model, task_data)
}
```

## వాస్తవ ప్రపంచ అనువర్తనాలు {#real-world-applications}

### మొబైల్ మరియు ఎడ్జ్ అమలు

డిస్టిల్డ్ మోడల్స్ వనరు పరిమిత పరికరాల్లో AI సామర్థ్యాలను సాధ్యమవుతాయి:
- **స్మార్ట్‌ఫోన్ అనువర్తనాలు** రియల్-టైమ్ టెక్స్ట్ ప్రాసెసింగ్‌తో
- **IoT పరికరాలు** స్థానిక ఇన్ఫరెన్స్ నిర్వహణతో
- **ఎంబెడ్డెడ్ సిస్టమ్స్** పరిమిత గణన వనరులతో

### ఖర్చు-సమర్థవంతమైన ఉత్పత్తి వ్యవస్థలు

సంస్థలు ఆపరేషనల్ ఖర్చులను తగ్గించడానికి డిస్టిలేషన్ ఉపయోగిస్తాయి:
- **కస్టమర్ సర్వీస్ చాట్‌బాట్స్** వేగవంతమైన స్పందన సమయాలతో
- **కంటెంట్ మోడరేషన్ సిస్టమ్స్** అధిక వాల్యూమ్‌ను సమర్థవంతంగా ప్రాసెస్ చేస్తాయి
- **రియల్-టైమ్ అనువాద సేవలు** తక్కువ ఆలస్యం అవసరాలతో

### డొమైన్-స్పెసిఫిక్ అనువర్తనాలు

డిస్టిలేషన్ ప్రత్యేక మోడల్స్ సృష్టించడంలో సహాయపడుతుంది:
- **మెడికల్ డయాగ్నోసిస్ సహాయం** గోప్యతా పరిరక్షణతో స్థానిక ఇన్ఫరెన్స్
- **న్యాయ పత్రాల విశ్లేషణ** నిర్దిష్ట న్యాయ డొమైన్‌లకు అనుకూలంగా
- **ఆర్థిక రిస్క్ అంచనా** వేగవంతమైన నిర్ణయ సామర్థ్యాలతో

### కేసు స్టడీ: DeepSeek V3 → Phi-4-mini తో కస్టమర్ సపోర్ట్

ఒక సాంకేతిక సంస్థ తమ కస్టమర్ సపోర్ట్ వ్యవస్థ కోసం డిస్టిలేషన్ అమలు చేసింది:

**అమలు వివరాలు:**
- **టీచర్ మోడల్**: DeepSeek V3 (671B పరిమాణాలు) - క్లిష్టమైన కస్టమర్ ప్రశ్నలకు అద్భుత తర్కం
- **స్టూడెంట్ మోడల్**: Phi-4-mini - వేగవంతమైన ఇన్ఫరెన్స్ మరియు అమలుకు ఆప్టిమైజ్ చేయబడింది
- **శిక్షణ డేటా**: 50,000 కస్టమర్ సపోర్ట్ సంభాషణలు
- **పని**: సాంకేతిక సమస్య పరిష్కారంతో బహుళ-తిరుగుబాటు సంభాషణ సపోర్ట్

**సాధించిన ఫలితాలు:**
- **85% తగ్గింపు** ఇన్ఫరెన్స్ సమయంలో (3.2 సెకన్ల నుండి 0.48 సెకన్లకు)
- **95% తగ్గింపు** మెమరీ అవసరాల్లో (1.2TB నుండి 60GBకి)
- **92% నిలుపుదల** మోడల్ అసలు ఖచ్చితత్వంలో సపోర్ట్ పనులపై
- **60% తగ్గింపు** ఆపరేషనల్ ఖర్చుల్లో
- **మెరుగైన స్కేలబిలిటీ** - ఇప్పుడు 10 రెట్లు ఎక్కువ సమకాలీన వినియోగదారులను నిర్వహించగలదు

**పనితీరు వివరాలు:**
```python
# సరిపోల్చే ప్రమాణాలు
performance_comparison = {
    "DeepSeek V3 (Teacher)": {
        "parameters": "671B",
        "memory_usage": "1.2TB",
        "inference_time": "3.2s",
        "accuracy": "94.5%",
        "throughput": "50 queries/hour"
    },
    "Phi-4-mini (Distilled)": {
        "parameters": "14B",
        "memory_usage": "60GB", 
        "inference_time": "0.48s",
        "accuracy": "87.0%",
        "throughput": "500 queries/hour"
    }
}
```

## సంక్షేపం {#conclusion}

మోడల్ డిస్టిలేషన్ అధునాతన AI సామర్థ్యాలకు ప్రజలందరికీ సులభంగా అందుబాటులో ఉండేలా చేసే కీలక సాంకేతికత. పెద్ద మోడల్స్ పనితీరును ఎక్కువగా నిలుపుకుంటూ చిన్న, సమర్థవంతమైన మోడల్స్ సృష్టించడం ద్వారా, డిస్టిలేషన్ ప్రాక్టికల్ AI అమలుకు పెరుగుతున్న అవసరాన్ని తీర్చుతుంది.

### ముఖ్యాంశాలు

1. **డిస్టిలేషన్ మోడల్ పనితీరు మరియు ప్రాక్టికల్ పరిమితుల మధ్య గ్యాప్‌ను భర్తీ చేస్తుంది**
2. **రెండు దశల ప్రక్రియ టీచర్ నుండి స్టూడెంట్‌కు సమర్థవంతమైన జ్ఞాన బదిలీని నిర్ధారిస్తుంది**
3. **Azure ML డిస్టిలేషన్ వర్క్‌ఫ్లోలను అమలు చేయడానికి బలమైన ఇన్‌ఫ్రాస్ట్రక్చర్ అందిస్తుంది**
4. **సరైన మూల్యాంకనం మరియు ఆప్టిమైజేషన్ విజయవంతమైన డిస్టిలేషన్‌కు అవసరం**
5. **వాస్తవ ప్రపంచ అనువర్తనాలు ఖర్చు, వేగం మరియు ప్రాప్యతలో గణనీయమైన లాభాలను చూపిస్తాయి**

### భవిష్యత్తు దిశలు

ఈ రంగం అభివృద్ధి చెందుతుండగా, మనం ఆశించవచ్చు:
- **మరింత మెరుగైన జ్ఞాన బదిలీ పద్ధతులతో అభివృద్ధి చెందిన డిస్టిలేషన్ సాంకేతికతలు**
- **బహుళ-టీచర్ డిస్టిలేషన్ ద్వారా మెరుగైన స్టూడెంట్ మోడల్ సామర్థ్యాలు**
- **డిస్టిలేషన్ ప్రక్రియ యొక్క ఆటోమేటెడ్ ఆప్టిమైజేషన్**
- **వివిధ ఆర్కిటెక్చర్లు మరియు డొమైన్‌లలో విస్తృత మోడల్ మద్దతు**

మోడల్ డిస్టిలేషన్ సంస్థలకు ఆధునిక భాషా మోడల్స్‌ను వాస్తవిక అమలులో ఉంచే పరిమితులను పాటిస్తూ, విస్తృత అనువర్తనాలు మరియు వాతావరణాలలో అందుబాటులో ఉంచే శక్తిని ఇస్తుంది.


## ➡️ తదుపరి ఏమిటి

- [03: ఫైన్-ట్యూనింగ్ - నిర్దిష్ట పనుల కోసం మోడల్స్ అనుకూలీకరణ](./03.SLMOps-Finetuing.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**అస్పష్టత**:  
ఈ పత్రాన్ని AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వానికి ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో పొరపాట్లు లేదా తప్పిదాలు ఉండవచ్చు. మూల పత్రం దాని స్వదేశీ భాషలో అధికారిక మూలంగా పరిగణించాలి. ముఖ్యమైన సమాచారానికి, ప్రొఫెషనల్ మానవ అనువాదం సిఫార్సు చేయబడుతుంది. ఈ అనువాదం వాడకంలో ఏర్పడిన ఏవైనా అపార్థాలు లేదా తప్పుదారుల కోసం మేము బాధ్యత వహించము.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->