<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "35b57399d00d5d7cf4a78fa05f5b631e",
  "translation_date": "2025-12-15T23:13:17+00:00",
  "source_file": "Module04/04.openvino.md",
  "language_code": "te"
}
-->
# విభాగం 4 : OpenVINO టూల్‌కిట్ ఆప్టిమైజేషన్ సూట్

## విషయ సూచిక
1. [పరిచయం](../../../Module04)
2. [OpenVINO అంటే ఏమిటి?](../../../Module04)
3. [ఇన్‌స్టాలేషన్](../../../Module04)
4. [త్వరిత ప్రారంభ గైడ్](../../../Module04)
5. [ఉదాహరణ: OpenVINOతో మోడల్స్ మార్చడం మరియు ఆప్టిమైజ్ చేయడం](../../../Module04)
6. [అధునాతన వినియోగం](../../../Module04)
7. [ఉత్తమ ఆచారాలు](../../../Module04)
8. [సమస్య పరిష్కారం](../../../Module04)
9. [అదనపు వనరులు](../../../Module04)

## పరిచయం

OpenVINO (Open Visual Inference and Neural Network Optimization) అనేది క్లౌడ్, ఆన్-ప్రెమైసెస్ మరియు ఎడ్జ్ వాతావరణాలలో ప్రదర్శనాత్మక AI పరిష్కారాలను అమలు చేయడానికి Intel యొక్క ఓపెన్-సోర్స్ టూల్‌కిట్. మీరు CPUలు, GPUలు, VPUలు లేదా ప్రత్యేక AI యాక్సిలరేటర్లను లక్ష్యంగా పెట్టుకున్నా, OpenVINO మోడల్ ఖచ్చితత్వాన్ని నిలుపుకుంటూ సమగ్ర ఆప్టిమైజేషన్ సామర్థ్యాలను అందిస్తుంది మరియు క్రాస్-ప్లాట్‌ఫారమ్ అమలును సులభతరం చేస్తుంది.

## OpenVINO అంటే ఏమిటి?

OpenVINO అనేది డెవలపర్లకు విభిన్న హార్డ్‌వేర్ ప్లాట్‌ఫారమ్‌లపై AI మోడల్స్‌ను సమర్థవంతంగా ఆప్టిమైజ్ చేయడం, మార్చడం మరియు అమలు చేయడానికి సహాయపడే ఓపెన్-సోర్స్ టూల్‌కిట్. ఇది మూడు ప్రధాన భాగాలుగా ఉంటుంది: ఇన్ఫరెన్స్ కోసం OpenVINO రన్‌టైమ్, మోడల్ ఆప్టిమైజేషన్ కోసం న్యూరల్ నెట్‌వర్క్ కంప్రెషన్ ఫ్రేమ్‌వర్క్ (NNCF), మరియు స్కేలబుల్ అమలుకు OpenVINO మోడల్ సర్వర్.

### ముఖ్య లక్షణాలు

- **క్రాస్-ప్లాట్‌ఫారమ్ అమలు**: లినక్స్, విండోస్, మరియు మాక్‌ఓఎస్‌ను Python, C++, మరియు C APIs తో మద్దతు
- **హార్డ్‌వేర్ యాక్సిలరేషన్**: CPU, GPU, VPU, మరియు AI యాక్సిలరేటర్ల కోసం ఆటోమేటిక్ డివైస్ కనుగొనడం మరియు ఆప్టిమైజేషన్
- **మోడల్ కంప్రెషన్ ఫ్రేమ్‌వర్క్**: NNCF ద్వారా అధునాతన క్వాంటైజేషన్, ప్రూనింగ్, మరియు ఆప్టిమైజేషన్ సాంకేతికతలు
- **ఫ్రేమ్‌వర్క్ అనుకూలత**: TensorFlow, ONNX, PaddlePaddle, మరియు PyTorch మోడల్స్‌కు ప్రత్యక్ష మద్దతు
- **జెనరేటివ్ AI మద్దతు**: పెద్ద భాషా మోడల్స్ మరియు జెనరేటివ్ AI అప్లికేషన్ల కోసం ప్రత్యేక OpenVINO GenAI

### లాభాలు

- **ప్రదర్శన ఆప్టిమైజేషన్**: కనిష్ట ఖచ్చితత్వ నష్టంతో గణనీయమైన వేగం మెరుగుదలలు
- **తగ్గిన అమలు ఫుట్‌ప్రింట్**: కనిష్ట బాహ్య ఆధారాలు ఇన్‌స్టాలేషన్ మరియు అమలును సులభతరం చేస్తాయి
- **మెరుగైన స్టార్ట్-అప్ సమయం**: వేగవంతమైన అప్లికేషన్ ప్రారంభానికి ఆప్టిమైజ్ చేసిన మోడల్ లోడింగ్ మరియు క్యాచింగ్
- **స్కేలబుల్ అమలు**: ఎడ్జ్ డివైసుల నుండి క్లౌడ్ ఇన్‌ఫ్రాస్ట్రక్చర్ వరకు సुसంగత APIs తో
- **ఉత్పత్తి సిద్ధం**: సమగ్ర డాక్యుమెంటేషన్ మరియు కమ్యూనిటీ మద్దతుతో ఎంటర్‌ప్రైజ్-గ్రేడ్ నమ్మకదారితనం

## ఇన్‌స్టాలేషన్

### ముందస్తు అవసరాలు

- Python 3.8 లేదా అంతకంటే పైగా
- pip ప్యాకేజ్ మేనేజర్
- వర్చువల్ ఎన్విరాన్‌మెంట్ (సిఫార్సు చేయబడింది)
- అనుకూల హార్డ్‌వేర్ (Intel CPUలు సిఫార్సు చేయబడినవి, కానీ వివిధ ఆర్కిటెక్చర్లకు మద్దతు)

### ప్రాథమిక ఇన్‌స్టాలేషన్

వర్చువల్ ఎన్విరాన్‌మెంట్ సృష్టించి యాక్టివేట్ చేయండి:

```bash
# వర్చువల్ ఎన్విరాన్‌మెంట్ సృష్టించండి
python -m venv openvino-env

# వర్చువల్ ఎన్విరాన్‌మెంట్‌ను యాక్టివేట్ చేయండి
# విండోస్‌లో:
openvino-env\Scripts\activate
# మాక్‌ఓఎస్/లినక్స్‌లో:
source openvino-env/bin/activate
```

OpenVINO రన్‌టైమ్ ఇన్‌స్టాల్ చేయండి:

```bash
pip install openvino
```

మోడల్ ఆప్టిమైజేషన్ కోసం NNCF ఇన్‌స్టాల్ చేయండి:

```bash
pip install nncf
```

### OpenVINO GenAI ఇన్‌స్టాలేషన్

జెనరేటివ్ AI అప్లికేషన్ల కోసం:

```bash
pip install openvino-genai
```

### ఐచ్ఛిక ఆధారాలు

ప్రత్యేక వినియోగాల కోసం అదనపు ప్యాకేజీలు:

```bash
# జూపిటర్ నోట్‌బుక్స్ మరియు అభివృద్ధి సాధనాల కోసం
pip install openvino[dev]

# టెన్సర్‌ఫ్లో మోడల్ మద్దతు కోసం
pip install openvino[tensorflow]

# పైటార్చ్ మోడల్ మద్దతు కోసం
pip install openvino[pytorch]

# ONNX మోడల్ మద్దతు కోసం
pip install openvino[onnx]
```

### ఇన్‌స్టాలేషన్ నిర్ధారణ

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

విజయవంతమైతే, మీరు OpenVINO వెర్షన్ సమాచారం చూడగలరు.

## త్వరిత ప్రారంభ గైడ్

### మీ మొదటి మోడల్ ఆప్టిమైజేషన్

OpenVINO ఉపయోగించి Hugging Face మోడల్‌ను మార్చి ఆప్టిమైజ్ చేద్దాం:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# మోడల్‌ను లోడ్ చేసి OpenVINO IR ఫార్మాట్‌కు మార్చండి
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# టోకనైజర్‌ను లోడ్ చేయండి
tokenizer = AutoTokenizer.from_pretrained(model_id)

# మార్చిన మోడల్‌ను సేవ్ చేయండి
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# ఇన్ఫరెన్స్ కోసం లోడ్ చేసి కంపైల్ చేయండి
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # లేదా "GPU", "AUTO"
)

# ఇన్ఫరెన్స్ పైప్‌లైన్‌ను సృష్టించండి
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### ఈ ప్రక్రియ ఏమి చేస్తుంది

ఆప్టిమైజేషన్ వర్క్‌ఫ్లోలో: Hugging Face నుండి అసలు మోడల్ లోడ్ చేయడం, OpenVINO ఇంటర్మీడియట్ రిప్రజెంటేషన్ (IR) ఫార్మాట్‌కు మార్చడం, డిఫాల్ట్ ఆప్టిమైజేషన్లు వర్తించడం, మరియు లక్ష్య హార్డ్‌వేర్ కోసం కంపైల్ చేయడం.

### ముఖ్య పారామితులు వివరణ

- `export=True`: మోడల్‌ను OpenVINO IR ఫార్మాట్‌కు మార్చడం
- `compile=False`: కంపైల్‌ను రన్‌టైమ్ వరకు ఆలస్యం చేయడం, ఫ్లెక్సిబిలిటీ కోసం
- `device`: లక్ష్య హార్డ్‌వేర్ ("CPU", "GPU", "AUTO" ఆటోమేటిక్ ఎంపిక కోసం)
- `save_pretrained()`: ఆప్టిమైజ్ చేసిన మోడల్‌ను పునఃఉపయోగం కోసం సేవ్ చేయడం

## ఉదాహరణ: OpenVINOతో మోడల్స్ మార్చడం మరియు ఆప్టిమైజ్ చేయడం

### దశ 1: NNCF క్వాంటైజేషన్‌తో మోడల్ మార్చడం

NNCF ఉపయోగించి పోస్ట్-ట్రైనింగ్ క్వాంటైజేషన్ ఎలా చేయాలో ఇక్కడ ఉంది:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# క్వాంటైజేషన్ కోసం NNCF ను ప్రారంభించండి
model_id = "microsoft/DialoGPT-small"

# OpenVINO ఫార్మాట్‌లో మోడల్‌ను లోడ్ చేయండి
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# క్వాంటైజేషన్ కోసం క్యాలిబ్రేషన్ డేటాసెట్‌ను సృష్టించండి
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# పోస్ట్-ట్రైనింగ్ క్వాంటైజేషన్‌ను వర్తింపజేయండి
core = Core()
model = core.read_model(ov_model.model_path)

# క్వాంటైజేషన్‌ను కాన్ఫిగర్ చేయండి
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # బ్యాచ్_సైజ్, సీక్వెన్స్_లెంగ్త్
        type="long"
    )
)

# క్వాంటైజ్డ్ మోడల్‌ను సృష్టించండి
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# క్వాంటైజ్డ్ మోడల్‌ను సేవ్ చేయండి
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### దశ 2: వెయిట్ కంప్రెషన్‌తో అధునాతన ఆప్టిమైజేషన్

ట్రాన్స్‌ఫార్మర్-ఆధారిత మోడల్స్ కోసం వెయిట్ కంప్రెషన్ వర్తించండి:

```python
import nncf
from openvino import Core

# మోడల్ లోడ్ చేయండి
core = Core()
model = core.read_model("models/dialogpt-openvino")

# LLMs కోసం వెయిట్ కంప్రెషన్ వర్తించండి
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # లేదా INT4_ASYM, INT8
    ratio=0.8,  # కంప్రెషన్ నిష్పత్తి
    group_size=128  # క్వాంటైజేషన్ కోసం గ్రూప్ పరిమాణం
)

# కంప్రెస్ చేసిన మోడల్ సేవ్ చేయండి
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### దశ 3: ఆప్టిమైజ్ చేసిన మోడల్‌తో ఇన్ఫరెన్స్

```python
from openvino import Core
import numpy as np

# OpenVINO కోర్‌ను ప్రారంభించండి
core = Core()

# ఆప్టిమైజ్ చేసిన మోడల్‌ను లోడ్ చేయండి
model = core.read_model("models/dialogpt-compressed.xml")

# లక్ష్య పరికరం కోసం మోడల్‌ను కంపైల్ చేయండి
compiled_model = core.compile_model(model, "CPU")

# ఇన్‌పుట్/ఆుట్‌పుట్ సమాచారం పొందండి
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# ఇన్‌పుట్ డేటాను సిద్ధం చేయండి
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# ఇన్ఫరెన్స్‌ను నడపండి
result = compiled_model([tokens])[output_layer]

# అవుట్‌పుట్‌ను డీకోడ్ చేయండి
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### అవుట్‌పుట్ నిర్మాణం

ఆప్టిమైజేషన్ తర్వాత, మీ మోడల్ డైరెక్టరీలో ఈ ఫైళ్లు ఉంటాయి:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## అధునాతన వినియోగం

### NNCF YAML తో కాన్ఫిగరేషన్

సంక్లిష్ట ఆప్టిమైజేషన్ వర్క్‌ఫ్లోల కోసం NNCF కాన్ఫిగరేషన్ ఫైళ్లను ఉపయోగించండి:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

కాన్ఫిగరేషన్ వర్తించండి:

```python
import nncf
from openvino import Core

# మోడల్ మరియు కాన్ఫిగ్‌ను లోడ్ చేయండి
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# కంప్రెషన్‌ను వర్తించండి
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU ఆప్టిమైజేషన్

GPU యాక్సిలరేషన్ కోసం:

```python
from optimum.intel import OVModelForCausalLM

# GPU పరికరంతో మోడల్‌ను లోడ్ చేయండి
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# అధిక ఉత్పాదకత కోసం కాన్ఫిగర్ చేయండి
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### బ్యాచ్ ప్రాసెసింగ్ ఆప్టిమైజేషన్

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# బ్యాచ్ ప్రాసెసింగ్ కోసం కాన్ఫిగర్ చేయండి
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# బహుళ ఇన్‌పుట్‌లను ప్రాసెస్ చేయండి
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### మోడల్ సర్వర్ అమలు

OpenVINO మోడల్ సర్వర్‌తో ఆప్టిమైజ్ చేసిన మోడల్స్‌ను అమలు చేయండి:

```bash
# OpenVINO మోడల్ సర్వర్‌ను ఇన్‌స్టాల్ చేయండి
pip install ovms

# మోడల్ సర్వర్‌ను ప్రారంభించండి
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

మోడల్ సర్వర్ కోసం క్లయింట్ కోడ్:

```python
import requests
import json

# అభ్యర్థన సిద్ధం చేయండి
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # టోకెన్ IDలు
    }
}

# మోడల్ సర్వర్‌కు అభ్యర్థన పంపండి
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## ఉత్తమ ఆచారాలు

### 1. మోడల్ ఎంపిక మరియు సిద్ధం
- మద్దతు ఉన్న ఫ్రేమ్‌వర్క్‌ల (PyTorch, TensorFlow, ONNX) నుండి మోడల్స్ ఉపయోగించండి
- మోడల్ ఇన్‌పుట్స్‌కు స్థిరమైన లేదా తెలిసిన డైనమిక్ ఆకారాలు ఉండేలా చూసుకోండి
- కాలిబ్రేషన్ కోసం ప్రాతినిధ్యాత్మక డేటాసెట్‌లతో పరీక్షించండి

### 2. ఆప్టిమైజేషన్ వ్యూహం ఎంపిక
- **పోస్ట్-ట్రైనింగ్ క్వాంటైజేషన్**: త్వరిత ఆప్టిమైజేషన్ కోసం ఇక్కడ ప్రారంభించండి
- **వెయిట్ కంప్రెషన్**: పెద్ద భాషా మోడల్స్ మరియు ట్రాన్స్‌ఫార్మర్లకు అనుకూలం
- **క్వాంటైజేషన్-అవేర్ ట్రైనింగ్**: ఖచ్చితత్వం ముఖ్యమైనప్పుడు ఉపయోగించండి

### 3. హార్డ్‌వేర్-ప్రత్యేక ఆప్టిమైజేషన్
- **CPU**: సమతుల్య ప్రదర్శన కోసం INT8 క్వాంటైజేషన్ ఉపయోగించండి
- **GPU**: FP16 ఖచ్చితత్వం మరియు బ్యాచ్ ప్రాసెసింగ్ ఉపయోగించండి
- **VPU**: మోడల్ సరళీకరణ మరియు లేయర్ ఫ్యూజన్ పై దృష్టి పెట్టండి

### 4. ప్రదర్శన ట్యూనింగ్
- **థ్రూపుట్ మోడ్**: అధిక వాల్యూమ్ బ్యాచ్ ప్రాసెసింగ్ కోసం
- **లేటెన్సీ మోడ్**: రియల్-టైమ్ ఇంటరాక్టివ్ అప్లికేషన్ల కోసం
- **AUTO డివైస్**: OpenVINO ఉత్తమ హార్డ్‌వేర్‌ను ఎంచుకోనివ్వండి

### 5. మెమరీ నిర్వహణ
- మెమరీ ఓవర్‌హెడ్ నివారించడానికి డైనమిక్ ఆకారాలను జాగ్రత్తగా ఉపయోగించండి
- వేగవంతమైన తదుపరి లోడింగ్ కోసం మోడల్ క్యాచింగ్ అమలు చేయండి
- ఆప్టిమైజేషన్ సమయంలో మెమరీ వినియోగాన్ని పర్యవేక్షించండి

### 6. ఖచ్చితత్వ ధృవీకరణ
- ఎప్పుడూ ఆప్టిమైజ్ చేసిన మోడల్స్‌ను అసలు ప్రదర్శనతో సరిపోల్చండి
- మూల్యాంకన కోసం ప్రాతినిధ్యాత్మక టెస్ట్ డేటాసెట్‌లను ఉపయోగించండి
- క్రమంగా ఆప్టిమైజేషన్ (సావధానమైన సెట్టింగ్స్‌తో ప్రారంభించండి) పరిగణించండి

## సమస్య పరిష్కారం

### సాధారణ సమస్యలు

#### 1. ఇన్‌స్టాలేషన్ సమస్యలు
```bash
# పిప్ క్యాషేను క్లియర్ చేసి మళ్లీ ఇన్‌స్టాల్ చేయండి
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. మోడల్ మార్చే లోపాలు
```python
# మోడల్ అనుకూలతను తనిఖీ చేయండి
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. ప్రదర్శన సమస్యలు
```python
# పనితీరు సూచనలను ప్రారంభించండి
config = {
    "PERFORMANCE_HINT": "LATENCY",  # లేదా "థ్రూపుట్"
    "INFERENCE_PRECISION_HINT": "f32"  # లేదా "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. మెమరీ సమస్యలు
- ఆప్టిమైజేషన్ సమయంలో మోడల్ బ్యాచ్ సైజ్ తగ్గించండి
- పెద్ద డేటాసెట్‌ల కోసం స్ట్రీమింగ్ ఉపయోగించండి
- మోడల్ క్యాచింగ్ ఎనేబుల్ చేయండి: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. ఖచ్చితత్వ తగ్గుదల
- ఎక్కువ ఖచ్చితత్వం కోసం (INT4 కాకుండా INT8) ఉపయోగించండి
- కాలిబ్రేషన్ డేటాసెట్ పరిమాణం పెంచండి
- మిక్స్‌డ్ ప్రిసిషన్ ఆప్టిమైజేషన్ వర్తించండి

### ప్రదర్శన పర్యవేక్షణ

```python
# అంచనా పనితీరు పర్యవేక్షించండి
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### సహాయం పొందడం

- **డాక్యుమెంటేషన్**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub Issues**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **కమ్యూనిటీ ఫోరం**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## అదనపు వనరులు

### అధికారిక లింకులు
- **OpenVINO హోమ్‌పేజీ**: [openvino.ai](https://openvino.ai/)
- **GitHub రిపోజిటరీ**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF రిపోజిటరీ**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **మోడల్ జూ**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### నేర్చుకునే వనరులు
- **OpenVINO నోట్‌బుక్స్**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **త్వరిత ప్రారంభ గైడ్**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **ఆప్టిమైజేషన్ గైడ్**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### ఇంటిగ్రేషన్ టూల్స్
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO మోడల్ సర్వర్**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### ప్రదర్శన బెంచ్‌మార్కులు
- **అధికారిక బెంచ్‌మార్కులు**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF మోడల్ జూ**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### కమ్యూనిటీ ఉదాహరణలు
- **జుపిటర్ నోట్‌బుక్స్**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - OpenVINO నోట్‌బుక్స్ రిపోజిటరీలో సమగ్ర ట్యుటోరియల్స్ అందుబాటులో ఉన్నాయి
- **సాంపిల్ అప్లికేషన్లు**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - వివిధ డొమైన్‌ల కోసం వాస్తవ ప్రపంచ ఉదాహరణలు (కంప్యూటర్ విజన్, NLP, ఆడియో)
- **బ్లాగ్ పోస్టులు**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - Intel AI మరియు కమ్యూనిటీ బ్లాగ్ పోస్టులు వివరణాత్మక వినియోగాలతో

### సంబంధిత టూల్స్
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - Intel హార్డ్‌వేర్ కోసం అదనపు ఆప్టిమైజేషన్ సాంకేతికతలు
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - మొబైల్ మరియు ఎడ్జ్ అమలుల కోసం పోలికలు
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - క్రాస్-ప్లాట్‌ఫారమ్ ఇన్ఫరెన్స్ ఇంజిన్ ప్రత్యామ్నాయాలు

## ➡️ తదుపరి ఏమిటి

- [05: Apple MLX Framework లో లోతైన అవగాహన](./05.AppleMLX.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**అస్పష్టత**:  
ఈ పత్రాన్ని AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వానికి ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో పొరపాట్లు లేదా తప్పిదాలు ఉండవచ్చు. మూల పత్రం దాని స్వదేశీ భాషలో అధికారిక మూలంగా పరిగణించాలి. ముఖ్యమైన సమాచారానికి, ప్రొఫెషనల్ మానవ అనువాదం సిఫార్సు చేయబడుతుంది. ఈ అనువాదం వాడకంలో ఏర్పడిన ఏవైనా అపార్థాలు లేదా తప్పుదారితీసే అర్థాలు కోసం మేము బాధ్యత వహించము.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->