<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-12-15T23:06:30+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "te"
}
-->
# Section 3 : Microsoft Olive Optimization Suite

## Table of Contents
1. [Introduction](../../../Module04)
2. [What is Microsoft Olive?](../../../Module04)
3. [Installation](../../../Module04)
4. [Quick Start Guide](../../../Module04)
5. [Example: Converting Qwen3 to ONNX INT4](../../../Module04)
6. [Advanced Usage](../../../Module04)
7. [Olive Recipes Repository](../../../Module04)
8. [Best Practices](../../../Module04)
9. [Troubleshooting](../../../Module04)
10. [Additional Resources](../../../Module04)

## Introduction

Microsoft Olive అనేది శక్తివంతమైన, సులభంగా ఉపయోగించగల హార్డ్‌వేర్-అవేర్ మోడల్ ఆప్టిమైజేషన్ టూల్‌కిట్, ఇది వివిధ హార్డ్‌వేర్ ప్లాట్‌ఫారమ్‌లపై మిషన్ లెర్నింగ్ మోడల్స్‌ను డిప్లాయ్ చేయడానికి ఆప్టిమైజ్ చేయడం సులభతరం చేస్తుంది. మీరు CPUలు, GPUలు లేదా ప్రత్యేక AI యాక్సిలరేటర్లను లక్ష్యంగా పెట్టుకున్నా, Olive మీకు మోడల్ ఖచ్చితత్వాన్ని నిలుపుకుంటూ ఉత్తమ పనితీరును సాధించడంలో సహాయపడుతుంది.

## What is Microsoft Olive?

Olive అనేది సులభంగా ఉపయోగించగల హార్డ్‌వేర్-అవేర్ మోడల్ ఆప్టిమైజేషన్ టూల్, ఇది మోడల్ కంప్రెషన్, ఆప్టిమైజేషన్ మరియు కంపైలేషన్‌లో పరిశ్రమలో అగ్రగామి సాంకేతికతలను కలిపి పనిచేస్తుంది. ఇది ONNX Runtime తో E2E ఇన్ఫరెన్స్ ఆప్టిమైజేషన్ పరిష్కారంగా పనిచేస్తుంది.

### Key Features

- **హార్డ్‌వేర్-అవేర్ ఆప్టిమైజేషన్**: మీ లక్ష్య హార్డ్‌వేర్ కోసం ఉత్తమ ఆప్టిమైజేషన్ సాంకేతికతలను ఆటోమేటిక్‌గా ఎంచుకుంటుంది
- **40+ బిల్ట్-ఇన్ ఆప్టిమైజేషన్ కంపోనెంట్లు**: మోడల్ కంప్రెషన్, క్వాంటైజేషన్, గ్రాఫ్ ఆప్టిమైజేషన్ మరియు మరిన్ని కవర్ చేస్తుంది
- **సులభ CLI ఇంటర్‌ఫేస్**: సాధారణ ఆప్టిమైజేషన్ పనుల కోసం సింపుల్ కమాండ్లు
- **మల్టీ-ఫ్రేమ్‌వర్క్ సపోర్ట్**: PyTorch, Hugging Face మోడల్స్ మరియు ONNX తో పనిచేస్తుంది
- **పాపులర్ మోడల్ సపోర్ట్**: Olive ఆటోమేటిక్‌గా Llama, Phi, Qwen, Gemma వంటి ప్రాచుర్యం పొందిన మోడల్ ఆర్కిటెక్చర్లను బాక్స్ నుండి ఆప్టిమైజ్ చేయగలదు

### Benefits

- **డెవలప్‌మెంట్ సమయం తగ్గింపు**: వివిధ ఆప్టిమైజేషన్ సాంకేతికతలతో మాన్యువల్ ప్రయోగం అవసరం లేదు
- **పనితీరు మెరుగుదల**: కొన్ని సందర్భాల్లో 6 రెట్లు వేగవంతం
- **క్రాస్-ప్లాట్‌ఫారమ్ డిప్లాయ్‌మెంట్**: ఆప్టిమైజ్ చేసిన మోడల్స్ వివిధ హార్డ్‌వేర్ మరియు ఆపరేటింగ్ సిస్టమ్స్‌లో పనిచేస్తాయి
- **ఖచ్చితత్వం నిలుపుకోవడం**: ఆప్టిమైజేషన్లు పనితీరును మెరుగుపరచడంలో మోడల్ నాణ్యతను కాపాడుతాయి

## Installation

### Prerequisites

- Python 3.8 లేదా అంతకంటే పైగా
- pip ప్యాకేజ్ మేనేజర్
- వర్చువల్ ఎన్విరాన్‌మెంట్ (సిఫార్సు చేయబడింది)

### Basic Installation

వర్చువల్ ఎన్విరాన్‌మెంట్ సృష్టించి యాక్టివేట్ చేయండి:

```bash
# వర్చువల్ ఎన్విరాన్‌మెంట్ సృష్టించండి
python -m venv olive-env

# వర్చువల్ ఎన్విరాన్‌మెంట్‌ను యాక్టివేట్ చేయండి
# విండోస్‌లో:
olive-env\Scripts\activate
# మాక్‌ఓఎస్/లినక్స్‌లో:
source olive-env/bin/activate
```

ఆటో-ఆప్టిమైజేషన్ ఫీచర్లతో Olive ఇన్‌స్టాల్ చేయండి:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Optional Dependencies

అదనపు ఫీచర్ల కోసం Olive వివిధ ఐచ్ఛిక డిపెండెన్సీలను అందిస్తుంది:

```bash
# Azure ML సమీకరణ కోసం
pip install olive-ai[azureml]

# DirectML (విండోస్ GPU వేగవంతీకరణ) కోసం
pip install olive-ai[directml]

# CPU ఆప్టిమైజేషన్ కోసం
pip install olive-ai[cpu]

# అన్ని ఫీచర్ల కోసం
pip install olive-ai[all]
```

### Verify Installation

```bash
olive --help
```

సఫలమైతే, మీరు Olive CLI సహాయం సందేశాన్ని చూడగలరు.

## Quick Start Guide

### Your First Optimization

Olive ఆటో-ఆప్టిమైజేషన్ ఫీచర్ ఉపయోగించి చిన్న భాషా మోడల్‌ను ఆప్టిమైజ్ చేద్దాం:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### What This Command Does

ఆప్టిమైజేషన్ ప్రక్రియలో: మోడల్‌ను లోకల్ క్యాష్ నుండి పొందడం, ONNX గ్రాఫ్‌ను క్యాప్చర్ చేసి ONNX డేటా ఫైల్‌లో వెయిట్స్ నిల్వ చేయడం, ONNX గ్రాఫ్‌ను ఆప్టిమైజ్ చేయడం, మరియు RTN పద్ధతితో మోడల్‌ను int4 కు క్వాంటైజ్ చేయడం జరుగుతుంది.

### Command Parameters Explained

- `--model_name_or_path`: Hugging Face మోడల్ గుర్తింపు లేదా లోకల్ పాథ్
- `--output_path`: ఆప్టిమైజ్ చేసిన మోడల్ సేవ్ చేయబడే డైరెక్టరీ
- `--device`: లక్ష్య పరికరం (cpu, gpu)
- `--provider`: ఎగ్జిక్యూషన్ ప్రొవైడర్ (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ఇన్ఫరెన్స్ కోసం ONNX Runtime Generate AI ఉపయోగించండి
- `--precision`: క్వాంటైజేషన్ ప్రిసిషన్ (int4, int8, fp16)
- `--log_level`: లాగింగ్ verbosity (0=కనిష్టం, 1=విస్తృతం)

## Example: Converting Qwen3 to ONNX INT4

[lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) వద్ద అందించిన Hugging Face ఉదాహరణ ఆధారంగా, Qwen3 మోడల్‌ను ఎలా ఆప్టిమైజ్ చేయాలో ఇక్కడ ఉంది:

### Step 1: Download Model (Optional)

డౌన్లోడ్ సమయాన్ని తగ్గించడానికి, అవసరమైన ఫైళ్లను మాత్రమే క్యాష్ చేయండి:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Step 2: Optimize Qwen3 Model

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Step 3: Test the Optimized Model

మీ ఆప్టిమైజ్ చేసిన మోడల్‌ను పరీక్షించడానికి సింపుల్ Python స్క్రిప్ట్ సృష్టించండి:

```python
import onnxruntime_genai as og

# ఆప్టిమైజ్ చేసిన మోడల్‌ను లోడ్ చేయండి
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# చాట్ టెంప్లేట్‌ను సృష్టించండి
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# టెక్స్ట్‌ను ఉత్పత్తి చేయండి
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Output Structure

ఆప్టిమైజేషన్ తర్వాత, మీ అవుట్‌పుట్ డైరెక్టరీలో ఈ ఫైళ్లు ఉంటాయి:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Advanced Usage

### Configuration Files

సంక్లిష్టమైన ఆప్టిమైజేషన్ వర్క్‌ఫ్లోల కోసం, మీరు JSON కాన్ఫిగరేషన్ ఫైళ్లను ఉపయోగించవచ్చు:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

కాన్ఫిగరేషన్‌తో రన్ చేయండి:

```bash
olive run --config config.json
```

### GPU Optimization

CUDA GPU ఆప్టిమైజేషన్ కోసం:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (విండోస్) కోసం:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fine-tuning with Olive

Olive మోడల్స్‌ను ఫైన్-ట్యూన్ చేయడాన్ని కూడా మద్దతు ఇస్తుంది:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Best Practices

### 1. Model Selection
- పరీక్ష కోసం చిన్న మోడల్స్‌తో ప్రారంభించండి (ఉదా: 0.5B-7B పరిమాణాలు)
- మీ లక్ష్య మోడల్ ఆర్కిటెక్చర్ Olive మద్దతు ఉన్నదని నిర్ధారించుకోండి

### 2. Hardware Considerations
- మీ ఆప్టిమైజేషన్ లక్ష్యాన్ని డిప్లాయ్‌మెంట్ హార్డ్‌వేర్‌కు సరిపోల్చండి
- CUDA-సమర్థ హార్డ్‌వేర్ ఉంటే GPU ఆప్టిమైజేషన్ ఉపయోగించండి
- విండోస్ యంత్రాల కోసం ఇంటిగ్రేటెడ్ గ్రాఫిక్స్‌తో DirectML పరిగణించండి

### 3. Precision Selection
- **INT4**: గరిష్ట కంప్రెషన్, కొంత ఖచ్చితత్వ నష్టం
- **INT8**: పరిమాణం మరియు ఖచ్చితత్వం మధ్య మంచి సమతుల్యం
- **FP16**: తక్కువ ఖచ్చితత్వ నష్టం, మోస్తరు పరిమాణ తగ్గింపు

### 4. Testing and Validation
- మీ ప్రత్యేక ఉపయోగాల కోసం ఆప్టిమైజ్ చేసిన మోడల్స్‌ను ఎప్పుడూ పరీక్షించండి
- పనితీరు మెట్రిక్స్ (లేటెన్సీ, థ్రూపుట్, ఖచ్చితత్వం)ని పోల్చండి
- మూల్యాంకన కోసం ప్రాతినిధ్య ఇన్‌పుట్ డేటాను ఉపయోగించండి

### 5. Iterative Optimization
- త్వరిత ఫలితాల కోసం ఆటో-ఆప్టిమైజేషన్‌తో ప్రారంభించండి
- సన్నిహిత నియంత్రణ కోసం కాన్ఫిగరేషన్ ఫైళ్లను ఉపయోగించండి
- వివిధ ఆప్టిమైజేషన్ పాస్లతో ప్రయోగాలు చేయండి

## Troubleshooting

### Common Issues

#### 1. Installation Problems
```bash
# మీరు ఆధారిత సంబంధిత విరుద్ధతలను ఎదుర్కొంటే:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU Issues
```bash
# CUDA ఇన్‌స్టాలేషన్‌ను ధృవీకరించండి:
nvidia-smi

# సరైన ONNX Runtime GPU ప్యాకేజీని ఇన్‌స్టాల్ చేయండి:
pip install onnxruntime-gpu
```

#### 3. Memory Issues
- ఆప్టిమైజేషన్ సమయంలో చిన్న బ్యాచ్ సైజ్‌లను ఉపయోగించండి
- మొదట ఎక్కువ ప్రిసిషన్‌తో క్వాంటైజేషన్ ప్రయత్నించండి (int8, int4 కాకుండా)
- మోడల్ క్యాషింగ్ కోసం సరిపడా డిస్క్ స్థలం ఉన్నదని నిర్ధారించుకోండి

#### 4. Model Loading Errors
- మోడల్ పాథ్ మరియు యాక్సెస్ అనుమతులను తనిఖీ చేయండి
- మోడల్‌కు `trust_remote_code=True` అవసరమో చూడండి
- అవసరమైన అన్ని మోడల్ ఫైళ్లు డౌన్లోడ్ అయ్యాయో నిర్ధారించుకోండి

### Getting Help

- **డాక్యుమెంటేషన్**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **ఉదాహరణలు**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Recipes Repository

### Introduction to Olive Recipes

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) రిపాజిటరీ ప్రధాన Olive టూల్‌కిట్‌కు అనుబంధంగా, ప్రాచుర్యం పొందిన AI మోడల్స్ కోసం ఉపయోగించడానికి సిద్ధంగా ఉన్న ఆప్టిమైజేషన్ రెసిపీల సమాహారాన్ని అందిస్తుంది. ఈ రిపాజిటరీ పబ్లిక్‌గా అందుబాటులో ఉన్న మోడల్స్‌ను ఆప్టిమైజ్ చేయడంలో మరియు ప్రైవేటు మోడల్స్ కోసం ఆప్టిమైజేషన్ వర్క్‌ఫ్లోలను సృష్టించడంలో ప్రాక్టికల్ రిఫరెన్స్‌గా పనిచేస్తుంది.

### Key Features

- **100+ ప్రీ-బిల్ట్ రెసిపీలు**: ప్రాచుర్యం పొందిన మోడల్స్ కోసం సిద్ధంగా ఉన్న ఆప్టిమైజేషన్ కాన్ఫిగరేషన్లు
- **మల్టీ-ఆర్కిటెక్చర్ సపోర్ట్**: ట్రాన్స్‌ఫార్మర్ మోడల్స్, విజన్ మోడల్స్, మల్టిమోడల్ ఆర్కిటెక్చర్లను కవర్ చేస్తుంది
- **హార్డ్‌వేర్-స్పెసిఫిక్ ఆప్టిమైజేషన్లు**: CPU, GPU, మరియు ప్రత్యేక యాక్సిలరేటర్ల కోసం రెసిపీలు
- **పాపులర్ మోడల్ ఫ్యామిలీస్**: Phi, Llama, Qwen, Gemma, Mistral మరియు మరిన్ని

### Supported Model Families

రిపాజిటరీలో ఈ మోడల్ ఫ్యామిలీస్ కోసం ఆప్టిమైజేషన్ రెసిపీలు ఉన్నాయి:

#### Language Models
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 సిరీస్ (0.5B నుండి 14B వరకు)
- **Google Gemma**: వివిధ Gemma మోడల్ కాన్ఫిగరేషన్లు
- **Mistral AI**: Mistral-7B సిరీస్
- **DeepSeek**: R1-Distill సిరీస్ మోడల్స్

#### Vision and Multimodal Models
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP Models**: వివిధ CLIP-ViT కాన్ఫిగరేషన్లు
- **ResNet**: ResNet-50 ఆప్టిమైజేషన్లు
- **Vision Transformers**: ViT-base-patch16-224

#### Specialized Models
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: బేస్ మరియు మల్టీలింగ్వల్ వేరియంట్లు
- **Sentence Transformers**: all-MiniLM-L6-v2

### Using Olive Recipes

#### Method 1: Clone Specific Recipe

```bash
# రెసిపీలు రిపాజిటరీని క్లోన్ చేయండి
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# ఒక నిర్దిష్ట మోడల్ రెసిపీకి నావిగేట్ చేయండి
cd microsoft-Phi-4-mini-instruct

# ఆప్టిమైజేషన్‌ను నడపండి
olive run --config olive_config.json
```

#### Method 2: Use Recipe as Template

```bash
# మీ మోడల్ కోసం ఒక రెసిపీ కాన్ఫిగరేషన్‌ను కాపీ చేయండి
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# మీ అవసరాలకు అనుగుణంగా కాన్ఫిగరేషన్‌ను మార్చండి
# మోడల్ మార్గాలు, ఆప్టిమైజేషన్ పరామితులు మొదలైనవి నవీకరించండి

# మీ కస్టమ్ కాన్ఫిగరేషన్‌తో రన్ చేయండి
olive run --config my_config.json
```

### Recipe Structure

ప్రతి రెసిపీ డైరెక్టరీ సాధారణంగా ఈ ఫైళ్లను కలిగి ఉంటుంది:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Example: Using Phi-4-mini Recipe

Phi-4-mini రెసిపీని ఉదాహరణగా ఉపయోగిద్దాం:

```bash
# రిపోజిటరీని క్లోన్ చేయండి
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# ఆధారాలను ఇన్‌స్టాల్ చేయండి
pip install -r requirements.txt

# ఆప్టిమైజేషన్‌ను నడపండి
olive run --config olive_config.json
```

కాన్ఫిగరేషన్ ఫైల్ సాధారణంగా ఈ అంశాలను కలిగి ఉంటుంది:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Customizing Recipes

#### Modifying Target Hardware

లక్ష్య హార్డ్‌వేర్ మార్చడానికి, `systems` సెక్షన్‌ను అప్‌డేట్ చేయండి:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Adjusting Optimization Parameters

వివిధ ఆప్టిమైజేషన్ స్థాయిల కోసం `passes` సెక్షన్‌ను మార్చండి:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Creating Your Own Recipe

1. **సమానమైన మోడల్‌తో ప్రారంభించండి**: సమాన ఆర్కిటెక్చర్ ఉన్న రెసిపీని కనుగొనండి
2. **మోడల్ కాన్ఫిగరేషన్ అప్‌డేట్ చేయండి**: కాన్ఫిగరేషన్‌లో మోడల్ పేరు/పాథ్ మార్చండి
3. **పరామితులు సర్దుబాటు చేయండి**: అవసరమైనట్లుగా ఆప్టిమైజేషన్ పరామితులను మార్చండి
4. **పరీక్షించి ధృవీకరించండి**: ఆప్టిమైజేషన్ నడిపించి ఫలితాలను ధృవీకరించండి
5. **కమ్యూనిటీకి కాంట్రిబ్యూట్ చేయండి**: మీ రెసిపీని రిపాజిటరీకి సమర్పించండి

### Benefits of Using Recipes

#### 1. **సిద్ధమైన కాన్ఫిగరేషన్లు**
- నిర్దిష్ట మోడల్స్ కోసం పరీక్షించిన ఆప్టిమైజేషన్ సెట్టింగ్స్
- ఉత్తమ పరామితులను కనుగొనడంలో ప్రయోగాలు తప్పించుకోవచ్చు

#### 2. **హార్డ్‌వేర్-స్పెసిఫిక్ ట్యూనింగ్**
- వివిధ ఎగ్జిక్యూషన్ ప్రొవైడర్ల కోసం ప్రీ-ఆప్టిమైజ్డ్
- CPU, GPU, NPU లక్ష్యాల కోసం సిద్ధంగా ఉన్న కాన్ఫిగరేషన్లు

#### 3. **విస్తృత కవరేజ్**
- అత్యంత ప్రాచుర్యం పొందిన ఓపెన్-సోర్స్ మోడల్స్‌కు మద్దతు
- కొత్త మోడల్ విడుదలలతో రెగ్యులర్ అప్‌డేట్లు

#### 4. **కమ్యూనిటీ కాంట్రిబ్యూషన్స్**
- AI కమ్యూనిటీతో సహకార అభివృద్ధి
- పంచుకున్న జ్ఞానం మరియు ఉత్తమ పద్ధతులు

### Contributing to Olive Recipes

మీరు రిపాజిటరీలో లేని మోడల్‌ను ఆప్టిమైజ్ చేసినట్లయితే:

1. **రిపాజిటరీని ఫోర్క్ చేయండి**: olive-recipes యొక్క మీ స్వంత ఫోర్క్ సృష్టించండి
2. **రెసిపీ డైరెక్టరీ సృష్టించండి**: మీ మోడల్ కోసం కొత్త డైరెక్టరీ జోడించండి
3. **కాన్ఫిగరేషన్ చేర్చండి**: olive_config.json మరియు మద్దతు ఫైళ్లను జోడించండి
4. **వినియోగం డాక్యుమెంట్ చేయండి**: స్పష్టమైన READMEతో సూచనలు ఇవ్వండి
5. **పుల్ రిక్వెస్ట్ సమర్పించండి**: కమ్యూనిటీకి తిరిగి కాంట్రిబ్యూట్ చేయండి

### Performance Benchmarks

చాలా రెసిపీలు పనితీరు బెంచ్‌మార్క్‌లను చూపిస్తాయి:
- **లేటెన్సీ మెరుగుదల**: సాధారణంగా 2-6 రెట్లు వేగవంతం
- **మెమరీ తగ్గింపు**: క్వాంటైజేషన్‌తో 50-75% మెమరీ వినియోగం తగ్గింపు
- **ఖచ్చితత్వం నిలుపుకోవడం**: 95-99% ఖచ్చితత్వం పరిరక్షణ

### Integration with AI Toolkit

రెసిపీలు ఈ టూల్‌కిట్లతో సజావుగా పనిచేస్తాయి:
- **VS Code AI Toolkit**: మోడల్ ఆప్టిమైజేషన్ కోసం డైరెక్ట్ ఇంటిగ్రేషన్
- **Azure Machine Learning**: క్లౌడ్ ఆధారిత ఆప్టిమైజేషన్ వర్క్‌ఫ్లోలు
- **ONNX Runtime**: ఆప్టిమైజ్డ్ ఇన్ఫరెన్స్ డిప్లాయ్‌మెంట్

## Additional Resources

### Official Links
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipes Repository**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime Documentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Example**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Community Examples
- **Jupyter Notebooks**: Olive GitHub రిపాజిటరీలో అందుబాటులో — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: VS Code కోసం AI Toolkit అవలోకనం — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blog Posts**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Related Tools
- **ONNX Runtime**: హై-పర్ఫార్మెన్స్ ఇన్ఫరెన్స్ ఇంజిన్ — https://onnxruntime.ai/
- **Hugging Face Transformers**: అనేక అనుకూల మోడల్స్ మూలం — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: క్లౌడ్ ఆధారిత ఆప్టిమైజేషన్ వర్క్‌ఫ్లోలు — https://learn.microsoft.com/azure/machine-learning/


## ➡️ What's next

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**అస్పష్టత**:  
ఈ డాక్యుమెంట్‌ను AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వానికి ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో పొరపాట్లు లేదా తప్పిదాలు ఉండవచ్చు. అసలు డాక్యుమెంట్ దాని స్వదేశీ భాషలోనే అధికారిక మూలంగా పరిగణించాలి. ముఖ్యమైన సమాచారానికి, ప్రొఫెషనల్ మానవ అనువాదం సిఫార్సు చేయబడుతుంది. ఈ అనువాదం వాడకంలో ఏర్పడిన ఏవైనా అపార్థాలు లేదా తప్పుదారితీసే అర్థాలు కోసం మేము బాధ్యత వహించము.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->