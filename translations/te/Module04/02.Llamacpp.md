<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-12-15T23:51:14+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "te"
}
-->
# సెక్షన్ 2 : Llama.cpp అమలు మార్గదర్శకం

## విషయ సూచిక
1. [పరిచయం](../../../Module04)
2. [Llama.cpp అంటే ఏమిటి?](../../../Module04)
3. [ఇన్‌స్టాలేషన్](../../../Module04)
4. [మూలం నుండి నిర్మాణం](../../../Module04)
5. [మోడల్ క్వాంటైజేషన్](../../../Module04)
6. [ప్రాథమిక ఉపయోగం](../../../Module04)
7. [అధునాతన ఫీచర్లు](../../../Module04)
8. [పైథాన్ ఇంటిగ్రేషన్](../../../Module04)
9. [సమస్య పరిష్కారం](../../../Module04)
10. [ఉత్తమ ఆచారాలు](../../../Module04)

## పరిచయం

ఈ సమగ్ర పాఠం Llama.cpp గురించి మీరు తెలుసుకోవలసిన ప్రతిదీ, ప్రాథమిక ఇన్‌స్టాలేషన్ నుండి అధునాతన ఉపయోగ పరిస్థితుల వరకు, మీకు మార్గనిర్దేశం చేస్తుంది. Llama.cpp అనేది శక్తివంతమైన C++ అమలు, ఇది తక్కువ సెటప్‌తో మరియు వివిధ హార్డ్‌వేర్ కాన్ఫిగరేషన్లపై అద్భుతమైన పనితీరు కలిగి ఉన్న పెద్ద భాషా మోడల్స్ (LLMs) యొక్క సమర్థవంతమైన ఇన్ఫరెన్స్‌ను సాధ్యమవుతుంది.

## Llama.cpp అంటే ఏమిటి?

Llama.cpp అనేది C/C++ లో రాయబడిన LLM ఇన్ఫరెన్స్ ఫ్రేమ్‌వర్క్, ఇది తక్కువ సెటప్‌తో మరియు విస్తృత హార్డ్‌వేర్‌పై ఆధునిక పనితీరు కలిగి ఉన్న పెద్ద భాషా మోడల్స్‌ను స్థానికంగా నడపడానికి అనుమతిస్తుంది. ముఖ్య ఫీచర్లు:

### ప్రధాన ఫీచర్లు
- **డిపెండెన్సీల్లేని సాధారణ C/C++ అమలు**
- **క్రాస్-ప్లాట్‌ఫారమ్ అనుకూలత** (విండోస్, మాకోస్, లినక్స్)
- **వివిధ ఆర్కిటెక్చర్ల కోసం హార్డ్‌వేర్ ఆప్టిమైజేషన్**
- **క్వాంటైజేషన్ మద్దతు** (1.5-బిట్ నుండి 8-బిట్ ఇంటిజర్ క్వాంటైజేషన్)
- **CPU మరియు GPU వేగవంతీకరణ మద్దతు**
- **పరిమిత వాతావరణాల కోసం మెమరీ సామర్థ్యం**

### లాభాలు
- ప్రత్యేక హార్డ్‌వేర్ అవసరం లేకుండా CPU పై సమర్థవంతంగా నడుస్తుంది
- బహుళ GPU బ్యాక్‌ఎండ్లను మద్దతు ఇస్తుంది (CUDA, Metal, OpenCL, Vulkan)
- తేలికపాటి మరియు పోర్టబుల్
- ఆపిల్ సిలికాన్ మొదటి తరగతి పౌరుడు - ARM NEON, Accelerate మరియు Metal ఫ్రేమ్‌వర్క్‌ల ద్వారా ఆప్టిమైజ్ చేయబడింది
- తగ్గించిన మెమరీ వినియోగం కోసం వివిధ క్వాంటైజేషన్ స్థాయిలను మద్దతు ఇస్తుంది

## ఇన్‌స్టాలేషన్

### పద్ధతి 1: ముందుగా నిర్మించిన బైనరీలు (ఆరంభకులకు సిఫార్సు)

#### GitHub విడుదలల నుండి డౌన్‌లోడ్ చేయండి
1. [Llama.cpp GitHub విడుదలలు](https://github.com/ggml-org/llama.cpp/releases) సందర్శించండి
2. మీ సిస్టమ్‌కు సరిపోయే బైనరీని డౌన్‌లోడ్ చేయండి:
   - విండోస్ కోసం `llama-<version>-bin-win-<feature>-<arch>.zip`
   - మాకోస్ కోసం `llama-<version>-bin-macos-<feature>-<arch>.zip`
   - లినక్స్ కోసం `llama-<version>-bin-linux-<feature>-<arch>.zip`

3. ఆర్కైవ్‌ను ఎక్స్‌ట్రాక్ట్ చేసి డైరెక్టరీని మీ సిస్టమ్ PATH లో చేర్చండి

#### ప్యాకేజ్ మేనేజర్లను ఉపయోగించడం

**మాకోస్ (Homebrew):**
```bash
brew install llama.cpp
```

**లినక్స్ (వివిధ డిస్ట్రిబ్యూషన్లు):**
```bash
# ఉబుంటు/డెబియన్
sudo apt install llama.cpp

# ఆర్చ్ లినక్స్
sudo pacman -S llama.cpp
```

### పద్ధతి 2: పైథాన్ ప్యాకేజ్ (llama-cpp-python)

#### ప్రాథమిక ఇన్‌స్టాలేషన్
```bash
pip install llama-cpp-python
```

#### హార్డ్‌వేర్ వేగవంతీకరణతో
```bash
# CUDA (NVIDIA GPU ల కోసం)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# మెటల్ (ఆపిల్ సిలికాన్ కోసం)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# OpenBLAS (CPU ఆప్టిమైజేషన్ కోసం)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## మూలం నుండి నిర్మాణం

### ముందస్తు అవసరాలు

**సిస్టమ్ అవసరాలు:**
- C++ కంపైలర్ (GCC, Clang, లేదా MSVC)
- CMake (సంస్కరణ 3.14 లేదా అంతకంటే పైగా)
- Git
- మీ ప్లాట్‌ఫారమ్ కోసం బిల్డ్ టూల్స్

**ముందస్తు అవసరాలను ఇన్‌స్టాల్ చేయడం:**

**మాకోస్:**
```bash
xcode-select --install
```

**ఉబుంటు/డెబియన్:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**విండోస్:**
- C++ అభివృద్ధి టూల్స్‌తో Visual Studio 2022 ఇన్‌స్టాల్ చేయండి
- అధికారిక వెబ్‌సైట్ నుండి CMake ఇన్‌స్టాల్ చేయండి
- Git ఇన్‌స్టాల్ చేయండి

### ప్రాథమిక బిల్డ్ ప్రక్రియ

1. **రిపోజిటరీని క్లోన్ చేయండి:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **బిల్డ్‌ను కాన్ఫిగర్ చేయండి:**
```bash
cmake -B build
```

3. **ప్రాజెక్ట్‌ను బిల్డ్ చేయండి:**
```bash
cmake --build build --config Release
```

వేగవంతమైన కంపైల్ కోసం, సమాంతర జాబ్స్ ఉపయోగించండి:
```bash
cmake --build build --config Release -j 8
```

### హార్డ్‌వేర్-స్పెసిఫిక్ బిల్డ్స్

#### CUDA మద్దతు (NVIDIA GPUs)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal మద్దతు (ఆపిల్ సిలికాన్)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS మద్దతు (CPU ఆప్టిమైజేషన్)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan మద్దతు
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### అధునాతన బిల్డ్ ఎంపికలు

#### డీబగ్ బిల్డ్
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### అదనపు ఫీచర్లతో
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## మోడల్ క్వాంటైజేషన్

### GGUF ఫార్మాట్ అర్థం చేసుకోవడం

GGUF (Generalized GGML Unified Format) అనేది Llama.cpp మరియు ఇతర ఫ్రేమ్‌వర్క్‌లను ఉపయోగించి పెద్ద భాషా మోడల్స్‌ను సమర్థవంతంగా నడపడానికి రూపొందించిన ఆప్టిమైజ్డ్ ఫైల్ ఫార్మాట్. ఇది అందిస్తుంది:

- ప్రమాణీకృత మోడల్ వెయిట్ నిల్వ
- ప్లాట్‌ఫారమ్‌ల మధ్య మెరుగైన అనుకూలత
- పెరిగిన పనితీరు
- సమర్థవంతమైన మెటాడేటా నిర్వహణ

### క్వాంటైజేషన్ రకాలు

Llama.cpp వివిధ క్వాంటైజేషన్ స్థాయిలను మద్దతు ఇస్తుంది:

| రకం | బిట్లు | వివరణ | ఉపయోగం |
|------|------|-------------|----------|
| F16 | 16 | హాఫ్ ప్రెసిషన్ | ఉన్నత నాణ్యత, పెద్ద మెమరీ |
| Q8_0 | 8 | 8-బిట్ క్వాంటైజేషన్ | మంచి సమతుల్యం |
| Q4_0 | 4 | 4-బిట్ క్వాంటైజేషన్ | మోస్తరు నాణ్యత, చిన్న పరిమాణం |
| Q2_K | 2 | 2-బిట్ క్వాంటైజేషన్ | అత్యల్ప పరిమాణం, తక్కువ నాణ్యత |

### మోడల్స్ మార్చడం

#### PyTorch నుండి GGUF కి
```bash
# హగ్గింగ్ ఫేస్ మోడల్‌ను మార్చండి
python convert_hf_to_gguf.py path/to/model --outdir ./models

# మోడల్‌ను క్వాంటైజ్ చేయండి
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Hugging Face నుండి నేరుగా డౌన్‌లోడ్
చాలా మోడల్స్ Hugging Face లో GGUF ఫార్మాట్‌లో అందుబాటులో ఉన్నాయి:
- "GGUF" అనే పేరుతో మోడల్స్ కోసం శోధించండి
- సరైన క్వాంటైజేషన్ స్థాయిని డౌన్‌లోడ్ చేసుకోండి
- llama.cpp తో నేరుగా ఉపయోగించండి

## ప్రాథమిక ఉపయోగం

### కమాండ్ లైన్ ఇంటర్‌ఫేస్

#### సాదా టెక్స్ట్ జనరేషన్
```bash
# ప్రాథమిక టెక్స్ట్ పూర్తి చేయడం
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# ఇంటరాక్టివ్ చాట్ మోడ్
./llama-cli -m model.gguf -cnv
```

#### Hugging Face నుండి మోడల్స్ ఉపయోగించడం
```bash
# డౌన్లోడ్ చేసి నేరుగా నడపండి
./llama-cli -hf microsoft/DialoGPT-medium
```

#### సర్వర్ మోడ్
```bash
# సర్వర్ ప్రారంభించండి
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# GPU వేగవంతతతో
./llama-server -m model.gguf --n-gpu-layers 32
```

### సాధారణ పారామితులు

| పారామితి | వివరణ | ఉదాహరణ |
|-----------|-------------|---------|
| `-m` | మోడల్ ఫైల్ మార్గం | `-m model.gguf` |
| `-p` | ప్రాంప్ట్ టెక్స్ట్ | `-p "Hello world"` |
| `-n` | జనరేట్ చేయవలసిన టోకెన్ల సంఖ్య | `-n 100` |
| `-c` | కాంటెక్స్ట్ సైజ్ | `-c 4096` |
| `-t` | థ్రెడ్ల సంఖ్య | `-t 8` |
| `-ngl` | GPU లేయర్లు | `-ngl 32` |
| `-temp` | ఉష్ణోగ్రత | `-temp 0.7` |

### ఇంటరాక్టివ్ మోడ్

```bash
# ఇంటరాక్టివ్ సెషన్ ప్రారంభించండి
./llama-cli -m model.gguf -cnv

# ఉదాహరణ సంభాషణ:
# > హలో, మీరు ఎలా ఉన్నారు?
# హాయ్! నేను బాగున్నాను, అడిగినందుకు ధన్యవాదాలు...
# > మీరు నాకు ఏం సహాయం చేయగలరు?
# నేను వివిధ పనుల్లో సహాయం చేయగలను...
```

## అధునాతన ఫీచర్లు

### సర్వర్ API

#### సర్వర్ ప్రారంభించడం
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API ఉపయోగం
```bash
# చాట్ పూర్తి చేయడం
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# టెక్స్ట్ పూర్తి చేయడం
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### పనితీరు ఆప్టిమైజేషన్

#### మెమరీ నిర్వహణ
```bash
# సందర్భం పరిమాణాన్ని సెట్ చేయండి
./llama-cli -m model.gguf -c 2048

# మెమరీ మ్యాపింగ్‌ను ప్రారంభించండి
./llama-cli -m model.gguf --mlock
```

#### బహుళ థ్రెడింగ్
```bash
# అన్ని CPU కోర్లను ఉపయోగించండి
./llama-cli -m model.gguf -t $(nproc)

# నిర్దిష్ట థ్రెడ్ సంఖ్య
./llama-cli -m model.gguf -t 8
```

#### GPU వేగవంతీకరణ
```bash
# లేయర్లను GPUకి ఆఫ్‌లోడ్ చేయండి
./llama-cli -m model.gguf -ngl 32

# నిర్దిష్ట GPUని ఉపయోగించండి
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## పైథాన్ ఇంటిగ్రేషన్

### llama-cpp-python తో ప్రాథమిక ఉపయోగం

```python
from llama_cpp import Llama

# మోడల్‌ను ప్రారంభించండి
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# టెక్స్ట్‌ను ఉత్పత్తి చేయండి
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### చాట్ ఇంటర్‌ఫేస్

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# చాట్ పూర్తి చేయడం
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### స్ట్రీమింగ్ ప్రతిస్పందనలు

```python
# స్ట్రీమింగ్ టెక్స్ట్ జనరేషన్
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### LangChain తో ఇంటిగ్రేషన్

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# LLM ను ప్రారంభించండి
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# ప్రాంప్ట్ టెంప్లేట్ సృష్టించండి
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# చైన్ సృష్టించండి
chain = LLMChain(llm=llm, prompt=prompt)

# చైన్ ఉపయోగించండి
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## సమస్య పరిష్కారం

### సాధారణ సమస్యలు మరియు పరిష్కారాలు

#### బిల్డ్ లోపాలు

**సమస్య: CMake కనుగొనబడలేదు**
```bash
# పరిష్కారం: CMake ను ఇన్‌స్టాల్ చేయండి
# ఉబుంటు/డెబియన్
sudo apt install cmake

# మాక్‌ఓఎస్
brew install cmake
```

**సమస్య: కంపైలర్ కనుగొనబడలేదు**
```bash
# పరిష్కారం: బిల్డ్ టూల్స్ ఇన్‌స్టాల్ చేయండి
# ఉబుంటు/డెబియన్
sudo apt install build-essential

# మాక్‌ఓఎస్
xcode-select --install
```

#### రన్‌టైమ్ సమస్యలు

**సమస్య: మోడల్ లోడ్ అవడం విఫలమవుతుంది**
- మోడల్ ఫైల్ మార్గాన్ని ధృవీకరించండి
- ఫైల్ అనుమతులను తనిఖీ చేయండి
- సరిపడా RAM ఉందో చూసుకోండి
- వేరే క్వాంటైజేషన్ స్థాయిలను ప్రయత్నించండి

**సమస్య: పనితీరు తక్కువగా ఉంది**
- హార్డ్‌వేర్ వేగవంతీకరణను ప్రారంభించండి
- థ్రెడ్ల సంఖ్యను పెంచండి
- సరైన క్వాంటైజేషన్ ఉపయోగించండి
- GPU మెమరీ వినియోగాన్ని తనిఖీ చేయండి

#### మెమరీ సమస్యలు

**సమస్య: మెమరీ తక్కువగా ఉంది**
```bash
# పరిష్కారాలు:
# 1. చిన్న క్వాంటైజేషన్ ఉపయోగించండి
./llama-cli -m model-q4_0.gguf

# 2. సందర్భ పరిమాణాన్ని తగ్గించండి
./llama-cli -m model.gguf -c 1024

# 3. GPU కి ఆఫ్‌లోడ్ చేయండి
./llama-cli -m model.gguf -ngl 32
```

### ప్లాట్‌ఫారమ్-స్పెసిఫిక్ సమస్యలు

#### విండోస్
- MinGW లేదా Visual Studio కంపైలర్ ఉపయోగించండి
- సరైన PATH కాన్ఫిగరేషన్ ఉన్నదని నిర్ధారించండి
- యాంటీవైరస్ జోక్యం ఉన్నదో చూసుకోండి

#### మాకోస్
- ఆపిల్ సిలికాన్ కోసం Metal ప్రారంభించండి
- అవసరమైతే Rosetta 2 ఉపయోగించి అనుకూలత పొందండి
- Xcode కమాండ్ లైన్ టూల్స్ తనిఖీ చేయండి

#### లినక్స్
- అభివృద్ధి ప్యాకేజీలను ఇన్‌స్టాల్ చేయండి
- GPU డ్రైవర్ సంస్కరణలను తనిఖీ చేయండి
- CUDA టూల్‌కిట్ ఇన్‌స్టాలేషన్‌ను ధృవీకరించండి

## ఉత్తమ ఆచారాలు

### మోడల్ ఎంపిక
1. మీ హార్డ్‌వేర్ ఆధారంగా సరైన క్వాంటైజేషన్ ఎంచుకోండి
2. మోడల్ పరిమాణం మరియు నాణ్యత మధ్య వ్యత్యాసాలను పరిగణించండి
3. మీ ప్రత్యేక ఉపయోగానికి వేరే వేరే మోడల్స్‌ను పరీక్షించండి

### పనితీరు ఆప్టిమైజేషన్
1. GPU వేగవంతీకరణ అందుబాటులో ఉన్నప్పుడు ఉపయోగించండి
2. మీ CPU కోసం థ్రెడ్ల సంఖ్యను ఆప్టిమైజ్ చేయండి
3. మీ ఉపయోగానికి సరిపోయే కాంటెక్స్ట్ సైజ్ సెట్ చేయండి
4. పెద్ద మోడల్స్ కోసం మెమరీ మ్యాపింగ్ ప్రారంభించండి

### ఉత్పత్తి అమలు
1. API యాక్సెస్ కోసం సర్వర్ మోడ్ ఉపయోగించండి
2. సరైన లోపాల నిర్వహణను అమలు చేయండి
3. వనరుల వినియోగాన్ని పర్యవేక్షించండి
4. లాగింగ్ మరియు మానిటరింగ్ ఏర్పాటు చేయండి

### అభివృద్ధి వర్క్‌ఫ్లో
1. పరీక్ష కోసం చిన్న మోడల్స్‌తో ప్రారంభించండి
2. మోడల్ కాన్ఫిగరేషన్ల కోసం వెర్షన్ కంట్రోల్ ఉపయోగించండి
3. మీ కాన్ఫిగరేషన్లను డాక్యుమెంట్ చేయండి
4. వివిధ ప్లాట్‌ఫారమ్‌లపై పరీక్షించండి

### భద్రతా పరిగణనలు
1. ఇన్‌పుట్ ప్రాంప్ట్‌లను ధృవీకరించండి
2. రేట్ లిమిటింగ్ అమలు చేయండి
3. API ఎండ్‌పాయింట్లను సురక్షితం చేయండి
4. దుర్వినియోగ నమూనాలను పర్యవేక్షించండి

## ముగింపు

Llama.cpp వివిధ హార్డ్‌వేర్ కాన్ఫిగరేషన్లపై పెద్ద భాషా మోడల్స్‌ను స్థానికంగా నడపడానికి శక్తివంతమైన మరియు సమర్థవంతమైన మార్గాన్ని అందిస్తుంది. మీరు AI అప్లికేషన్లు అభివృద్ధి చేస్తున్నారా, పరిశోధన చేస్తున్నారా లేదా LLMs తో ప్రయోగాలు చేస్తున్నారా, ఈ ఫ్రేమ్‌వర్క్ విస్తృత ఉపయోగాల కోసం అవసరమైన అనుకూలత మరియు పనితీరు అందిస్తుంది.

ముఖ్యమైన విషయాలు:
- మీ అవసరాలకు సరిపోయే ఇన్‌స్టాలేషన్ పద్ధతిని ఎంచుకోండి
- మీ ప్రత్యేక హార్డ్‌వేర్ కాన్ఫిగరేషన్ కోసం ఆప్టిమైజ్ చేయండి
- ప్రాథమిక ఉపయోగంతో ప్రారంభించి, క్రమంగా అధునాతన ఫీచర్లను అన్వేషించండి
- సులభ ఇంటిగ్రేషన్ కోసం పైథాన్ బైండింగ్స్ ఉపయోగించండి
- ఉత్పత్తి అమలుల కోసం ఉత్తమ ఆచారాలను అనుసరించండి

మరింత సమాచారం మరియు నవీకరణల కోసం, [అధికారిక Llama.cpp రిపోజిటరీ](https://github.com/ggml-org/llama.cpp) ను సందర్శించండి మరియు సమగ్ర డాక్యుమెంటేషన్ మరియు కమ్యూనిటీ వనరులను చూడండి.


## ➡️ తదుపరి ఏమిటి

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**అస్పష్టత**:  
ఈ పత్రాన్ని AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వానికి ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో పొరపాట్లు లేదా తప్పిదాలు ఉండవచ్చు. మూల పత్రం దాని స్వదేశీ భాషలో అధికారిక మూలంగా పరిగణించాలి. ముఖ్యమైన సమాచారానికి, ప్రొఫెషనల్ మానవ అనువాదం సిఫార్సు చేయబడుతుంది. ఈ అనువాదం వాడకంలో ఏర్పడిన ఏవైనా అపార్థాలు లేదా తప్పుదారితీసే అర్థాలు కోసం మేము బాధ్యత వహించము.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->