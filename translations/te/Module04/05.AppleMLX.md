<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7084ef66b0602790ea6a864a89b6fd0c",
  "translation_date": "2025-12-15T23:20:47+00:00",
  "source_file": "Module04/05.AppleMLX.md",
  "language_code": "te"
}
-->
# Section 4 : ఆపిల్ MLX ఫ్రేమ్‌వర్క్ లో లోతైన అవగాహన

## Table of Contents
1. [ఆపిల్ MLX పరిచయం](../../../Module04)
2. [LLM అభివృద్ధికి ముఖ్య లక్షణాలు](../../../Module04)
3. [ఇన్‌స్టాలేషన్ గైడ్](../../../Module04)
4. [MLX తో ప్రారంభించడం](../../../Module04)
5. [MLX-LM: భాషా మోడల్స్](../../../Module04)
6. [పెద్ద భాషా మోడల్స్ తో పని చేయడం](../../../Module04)
7. [Hugging Face ఇంటిగ్రేషన్](../../../Module04)
8. [మోడల్ మార్పిడి మరియు క్వాంటైజేషన్](../../../Module04)
9. [భాషా మోడల్స్ ఫైన్-ట్యూనింగ్](../../../Module04)
10. [అధునాతన LLM లక్షణాలు](../../../Module04)
11. [LLMs కోసం ఉత్తమ ఆచారాలు](../../../Module04)
12. [సమస్య పరిష్కారం](../../../Module04)
13. [అదనపు వనరులు](../../../Module04)

## ఆపిల్ MLX పరిచయం

ఆపిల్ MLX అనేది ఆపిల్ సిలికాన్ పై సమర్థవంతమైన మరియు సౌకర్యవంతమైన మెషీన్ లెర్నింగ్ కోసం ప్రత్యేకంగా రూపొందించిన అర్రే ఫ్రేమ్‌వర్క్, ఇది ఆపిల్ మెషీన్ లెర్నింగ్ రీసెర్చ్ ద్వారా అభివృద్ధి చేయబడింది. 2023 డిసెంబర్ లో విడుదలైన MLX, PyTorch మరియు TensorFlow వంటి ఫ్రేమ్‌వర్క్‌లకు ఆపిల్ యొక్క సమాధానంగా నిలుస్తుంది, ముఖ్యంగా Mac కంప్యూటర్లపై శక్తివంతమైన పెద్ద భాషా మోడల్ సామర్థ్యాలను అందించడంపై ప్రత్యేక దృష్టి పెట్టింది.

### LLMs కోసం MLX ప్రత్యేకత ఏమిటి?

MLX ఆపిల్ సిలికాన్ యొక్క యూనిఫైడ్ మెమరీ ఆర్కిటెక్చర్‌ను పూర్తిగా వినియోగించుకునేందుకు రూపొందించబడింది, ఇది Mac కంప్యూటర్లపై పెద్ద భాషా మోడల్స్‌ను స్థానికంగా నడపడం మరియు ఫైన్-ట్యూన్ చేయడంలో ప్రత్యేకంగా అనుకూలంగా ఉంటుంది. ఈ ఫ్రేమ్‌వర్క్ Mac వినియోగదారులు LLMs తో పని చేసే సమయంలో సాధారణంగా ఎదుర్కొన్న అనేక అనుకూలత సమస్యలను తొలగిస్తుంది.

### LLMs కోసం MLX ను ఎవరు ఉపయోగించాలి?

- **Mac వినియోగదారులు** క్లౌడ్ ఆధారితత లేకుండా LLMs స్థానికంగా నడపాలనుకునేవారు
- **గవేషకులు** భాషా మోడల్ ఫైన్-ట్యూనింగ్ మరియు అనుకూలీకరణలో ప్రయోగాలు చేస్తున్నవారు
- **డెవలపర్లు** భాషా మోడల్ సామర్థ్యాలతో AI అప్లికేషన్లు నిర్మిస్తున్నవారు
- **ఎవరైనా** టెక్స్ట్ జనరేషన్, చాట్ మరియు భాషా పనుల కోసం ఆపిల్ సిలికాన్ ఉపయోగించాలనుకునేవారు

## LLM అభివృద్ధికి ముఖ్య లక్షణాలు

### 1. యూనిఫైడ్ మెమరీ ఆర్కిటెక్చర్
ఆపిల్ సిలికాన్ యొక్క యూనిఫైడ్ మెమరీ MLX కు పెద్ద భాషా మోడల్స్‌ను మెమరీ కాపీ చేయాల్సిన అదనపు భారంలేకుండా సమర్థవంతంగా నిర్వహించడానికి అనుమతిస్తుంది. దీని అర్థం మీరు అదే హార్డ్వేర్ పై పెద్ద మోడల్స్ తో పని చేయవచ్చు.

### 2. స్థానిక ఆపిల్ సిలికాన్ ఆప్టిమైజేషన్
MLX ఆపిల్ M-సిరీస్ చిప్స్ కోసం మట్టితోనే నిర్మించబడింది, భాషా మోడల్స్ లో సాధారణంగా ఉపయోగించే ట్రాన్స్‌ఫార్మర్ ఆర్కిటెక్చర్లకు ఉత్తమ పనితీరు అందిస్తుంది.

### 3. క్వాంటైజేషన్ మద్దతు
4-బిట్ మరియు 8-బిట్ క్వాంటైజేషన్ కోసం బిల్ట్-ఇన్ మద్దతు మెమరీ అవసరాలను తగ్గిస్తుంది మరియు మోడల్ నాణ్యతను నిలుపుకుంటూ, వినియోగదారుల హార్డ్వేర్ పై పెద్ద మోడల్స్ నడపడానికి వీలు కల్పిస్తుంది.

### 4. Hugging Face ఇంటిగ్రేషన్
Hugging Face ఎకోసిస్టమ్ తో సులభమైన ఇంటిగ్రేషన్ వేలాది ప్రీ-ట్రెయిన్డ్ భాషా మోడల్స్ కు సులభ మార్పిడి సాధనాలను అందిస్తుంది.

### 5. LoRA ఫైన్-ట్యూనింగ్
లో-రాంక్ అడాప్టేషన్ (LoRA) మద్దతు తక్కువ కంప్యూటేషనల్ వనరులతో పెద్ద మోడల్స్ ను సమర్థవంతంగా ఫైన్-ట్యూన్ చేయడానికి అనుమతిస్తుంది.

## ఇన్‌స్టాలేషన్ గైడ్

### సిస్టమ్ అవసరాలు
- **macOS 13.0+** (ఆపిల్ సిలికాన్ ఆప్టిమైజేషన్ కోసం)
- **Python 3.8+**
- **ఆపిల్ సిలికాన్** (M1, M2, M3, M4 సిరీస్)
- **స్థానిక ARM వాతావరణం** (Rosetta కింద నడవకూడదు)
- **8GB+ RAM** (పెద్ద మోడల్స్ కోసం 16GB+ సిఫార్సు)

### LLMs కోసం త్వరిత ఇన్‌స్టాలేషన్

భాషా మోడల్స్ తో ప్రారంభించడానికి సులభమైన మార్గం MLX-LM ఇన్‌స్టాల్ చేయడం:

```bash
pip install mlx-lm
```

ఈ ఒకే కమాండ్ MLX కోర్ ఫ్రేమ్‌వర్క్ మరియు భాషా మోడల్ యుటిలిటీలను ఇన్‌స్టాల్ చేస్తుంది.

### వర్చువల్ ఎన్విరాన్‌మెంట్ సెట్ అప్ (సిఫార్సు చేయబడింది)

```bash
# వర్చువల్ ఎన్విరాన్‌మెంట్ సృష్టించి యాక్టివేట్ చేయండి
python -m venv mlx-llm-env
source mlx-llm-env/bin/activate

# MLX-LM ఇన్‌స్టాల్ చేయండి
pip install mlx-lm

# ఇన్‌స్టాలేషన్‌ను ధృవీకరించండి
python -c "from mlx_lm import load; print('MLX-LM installed successfully')"
```

### ఆడియో మోడల్స్ కోసం అదనపు డిపెండెన్సీలు

మీరు Whisper వంటి స్పీచ్ మోడల్స్ తో పని చేయాలనుకుంటే:

```bash
pip install mlx-lm[whisper]
# లేదా
pip install mlx-lm ffmpeg-python
```

## MLX తో ప్రారంభించడం

### మీ మొదటి భాషా మోడల్

సాధారణ టెక్స్ట్ జనరేషన్ ఉదాహరణను నడపడం ప్రారంభిద్దాం:

```bash
# కమాండ్ లైన్ నుండి వేగవంతమైన టెక్స్ట్ ఉత్పత్తి
python -m mlx_lm.generate --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --prompt "Explain artificial intelligence in simple terms:"
```

### Python API ఉదాహరణ

```python
from mlx_lm import load, generate

# క్వాంటైజ్డ్ మోడల్‌ను లోడ్ చేయండి (తక్కువ మెమరీ ఉపయోగిస్తుంది)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# టెక్స్ట్‌ను ఉత్పత్తి చేయండి
prompt = "Write a short story about a robot learning to understand emotions:"
response = generate(
    model, 
    tokenizer, 
    prompt=prompt, 
    verbose=True,
    max_tokens=300,
    temp=0.7
)
print(response)
```

### మోడల్ లోడింగ్ అర్థం చేసుకోవడం

```python
from mlx_lm import load

# మోడల్స్ లోడ్ చేయడానికి వివిధ మార్గాలు
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # పూర్తి ఖచ్చితత్వం
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # క్వాంటైజ్డ్

# కస్టమ్ సెట్టింగ్స్‌తో లోడ్ చేయండి
model, tokenizer = load(
    "qwen/Qwen-7B-Chat",
    tokenizer_config={
        "eos_token": "<|endoftext|>",
        "trust_remote_code": True
    }
)
```

## MLX-LM: భాషా మోడల్స్

### మద్దతు పొందిన మోడల్ ఆర్కిటెక్చర్లు

MLX-LM విస్తృత శ్రేణి ప్రాచుర్యం పొందిన భాషా మోడల్ ఆర్కిటెక్చర్లను మద్దతు ఇస్తుంది:

- **LLaMA మరియు LLaMA 2** - మెటా యొక్క ప్రాథమిక మోడల్స్
- **Mistral మరియు Mixtral** - సమర్థవంతమైన మరియు శక్తివంతమైన మోడల్స్
- **Phi-3** - మైక్రోసాఫ్ట్ యొక్క కాంపాక్ట్ భాషా మోడల్స్
- **Qwen** - అలీబాబా యొక్క బహుభాషా మోడల్స్
- **Code Llama** - కోడ్ జనరేషన్ కోసం ప్రత్యేకంగా
- **Gemma** - గూగుల్ యొక్క ఓపెన్ భాషా మోడల్స్

### కమాండ్ లైన్ ఇంటర్‌ఫేస్

MLX-LM కమాండ్ లైన్ ఇంటర్‌ఫేస్ భాషా మోడల్స్ తో పని చేయడానికి శక్తివంతమైన సాధనాలను అందిస్తుంది:

```bash
# ప్రాథమిక పాఠ్య ఉత్పత్తి
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "Hello, how are you?"

# నిర్దిష్ట పారామితులతో ఉత్పత్తి చేయండి
python -m mlx_lm.generate \
    --model mlx-community/CodeLlama-7b-Instruct-hf-4bit \
    --prompt "Write a Python function to calculate fibonacci numbers:" \
    --max-tokens 500 \
    --temp 0.3

# ఇంటరాక్టివ్ చాట్ మోడ్
python -m mlx_lm.generate --model mistralai/Mistral-7B-Instruct-v0.2 --prompt "You are a helpful assistant." --max-tokens 100

# అన్ని ఎంపికల కోసం సహాయం పొందండి
python -m mlx_lm.generate --help
```

### అధునాతన వినియోగాల కోసం Python API

```python
from mlx_lm import load, generate

# బహుళ తరం కోసం ఒకసారి మోడల్ లోడ్ చేయండి
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# ఒకే ప్రాంప్ట్ తరం
def generate_response(prompt, max_tokens=200, temperature=0.7):
    return generate(
        model, 
        tokenizer, 
        prompt=prompt,
        max_tokens=max_tokens,
        temp=temperature,
        verbose=True
    )

# బ్యాచ్ తరం
prompts = [
    "Explain quantum computing:",
    "Write a haiku about technology:",
    "What are the benefits of renewable energy?"
]

responses = [generate_response(prompt) for prompt in prompts]
```

## పెద్ద భాషా మోడల్స్ తో పని చేయడం

### టెక్స్ట్ జనరేషన్ నమూనాలు

#### సింగిల్-టర్న్ జనరేషన్
```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Summarize the key principles of sustainable development:"
response = generate(model, tokenizer, prompt=prompt, max_tokens=300)
```

#### సూచన అనుసరణ
```python
# సూచన-అనుసరించే మోడల్స్ కోసం ప్రాంప్ట్‌లను ఫార్మాట్ చేయండి
instruction_prompt = """<s>[INST] You are a helpful coding assistant. 
Write a Python function that takes a list of numbers and returns the median value. 
Include comments explaining your code. [/INST]"""

response = generate(model, tokenizer, prompt=instruction_prompt, max_tokens=400)
```

#### సృజనాత్మక రచన
```python
creative_prompt = """Write a creative story beginning with: 
"The last library on Earth had been closed for fifty years when Sarah discovered the hidden door..."
Continue the story for about 200 words."""

story = generate(
    model, 
    tokenizer, 
    prompt=creative_prompt, 
    max_tokens=250, 
    temp=0.8  # మరింత సృజనాత్మకత కోసం ఎక్కువ ఉష్ణోగ్రత
)
```

### బహు-టర్న్ సంభాషణలు

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# సంభాషణ చరిత్ర నిర్వహణ
class Conversation:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.history = []
    
    def add_message(self, role, content):
        self.history.append({"role": role, "content": content})
    
    def generate_response(self, user_input):
        self.add_message("user", user_input)
        
        # మోడల్ కోసం సంభాషణను ఫార్మాట్ చేయండి
        conversation_text = self.format_conversation()
        
        response = generate(
            self.model,
            self.tokenizer,
            prompt=conversation_text,
            max_tokens=300,
            temp=0.7
        )
        
        self.add_message("assistant", response)
        return response
    
    def format_conversation(self):
        formatted = ""
        for message in self.history:
            if message["role"] == "user":
                formatted += f"[INST] {message['content']} [/INST]"
            else:
                formatted += f" {message['content']} "
        return formatted

# ఉపయోగం
chat = Conversation(model, tokenizer)
response1 = chat.generate_response("What is machine learning?")
response2 = chat.generate_response("Can you give me a practical example?")
```

## Hugging Face ఇంటిగ్రేషన్

### MLX-అనుకూల మోడల్స్ కనుగొనడం

MLX Hugging Face ఎకోసిస్టమ్ తో సులభంగా పని చేస్తుంది:

- **MLX మోడల్స్ బ్రౌజ్ చేయండి**: https://huggingface.co/models?library=mlx&sort=trending
- **MLX కమ్యూనిటీ**: https://huggingface.co/mlx-community (పూర్వం మార్పిడి చేసిన మోడల్స్)
- **మూల మోడల్స్**: ఎక్కువ LLaMA, Mistral, Phi, మరియు Qwen మోడల్స్ మార్పిడి తో పనిచేస్తాయి

### Hugging Face నుండి మోడల్స్ లోడ్ చేయడం

```python
from mlx_lm import load

# ముందుగా మార్చిన MLX మోడల్స్‌ను లోడ్ చేయండి (సిఫార్సు చేయబడింది)
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")

# అసలు Hugging Face మోడల్స్‌ను లోడ్ చేయండి (స్వయంచాలకంగా మార్చబడతాయి)
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")
model, tokenizer = load("microsoft/Phi-3-mini-4k-instruct")
```

### ఆఫ్‌లైన్ ఉపయోగం కోసం మోడల్స్ డౌన్లోడ్ చేయడం

```bash
# హగ్గింగ్ ఫేస్ CLIని ఇన్‌స్టాల్ చేయండి
pip install huggingface_hub

# ఆఫ్‌లైన్ ఉపయోగానికి ఒక మోడల్ డౌన్‌లోడ్ చేయండి
huggingface-cli download mlx-community/Mistral-7B-Instruct-v0.3-4bit --local-dir ./models/mistral-7b

# డౌన్‌లోడ్ చేసిన మోడల్‌ను ఉపయోగించండి
python -m mlx_lm.generate --model ./models/mistral-7b --prompt "Hello world"
```

## మోడల్ మార్పిడి మరియు క్వాంటైజేషన్

### Hugging Face మోడల్స్ ను MLX కి మార్చడం

```bash
# ప్రాథమిక మార్పిడి
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2

# క్వాంటైజేషన్‌తో మార్పిడి చేయండి (మెమరీ సామర్థ్యానికి సిఫార్సు చేయబడింది)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-Instruct-v0.2 -q

# మార్పిడి చేసి Hugging Face Hub కు అప్లోడ్ చేయండి
python -m mlx_lm.convert \
    --hf-path microsoft/Phi-3-mini-4k-instruct \
    -q \
    --upload-repo your-username/phi-3-mini-4k-instruct-mlx
```

### క్వాంటైజేషన్ అర్థం చేసుకోవడం

క్వాంటైజేషన్ మోడల్ పరిమాణం మరియు మెమరీ వినియోగాన్ని తగ్గిస్తుంది, నాణ్యతలో తక్కువ నష్టంతో:

```python
# మోడల్ పరిమాణాలు మరియు మెమరీ వినియోగం యొక్క తులన

# అసలు మోడల్ (ఫ్లోట్32): 7B పరామితుల కోసం సుమారు 14GB
model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")

# 4-బిట్ క్వాంటైజ్డ్: 7B పరామితుల కోసం సుమారు 4GB
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# 8-బిట్ క్వాంటైజ్డ్: 7B పరామితుల కోసం సుమారు 7GB (4-బిట్ కంటే మెరుగైన నాణ్యత)
# python -m mlx_lm.convert --hf-path model_name --quantize-bits 8
```

### కస్టమ్ క్వాంటైజేషన్

```bash
# విభిన్న క్వాంటైజేషన్ ఎంపికలు
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 4
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 --quantize-bits 8

# గ్రూప్ సైజ్ క్వాంటైజేషన్ (మరింత ఖచ్చితమైన)
python -m mlx_lm.convert --hf-path mistralai/Mistral-7B-v0.1 -q --q-group-size 64
```

## భాషా మోడల్స్ ఫైన్-ట్యూనింగ్

### LoRA (లో-రాంక్ అడాప్టేషన్) ఫైన్-ట్యూనింగ్

MLX LoRA ఉపయోగించి సమర్థవంతమైన ఫైన్-ట్యూనింగ్ మద్దతు ఇస్తుంది, ఇది తక్కువ కంప్యూటేషనల్ వనరులతో పెద్ద మోడల్స్ ను అనుకూలీకరించడానికి అనుమతిస్తుంది:

```python
# ప్రాథమిక LoRA ఫైన్-ట్యూనింగ్ సెటప్
from mlx_lm import load
from mlx_lm.utils import load_dataset

# బేస్ మోడల్ లోడ్ చేయండి
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# మీ డేటాసెట్ (JSON ఫార్మాట్) సిద్ధం చేయండి
# ప్రతి ఎంట్రీలో మీ శిక్షణ ఉదాహరణలతో 'text' ఫీల్డ్ ఉండాలి
dataset_path = "your_training_data.json"
```

### శిక్షణ డేటా సిద్ధం చేయడం

మీ శిక్షణ ఉదాహరణలతో JSON ఫైల్ సృష్టించండి:

```json
[
    {
        "text": "[INST] What is the capital of France? [/INST] The capital of France is Paris."
    },
    {
        "text": "[INST] Explain photosynthesis briefly. [/INST] Photosynthesis is the process by which plants convert sunlight, carbon dioxide, and water into glucose and oxygen."
    }
]
```

### ఫైన్-ట్యూనింగ్ కమాండ్

```bash
# LoRA తో సరిగ్గా సర్దుబాటు చేయండి
python -m mlx_lm.lora \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --train \
    --data your_training_data.json \
    --lora-layers 16 \
    --batch-size 4 \
    --learning-rate 1e-5 \
    --steps 1000 \
    --save-every 100 \
    --adapter-path ./fine_tuned_model
```

### ఫైన్-ట్యూన్ చేసిన మోడల్స్ ఉపయోగించడం

```python
from mlx_lm import load

# ఫైన్-ట్యూన్ చేసిన అడాప్టర్‌తో బేస్ మోడల్‌ను లోడ్ చేయండి
model, tokenizer = load(
    "mlx-community/Mistral-7B-Instruct-v0.3-4bit",
    adapter_path="./fine_tuned_model"
)

# మీ ఫైన్-ట్యూన్ చేసిన మోడల్‌తో జనరేట్ చేయండి
response = generate(model, tokenizer, prompt="Your custom prompt", max_tokens=200)
```

## అధునాతన LLM లక్షణాలు

### సమర్థత కోసం ప్రాంప్ట్ క్యాచింగ్

అదే సందర్భాన్ని పునరావృతంగా ఉపయోగించేటప్పుడు, MLX ప్రాంప్ట్ క్యాచింగ్ మద్దతు ఇస్తుంది పనితీరు మెరుగుపరచడానికి:

```bash
# సిస్టమ్ ప్రాంప్ట్‌ను సృష్టించి క్యాష్ చేయండి
python -m mlx_lm.generate \
    --model mlx-community/Mistral-7B-Instruct-v0.3-4bit \
    --prompt "You are a helpful coding assistant. You provide clean, well-commented code solutions." \
    --save-prompt-cache coding_assistant.safetensors

# కొత్త ప్రశ్నలతో క్యాష్ చేసిన ప్రాంప్ట్‌ను ఉపయోగించండి
python -m mlx_lm.generate \
    --prompt-cache-file coding_assistant.safetensors \
    --prompt "Write a Python function to sort a list of dictionaries by a specific key."
```

### స్ట్రీమింగ్ టెక్స్ట్ జనరేషన్

```python
from mlx_lm import load, stream_generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

prompt = "Write a detailed explanation of renewable energy sources:"

# టోకెన్లు ఉత్పత్తి అవుతున్నప్పుడు స్ట్రీమ్ చేయండి
for token in stream_generate(model, tokenizer, prompt, max_tokens=500):
    print(token, end='', flush=True)
```

### కోడ్ జనరేషన్ మోడల్స్ తో పని చేయడం

```python
from mlx_lm import load, generate

# కోడ్-ప్రత్యేక మోడల్‌ను లోడ్ చేయండి
model, tokenizer = load("mlx-community/CodeLlama-7b-Instruct-hf-4bit")

# కోడ్ ఉత్పత్తి ప్రాంప్ట్
code_prompt = """Write a Python class that implements a simple cache with the following features:
- Get and set methods
- Maximum size limit
- LRU (Least Recently Used) eviction policy
Include proper documentation and error handling."""

code_response = generate(
    model, 
    tokenizer, 
    prompt=code_prompt, 
    max_tokens=800,
    temp=0.3  # మరింత ఖచ్చితమైన కోడ్ కోసం తక్కువ ఉష్ణోగ్రత
)

print(code_response)
```

### చాట్ మోడల్స్ తో పని చేయడం

```python
from mlx_lm import load, generate

model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")

# మిస్ట్రాల్ మోడల్స్ కోసం సరైన చాట్ ఫార్మాటింగ్
def format_chat_prompt(messages):
    formatted_prompt = ""
    for message in messages:
        if message["role"] == "user":
            formatted_prompt += f"[INST] {message['content']} [/INST]"
        elif message["role"] == "assistant":
            formatted_prompt += f" {message['content']} "
    return formatted_prompt

# బహుళ-తిరుగుడు సంభాషణ
messages = [
    {"role": "user", "content": "What are the main components of a computer?"},
    {"role": "assistant", "content": "The main components of a computer include the CPU, RAM, storage, motherboard, and power supply."},
    {"role": "user", "content": "Can you explain what RAM does in more detail?"}
]

chat_prompt = format_chat_prompt(messages)
response = generate(model, tokenizer, prompt=chat_prompt, max_tokens=300)
```

## LLMs కోసం ఉత్తమ ఆచారాలు

### మెమరీ నిర్వహణ

```python
import psutil

def check_memory_usage():
    memory = psutil.virtual_memory()
    print(f"Memory usage: {memory.percent}%")
    print(f"Available memory: {memory.available / (1024**3):.2f} GB")

# పెద్ద మోడల్స్ లోడ్ చేయడానికి ముందు మెమరీని తనిఖీ చేయండి
check_memory_usage()

# మెమరీ సామర్థ్యాన్ని మెరుగుపరచడానికి క్వాంటైజ్డ్ మోడల్స్ ఉపయోగించండి
model, tokenizer = load("mlx-community/Mistral-7B-Instruct-v0.3-4bit")  # సుమారు 4GB
# వర్సెస్
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # సుమారు 14GB
```

### మోడల్ ఎంపిక మార్గదర్శకాలు

**ప్రయోగం మరియు నేర్చుకోవడానికి:**
- 4-బిట్ క్వాంటైజ్డ్ మోడల్స్ ఉపయోగించండి (ఉదా: `mlx-community/Mistral-7B-Instruct-v0.3-4bit`)
- Phi-3-mini వంటి చిన్న మోడల్స్ తో ప్రారంభించండి

**ఉత్పత్తి అప్లికేషన్ల కోసం:**
- మోడల్ పరిమాణం మరియు నాణ్యత మధ్య వ్యాపారాన్ని పరిగణించండి
- క్వాంటైజ్డ్ మరియు పూర్తి-ప్రిసిషన్ మోడల్స్ రెండింటినీ పరీక్షించండి
- మీ ప్రత్యేక వినియోగాలపై బెంచ్‌మార్క్ చేయండి

**ప్రత్యేక పనుల కోసం:**
- **కోడ్ జనరేషన్**: CodeLlama, Code Llama Instruct
- **సాధారణ చాట్**: Mistral-7B-Instruct, Phi-3
- **బహుభాషా**: Qwen మోడల్స్
- **సృజనాత్మక రచన**: Mistral లేదా LLaMA తో అధిక ఉష్ణోగ్రత సెట్టింగ్స్

### ప్రాంప్ట్ ఇంజనీరింగ్ ఉత్తమ ఆచారాలు

```python
# సూచన-అనుసరించే మోడల్స్ కోసం మంచి ప్రాంప్ట్ నిర్మాణం
def create_instruction_prompt(instruction, context="", examples=""):
    prompt = f"[INST] "
    
    if context:
        prompt += f"Context: {context}\n\n"
    
    if examples:
        prompt += f"Examples:\n{examples}\n\n"
    
    prompt += f"Instruction: {instruction} [/INST]"
    
    return prompt

# ఉదాహరణ ఉపయోగం
prompt = create_instruction_prompt(
    instruction="Summarize the following text in 2-3 sentences:",
    context="You are a helpful assistant that provides concise summaries.",
    examples="Text: 'Long article...' Summary: 'Brief summary...'"
)
```

### పనితీరు ఆప్టిమైజేషన్

```python
# ఉపయోగ కేసు ఆధారంగా ఉత్పత్తి పరామితులను ఆప్టిమైజ్ చేయండి
def optimize_for_use_case(use_case):
    params = {
        "max_tokens": 200,
        "temp": 0.7,
        "top_p": 0.9
    }
    
    if use_case == "code_generation":
        params.update({"temp": 0.3, "max_tokens": 500})
    elif use_case == "creative_writing":
        params.update({"temp": 0.9, "max_tokens": 800})
    elif use_case == "factual_qa":
        params.update({"temp": 0.3, "max_tokens": 150})
    elif use_case == "summarization":
        params.update({"temp": 0.5, "max_tokens": 300})
    
    return params

# ఉపయోగం
code_params = optimize_for_use_case("code_generation")
response = generate(model, tokenizer, prompt=prompt, **code_params)
```

## సమస్య పరిష్కారం

### సాధారణ సమస్యలు మరియు పరిష్కారాలు

#### ఇన్‌స్టాలేషన్ సమస్యలు

**సమస్య**: "mlx-lm కోసం సరిపోయే డిస్ట్రిబ్యూషన్ కనబడలేదు"
```bash
# పైథాన్ ఆర్కిటెక్చర్‌ను తనిఖీ చేయండి
python -c "import platform; print(platform.processor())"
# 'i386' కాకుండా 'arm' ను అవుట్‌పుట్ చేయాలి

# అవుట్‌పుట్ 'i386' అయితే, మీరు రోసెట్టా క్రింద x86 పైథాన్ ఉపయోగిస్తున్నారు
# స్థానిక ARM పైథాన్‌ను ఇన్‌స్టాల్ చేయండి లేదా కాండా ఉపయోగించండి
```

**పరిష్కారం**: స్థానిక ARM Python లేదా Miniconda ఉపయోగించండి:
```bash
# ARM64 కోసం Miniconda ను ఇన్‌స్టాల్ చేయండి
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh
bash Miniconda3-latest-MacOSX-arm64.sh

# కొత్త వాతావరణం సృష్టించండి
conda create -n mlx python=3.11
conda activate mlx
pip install mlx-lm
```

#### మెమరీ సమస్యలు

**సమస్య**: "RuntimeError: Out of memory"
```python
# చిన్న లేదా క్వాంటైజ్డ్ మోడల్స్ ఉపయోగించండి
model, tokenizer = load("mlx-community/Phi-3-mini-4k-instruct-4bit")  # ~2GB
# బదులుగా
# model, tokenizer = load("mistralai/Mistral-7B-Instruct-v0.2")  # ~14GB

# macOS 15+ కోసం, వైర్డ్ మెమరీ పరిమితిని పెంచండి
# sudo sysctl iogpu.wired_limit_mb=8192  # మీ RAM ఆధారంగా సర్దుబాటు చేయండి
```

#### మోడల్ లోడింగ్ సమస్యలు

**సమస్య**: మోడల్ లోడ్ అవ్వడం లేదా తక్కువ నాణ్యత అవుట్పుట్
```python
# మోడల్ సమగ్రతను నిర్ధారించండి
from mlx_lm import load

try:
    model, tokenizer = load("model_name")
    print("Model loaded successfully")
except Exception as e:
    print(f"Error loading model: {e}")
    
# ఒక సులభమైన ప్రాంప్ట్‌తో పరీక్షించండి
test_response = generate(model, tokenizer, prompt="Hello", max_tokens=10)
print(f"Test response: {test_response}")
```

#### పనితీరు సమస్యలు

**సమస్య**: నెమ్మదిగా జనరేషన్ వేగం
- ఇతర మెమరీ-భారీ అప్లికేషన్లను మూసివేయండి
- సాధ్యమైనప్పుడు క్వాంటైజ్డ్ మోడల్స్ ఉపయోగించండి
- మీరు Rosetta కింద నడవడం లేదని నిర్ధారించుకోండి
- మోడల్స్ లోడ్ చేసేముందు అందుబాటులో ఉన్న మెమరీని తనిఖీ చేయండి

### డీబగ్గింగ్ సూచనలు

```python
# డీబగ్గింగ్ కోసం విస్తృత అవుట్పుట్‌ను ప్రారంభించండి
response = generate(
    model, 
    tokenizer, 
    prompt="Test prompt", 
    verbose=True,  # జనరేషన్ పురోగతిని చూపిస్తుంది
    max_tokens=50
)

# సిస్టమ్ వనరులను పర్యవేక్షించండి
import psutil
import time

def monitor_generation():
    start_time = time.time()
    start_memory = psutil.virtual_memory().percent
    
    response = generate(model, tokenizer, prompt="Long prompt...", max_tokens=200)
    
    end_time = time.time()
    end_memory = psutil.virtual_memory().percent
    
    print(f"Generation time: {end_time - start_time:.2f} seconds")
    print(f"Memory change: {end_memory - start_memory:.1f}%")
    
    return response
```

## అదనపు వనరులు

### అధికారిక డాక్యుమెంటేషన్ మరియు రిపాజిటరీలు

- **MLX GitHub రిపాజిటరీ**: https://github.com/ml-explore/mlx
- **MLX-LM ఉదాహరణలు**: https://github.com/ml-explore/mlx-examples/tree/main/llms
- **MLX డాక్యుమెంటేషన్**: https://ml-explore.github.io/mlx/
- **Hugging Face MLX ఇంటిగ్రేషన్**: https://huggingface.co/docs/hub/en/mlx

### మోడల్ సేకరణలు

- **MLX కమ్యూనిటీ మోడల్స్**: https://huggingface.co/mlx-community
- **ట్రెండింగ్ MLX మోడల్స్**: https://huggingface.co/models?library=mlx&sort=trending

### ఉదాహరణ అప్లికేషన్లు

1. **వ్యక్తిగత AI అసిస్టెంట్**: సంభాషణ మెమరీతో స్థానిక చాట్‌బాట్ నిర్మించండి
2. **కోడ్ సహాయకుడు**: మీ అభివృద్ధి వర్క్‌ఫ్లో కోసం కోడింగ్ అసిస్టెంట్ సృష్టించండి
3. **కంటెంట్ జనరేటర్**: రచన, సారాంశం మరియు కంటెంట్ సృష్టి కోసం సాధనాలు అభివృద్ధి చేయండి
4. **కస్టమ్ ఫైన్-ట్యూన్ చేసిన మోడల్స్**: డొమైన్-స్పెసిఫిక్ పనుల కోసం మోడల్స్ అనుకూలీకరించండి
5. **బహుముఖ అప్లికేషన్లు**: టెక్స్ట్ జనరేషన్ ను ఇతర MLX సామర్థ్యాలతో కలపండి

### కమ్యూనిటీ మరియు నేర్చుకోవడం

- **MLX కమ్యూనిటీ చర్చలు**: GitHub Issues మరియు Discussions
- **Hugging Face ఫోరమ్స్**: కమ్యూనిటీ మద్దతు మరియు మోడల్ షేరింగ్
- **ఆపిల్ డెవలపర్ డాక్యుమెంటేషన్**: అధికారిక ఆపిల్ ML వనరులు

### ఉల్లేఖనం

మీరు మీ పరిశోధనలో MLX ఉపయోగిస్తే, దయచేసి ఉల్లేఖించండి:

```bibtex
@software{mlx2023,
    author = {Awni Hannun and Jagrit Digani and Angelos Katharopoulos and Ronan Collobert},
    title = {{MLX}: Efficient and flexible machine learning on Apple silicon},
    url = {https://github.com/ml-explore},
    version = {0.26.5},
    year = {2023},
}
```

---

## ముగింపు

ఆపిల్ MLX Mac కంప్యూటర్లపై పెద్ద భాషా మోడల్స్ నడపడం లో విప్లవాత్మక మార్పు తీసుకొచ్చింది. స్థానిక ఆపిల్ సిలికాన్ ఆప్టిమైజేషన్, సులభమైన Hugging Face ఇంటిగ్రేషన్, మరియు క్వాంటైజేషన్ మరియు LoRA ఫైన్-ట్యూనింగ్ వంటి శక్తివంతమైన లక్షణాలను అందించడం ద్వారా, MLX స్థానికంగా సున్నితమైన భాషా మోడల్స్ నడపడానికి అద్భుతమైన పనితీరు అందిస్తుంది.

మీరు చాట్‌బాట్స్, కోడ్ అసిస్టెంట్లు, కంటెంట్ జనరేటర్లు లేదా కస్టమ్ ఫైన్-ట్యూన్ చేసిన మోడల్స్ నిర్మిస్తున్నా, MLX మీ ఆపిల్ సిలికాన్ Mac యొక్క పూర్తి సామర్థ్యాన్ని భాషా మోడల్ అప్లికేషన్ల కోసం వినియోగించడానికి అవసరమైన సాధనాలు మరియు పనితీరును అందిస్తుంది. ఈ ఫ్రేమ్‌వర్క్ సమర్థత మరియు సౌకర్యంపై దృష్టి పెట్టడం ద్వారా పరిశోధన మరియు ఉత్పత్తి అప్లికేషన్లకు అద్భుతమైన ఎంపిక అవుతుంది.

ఈ ట్యుటోరియల్ లోని ప్రాథమిక ఉదాహరణలతో ప్రారంభించి, Hugging Face పై పూర్వం మార్పిడి చేసిన మోడల్స్ యొక్క సమృద్ధి ఎకోసిస్టమ్ ను అన్వేషించండి, మరియు క్రమంగా ఫైన్-ట్యూనింగ్ మరియు కస్టమ్ మోడల్ అభివృద్ధి వంటి అధునాతన లక్షణాలకు ఎదగండి. MLX ఎకోసిస్టమ్ పెరుగుతూ ఉండటంతో, ఇది ఆపిల్ హార్డ్వేర్ పై భాషా మోడల్ అభివృద్ధికి మరింత శక్తివంతమైన వేదికగా మారుతోంది.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**అస్పష్టత**:  
ఈ పత్రాన్ని AI అనువాద సేవ [Co-op Translator](https://github.com/Azure/co-op-translator) ఉపయోగించి అనువదించబడింది. మేము ఖచ్చితత్వానికి ప్రయత్నించినప్పటికీ, ఆటోమేటెడ్ అనువాదాల్లో పొరపాట్లు లేదా తప్పిదాలు ఉండవచ్చు. మూల పత్రం దాని స్వదేశీ భాషలో అధికారిక మూలంగా పరిగణించాలి. ముఖ్యమైన సమాచారానికి, ప్రొఫెషనల్ మానవ అనువాదం సిఫార్సు చేయబడుతుంది. ఈ అనువాదం వాడకంలో ఏర్పడిన ఏవైనా అపార్థాలు లేదా తప్పుదారుల కోసం మేము బాధ్యత వహించము.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->