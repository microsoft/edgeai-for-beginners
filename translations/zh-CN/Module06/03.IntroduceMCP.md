# 第三章 - 模型上下文协议 (MCP) 集成

## MCP（模型上下文协议）简介

模型上下文协议（MCP）是一种开源标准，用于将人工智能应用程序连接到外部系统。通过使用 MCP，像 Claude 或 ChatGPT 这样的 AI 应用程序可以连接到数据源（例如本地文件、数据库）、工具（例如搜索引擎、计算器）以及工作流（例如专业提示），从而访问关键信息并执行任务。

可以将 MCP 想象成 AI 应用程序的 **USB-C 接口**。正如 USB-C 提供了一种标准化的方式来连接电子设备，MCP 提供了一种标准化的方式来连接 AI 应用程序与外部系统。

### MCP 能实现什么？

MCP 为 AI 应用程序解锁了强大的功能：

- **个性化 AI 助手**：代理可以访问您的 Google 日历和 Notion，成为更个性化的 AI 助手
- **高级代码生成**：Claude Code 可以根据 Figma 设计生成完整的 Web 应用程序
- **企业数据集成**：企业聊天机器人可以连接到组织内的多个数据库，帮助用户通过聊天分析数据
- **创意工作流**：AI 模型可以在 Blender 上创建 3D 设计，并使用 3D 打印机打印出来
- **实时信息访问**：连接到外部数据源以获取最新信息
- **复杂的多步骤操作**：结合多个工具和系统执行复杂的工作流

### MCP 的重要性

MCP 在整个生态系统中提供了诸多优势：

**对开发者而言**：MCP 减少了构建或集成 AI 应用程序或代理时的开发时间和复杂性。

**对 AI 应用程序而言**：MCP 提供了访问数据源、工具和应用程序生态系统的能力，从而增强功能并改善最终用户体验。

**对最终用户而言**：MCP 使 AI 应用程序或代理能够更强大，可以在必要时访问您的数据并代表您采取行动。

## MCP 中的小型语言模型（SLM）

小型语言模型代表了一种高效的 AI 部署方法，具有以下几个优势：

### SLM 的优势
- **资源效率**：较低的计算需求
- **响应速度更快**：实时应用的延迟更低  
- **成本效益**：基础设施需求较少
- **隐私保护**：可以在本地运行，无需数据传输
- **定制化**：更容易针对特定领域进行微调

### SLM 与 MCP 的良好结合

SLM 与 MCP 的结合创造了一个强大的组合，模型的推理能力通过外部工具得到了增强，从而弥补了其参数较少的不足。

## Python MCP SDK 概述

Python MCP SDK 为构建支持 MCP 的应用程序提供了基础。该 SDK 包括：

- **客户端库**：用于连接 MCP 服务器
- **服务器框架**：用于创建自定义 MCP 服务器
- **协议处理器**：用于管理通信
- **工具集成**：用于执行外部功能

## 实际应用：Phi-4 MCP 客户端

让我们通过微软的 Phi-4 小型模型与 MCP 功能集成的实际应用来进行探索。

### MCP 架构概述

MCP 遵循 **客户端-服务器架构**，其中 MCP 主机（如 Claude Code 或 Claude Desktop 等 AI 应用程序）与一个或多个 MCP 服务器建立连接。MCP 主机通过为每个 MCP 服务器创建一个 MCP 客户端来实现这一点。

#### 关键参与者

- **MCP 主机**：协调和管理一个或多个 MCP 客户端的 AI 应用程序
- **MCP 客户端**：维护与 MCP 服务器的连接，并从 MCP 服务器获取上下文供 MCP 主机使用
- **MCP 服务器**：提供上下文给 MCP 客户端的程序

#### 双层架构

MCP 包括两个不同的层：

**数据层**：定义基于 JSON-RPC 的客户端-服务器通信协议，包括：
- 生命周期管理（连接初始化、能力协商）
- 核心原语（工具、资源、提示）
- 客户端功能（采样、引导、日志记录）
- 实用功能（通知、进度跟踪）

**传输层**：定义通信机制和渠道：
- **STDIO 传输**：使用标准输入/输出流进行本地进程通信（性能最佳，无网络开销）
- **可流式 HTTP 传输**：使用 HTTP POST 和可选的服务器发送事件进行远程服务器通信（支持标准 HTTP 认证）

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### MCP 核心原语

MCP 定义了原语，指定了可以与 AI 应用程序共享的上下文信息类型以及可以执行的操作范围。

#### 服务器原语

MCP 定义了服务器可以公开的三个核心原语：

**工具**：AI 应用程序可以调用的可执行功能以执行操作
- 示例：文件操作、API 调用、数据库查询
- 方法：`tools/list`，`tools/call`
- 支持动态发现和执行

**资源**：为 AI 应用程序提供上下文信息的数据源
- 示例：文件内容、数据库记录、API 响应
- 方法：`resources/list`，`resources/read`
- 允许访问结构化数据

**提示**：帮助结构化与语言模型交互的可重用模板
- 示例：系统提示、少样本示例
- 方法：`prompts/list`，`prompts/get`
- 标准化 AI 交互模式

#### 客户端原语

MCP 还定义了客户端可以公开的原语，以实现更丰富的交互：

**采样**：允许服务器从客户端的 AI 应用程序请求语言模型完成
- 方法：`sampling/complete`
- 实现与模型无关的服务器开发
- 提供对主机语言模型的访问

**引导**：允许服务器向用户请求额外信息
- 方法：`elicitation/request`
- 实现用户交互和确认
- 支持动态信息收集

**日志记录**：允许服务器向客户端发送日志消息
- 用于调试和监控
- 提供对服务器操作的可见性

### MCP 协议生命周期

#### 初始化和能力协商

MCP 是一个需要生命周期管理的有状态协议。初始化过程具有以下几个关键目的：

1. **协议版本协商**：确保客户端和服务器使用兼容的协议版本（例如 "2025-06-18"）
2. **能力发现**：每一方声明支持的功能和原语
3. **身份交换**：提供身份和版本信息

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### 工具发现与执行

初始化后，客户端可以发现并执行工具：

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### 实时通知

MCP 支持实时通知以实现动态更新：

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## 入门指南：分步操作

### 第一步：环境设置

安装所需依赖项：
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### 第二步：基本配置

设置环境变量：
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### 第三步：运行您的第一个 MCP 客户端

**基本 Ollama 设置：**
```bash
python ghmodel_mcp_demo.py
```

**使用 vLLM 后端：**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**服务器发送事件连接：**
```bash
python ghmodel_mcp_demo.py --run sse
```

**自定义 MCP 服务器：**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### 第四步：编程使用

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## 高级功能

### 多后端支持

实现支持 Ollama 和 vLLM 后端，您可以根据需求进行选择：

- **Ollama**：更适合本地开发和测试
- **vLLM**：针对生产和高吞吐量场景进行了优化

### 灵活的连接协议

支持两种连接模式：

**STDIO 模式**：直接进程通信
- 更低的延迟
- 适合本地工具
- 设置简单

**SSE 模式**：基于 HTTP 的流式传输
- 支持网络连接
- 更适合分布式系统
- 实时更新

### 工具集成能力

系统可以与各种工具集成：
- Web 自动化（Playwright）
- 文件操作
- API 交互
- 系统命令
- 自定义功能

## 错误处理和最佳实践

### 全面的错误管理

实现包括以下方面的强大错误处理：

**连接错误：**
- MCP 服务器故障
- 网络超时
- 连接问题

**工具执行错误：**
- 缺少工具
- 参数验证
- 执行失败

**响应处理错误：**
- JSON 解析问题
- 格式不一致
- LLM 响应异常

### 最佳实践

1. **资源管理**：使用异步上下文管理器
2. **错误处理**：实现全面的 try-catch 块
3. **日志记录**：启用适当的日志级别
4. **安全性**：验证输入并清理输出
5. **性能优化**：使用连接池和缓存

## 实际应用

### Web 自动化
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### 数据处理
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API 集成
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## 性能优化

### 内存管理
- 高效的消息历史处理
- 适当的资源清理
- 连接池化

### 网络优化
- 异步 HTTP 操作
- 可配置的超时
- 优雅的错误恢复

### 并发处理
- 非阻塞 I/O
- 并行工具执行
- 高效的异步模式

## 安全性考虑

### 数据保护
- 安全的 API 密钥管理
- 输入验证
- 输出清理

### 网络安全
- 支持 HTTPS
- 默认本地端点
- 安全令牌处理

### 执行安全
- 工具过滤
- 沙盒环境
- 审计日志记录

## MCP 生态系统与开发

### MCP 项目范围

模型上下文协议生态系统包括以下关键组件：

- **[MCP 规范](https://modelcontextprotocol.io/specification/latest)**：官方规范，概述客户端和服务器的实现要求
- **[MCP SDKs](https://modelcontextprotocol.io/docs/sdk)**：实现 MCP 的不同编程语言 SDK
- **MCP 开发工具**：开发 MCP 服务器和客户端的工具，包括 [MCP Inspector](https://github.com/modelcontextprotocol/inspector)
- **[MCP 参考服务器实现](https://github.com/modelcontextprotocol/servers)**：MCP 服务器的参考实现

### 开始 MCP 开发

开始使用 MCP 构建：

**构建服务器**：[创建 MCP 服务器](https://modelcontextprotocol.io/docs/develop/build-server)，以公开您的数据和工具

**构建客户端**：[开发应用程序](https://modelcontextprotocol.io/docs/develop/build-client)，以连接到 MCP 服务器

**学习概念**：[了解 MCP 的核心概念](https://modelcontextprotocol.io/docs/learn/architecture)和架构

## 结论

与 MCP 集成的小型语言模型代表了 AI 应用程序开发的范式转变。通过将小型模型的效率与外部工具的强大功能相结合，开发者可以创建既资源高效又功能强大的智能系统。

模型上下文协议提供了一种标准化方式，将 AI 应用程序连接到外部系统，就像 USB-C 为电子设备提供通用连接标准一样。这种标准化实现了：

- **无缝集成**：将 AI 模型连接到多样化的数据源和工具
- **生态系统增长**：一次构建，可用于多个 AI 应用程序
- **增强功能**：通过外部功能扩展 SLM 的能力
- **实时更新**：支持动态响应的 AI 应用程序

关键要点：
- MCP 是一种连接 AI 应用程序与外部系统的开放标准
- 协议支持工具、资源和提示作为核心原语
- 实时通知支持动态响应的应用程序
- 适当的生命周期管理和错误处理对于生产使用至关重要
- 生态系统提供了全面的 SDK 和开发工具

## 参考资料与进一步阅读

### 官方 MCP 文档

- **[模型上下文协议官网](https://modelcontextprotocol.io/)** - 完整的文档和规范
- **[MCP 入门指南](https://modelcontextprotocol.io/docs/getting-started/intro)** - 介绍和核心概念
- **[MCP 架构概述](https://modelcontextprotocol.io/docs/learn/architecture)** - 详细的技术架构
- **[MCP 规范](https://modelcontextprotocol.io/specification/latest)** - 官方协议规范
- **[MCP SDK 文档](https://modelcontextprotocol.io/docs/sdk)** - 特定语言的 SDK 指南

### 开发资源

- **[MCP 初学者指南](https://aka.ms/mcp-for-beginners)** - 面向初学者的模型上下文协议综合指南
- **[MCP GitHub 组织](https://github.com/modelcontextprotocol)** - 官方代码库和示例
- **[MCP 服务器代码库](https://github.com/modelcontextprotocol/servers)** - 参考服务器实现
- **[MCP Inspector](https://github.com/modelcontextprotocol/inspector)** - 开发和调试工具
- **[构建 MCP 服务器指南](https://modelcontextprotocol.io/docs/develop/build-server)** - 服务器开发教程
- **[构建 MCP 客户端指南](https://modelcontextprotocol.io/docs/develop/build-client)** - 客户端开发教程

### 小型语言模型与边缘 AI

- **[微软 Phi 模型](https://aka.ms/phicookbook)** - Phi 模型系列
- **[Foundry Local 文档](https://github.com/microsoft/Foundry-Local)** - 微软的边缘 AI 运行时
- **[Ollama 文档](https://ollama.ai/docs)** - 本地 LLM 部署平台  
- **[vLLM 文档](https://docs.vllm.ai/)** - 高性能 LLM 服务  

### 技术标准和协议  

- **[JSON-RPC 2.0 规范](https://www.jsonrpc.org/)** - MCP 使用的底层 RPC 协议  
- **[JSON Schema](https://json-schema.org/)** - MCP 工具的模式定义标准  
- **[OpenAPI 规范](https://swagger.io/specification/)** - API 文档标准  
- **[服务器发送事件 (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - 实时更新的 Web 标准  

### AI 代理开发  

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - 面向生产环境的代理开发框架  
- **[LangChain 文档](https://docs.langchain.com/)** - 代理和工具集成框架  
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - 微软的 AI 编排 SDK  

### 行业报告和研究  

- **[Anthropic 的模型上下文协议公告](https://www.anthropic.com/news/model-context-protocol)** - MCP 的初次介绍  
- **[小型语言模型调查](https://arxiv.org/abs/2410.20011)** - 关于 SLM 研究的学术调查  
- **[边缘 AI 市场分析](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - 行业趋势和预测  
- **[AI 代理开发最佳实践](https://arxiv.org/abs/2309.02427)** - 关于代理架构的研究  

本部分为构建您自己的基于 SLM 的 MCP 应用程序提供了基础，开启了自动化、数据处理和智能系统集成的可能性。  

## ➡️ 下一步  

- [模块 7. 边缘 AI 示例](../Module07/README.md)  

---

**免责声明**：  
本文档使用AI翻译服务[Co-op Translator](https://github.com/Azure/co-op-translator)进行翻译。尽管我们努力确保翻译的准确性，但请注意，自动翻译可能包含错误或不准确之处。原始语言的文档应被视为权威来源。对于重要信息，建议使用专业人工翻译。我们不对因使用此翻译而产生的任何误解或误读承担责任。