<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8c30436578b1bd604c48233ecdd39701",
  "translation_date": "2025-11-11T21:23:12+00:00",
  "source_file": "Workshop/Session01-GettingStartedFoundryLocal.md",
  "language_code": "es"
}
-->
# Sesi√≥n 1: Introducci√≥n a Foundry Local

## Resumen

Aprende a instalar, configurar y ejecutar tus primeros modelos de IA utilizando Microsoft Foundry Local. Esta sesi√≥n pr√°ctica ofrece una introducci√≥n paso a paso a la inferencia local, desde la instalaci√≥n hasta la creaci√≥n de tu primera aplicaci√≥n de chat utilizando modelos como Phi-4, Qwen y DeepSeek.

## Objetivos de Aprendizaje

Al finalizar esta sesi√≥n, podr√°s:

- **Instalar y Configurar**: Configurar Foundry Local con verificaci√≥n adecuada de instalaci√≥n
- **Dominar Operaciones CLI**: Usar la CLI de Foundry Local para la gesti√≥n y despliegue de modelos
- **Ejecutar tu Primer Modelo**: Desplegar e interactuar exitosamente con un modelo de IA local
- **Crear una Aplicaci√≥n de Chat**: Desarrollar una aplicaci√≥n b√°sica de chat utilizando el SDK de Python de Foundry Local
- **Comprender la IA Local**: Entender los fundamentos de la inferencia local y la gesti√≥n de modelos

## Requisitos Previos

### Requisitos del Sistema

- **Windows**: Windows 11 (22H2 o posterior) O **macOS**: macOS 11+ (soporte limitado)
- **RAM**: M√≠nimo 8GB, recomendado 16GB+
- **Almacenamiento**: 10GB+ de espacio libre para modelos
- **Python**: Instalado 3.10 o posterior
- **Acceso de Administrador**: Privilegios de administrador para la instalaci√≥n

### Entorno de Desarrollo

- Visual Studio Code con extensi√≥n de Python (recomendado)
- Acceso a l√≠nea de comandos (PowerShell en Windows, Terminal en macOS)
- Git para clonar repositorios (opcional)

## Flujo del Taller (30 minutos)

### Paso 1: Instalar Foundry Local (5 minutos)

#### Instalaci√≥n en Windows

Instala Foundry Local utilizando el gestor de paquetes de Windows:

```powershell
# Install via winget (recommended)
winget install Microsoft.FoundryLocal
```

Alternativa: Descarga directamente desde [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)

#### Instalaci√≥n en macOS (Soporte Limitado)

> [!NOTE] 
> El soporte para macOS est√° actualmente en vista previa. Consulta la documentaci√≥n oficial para la √∫ltima disponibilidad.

Si est√° disponible, instala usando Homebrew:

```bash
# If Homebrew formula is available
brew update
brew install foundry-local

# Or manual download (check official docs for latest)
curl -L -o foundry-local.tar.gz "https://download.microsoft.com/foundry-local/latest/macos/foundry-local.tar.gz"
tar -xzf foundry-local.tar.gz
sudo ./install.sh
```

**Alternativa para usuarios de macOS:**
- Usa una m√°quina virtual de Windows 11 (Parallels/UTM) y sigue los pasos de Windows
- Ejecuta mediante contenedor si est√° disponible y configura `FOUNDRY_LOCAL_ENDPOINT`

### Paso 2: Verificar la Instalaci√≥n (3 minutos)

Despu√©s de la instalaci√≥n, reinicia tu terminal y verifica que Foundry Local est√© funcionando:

```powershell
# Check if Foundry Local is installed correctly
foundry --version

# View available commands
foundry --help
```

El resultado esperado deber√≠a mostrar informaci√≥n de la versi√≥n y los comandos disponibles.

### Paso 3: Configurar el Entorno de Python (5 minutos)

Crea un entorno de Python dedicado para este taller:

**Windows:**
```powershell
# Create virtual environment
py -m venv .venv

# Activate environment
.\.venv\Scripts\Activate.ps1

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

**macOS/Linux:**
```bash
# Create virtual environment
python3 -m venv .venv

# Activate environment
source .venv/bin/activate

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

### Paso 4: Ejecutar tu Primer Modelo (7 minutos)

¬°Ahora ejecutemos nuestro primer modelo de IA local!

#### Comienza con Phi-4 Mini (Modelo Recomendado)

```powershell
# Download and start phi-4-mini (lightweight, fast)
foundry model run phi-4-mini

# Test the model with a simple prompt
foundry model run phi-4-mini --prompt "Hello, introduce yourself in one sentence"
```

> [!TIP]
> Este comando descarga el modelo (la primera vez) y comienza autom√°ticamente el servicio de Foundry Local.

#### Verifica lo que est√° en ejecuci√≥n

```powershell
# List available models (shows downloaded models)
foundry model list

# Check service status
foundry service status

# See what models are cached locally
foundry cache list
```

#### Prueba Diferentes Modelos

Una vez que phi-4-mini est√© funcionando, experimenta con otros modelos:

```powershell
# Larger model with better capabilities
foundry model run gpt-oss-20b --prompt "Explain edge AI in simple terms"

# Fast, efficient model
foundry model run qwen2.5-0.5b --prompt "What are the benefits of local AI inference?"
```

### Paso 5: Crear tu Primera Aplicaci√≥n de Chat (10 minutos)

Ahora vamos a crear una aplicaci√≥n en Python que utilice los modelos que acabamos de iniciar.

#### Crear el Script de Chat

Crea un nuevo archivo llamado `my_first_chat.py` (o utiliza el ejemplo proporcionado):

```python
#!/usr/bin/env python3
"""
My First Foundry Local Chat Application
Using FoundryLocalManager for automatic service management
"""

import os
from foundry_local import FoundryLocalManager
from openai import OpenAI

def main():
    # Get model alias from environment or use default
    alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-4-mini")
    
    try:
        # Initialize Foundry Local Manager (auto-starts service, downloads model)
        manager = FoundryLocalManager(alias)
        
        # Create OpenAI client pointing to local endpoint
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-needed"
        )
        
        # Get the actual model ID for this alias
        model_id = manager.get_model_info(alias).id
        
        print("ü§ñ Welcome to your first local AI chat!")
        print(f"ÔøΩ Using model: {alias} -> {model_id}")
        print(f"üåê Endpoint: {manager.endpoint}")
        print("ÔøΩüí° Type 'quit' to exit\n")
        
    except Exception as e:
        print(f"‚ùå Failed to initialize Foundry Local: {e}")
        print("üí° Make sure Foundry Local is installed: foundry --version")
        return
    
    while True:
        # Get user input
        user_message = input("You: ").strip()
        
        if user_message.lower() in ['quit', 'exit', 'bye']:
            print("üëã Goodbye!")
            break
            
        if not user_message:
            continue
            
        try:
            # Send message to local AI model
            response = client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": "You are a helpful AI assistant running locally."},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=200,
                temperature=0.7
            )
            
            # Display the response
            ai_response = response.choices[0].message.content
            print(f"ü§ñ AI: {ai_response}\n")
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            print("üí° Check service status: foundry service status\n")

if __name__ == "__main__":
    main()
```

> [!TIP]
> **Ejemplos Relacionados**: Para un uso m√°s avanzado, consulta:
>
> - **Ejemplo en Python**: `Workshop/samples/session01/chat_bootstrap.py` - Incluye respuestas en streaming y manejo de errores
> - **Notebook Jupyter**: `Workshop/notebooks/session01_chat_bootstrap.ipynb` - Versi√≥n interactiva con explicaciones detalladas

#### Prueba tu Aplicaci√≥n de Chat

```powershell
# No need to manually start models - FoundryLocalManager handles this!
# Just run your chat application
python my_first_chat.py
```

Alternativa: Usa directamente los ejemplos proporcionados

```powershell
# Try the complete sample with streaming support
cd Workshop/samples
python -m session01.chat_bootstrap "Your question here"
```

O explora el notebook interactivo  
Abre Workshop/notebooks/session01_chat_bootstrap.ipynb en VS Code

Prueba estas conversaciones de ejemplo:

- "¬øQu√© es Microsoft Foundry Local?"
- "Enumera 3 beneficios de ejecutar modelos de IA localmente"
- "Ay√∫dame a entender la IA en el edge"

## Lo que Has Logrado

¬°Felicidades! Has logrado:

1. ‚úÖ **Instalar Foundry Local** y verificar que funciona
2. ‚úÖ **Iniciar tu primer modelo de IA** (phi-4-mini) localmente
3. ‚úÖ **Probar diferentes modelos** mediante l√≠nea de comandos
4. ‚úÖ **Crear una aplicaci√≥n de chat** que se conecta a tu IA local
5. ‚úÖ **Experimentar la inferencia de IA local** sin dependencias en la nube

## Comprendiendo lo que Sucedi√≥

### Inferencia de IA Local

- Tus modelos de IA se ejecutan completamente en tu computadora
- No se env√≠an datos a la nube
- Las respuestas se generan localmente utilizando tu CPU/GPU
- Se mantiene la privacidad y seguridad

### Gesti√≥n de Modelos

- `foundry model run` descarga e inicia modelos
- **FoundryLocalManager SDK** maneja autom√°ticamente el inicio del servicio y la carga de modelos
- Los modelos se almacenan en cach√© localmente para uso futuro
- Se pueden descargar m√∫ltiples modelos, pero t√≠picamente se ejecuta uno a la vez
- El servicio gestiona autom√°ticamente el ciclo de vida del modelo

### Enfoques SDK vs CLI

- **Enfoque CLI**: Gesti√≥n manual de modelos con `foundry model run <model>`
- **Enfoque SDK**: Gesti√≥n autom√°tica de servicios y modelos con `FoundryLocalManager(alias)`
- **Recomendaci√≥n**: Usa SDK para aplicaciones, CLI para pruebas y exploraci√≥n

## Referencia de Comandos Comunes

### Comandos Esenciales de CLI

```powershell
# Installation & Setup
foundry --version              # Check installation
foundry --help                 # View all commands

# Model Management
foundry model list             # List available models
foundry model run <model>      # Download and start a model
foundry model run <model> --prompt "text"  # One-shot prompt
foundry cache list             # Show downloaded models

# Service Management
foundry service status         # Check if service is running
foundry service start          # Start the service manually
foundry service stop           # Stop the service
```

### Recomendaciones de Modelos

- **phi-4-mini**: Mejor modelo inicial - r√°pido, ligero, buena calidad
- **qwen2.5-0.5b**: Inferencia m√°s r√°pida, uso m√≠nimo de memoria
- **gpt-oss-20b**: Respuestas de mayor calidad, requiere m√°s recursos
- **deepseek-coder-1.3b**: Optimizado para tareas de programaci√≥n y c√≥digo

## Soluci√≥n de Problemas

### "Comando Foundry no encontrado"

**Soluci√≥n:**

```powershell
# Restart your terminal after installation
# Or manually add to PATH (Windows)
$env:PATH += ";C:\Program Files\Microsoft\FoundryLocal"
```

### "Error al cargar el modelo"

**Soluci√≥n:**

```powershell
# Check available system memory
foundry service status

# Try a smaller model first
foundry model run phi-4-mini

# Check disk space for model downloads
# Models are stored in: %USERPROFILE%\.foundry\models (Windows)
```

### "Conexi√≥n rechazada en localhost"

**Soluci√≥n:**

```powershell
# Check if service is running
foundry service status

# Start service if needed
foundry service start

# Verify the port (default is 5273)
# Check for port conflicts with: netstat -an | findstr 5273
```

## Pr√≥ximos Pasos

### Acciones Inmediatas

1. **Experimenta** con diferentes modelos y prompts
2. **Modifica** tu aplicaci√≥n de chat para probar diferentes modelos
3. **Crea** tus propios prompts y prueba las respuestas
4. **Explora** la Sesi√≥n 2: Construcci√≥n de aplicaciones RAG

### Ruta de Aprendizaje Avanzada

1. **Sesi√≥n 2**: Construye soluciones de IA con RAG (Generaci√≥n Aumentada por Recuperaci√≥n)
2. **Sesi√≥n 3**: Compara diferentes modelos de c√≥digo abierto
3. **Sesi√≥n 4**: Trabaja con modelos de √∫ltima generaci√≥n
4. **Sesi√≥n 5**: Construye sistemas de IA multiagente

## Variables de Entorno (Opcional)

Para un uso m√°s avanzado, puedes configurar estas variables de entorno:

| Variable | Prop√≥sito | Ejemplo |
|----------|-----------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Modelo predeterminado a usar | `phi-4-mini` |
| `FOUNDRY_LOCAL_ENDPOINT` | Sobrescribir URL del endpoint | `http://localhost:5273/v1` |

Crea un archivo `.env` en tu directorio de proyecto:
```
FOUNDRY_LOCAL_ALIAS=phi-4-mini
FOUNDRY_LOCAL_ENDPOINT=auto
```

## Recursos Adicionales

### Documentaci√≥n

- [Referencia del SDK de Python de Foundry Local](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk?pivots=programming-language-python)
- [Gu√≠a de Instalaci√≥n de Foundry Local](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)
- [Cat√°logo de Modelos](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/models)

### C√≥digo de Ejemplo

- **Ejemplo en Python de la Sesi√≥n01**: `Workshop/samples/session01/chat_bootstrap.py` - Aplicaci√≥n de chat completa con streaming
- **Notebook de la Sesi√≥n01**: `Workshop/notebooks/session01_chat_bootstrap.ipynb` - Tutorial interactivo  
- [Ejemplo del M√≥dulo08 01](../Module08/samples/01/README.md) - Inicio r√°pido de chat REST
- [Ejemplo del M√≥dulo08 02](../Module08/samples/02/README.md) - Integraci√≥n con OpenAI SDK
- [Ejemplo del M√≥dulo08 03](../Module08/samples/03/README.md) - Descubrimiento y evaluaci√≥n de modelos

### Comunidad

- [Discusiones en GitHub de Foundry Local](https://github.com/microsoft/Foundry-Local/discussions)
- [Comunidad de Azure AI](https://techcommunity.microsoft.com/category/artificialintelligence)

---

**Duraci√≥n de la Sesi√≥n**: 30 minutos pr√°cticos + 15 minutos de preguntas y respuestas  
**Nivel de Dificultad**: Principiante  
**Requisitos Previos**: Windows 11/macOS 11+, Python 3.10+, Acceso de administrador

## Escenario de Ejemplo del Taller

### Contexto Real

**Escenario**: Un equipo de TI empresarial necesita evaluar la inferencia de IA en el dispositivo para procesar comentarios sensibles de empleados sin enviar datos a servicios externos.

**Tu Objetivo**: Demostrar que los modelos de IA locales pueden proporcionar respuestas de calidad con latencia inferior a un segundo mientras se mantiene la privacidad total de los datos.

### Prompts de Prueba

Usa estos prompts para validar tu configuraci√≥n:

```json
[
    "List two benefits of local inference.",
    "Summarize why keeping data on device improves privacy.",
    "Give one trade-off when choosing a small model over a large model."
]
```

### Criterios de √âxito

- ‚úÖ Todos los prompts obtienen respuestas en menos de 2 segundos
- ‚úÖ No se env√≠an datos fuera de tu m√°quina local
- ‚úÖ Las respuestas son relevantes y √∫tiles
- ‚úÖ Tu aplicaci√≥n de chat funciona sin problemas

Esta validaci√≥n asegura que tu configuraci√≥n de Foundry Local est√° lista para los talleres avanzados de las Sesiones 2-6.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Descargo de responsabilidad**:  
Este documento ha sido traducido utilizando el servicio de traducci√≥n autom√°tica [Co-op Translator](https://github.com/Azure/co-op-translator). Aunque nos esforzamos por lograr precisi√≥n, tenga en cuenta que las traducciones autom√°ticas pueden contener errores o imprecisiones. El documento original en su idioma nativo debe considerarse la fuente autorizada. Para informaci√≥n cr√≠tica, se recomienda una traducci√≥n profesional realizada por humanos. No nos hacemos responsables de malentendidos o interpretaciones err√≥neas que surjan del uso de esta traducci√≥n.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->