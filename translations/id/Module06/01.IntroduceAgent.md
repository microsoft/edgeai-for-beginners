<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T13:40:31+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "id"
}
-->
# Agen AI dan Model Bahasa Kecil: Panduan Komprehensif

## Pendahuluan

Dalam tutorial ini, kita akan membahas Agen AI dan Model Bahasa Kecil (SLM) serta strategi implementasi canggihnya untuk lingkungan komputasi edge. Kita akan mengulas konsep dasar AI agentik, teknik optimasi SLM, strategi penerapan praktis untuk perangkat dengan sumber daya terbatas, dan Microsoft Agent Framework untuk membangun sistem agen siap produksi.

Lanskap kecerdasan buatan mengalami pergeseran paradigma pada tahun 2025. Sementara tahun 2023 adalah tahun chatbot dan 2024 melihat ledakan copilot, 2025 menjadi milik agen AI â€” sistem cerdas yang berpikir, bernalar, merencanakan, menggunakan alat, dan menjalankan tugas dengan sedikit masukan manusia, didukung oleh Model Bahasa Kecil yang semakin efisien. Microsoft Agent Framework muncul sebagai solusi terdepan untuk membangun sistem cerdas ini dengan kemampuan berbasis edge offline.

## Tujuan Pembelajaran

Pada akhir tutorial ini, Anda akan dapat:

- ğŸ¤– Memahami konsep dasar agen AI dan sistem agentik
- ğŸ”¬ Mengidentifikasi keunggulan Model Bahasa Kecil dibandingkan Model Bahasa Besar dalam aplikasi agentik
- ğŸš€ Mempelajari strategi penerapan SLM canggih untuk lingkungan komputasi edge
- ğŸ“± Menerapkan agen berbasis SLM untuk aplikasi dunia nyata
- ğŸ—ï¸ Membangun agen siap produksi menggunakan Microsoft Agent Framework
- ğŸŒ Menerapkan agen berbasis edge offline dengan integrasi LLM dan SLM lokal
- ğŸ”§ Mengintegrasikan Microsoft Agent Framework dengan Foundry Local untuk penerapan edge

## Memahami Agen AI: Dasar dan Klasifikasi

### Definisi dan Konsep Inti

Agen kecerdasan buatan (AI) merujuk pada sistem atau program yang mampu secara otonom melakukan tugas atas nama pengguna atau sistem lain dengan merancang alur kerjanya dan memanfaatkan alat yang tersedia. Berbeda dengan AI tradisional yang hanya merespons pertanyaan Anda, agen dapat bertindak secara mandiri untuk mencapai tujuan.

### Kerangka Klasifikasi Agen

Memahami batasan agen membantu dalam memilih jenis agen yang sesuai untuk berbagai skenario komputasi:

- **ğŸ”¬ Agen Refleks Sederhana**: Sistem berbasis aturan yang merespons persepsi langsung (termostat, otomatisasi dasar)
- **ğŸ“± Agen Berbasis Model**: Sistem yang mempertahankan keadaan internal dan memori (robot vacuum, sistem navigasi)
- **âš–ï¸ Agen Berbasis Tujuan**: Sistem yang merencanakan dan menjalankan urutan untuk mencapai tujuan (perencana rute, penjadwal tugas)
- **ğŸ§  Agen Pembelajaran**: Sistem adaptif yang meningkatkan kinerja seiring waktu (sistem rekomendasi, asisten pribadi)

### Keunggulan Utama Agen AI

Agen AI menawarkan beberapa keunggulan mendasar yang membuatnya ideal untuk aplikasi komputasi edge:

**Otonomi Operasional**: Agen menyediakan eksekusi tugas independen tanpa pengawasan manusia yang terus-menerus, membuatnya ideal untuk aplikasi waktu nyata. Mereka membutuhkan pengawasan minimal sambil mempertahankan perilaku adaptif, memungkinkan penerapan pada perangkat dengan sumber daya terbatas dengan pengurangan beban operasional.

**Fleksibilitas Penerapan**: Sistem ini memungkinkan kemampuan AI di perangkat tanpa kebutuhan konektivitas internet, meningkatkan privasi dan keamanan melalui pemrosesan lokal, dapat disesuaikan untuk aplikasi spesifik domain, dan cocok untuk berbagai lingkungan komputasi edge.

**Efisiensi Biaya**: Sistem agen menawarkan penerapan yang hemat biaya dibandingkan solusi berbasis cloud, dengan pengurangan biaya operasional dan kebutuhan bandwidth yang lebih rendah untuk aplikasi edge.

## Strategi Model Bahasa Kecil Lanjutan

### Dasar-Dasar SLM (Model Bahasa Kecil)

Model Bahasa Kecil (SLM) adalah model bahasa yang dapat dimuat pada perangkat elektronik konsumen umum dan melakukan inferensi dengan latensi yang cukup rendah untuk melayani permintaan agentik dari satu pengguna. Dalam praktiknya, SLM biasanya adalah model dengan kurang dari 10 miliar parameter.

**Fitur Penemuan Format**: SLM menawarkan dukungan canggih untuk berbagai tingkat kuantisasi, kompatibilitas lintas platform, optimasi kinerja waktu nyata, dan kemampuan penerapan edge. Pengguna dapat mengakses privasi yang ditingkatkan melalui pemrosesan lokal dan dukungan WebGPU untuk penerapan berbasis browser.

**Koleksi Tingkat Kuantisasi**: Format SLM populer meliputi Q4_K_M untuk kompresi seimbang dalam aplikasi mobile, seri Q5_K_S untuk penerapan edge yang berfokus pada kualitas, Q8_0 untuk presisi mendekati asli pada perangkat edge yang kuat, dan format eksperimental seperti Q2_K untuk skenario sumber daya ultra-rendah.

### GGUF (Format Universal GGML Umum) untuk Penerapan SLM

GGUF berfungsi sebagai format utama untuk menerapkan SLM yang dikuantisasi pada perangkat CPU dan edge, yang dioptimalkan khusus untuk aplikasi agentik:

**Fitur yang Dioptimalkan untuk Agen**: Format ini menyediakan sumber daya komprehensif untuk konversi dan penerapan SLM dengan dukungan yang ditingkatkan untuk pemanggilan alat, pembuatan output terstruktur, dan percakapan multi-putaran. Kompatibilitas lintas platform memastikan perilaku agen yang konsisten di berbagai perangkat edge.

**Optimasi Kinerja**: GGUF memungkinkan penggunaan memori yang efisien untuk alur kerja agen, mendukung pemuatan model dinamis untuk sistem multi-agen, dan menyediakan inferensi yang dioptimalkan untuk interaksi agen waktu nyata.

### Kerangka SLM yang Dioptimalkan untuk Edge

#### Optimasi Llama.cpp untuk Agen

Llama.cpp menyediakan teknik kuantisasi mutakhir yang dioptimalkan khusus untuk penerapan SLM agentik:

**Kuantisasi Khusus Agen**: Kerangka ini mendukung Q4_0 (optimal untuk penerapan agen mobile dengan pengurangan ukuran 75%), Q5_1 (kualitas-kompresi seimbang untuk agen inferensi edge), dan Q8_0 (kualitas mendekati asli untuk sistem agen produksi). Format canggih memungkinkan agen yang sangat terkompresi untuk skenario edge ekstrem.

**Manfaat Implementasi**: Inferensi yang dioptimalkan CPU dengan akselerasi SIMD menyediakan eksekusi agen yang efisien memori. Kompatibilitas lintas platform di arsitektur x86, ARM, dan Apple Silicon memungkinkan kemampuan penerapan agen universal.

#### Kerangka Apple MLX untuk Agen SLM

Apple MLX menyediakan optimasi asli yang dirancang khusus untuk agen berbasis SLM pada perangkat Apple Silicon:

**Optimasi Agen Apple Silicon**: Kerangka ini memanfaatkan arsitektur memori terpadu dengan integrasi Metal Performance Shaders, presisi campuran otomatis untuk inferensi agen, dan bandwidth memori yang dioptimalkan untuk sistem multi-agen. Agen SLM menunjukkan kinerja luar biasa pada chip seri M.

**Fitur Pengembangan**: Dukungan API Python dan Swift dengan optimasi khusus agen, diferensiasi otomatis untuk pembelajaran agen, dan integrasi mulus dengan alat pengembangan Apple menyediakan lingkungan pengembangan agen yang komprehensif.

#### ONNX Runtime untuk Agen SLM Lintas Platform

ONNX Runtime menyediakan mesin inferensi universal yang memungkinkan agen SLM berjalan secara konsisten di berbagai platform perangkat keras dan sistem operasi:

**Penerapan Universal**: ONNX Runtime memastikan perilaku agen SLM yang konsisten di platform Windows, Linux, macOS, iOS, dan Android. Kompatibilitas lintas platform ini memungkinkan pengembang untuk menulis sekali dan menerapkan di mana saja, secara signifikan mengurangi beban pengembangan dan pemeliharaan untuk aplikasi multi-platform.

**Opsi Akselerasi Perangkat Keras**: Kerangka ini menyediakan penyedia eksekusi yang dioptimalkan untuk berbagai konfigurasi perangkat keras termasuk CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm), dan akselerator khusus (Intel VPU, Qualcomm NPU). Agen SLM dapat secara otomatis memanfaatkan perangkat keras terbaik yang tersedia tanpa perubahan kode.

**Fitur Siap Produksi**: ONNX Runtime menawarkan fitur tingkat perusahaan yang penting untuk penerapan agen produksi termasuk optimasi graf untuk inferensi lebih cepat, manajemen memori untuk lingkungan dengan sumber daya terbatas, dan alat profil yang komprehensif untuk analisis kinerja. Kerangka ini mendukung API Python dan C++ untuk integrasi yang fleksibel.

## SLM vs LLM dalam Sistem Agentik: Perbandingan Lanjutan

### Keunggulan SLM dalam Aplikasi Agen

**Efisiensi Operasional**: SLM memberikan pengurangan biaya 10-30Ã— dibandingkan LLM untuk tugas agen, memungkinkan respons agentik waktu nyata dalam skala besar. Mereka menawarkan waktu inferensi yang lebih cepat karena kompleksitas komputasi yang lebih rendah, membuatnya ideal untuk aplikasi agen interaktif.

**Kemampuan Penerapan Edge**: SLM memungkinkan eksekusi agen di perangkat tanpa ketergantungan internet, privasi yang ditingkatkan melalui pemrosesan agen lokal, dan kustomisasi untuk aplikasi agen spesifik domain yang cocok untuk berbagai lingkungan komputasi edge.

**Optimasi Khusus Agen**: SLM unggul dalam pemanggilan alat, pembuatan output terstruktur, dan alur kerja pengambilan keputusan rutin yang mencakup 70-80% tugas agen tipikal.

### Kapan Menggunakan SLM vs LLM dalam Sistem Agen

**Cocok untuk SLM**:
- **Tugas agen berulang**: Entri data, pengisian formulir, panggilan API rutin
- **Integrasi alat**: Query database, operasi file, interaksi sistem
- **Alur kerja terstruktur**: Mengikuti proses agen yang telah ditentukan
- **Agen spesifik domain**: Layanan pelanggan, penjadwalan, analisis dasar
- **Pemrosesan lokal**: Operasi agen yang sensitif terhadap privasi

**Lebih Baik untuk LLM**:
- **Penalaran kompleks**: Pemecahan masalah baru, perencanaan strategis
- **Percakapan terbuka**: Obrolan umum, diskusi kreatif
- **Tugas pengetahuan luas**: Penelitian yang membutuhkan pengetahuan umum yang luas
- **Situasi baru**: Menangani skenario agen yang sepenuhnya baru

### Arsitektur Agen Hibrida

Pendekatan optimal menggabungkan SLM dan LLM dalam sistem agentik heterogen:

**Orkestrasi Agen Cerdas**:
1. **SLM sebagai utama**: Menangani 70-80% tugas agen rutin secara lokal
2. **LLM saat diperlukan**: Mengarahkan query kompleks ke model besar berbasis cloud
3. **SLM khusus**: Model kecil yang berbeda untuk domain agen yang berbeda
4. **Optimasi biaya**: Meminimalkan panggilan LLM yang mahal melalui pengalihan cerdas

## Strategi Penerapan Agen SLM Produksi

### Foundry Local: Runtime AI Edge Tingkat Perusahaan

Foundry Local (https://github.com/microsoft/foundry-local) berfungsi sebagai solusi unggulan Microsoft untuk menerapkan Model Bahasa Kecil di lingkungan edge produksi. Ini menyediakan lingkungan runtime lengkap yang dirancang khusus untuk agen berbasis SLM dengan fitur tingkat perusahaan dan kemampuan integrasi yang mulus.

**Arsitektur Inti dan Fitur**:
- **API Kompatibel OpenAI**: Kompatibilitas penuh dengan SDK OpenAI dan integrasi Agent Framework
- **Optimasi Perangkat Keras Otomatis**: Pemilihan cerdas varian model berdasarkan perangkat keras yang tersedia (CUDA GPU, Qualcomm NPU, CPU)
- **Manajemen Model**: Pengunduhan, caching, dan manajemen siklus hidup model SLM secara otomatis
- **Penemuan Layanan**: Deteksi layanan tanpa konfigurasi untuk kerangka kerja agen
- **Optimasi Sumber Daya**: Manajemen memori cerdas dan efisiensi daya untuk penerapan edge

#### Instalasi dan Pengaturan

**Instalasi Lintas Platform**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Memulai Pengembangan Agen**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Integrasi Kerangka Agen

**Integrasi SDK Foundry Local**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Pemilihan Model Otomatis dan Optimasi Perangkat Keras**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Pola Penerapan Produksi

**Pengaturan Produksi Agen Tunggal**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Orkestrasi Produksi Multi-Agen**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### Fitur Tingkat Perusahaan dan Pemantauan

**Pemantauan Kesehatan dan Observabilitas**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Manajemen Sumber Daya dan Auto-scaling**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### Konfigurasi dan Optimasi Lanjutan

**Konfigurasi Model Kustom**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Daftar Periksa Penerapan Produksi**:

âœ… **Konfigurasi Layanan**:
- Konfigurasikan alias model yang sesuai untuk kasus penggunaan
- Tetapkan batas sumber daya dan ambang pemantauan
- Aktifkan pemeriksaan kesehatan dan pengumpulan metrik
- Konfigurasikan restart otomatis dan failover

âœ… **Pengaturan Keamanan**:
- Aktifkan akses API hanya lokal (tidak ada eksposur eksternal)
- Konfigurasikan manajemen kunci API yang sesuai
- Siapkan log audit untuk interaksi agen
- Terapkan pembatasan tingkat untuk penggunaan produksi

âœ… **Optimasi Kinerja**:
- Uji kinerja model di bawah beban yang diharapkan
- Konfigurasikan tingkat kuantisasi yang sesuai
- Siapkan strategi caching dan pemanasan model
- Pantau pola penggunaan memori dan CPU

âœ… **Pengujian Integrasi**:
- Uji integrasi kerangka kerja agen
- Verifikasi kemampuan operasi offline
- Uji skenario failover dan pemulihan
- Validasi alur kerja agen end-to-end

### Ollama: Penerapan Agen SLM yang Disederhanakan

### Ollama: Penerapan Agen SLM Berfokus Komunitas

Ollama menyediakan pendekatan yang digerakkan oleh komunitas untuk penerapan agen SLM dengan penekanan pada kesederhanaan, ekosistem model yang luas, dan alur kerja yang ramah pengembang. Sementara Foundry Local berfokus pada fitur tingkat perusahaan, Ollama unggul dalam pembuatan prototipe cepat, akses model komunitas, dan skenario penerapan yang disederhanakan.

**Arsitektur Inti dan Fitur**:
- **API Kompatibel OpenAI**: Kompatibilitas penuh REST API untuk integrasi kerangka kerja agen yang mulus
- **Perpustakaan Model yang Luas**: Akses ke ratusan model yang disumbangkan komunitas dan resmi
- **Manajemen Model Sederhana**: Instalasi dan penggantian model dengan satu perintah
- **Dukungan Lintas Platform**: Dukungan asli di Windows, macOS, dan Linux
- **Optimasi Sumber Daya**: Kuantisasi otomatis dan deteksi perangkat keras

#### Instalasi dan Pengaturan

**Instalasi Lintas Platform**:
```bash
# Windows
winget install Ollama.Ollama

# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Memulai Pengembangan Agen**:
```bash
# Start Ollama service
ollama serve

# Pull and run models for agent development
ollama pull phi3.5:3.8b-mini-instruct-q4_K_M    # Microsoft Phi-3.5 Mini
ollama pull qwen2.5:0.5b-instruct-q4_K_M        # Qwen2.5 0.5B
ollama pull llama3.2:1b-instruct-q4_K_M         # Llama 3.2 1B

# Test model availability
ollama list

# Test API endpoint
curl http://localhost:11434/api/generate -d '{
  "model": "phi3.5:3.8b-mini-instruct-q4_K_M",
  "prompt": "Hello, how can I help you today?"
}'
```

#### Integrasi Kerangka Agen

**Ollama dengan Microsoft Agent Framework**:
```python
from microsoft_agent_framework import Agent, Config
import openai
import requests
import json

class OllamaManager:
    def __init__(self, model_name: str, base_url: str = "http://localhost:11434"):
        self.model_name = model_name
        self.base_url = base_url
        self.api_url = f"{base_url}/api"
        self.openai_url = f"{base_url}/v1"
        
    def ensure_model_available(self) -> bool:
        """Ensure the model is pulled and available."""
        try:
            response = requests.post(f"{self.api_url}/pull", 
                json={"name": self.model_name})
            return response.status_code == 200
        except Exception as e:
            print(f"Failed to pull model {self.model_name}: {e}")
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for Ollama."""
        return openai.OpenAI(
            base_url=self.openai_url,
            api_key="ollama",  # Ollama doesn't require real API key
        )
    
    def health_check(self) -> bool:
        """Check if Ollama service is running."""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            return response.status_code == 200
        except:
            return False

# Initialize Ollama for agent development
ollama_manager = OllamaManager("phi3.5:3.8b-mini-instruct-q4_K_M")
ollama_manager.ensure_model_available()

# Configure agent with Ollama backend
agent_config = Config(
    name="ollama-agent",
    model_provider="ollama",
    model_id="phi3.5:3.8b-mini-instruct-q4_K_M",
    endpoint=ollama_manager.openai_url,
    api_key="ollama"
)

agent = Agent(config=agent_config)
```

**Pengaturan Agen Multi-Model dengan Ollama**:
```python
class OllamaMultiModelManager:
    def __init__(self):
        self.models = {
            "lightweight": "qwen2.5:0.5b-instruct-q4_K_M",      # 350MB
            "balanced": "phi3.5:3.8b-mini-instruct-q4_K_M",     # 2.3GB  
            "capable": "llama3.2:3b-instruct-q4_K_M",           # 1.9GB
            "coding": "codellama:7b-code-q4_K_M"                # 4.1GB
        }
        self.base_url = "http://localhost:11434"
        self.clients = {}
        self._initialize_models()
    
    def _initialize_models(self):
        """Pull all required models and create clients."""
        for category, model_name in self.models.items():
            # Pull model if not available
            self._pull_model(model_name)
            
            # Create OpenAI client for each model
            self.clients[category] = openai.OpenAI(
                base_url=f"{self.base_url}/v1",
                api_key="ollama"
            )
    
    def _pull_model(self, model_name: str):
        """Pull model if not already available."""
        try:
            response = requests.post(f"{self.base_url}/api/pull", 
                json={"name": model_name})
            if response.status_code == 200:
                print(f"Model {model_name} ready")
        except Exception as e:
            print(f"Failed to pull {model_name}: {e}")
    
    def get_agent_for_task(self, task_type: str) -> Agent:
        """Get appropriate agent based on task complexity."""
        model_category = self._classify_task(task_type)
        model_name = self.models[model_category]
        
        config = Config(
            name=f"ollama-{model_category}-agent",
            model_provider="ollama",
            model_id=model_name,
            endpoint=f"{self.base_url}/v1",
            api_key="ollama"
        )
        
        return Agent(config=config)
    
    def _classify_task(self, task_type: str) -> str:
        """Classify task to appropriate model category."""
        if any(keyword in task_type.lower() for keyword in ["simple", "route", "classify"]):
            return "lightweight"
        elif any(keyword in task_type.lower() for keyword in ["code", "programming", "debug"]):
            return "coding"
        elif any(keyword in task_type.lower() for keyword in ["complex", "analysis", "research"]):
            return "capable"
        else:
            return "balanced"

# Usage example
manager = OllamaMultiModelManager()

# Get appropriate agents for different tasks
routing_agent = manager.get_agent_for_task("simple routing")
coding_agent = manager.get_agent_for_task("code debugging")
analysis_agent = manager.get_agent_for_task("complex analysis")
```

#### Pola Penerapan Produksi

**Layanan Produksi dengan Ollama**:
```python
import asyncio
import logging
from typing import Dict, Optional
from microsoft_agent_framework import Agent, Config
import requests
import openai

class OllamaProductionService:
    def __init__(self, models_config: Dict[str, str]):
        self.models_config = models_config
        self.base_url = "http://localhost:11434"
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "errors": 0,
            "model_usage": {model: 0 for model in models_config.keys()}
        }
        self._initialize_production_agents()
    
    def _initialize_production_agents(self):
        """Initialize production agents with health checks."""
        for agent_type, model_name in self.models_config.items():
            try:
                # Ensure model is available
                self._ensure_model_ready(model_name)
                
                # Create production agent
                config = Config(
                    name=f"production-{agent_type}",
                    model_provider="ollama",
                    model_id=model_name,
                    endpoint=f"{self.base_url}/v1",
                    api_key="ollama",
                    max_tokens=512,
                    temperature=0.1,
                    timeout=30.0
                )
                
                agent = Agent(config=config)
                
                # Add production tools based on agent type
                self._add_production_tools(agent, agent_type)
                
                self.agents[agent_type] = agent
                logging.info(f"Initialized {agent_type} agent with model {model_name}")
                
            except Exception as e:
                logging.error(f"Failed to initialize {agent_type} agent: {e}")
    
    def _ensure_model_ready(self, model_name: str):
        """Ensure model is pulled and ready for use."""
        try:
            # Check if model exists
            response = requests.get(f"{self.base_url}/api/tags")
            models = response.json().get('models', [])
            
            model_exists = any(model['name'] == model_name for model in models)
            
            if not model_exists:
                logging.info(f"Pulling model {model_name}...")
                pull_response = requests.post(f"{self.base_url}/api/pull", 
                    json={"name": model_name})
                
                if pull_response.status_code != 200:
                    raise Exception(f"Failed to pull model {model_name}")
                    
        except Exception as e:
            raise Exception(f"Model setup failed for {model_name}: {e}")
    
    def _add_production_tools(self, agent: Agent, agent_type: str):
        """Add tools based on agent type."""
        if agent_type == "customer_service":
            @agent.tool
            def lookup_customer(customer_id: str) -> dict:
                """Look up customer information."""
                # Simulate database lookup
                return {"customer_id": customer_id, "status": "active", "tier": "premium"}
            
            @agent.tool
            def create_support_ticket(issue: str, priority: str = "medium") -> str:
                """Create a support ticket."""
                ticket_id = f"TICK-{hash(issue) % 10000:04d}"
                return f"Created ticket {ticket_id} with priority {priority}"
        
        elif agent_type == "technical_support":
            @agent.tool
            def run_diagnostics(system_info: str) -> dict:
                """Run system diagnostics."""
                return {"status": "healthy", "issues": [], "recommendations": []}
            
            @agent.tool
            def access_knowledge_base(query: str) -> str:
                """Search technical knowledge base."""
                return f"Knowledge base results for: {query}"
    
    async def process_request(self, request: str, agent_type: str = "customer_service") -> dict:
        """Process user request with monitoring and error handling."""
        start_time = time.time()
        
        try:
            if agent_type not in self.agents:
                raise ValueError(f"Agent type {agent_type} not available")
            
            agent = self.agents[agent_type]
            response = await agent.chat_async(request)
            
            # Update metrics
            self.metrics["requests_processed"] += 1
            self.metrics["model_usage"][agent_type] += 1
            
            processing_time = time.time() - start_time
            
            self._log_interaction(request, response, "success", processing_time, agent_type)
            
            return {
                "response": response,
                "status": "success",
                "processing_time": processing_time,
                "agent_type": agent_type
            }
            
        except Exception as e:
            self.metrics["errors"] += 1
            processing_time = time.time() - start_time
            
            self._log_interaction(request, str(e), "error", processing_time, agent_type)
            
            return {
                "response": "I'm experiencing technical difficulties. Please try again.",
                "status": "error",
                "error": str(e),
                "processing_time": processing_time
            }
    
    def _log_interaction(self, request: str, response: str, status: str, 
                        processing_time: float, agent_type: str):
        """Log interaction for monitoring and analysis."""
        logging.info(f"Agent: {agent_type}, Status: {status}, Time: {processing_time:.2f}s")
        
        # In production, this would write to a proper logging system
        log_entry = {
            "timestamp": time.time(),
            "agent_type": agent_type,
            "request_length": len(request),
            "response_length": len(response),
            "status": status,
            "processing_time": processing_time
        }
    
    def get_health_status(self) -> dict:
        """Get service health status."""
        try:
            # Check Ollama service health
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            ollama_healthy = response.status_code == 200
            
            # Check model availability
            available_models = []
            if ollama_healthy:
                models = response.json().get('models', [])
                available_models = [model['name'] for model in models]
            
            return {
                "service_status": "healthy" if ollama_healthy else "unhealthy",
                "ollama_endpoint": self.base_url,
                "available_models": available_models,
                "active_agents": list(self.agents.keys()),
                "metrics": self.metrics,
                "timestamp": time.time()
            }
            
        except Exception as e:
            return {
                "service_status": "error",
                "error": str(e),
                "timestamp": time.time()
            }

# Production deployment example
production_models = {
    "customer_service": "phi3.5:3.8b-mini-instruct-q4_K_M",
    "technical_support": "llama3.2:3b-instruct-q4_K_M",
    "routing": "qwen2.5:0.5b-instruct-q4_K_M"
}

service = OllamaProductionService(production_models)

# Process requests
result = await service.process_request(
    "I need help with my account settings", 
    "customer_service"
)
print(result)
```

#### Fitur Tingkat Perusahaan dan Pemantauan

**Pemantauan dan Observabilitas Ollama**:
```python
import time
import asyncio
import requests
from typing import Dict, List

class OllamaMonitoringService:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.metrics_history = []
        self.alert_thresholds = {
            "response_time_ms": 2000,
            "error_rate_percent": 5,
            "memory_usage_percent": 85
        }
    
    async def collect_metrics(self) -> dict:
        """Collect comprehensive metrics from Ollama service."""
        metrics = {
            "timestamp": time.time(),
            "service_status": "unknown",
            "models": {},
            "performance": {},
            "resources": {}
        }
        
        try:
            # Check service health
            health_response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            metrics["service_status"] = "healthy" if health_response.status_code == 200 else "unhealthy"
            
            if metrics["service_status"] == "healthy":
                # Get model information
                models_data = health_response.json().get('models', [])
                for model in models_data:
                    model_name = model['name']
                    metrics["models"][model_name] = {
                        "size_gb": model.get('size', 0) / (1024**3),
                        "modified": model.get('modified_at', ''),
                        "digest": model.get('digest', '')[:12]  # Short digest
                    }
                
                # Test inference performance
                start_time = time.time()
                test_response = requests.post(f"{self.base_url}/api/generate", 
                    json={
                        "model": list(metrics["models"].keys())[0] if metrics["models"] else "",
                        "prompt": "Hello",
                        "stream": False
                    }, timeout=10)
                
                if test_response.status_code == 200:
                    inference_time = (time.time() - start_time) * 1000
                    metrics["performance"] = {
                        "inference_time_ms": inference_time,
                        "tokens_per_second": self._calculate_tokens_per_second(test_response.json()),
                        "last_successful_inference": time.time()
                    }
            
        except Exception as e:
            metrics["service_status"] = "error"
            metrics["error"] = str(e)
        
        self.metrics_history.append(metrics)
        
        # Keep only last 100 metrics entries
        if len(self.metrics_history) > 100:
            self.metrics_history = self.metrics_history[-100:]
        
        return metrics
    
    def _calculate_tokens_per_second(self, response_data: dict) -> float:
        """Calculate approximate tokens per second from response."""
        try:
            # Estimate tokens (rough approximation)
            response_text = response_data.get('response', '')
            estimated_tokens = len(response_text.split())
            
            # Get timing info if available
            eval_duration = response_data.get('eval_duration', 0)
            if eval_duration > 0:
                # Convert nanoseconds to seconds
                duration_seconds = eval_duration / 1e9
                return estimated_tokens / duration_seconds if duration_seconds > 0 else 0
        except:
            pass
        return 0
    
    def check_alerts(self, current_metrics: dict) -> List[dict]:
        """Check current metrics against alert thresholds."""
        alerts = []
        
        # Check response time
        if current_metrics.get('performance', {}).get('inference_time_ms', 0) > self.alert_thresholds['response_time_ms']:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {current_metrics['performance']['inference_time_ms']:.0f}ms",
                "severity": "warning"
            })
        
        # Check service status
        if current_metrics.get('service_status') != 'healthy':
            alerts.append({
                "type": "availability",
                "message": f"Service unhealthy: {current_metrics.get('error', 'Unknown error')}",
                "severity": "critical"
            })
        
        return alerts
    
    def get_performance_summary(self, minutes: int = 60) -> dict:
        """Get performance summary for the last N minutes."""
        cutoff_time = time.time() - (minutes * 60)
        recent_metrics = [m for m in self.metrics_history if m['timestamp'] > cutoff_time]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        # Calculate averages
        response_times = [m.get('performance', {}).get('inference_time_ms', 0) 
                         for m in recent_metrics if m.get('performance')]
        
        healthy_checks = sum(1 for m in recent_metrics if m.get('service_status') == 'healthy')
        uptime_percent = (healthy_checks / len(recent_metrics)) * 100 if recent_metrics else 0
        
        return {
            "period_minutes": minutes,
            "total_checks": len(recent_metrics),
            "uptime_percent": uptime_percent,
            "avg_response_time_ms": sum(response_times) / len(response_times) if response_times else 0,
            "max_response_time_ms": max(response_times) if response_times else 0,
            "min_response_time_ms": min(response_times) if response_times else 0
        }

# Production monitoring setup
monitor = OllamaMonitoringService()

async def monitoring_loop():
    """Continuous monitoring loop."""
    while True:
        try:
            metrics = await monitor.collect_metrics()
            alerts = monitor.check_alerts(metrics)
            
            if alerts:
                for alert in alerts:
                    logging.warning(f"ALERT: {alert['message']} (Severity: {alert['severity']})")
            
            # Log performance summary every 10 minutes
            if int(time.time()) % 600 == 0:  # Every 10 minutes
                summary = monitor.get_performance_summary(10)
                logging.info(f"Performance Summary: {summary}")
            
        except Exception as e:
            logging.error(f"Monitoring error: {e}")
        
        await asyncio.sleep(30)  # Check every 30 seconds

# Start monitoring
# asyncio.create_task(monitoring_loop())
```

#### Konfigurasi dan Optimasi Lanjutan

**Manajemen Model Kustom dengan Ollama**:
```python
class OllamaModelManager:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.model_catalog = {
            # Lightweight models for fast responses
            "ultra_light": [
                "qwen2.5:0.5b-instruct-q4_K_M",
                "tinyllama:1.1b-chat-q4_K_M"
            ],
            # Balanced models for general use
            "balanced": [
                "phi3.5:3.8b-mini-instruct-q4_K_M",
                "llama3.2:3b-instruct-q4_K_M"
            ],
            # Specialized models for specific tasks
            "code_specialist": [
                "codellama:7b-code-q4_K_M",
                "codegemma:7b-code-q4_K_M"
            ],
            # High capability models
            "high_capability": [
                "llama3.1:8b-instruct-q4_K_M",
                "qwen2.5:7b-instruct-q4_K_M"
            ]
        }
    
    def setup_production_models(self, categories: List[str]) -> dict:
        """Set up models for production use."""
        setup_results = {}
        
        for category in categories:
            if category not in self.model_catalog:
                setup_results[category] = {"status": "error", "message": "Unknown category"}
                continue
            
            models = self.model_catalog[category]
            category_results = []
            
            for model in models:
                try:
                    # Pull model
                    response = requests.post(f"{self.base_url}/api/pull", 
                        json={"name": model})
                    
                    if response.status_code == 200:
                        category_results.append({"model": model, "status": "ready"})
                    else:
                        category_results.append({"model": model, "status": "failed"})
                        
                except Exception as e:
                    category_results.append({"model": model, "status": "error", "error": str(e)})
            
            setup_results[category] = category_results
        
        return setup_results
    
    def optimize_for_hardware(self) -> dict:
        """Recommend optimal models based on available hardware."""
        # This would typically check actual hardware specs
        # For demo purposes, we'll simulate hardware detection
        
        recommendations = {
            "low_resource": {
                "models": ["qwen2.5:0.5b-instruct-q4_K_M"],
                "max_concurrent": 1,
                "memory_usage": "< 1GB"
            },
            "medium_resource": {
                "models": ["phi3.5:3.8b-mini-instruct-q4_K_M", "llama3.2:3b-instruct-q4_K_M"],
                "max_concurrent": 2,
                "memory_usage": "2-4GB"
            },
            "high_resource": {
                "models": ["llama3.1:8b-instruct-q4_K_M", "codellama:7b-code-q4_K_M"],
                "max_concurrent": 3,
                "memory_usage": "6-12GB"
            }
        }
        
        return recommendations

# Production model setup
model_manager = OllamaModelManager()
setup_results = model_manager.setup_production_models(["balanced", "ultra_light"])
print(f"Model setup results: {setup_results}")
```

**Daftar Periksa Penerapan Produksi untuk Ollama**:

âœ… **Konfigurasi Layanan**:
- Instal layanan Ollama dengan integrasi sistem yang tepat
- Konfigurasikan model untuk kasus penggunaan agen tertentu
- Siapkan skrip startup dan manajemen layanan yang sesuai
- Uji pemuatan model dan ketersediaan API

âœ… **Manajemen Model**:
- Unduh model yang diperlukan dan verifikasi integritasnya
- Siapkan prosedur pembaruan dan rotasi model
- Konfigurasikan optimasi caching dan penyimpanan model
- Uji kinerja model di bawah beban yang diharapkan

âœ… **Pengaturan Keamanan**:
- Konfigurasikan aturan firewall untuk akses hanya lokal
- Siapkan kontrol akses API dan pembatasan tingkat
- Terapkan log audit untuk interaksi agen
- Konfigurasikan penyimpanan model yang aman dan akses

âœ… **Optimasi Kinerja**:
- Benchmark model untuk kasus penggunaan yang diharapkan
- Konfigurasikan akselerasi perangkat keras yang sesuai
- Siapkan strategi pemanasan dan caching model
- Pantau penggunaan sumber daya dan metrik kinerja

âœ… **Pengujian Integrasi**:
- Uji integrasi Microsoft Agent Framework
- Verifikasi kemampuan operasi offline
- Uji skenario failover dan penanganan kesalahan
- Validasi alur kerja agen end-to-end

**Perbandingan dengan Foundry Local**:

| Fitur | Foundry Local | Ollama |
|-------|---------------|--------|
| **Kasus Penggunaan Utama** | Produksi perusahaan | Pengembangan & komunitas |
| **Ekosistem Model** | Kurasi Microsoft | Komunitas yang luas |
| **Optimasi Perangkat Keras** | Otomatis (CUDA/NPU/CPU) | Konfigurasi manual |
| **Fitur Perusahaan** | Pemantauan bawaan, keamanan | Alat komunitas |
| **Kompleksitas Penerapan** | Sederhana (instalasi winget) | Sederhana (instalasi curl) |
| **Kompatibilitas API** | OpenAI + ekstensi | Standar OpenAI |
| **Dukungan** | Resmi Microsoft | Berbasis komunitas |
| **Terbaik Untuk** | Agen produksi | Prototipe, penelitian |

**Kapan Memilih Ollama**:
- **Pengembangan dan Prototipe**: Eksperimen cepat dengan berbagai model
- **Model Komunitas**: Akses ke model kontribusi komunitas terbaru
- **Penggunaan Pendidikan**: Belajar dan mengajar pengembangan agen AI
- **Proyek Penelitian**: Penelitian akademik yang membutuhkan akses ke model yang beragam
- **Model Kustom**: Membangun dan menguji model yang disesuaikan

### VLLM: Inferensi Agen SLM Berkinerja Tinggi

VLLM (Inferensi Model Bahasa Sangat Besar) menyediakan mesin inferensi throughput tinggi dan efisien memori yang dioptimalkan khusus untuk penerapan SLM produksi dalam skala besar. Sementara Foundry Local berfokus pada kemudahan penggunaan dan Ollama menekankan model komunitas, VLLM unggul dalam skenario berkinerja tinggi yang membutuhkan throughput maksimum dan pemanfaatan sumber daya yang efisien.

**Arsitektur dan Fitur Inti**:
- **PagedAttention**: Manajemen memori revolusioner untuk perhitungan perhatian yang efisien
- **Dynamic Batching**: Pengelompokan permintaan cerdas untuk throughput optimal
- **Optimasi GPU**: Kernel CUDA canggih dan dukungan paralelisme tensor
- **Kompatibilitas OpenAI**: Kompatibilitas API penuh untuk integrasi yang mulus
- **Speculative Decoding**: Teknik percepatan inferensi canggih
- **Dukungan Kuantisasi**: Kuantisasi INT4, INT8, dan FP16 untuk efisiensi memori

#### Instalasi dan Pengaturan

**Opsi Instalasi**:
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

**Memulai Cepat untuk Pengembangan Agen**:
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```

#### Integrasi Kerangka Agen

**VLLM dengan Microsoft Agent Framework**:
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```

**Pengaturan Multi-Agen Throughput Tinggi**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```

#### Pola Penerapan Produksi

**Layanan Produksi VLLM Perusahaan**:
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```

#### Fitur Perusahaan dan Pemantauan

**Pemantauan Kinerja VLLM Lanjutan**:
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```

#### Konfigurasi dan Optimasi Lanjutan

**Template Konfigurasi Produksi VLLM**:
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```

**Daftar Periksa Penerapan Produksi untuk VLLM**:

âœ… **Optimasi Perangkat Keras**:
- Konfigurasikan paralelisme tensor untuk pengaturan multi-GPU
- Aktifkan kuantisasi (AWQ/GPTQ) untuk efisiensi memori
- Atur pemanfaatan memori GPU optimal (85-95%)
- Konfigurasikan ukuran batch yang sesuai untuk throughput

âœ… **Penyetelan Kinerja**:
- Aktifkan caching prefiks untuk kueri berulang
- Konfigurasikan prefill tersegmentasi untuk urutan panjang
- Atur speculative decoding untuk inferensi lebih cepat
- Optimalkan max_num_seqs berdasarkan perangkat keras

âœ… **Fitur Produksi**:
- Atur pemantauan kesehatan dan pengumpulan metrik
- Konfigurasikan restart otomatis dan failover
- Implementasikan antrian permintaan dan penyeimbangan beban
- Atur pencatatan dan pemberitahuan yang komprehensif

âœ… **Keamanan dan Keandalan**:
- Konfigurasikan aturan firewall dan kontrol akses
- Atur pembatasan tingkat API dan autentikasi
- Implementasikan shutdown dan pembersihan yang mulus
- Konfigurasikan cadangan dan pemulihan bencana

âœ… **Pengujian Integrasi**:
- Uji integrasi Microsoft Agent Framework
- Validasi skenario throughput tinggi
- Uji prosedur failover dan pemulihan
- Benchmark kinerja di bawah beban

**Perbandingan dengan Solusi Lain**:

| Fitur | VLLM | Foundry Local | Ollama |
|-------|------|---------------|--------|
| **Kasus Penggunaan Utama** | Produksi throughput tinggi | Kemudahan penggunaan perusahaan | Pengembangan & komunitas |
| **Kinerja** | Throughput maksimum | Seimbang | Baik |
| **Efisiensi Memori** | Optimasi PagedAttention | Optimasi otomatis | Standar |
| **Kompleksitas Pengaturan** | Tinggi (banyak parameter) | Rendah (otomatis) | Rendah (sederhana) |
| **Skalabilitas** | Sangat baik (paralel tensor/pipeline) | Baik | Terbatas |
| **Kuantisasi** | Lanjutan (AWQ, GPTQ, FP8) | Otomatis | Standar GGUF |
| **Fitur Perusahaan** | Implementasi kustom diperlukan | Bawaan | Alat komunitas |
| **Terbaik Untuk** | Agen produksi skala besar | Produksi perusahaan | Pengembangan |

**Kapan Memilih VLLM**:
- **Kebutuhan Throughput Tinggi**: Memproses ratusan permintaan per detik
- **Penerapan Skala Besar**: Pengaturan multi-GPU, multi-node
- **Kritis Kinerja**: Waktu respons di bawah satu detik dalam skala besar
- **Optimasi Lanjutan**: Membutuhkan kuantisasi dan pengelompokan kustom
- **Efisiensi Sumber Daya**: Pemanfaatan maksimum perangkat keras GPU yang mahal

## Aplikasi Agen SLM di Dunia Nyata

### Agen SLM Layanan Pelanggan
- **Kemampuan SLM**: Pencarian akun, reset kata sandi, pemeriksaan status pesanan
- **Manfaat biaya**: Pengurangan biaya inferensi hingga 10x dibandingkan agen LLM
- **Kinerja**: Waktu respons lebih cepat dengan kualitas konsisten untuk kueri rutin

### Agen Proses Bisnis SLM
- **Agen pemrosesan faktur**: Ekstraksi data, validasi informasi, rute untuk persetujuan
- **Agen manajemen email**: Kategorisasi, prioritas, penyusunan respons otomatis
- **Agen penjadwalan**: Koordinasi rapat, pengelolaan kalender, pengiriman pengingat

### Asisten Digital Pribadi SLM
- **Agen manajemen tugas**: Membuat, memperbarui, mengatur daftar tugas secara efisien
- **Agen pengumpulan informasi**: Meneliti topik, merangkum temuan secara lokal
- **Agen komunikasi**: Menyusun email, pesan, posting media sosial secara pribadi

### Agen SLM Perdagangan dan Keuangan
- **Agen pemantauan pasar**: Melacak harga, mengidentifikasi tren secara real-time
- **Agen pembuatan laporan**: Membuat ringkasan harian/mingguan secara otomatis
- **Agen penilaian risiko**: Mengevaluasi posisi portofolio menggunakan data lokal

### Agen Dukungan Kesehatan SLM
- **Agen penjadwalan pasien**: Koordinasi janji temu, pengiriman pengingat otomatis
- **Agen dokumentasi**: Membuat ringkasan medis, laporan secara lokal
- **Agen manajemen resep**: Melacak pengisian ulang, memeriksa interaksi secara pribadi

## Microsoft Agent Framework: Pengembangan Agen Siap Produksi

### Ikhtisar dan Arsitektur

Microsoft Agent Framework menyediakan platform komprehensif kelas perusahaan untuk membangun, menerapkan, dan mengelola agen AI yang dapat beroperasi baik di cloud maupun lingkungan edge offline. Kerangka ini dirancang khusus untuk bekerja dengan Small Language Models dan skenario komputasi edge, menjadikannya ideal untuk penerapan yang sensitif terhadap privasi dan sumber daya terbatas.

**Komponen Inti Kerangka**:
- **Agent Runtime**: Lingkungan eksekusi ringan yang dioptimalkan untuk perangkat edge
- **Sistem Integrasi Alat**: Arsitektur plugin yang dapat diperluas untuk menghubungkan layanan eksternal dan API
- **Manajemen Status**: Memori agen yang persisten dan penanganan konteks antar sesi
- **Lapisan Keamanan**: Kontrol keamanan bawaan untuk penerapan perusahaan
- **Mesin Orkestrasi**: Koordinasi multi-agen dan manajemen alur kerja

### Fitur Utama untuk Penerapan Edge

**Arsitektur Offline-First**: Microsoft Agent Framework dirancang dengan prinsip offline-first, memungkinkan agen beroperasi secara efektif tanpa konektivitas internet yang konstan. Ini mencakup inferensi model lokal, basis pengetahuan yang di-cache, eksekusi alat offline, dan degradasi yang mulus saat layanan cloud tidak tersedia.

**Optimasi Sumber Daya**: Kerangka ini menyediakan manajemen sumber daya cerdas dengan optimasi memori otomatis untuk SLM, penyeimbangan beban CPU/GPU untuk perangkat edge, pemilihan model adaptif berdasarkan sumber daya yang tersedia, dan pola inferensi hemat daya untuk penerapan mobile.

**Keamanan dan Privasi**: Fitur keamanan kelas perusahaan mencakup pemrosesan data lokal untuk menjaga privasi, saluran komunikasi agen yang terenkripsi, kontrol akses berbasis peran untuk kemampuan agen, dan pencatatan audit untuk persyaratan kepatuhan.

### Integrasi dengan Foundry Local

Microsoft Agent Framework terintegrasi dengan mulus dengan Foundry Local untuk menyediakan solusi AI edge yang lengkap:

**Penemuan Model Otomatis**: Kerangka ini secara otomatis mendeteksi dan terhubung ke instance Foundry Local, menemukan model SLM yang tersedia, dan memilih model optimal berdasarkan kebutuhan agen dan kemampuan perangkat keras.

**Pemuatan Model Dinamis**: Agen dapat memuat model SLM yang berbeda secara dinamis untuk tugas tertentu, memungkinkan sistem agen multi-model di mana model yang berbeda menangani jenis permintaan yang berbeda, dan failover otomatis antar model berdasarkan ketersediaan dan kinerja.

**Optimasi Kinerja**: Mekanisme caching terintegrasi mengurangi waktu pemuatan model, pooling koneksi mengoptimalkan panggilan API ke Foundry Local, dan pengelompokan cerdas meningkatkan throughput untuk beberapa permintaan agen.

### Membangun Agen dengan Microsoft Agent Framework

#### Definisi dan Konfigurasi Agen

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```

#### Integrasi Alat untuk Skenario Edge

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```

#### Orkestrasi Multi-Agen

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```

### Pola Penerapan Edge Lanjutan

#### Arsitektur Agen Hierarkis

**Kluster Agen Lokal**: Terapkan beberapa agen SLM khusus pada perangkat edge, masing-masing dioptimalkan untuk tugas tertentu. Gunakan model ringan seperti Qwen2.5-0.5B untuk routing dan penjadwalan sederhana, model menengah seperti Phi-4-Mini untuk layanan pelanggan dan dokumentasi, dan model yang lebih besar untuk penalaran kompleks saat sumber daya memungkinkan.

**Koordinasi Edge-to-Cloud**: Implementasikan pola eskalasi cerdas di mana agen lokal menangani tugas rutin, agen cloud menyediakan penalaran kompleks saat konektivitas memungkinkan, dan penyerahan yang mulus antara pemrosesan edge dan cloud menjaga kontinuitas.

#### Konfigurasi Penerapan

**Penerapan Perangkat Tunggal**:
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```

**Penerapan Edge Terdistribusi**:
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```

### Optimasi Kinerja untuk Agen Edge

#### Strategi Pemilihan Model

**Penugasan Model Berdasarkan Tugas**: Microsoft Agent Framework memungkinkan pemilihan model cerdas berdasarkan kompleksitas tugas dan persyaratan:

- **Tugas Sederhana** (Q&A, routing): Qwen2.5-0.5B (500MB, <100ms respons)
- **Tugas Sedang** (layanan pelanggan, penjadwalan): Phi-4-Mini (2.4GB, 200-500ms respons)
- **Tugas Kompleks** (analisis teknis, perencanaan): Phi-4 (7GB, 1-3s respons saat sumber daya memungkinkan)

**Pengalihan Model Dinamis**: Agen dapat beralih antara model berdasarkan beban sistem saat ini, penilaian kompleksitas tugas, tingkat prioritas pengguna, dan sumber daya perangkat keras yang tersedia.

#### Manajemen Memori dan Sumber Daya

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

### Pola Integrasi Perusahaan

#### Keamanan dan Kepatuhan

**Pemrosesan Data Lokal**: Semua pemrosesan agen terjadi secara lokal, memastikan data sensitif tidak pernah meninggalkan perangkat edge. Ini mencakup perlindungan informasi pelanggan, kepatuhan HIPAA untuk agen kesehatan, keamanan data keuangan untuk agen perbankan, dan kepatuhan GDPR untuk penerapan di Eropa.

**Kontrol Akses**: Izin berbasis peran mengontrol alat mana yang dapat diakses agen, autentikasi pengguna untuk interaksi agen, dan jejak audit untuk semua tindakan dan keputusan agen.

#### Pemantauan dan Observabilitas

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```

### Contoh Implementasi Dunia Nyata

#### Sistem Agen Edge Ritel

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```

#### Agen Dukungan Kesehatan

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```

### Praktik Terbaik untuk Microsoft Agent Framework

#### Panduan Pengembangan

1. **Mulai Sederhana**: Mulailah dengan skenario agen tunggal sebelum membangun sistem multi-agen yang kompleks
2. **Penyesuaian Model**: Pilih model terkecil yang memenuhi persyaratan akurasi Anda
3. **Desain Alat**: Buat alat yang fokus dan memiliki tujuan tunggal daripada alat multi-fungsi yang kompleks
4. **Penanganan Kesalahan**: Implementasikan degradasi yang mulus untuk skenario offline dan kegagalan model
5. **Pengujian**: Uji agen secara ekstensif dalam kondisi offline dan lingkungan dengan sumber daya terbatas

#### Praktik Terbaik Penerapan

1. **Peluncuran Bertahap**: Terapkan ke kelompok pengguna kecil terlebih dahulu, pantau metrik kinerja dengan cermat
2. **Pemantauan Sumber Daya**: Atur pemberitahuan untuk ambang batas memori, CPU, dan waktu respons
3. **Strategi Cadangan**: Selalu miliki rencana cadangan untuk kegagalan model atau kehabisan sumber daya
4. **Keamanan Utama**: Implementasikan kontrol keamanan sejak awal, bukan sebagai pemikiran belakangan
5. **Dokumentasi**: Pertahankan dokumentasi yang jelas tentang kemampuan dan keterbatasan agen

### Peta Jalan dan Integrasi Masa Depan

Microsoft Agent Framework terus berkembang dengan optimasi SLM yang ditingkatkan, alat penerapan edge yang lebih baik, manajemen sumber daya yang lebih baik untuk lingkungan yang terbatas, dan ekosistem alat yang diperluas untuk skenario perusahaan umum.

**Fitur Mendatang**:
- **AutoML untuk Optimasi Agen**: Penyetelan otomatis SLM untuk tugas agen tertentu
- **Edge Mesh Networking**: Koordinasi antara beberapa penerapan agen edge
- **Telemetry Lanjutan**: Pemantauan dan analitik yang ditingkatkan untuk kinerja agen
- **Visual Agent Builder**: Alat pengembangan agen low-code/no-code

## Praktik Terbaik untuk Implementasi Agen SLM

### Panduan Pemilihan SLM untuk Agen

Saat memilih SLM untuk penerapan agen, pertimbangkan faktor berikut:

**Pertimbangan Ukuran Model**: Pilih model ultra-kompresi seperti Q2_K untuk aplikasi agen mobile yang sangat terbatas, model seimbang seperti Q4_K_M untuk skenario agen umum, dan model presisi tinggi seperti Q8_0 untuk aplikasi agen yang kritis terhadap kualitas.

**Keselarasan Kasus Penggunaan Agen**: Cocokkan kemampuan SLM dengan persyaratan agen tertentu, mempertimbangkan faktor seperti pelestarian akurasi untuk keputusan agen, kecepatan inferensi untuk interaksi agen real-time, kendala memori untuk penerapan agen edge, dan persyaratan operasi offline untuk agen yang berfokus pada privasi.

### Pemilihan Strategi Optimasi untuk Agen SLM

**Pendekatan Kuantisasi untuk Agen**: Pilih tingkat kuantisasi yang sesuai berdasarkan persyaratan kualitas agen dan kendala perangkat keras. Pertimbangkan Q4_0 untuk kompresi maksimum dalam agen mobile, Q5_1 untuk kualitas-kompresi seimbang dalam agen umum, dan Q8_0 untuk kualitas mendekati asli dalam aplikasi agen yang kritis.
**Pemilihan Kerangka Kerja untuk Penerapan Agen**: Pilih kerangka kerja optimasi berdasarkan perangkat keras target dan kebutuhan agen. Gunakan Llama.cpp untuk penerapan agen yang dioptimalkan CPU, Apple MLX untuk aplikasi agen Apple Silicon, dan ONNX untuk kompatibilitas agen lintas platform.

## Konversi Praktis Agen SLM dan Kasus Penggunaan

### Skenario Penerapan Agen di Dunia Nyata

**Aplikasi Agen Mobile**: Format Q4_K sangat cocok untuk aplikasi agen di smartphone dengan penggunaan memori yang minimal, sementara Q8_0 memberikan kinerja seimbang untuk sistem agen berbasis tablet. Format Q5_K menawarkan kualitas unggul untuk agen produktivitas mobile.

**Komputasi Agen Desktop dan Edge**: Q5_K memberikan kinerja optimal untuk aplikasi agen desktop, Q8_0 menyediakan inferensi berkualitas tinggi untuk lingkungan agen workstation, dan Q4_K memungkinkan pemrosesan yang efisien pada perangkat agen edge.

**Penelitian dan Eksperimen Agen**: Format kuantisasi canggih memungkinkan eksplorasi inferensi agen dengan presisi ultra-rendah untuk penelitian akademis dan aplikasi agen proof-of-concept yang membutuhkan sumber daya yang sangat terbatas.

### Tolok Ukur Kinerja Agen SLM

**Kecepatan Inferensi Agen**: Q4_K mencapai waktu respons agen tercepat pada CPU mobile, Q5_K memberikan rasio kecepatan-kualitas yang seimbang untuk aplikasi agen umum, Q8_0 menawarkan kualitas unggul untuk tugas agen yang kompleks, dan format eksperimental memberikan throughput maksimum untuk perangkat keras agen khusus.

**Kebutuhan Memori Agen**: Tingkat kuantisasi untuk agen berkisar dari Q2_K (di bawah 500MB untuk model agen kecil) hingga Q8_0 (sekitar 50% dari ukuran asli), dengan konfigurasi eksperimental mencapai kompresi maksimum untuk lingkungan agen dengan sumber daya terbatas.

## Tantangan dan Pertimbangan untuk Agen SLM

### Pertukaran Kinerja dalam Sistem Agen

Penerapan agen SLM melibatkan pertimbangan yang cermat antara ukuran model, kecepatan respons agen, dan kualitas output. Sementara Q4_K menawarkan kecepatan dan efisiensi luar biasa untuk agen mobile, Q8_0 memberikan kualitas unggul untuk tugas agen yang kompleks. Q5_K menjadi pilihan tengah yang cocok untuk sebagian besar aplikasi agen umum.

### Kompatibilitas Perangkat Keras untuk Agen SLM

Berbagai perangkat edge memiliki kemampuan yang berbeda untuk penerapan agen SLM. Q4_K berjalan efisien pada prosesor dasar untuk agen sederhana, Q5_K membutuhkan sumber daya komputasi moderat untuk kinerja agen yang seimbang, dan Q8_0 memanfaatkan perangkat keras kelas atas untuk kemampuan agen yang canggih.

### Keamanan dan Privasi dalam Sistem Agen SLM

Meskipun agen SLM memungkinkan pemrosesan lokal untuk meningkatkan privasi, langkah-langkah keamanan yang tepat harus diterapkan untuk melindungi model agen dan data di lingkungan edge. Hal ini sangat penting saat menerapkan format agen presisi tinggi di lingkungan perusahaan atau format agen terkompresi dalam aplikasi yang menangani data sensitif.

## Tren Masa Depan dalam Pengembangan Agen SLM

Lanskap agen SLM terus berkembang dengan kemajuan dalam teknik kompresi, metode optimasi, dan strategi penerapan edge. Perkembangan masa depan mencakup algoritma kuantisasi yang lebih efisien untuk model agen, metode kompresi yang lebih baik untuk alur kerja agen, dan integrasi yang lebih baik dengan akselerator perangkat keras edge untuk pemrosesan agen.

**Prediksi Pasar untuk Agen SLM**: Menurut penelitian terbaru, otomatisasi berbasis agen dapat menghilangkan 40â€“60% tugas kognitif berulang dalam alur kerja perusahaan pada tahun 2027, dengan SLM memimpin transformasi ini berkat efisiensi biaya dan fleksibilitas penerapannya.

**Tren Teknologi dalam Agen SLM**:
- **Agen SLM Khusus**: Model khusus domain yang dilatih untuk tugas agen tertentu dan industri tertentu
- **Komputasi Agen Edge**: Kemampuan agen di perangkat yang ditingkatkan dengan privasi yang lebih baik dan latensi yang berkurang
- **Orkestrasi Agen**: Koordinasi yang lebih baik antara beberapa agen SLM dengan pengaturan rute dinamis dan penyeimbangan beban
- **Demokratisasi**: Fleksibilitas SLM memungkinkan partisipasi yang lebih luas dalam pengembangan agen di berbagai organisasi

## Memulai dengan Agen SLM

### Langkah 1: Siapkan Lingkungan Kerangka Kerja Agen Microsoft

**Instalasi Dependensi**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inisialisasi Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Langkah 2: Pilih SLM untuk Aplikasi Agen
Pilihan populer untuk Kerangka Kerja Agen Microsoft:
- **Microsoft Phi-4 Mini (3.8B)**: Sangat baik untuk tugas agen umum dengan kinerja seimbang
- **Qwen2.5-0.5B (0.5B)**: Sangat efisien untuk agen routing dan klasifikasi sederhana
- **Qwen2.5-Coder-0.5B (0.5B)**: Khusus untuk tugas agen terkait kode
- **Phi-4 (7B)**: Penalaran tingkat lanjut untuk skenario edge yang kompleks jika sumber daya memungkinkan

### Langkah 3: Buat Agen Pertama Anda dengan Kerangka Kerja Agen Microsoft

**Pengaturan Agen Dasar**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Langkah 4: Tentukan Lingkup dan Kebutuhan Agen
Mulailah dengan aplikasi agen yang fokus dan terdefinisi dengan baik menggunakan Kerangka Kerja Agen Microsoft:
- **Agen domain tunggal**: Layanan pelanggan ATAU penjadwalan ATAU penelitian
- **Tujuan agen yang jelas**: Sasaran yang spesifik dan terukur untuk kinerja agen
- **Integrasi alat yang terbatas**: Maksimal 3-5 alat untuk penerapan agen awal
- **Batasan agen yang terdefinisi**: Jalur eskalasi yang jelas untuk skenario kompleks
- **Desain edge-first**: Prioritaskan fungsi offline dan pemrosesan lokal

### Langkah 5: Terapkan Penerapan Edge dengan Kerangka Kerja Agen Microsoft

**Konfigurasi Sumber Daya**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Terapkan Langkah Keamanan untuk Agen Edge**:
- **Validasi input lokal**: Periksa permintaan tanpa ketergantungan cloud
- **Penyaringan output offline**: Pastikan respons memenuhi standar kualitas secara lokal
- **Kontrol keamanan edge**: Terapkan keamanan tanpa memerlukan konektivitas internet
- **Pemantauan lokal**: Lacak kinerja dan tandai masalah menggunakan telemetri edge

### Langkah 6: Ukur dan Optimalkan Kinerja Agen Edge
- **Tingkat penyelesaian tugas agen**: Pantau tingkat keberhasilan dalam skenario offline
- **Waktu respons agen**: Pastikan waktu respons di bawah satu detik untuk penerapan edge
- **Pemanfaatan sumber daya**: Lacak penggunaan memori, CPU, dan baterai pada perangkat edge
- **Efisiensi biaya**: Bandingkan biaya penerapan edge dengan alternatif berbasis cloud
- **Keandalan offline**: Ukur kinerja agen selama gangguan jaringan

## Poin Penting untuk Implementasi Agen SLM

1. **SLM cukup untuk agen**: Untuk sebagian besar tugas agen, model kecil bekerja sebaik model besar sambil menawarkan keuntungan signifikan
2. **Efisiensi biaya dalam agen**: 10-30x lebih murah untuk menjalankan agen SLM, membuatnya layak secara ekonomi untuk penerapan luas
3. **Spesialisasi bekerja untuk agen**: SLM yang disesuaikan sering kali mengungguli LLM umum dalam aplikasi agen tertentu
4. **Arsitektur agen hibrida**: Gunakan SLM untuk tugas agen rutin, LLM untuk penalaran kompleks jika diperlukan
5. **Kerangka Kerja Agen Microsoft memungkinkan penerapan produksi**: Menyediakan alat kelas perusahaan untuk membangun, menerapkan, dan mengelola agen edge
6. **Prinsip desain edge-first**: Agen yang mampu offline dengan pemrosesan lokal memastikan privasi dan keandalan
7. **Integrasi Foundry Local**: Koneksi mulus antara Kerangka Kerja Agen Microsoft dan inferensi model lokal
8. **Masa depan adalah agen SLM**: Model bahasa kecil dengan kerangka kerja produksi adalah masa depan AI agen, memungkinkan penerapan agen yang demokratis dan efisien

## Referensi dan Bacaan Lanjutan

### Makalah Penelitian dan Publikasi Inti

#### Agen AI dan Sistem Agenik
- **"Language Agents as Optimizable Graphs"** (2024) - Penelitian mendasar tentang arsitektur agen dan strategi optimasi
  - Penulis: Wenyue Hua, Lishan Yang, et al.
  - Tautan: https://arxiv.org/abs/2402.16823
  - Wawasan Utama: Desain agen berbasis graf dan strategi optimasi

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Penulis: Zhiheng Xi, Wenxiang Chen, et al.
  - Tautan: https://arxiv.org/abs/2309.07864
  - Wawasan Utama: Survei komprehensif tentang kemampuan dan aplikasi agen berbasis LLM

- **"Cognitive Architectures for Language Agents"** (2024)
  - Penulis: Theodore Sumers, Shunyu Yao, et al.
  - Tautan: https://arxiv.org/abs/2309.02427
  - Wawasan Utama: Kerangka kerja kognitif untuk merancang agen cerdas

#### Model Bahasa Kecil dan Optimasi
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Penulis: Tim Penelitian Microsoft
  - Tautan: https://arxiv.org/abs/2404.14219
  - Wawasan Utama: Prinsip desain SLM dan strategi penerapan mobile

- **"Qwen2.5 Technical Report"** (2024)
  - Penulis: Tim Alibaba Cloud
  - Tautan: https://arxiv.org/abs/2407.10671
  - Wawasan Utama: Teknik pelatihan SLM canggih dan optimasi kinerja

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Penulis: Peiyuan Zhang, Guangtao Zeng, et al.
  - Tautan: https://arxiv.org/abs/2401.02385
  - Wawasan Utama: Desain model ultra-kompak dan efisiensi pelatihan

### Dokumentasi Resmi dan Kerangka Kerja

#### Kerangka Kerja Agen Microsoft
- **Dokumentasi Resmi**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **Repositori GitHub**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Repositori Utama**: https://github.com/microsoft/foundry-local
- **Dokumentasi**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Repositori Utama**: https://github.com/vllm-project/vllm
- **Dokumentasi**: https://docs.vllm.ai/


#### Ollama
- **Situs Resmi**: https://ollama.ai/
- **Repositori GitHub**: https://github.com/ollama/ollama

### Kerangka Kerja Optimasi Model

#### Llama.cpp
- **Repositori**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentasi**: https://microsoft.github.io/Olive/
- **Repositori GitHub**: https://github.com/microsoft/Olive

#### OpenVINO
- **Situs Resmi**: https://docs.openvino.ai/

#### Apple MLX
- **Repositori**: https://github.com/ml-explore/mlx

### Laporan Industri dan Analisis Pasar

#### Penelitian Pasar Agen AI
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Tautan: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - Wawasan Utama: Tren pasar dan pola adopsi perusahaan

#### Tolok Ukur Teknis

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Tautan: https://mlcommons.org/en/inference-edge/
  - Wawasan Utama: Metrik kinerja standar untuk penerapan edge

### Standar dan Spesifikasi

#### Format Model dan Standar
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Format model lintas platform untuk interoperabilitas
- **Spesifikasi GGUF**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Format model kuantisasi untuk inferensi CPU
- **Spesifikasi API OpenAI**: https://platform.openai.com/docs/api-reference
  - Format API standar untuk integrasi model bahasa

#### Keamanan dan Kepatuhan
- **Kerangka Kerja Manajemen Risiko AI NIST**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - Sistem AI**: Kerangka kerja untuk sistem AI dan keselamatan
- **Standar IEEE untuk AI**: https://standards.ieee.org/industry-connections/ai/

Peralihan menuju agen berbasis SLM mewakili perubahan mendasar dalam pendekatan kita terhadap penerapan AI. Kerangka Kerja Agen Microsoft, dikombinasikan dengan platform lokal dan Model Bahasa Kecil yang efisien, menyediakan solusi lengkap untuk membangun agen siap produksi yang beroperasi secara efektif di lingkungan edge. Dengan fokus pada efisiensi, spesialisasi, dan utilitas praktis, tumpukan teknologi ini membuat agen AI lebih mudah diakses, terjangkau, dan efektif untuk aplikasi dunia nyata di setiap industri dan lingkungan komputasi edge.

Seiring kemajuan hingga tahun 2025, kombinasi model kecil yang semakin mampu, kerangka kerja agen yang canggih seperti Kerangka Kerja Agen Microsoft, dan platform penerapan edge yang kuat akan membuka kemungkinan baru untuk sistem otonom yang dapat beroperasi secara efisien di perangkat edge sambil menjaga privasi, mengurangi biaya, dan memberikan pengalaman pengguna yang luar biasa.

**Langkah Selanjutnya untuk Implementasi**:
1. **Eksplorasi Function Calling**: Pelajari bagaimana SLM menangani integrasi alat dan output terstruktur
2. **Kuasi Model Context Protocol (MCP)**: Pahami pola komunikasi agen yang canggih
3. **Bangun Agen Produksi**: Gunakan Kerangka Kerja Agen Microsoft untuk penerapan kelas perusahaan
4. **Optimalkan untuk Edge**: Terapkan teknik optimasi canggih untuk lingkungan dengan sumber daya terbatas


## â¡ï¸ Apa yang berikutnya

- [02: Function Calling dalam Small Language Models (SLMs)](./02.FunctionCalling.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan layanan penerjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Meskipun kami berupaya untuk memberikan hasil yang akurat, harap diketahui bahwa terjemahan otomatis mungkin mengandung kesalahan atau ketidakakuratan. Dokumen asli dalam bahasa aslinya harus dianggap sebagai sumber yang otoritatif. Untuk informasi yang penting, disarankan menggunakan jasa penerjemahan manusia profesional. Kami tidak bertanggung jawab atas kesalahpahaman atau penafsiran yang timbul dari penggunaan terjemahan ini.