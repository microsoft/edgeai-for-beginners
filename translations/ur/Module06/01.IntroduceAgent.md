<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T11:04:24+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "ur"
}
-->
# Ø§Û’ Ø¢Ø¦ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ø§ÙˆØ± Ú†Ú¾ÙˆÙ¹Û’ Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ù…Ø§ÚˆÙ„Ø²: Ø§ÛŒÚ© Ø¬Ø§Ù…Ø¹ Ø±ÛÙ†Ù…Ø§

## ØªØ¹Ø§Ø±Ù

Ø§Ø³ Ù¹ÛŒÙˆÙ¹ÙˆØ±ÛŒÙ„ Ù…ÛŒÚºØŒ ÛÙ… Ø§Û’ Ø¢Ø¦ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ø§ÙˆØ± Ú†Ú¾ÙˆÙ¹Û’ Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ù…Ø§ÚˆÙ„Ø² (SLMs) Ú©Û’ Ø¬Ø¯ÛŒØ¯ Ù†ÙØ§Ø° Ú©ÛŒ Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒÙˆÚº Ú©Ùˆ Ø§ÛŒØ¬ Ú©Ù…Ù¾ÛŒÙˆÙ¹Ù†Ú¯ Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±ÛŒÚº Ú¯Û’Û” ÛÙ… Ø§ÛŒØ¬Ù†Ù¹Ú© Ø§Û’ Ø¢Ø¦ÛŒ Ú©Û’ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØªØµÙˆØ±Ø§ØªØŒ SLM Ú©ÛŒ Ø§ØµÙ„Ø§Ø­ÛŒ ØªÚ©Ù†ÛŒÚ©ØŒ ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ù…Ø­Ø¯ÙˆØ¯ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ú©Û’ Ù„ÛŒÛ’ Ø¹Ù…Ù„ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒÙˆÚºØŒ Ø§ÙˆØ± Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± Ø§ÛŒØ¬Ù†Ù¹ Ø³Ø³Ù¹Ù…Ø² Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ú©Ø§ Ø§Ø­Ø§Ø·Û Ú©Ø±ÛŒÚº Ú¯Û’Û”

Ù…ØµÙ†ÙˆØ¹ÛŒ Ø°ÛØ§Ù†Øª Ú©Ø§ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…Û 2025 Ù…ÛŒÚº Ø§ÛŒÚ© Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©Ø§ Ø³Ø§Ù…Ù†Ø§ Ú©Ø± Ø±ÛØ§ ÛÛ’Û” 2023 Ú†ÛŒÙ¹ Ø¨ÙˆÙ¹Ø³ Ú©Ø§ Ø³Ø§Ù„ ØªÚ¾Ø§ Ø§ÙˆØ± 2024 Ù…ÛŒÚº Ú©ÙˆÙ¾Ø§Ø¦Ù„Ù¹Ø³ Ú©Ø§ Ø¹Ø±ÙˆØ¬ Ø¯ÛŒÚ©Ú¾Ø§ Ú¯ÛŒØ§ØŒ Ø¬Ø¨Ú©Û 2025 Ø§Û’ Ø¢Ø¦ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ø§ Ø³Ø§Ù„ ÛÛ’ â€” Ø°ÛÛŒÙ† Ù†Ø¸Ø§Ù… Ø¬Ùˆ Ø³ÙˆÚ†ØªÛ’ ÛÛŒÚºØŒ Ù…Ù†ØµÙˆØ¨Û Ø¨Ù†Ø¯ÛŒ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ù¹ÙˆÙ„Ø² Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± Ú©Ù… Ø³Û’ Ú©Ù… Ø§Ù†Ø³Ø§Ù†ÛŒ Ù…Ø¯Ø§Ø®Ù„Øª Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ø§Ù… Ø§Ù†Ø¬Ø§Ù… Ø¯ÛŒØªÛ’ ÛÛŒÚºØŒ Ø¬Ùˆ Ø²ÛŒØ§Ø¯Û ØªØ± Ù…Ø¤Ø«Ø± Ú†Ú¾ÙˆÙ¹Û’ Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ú†Ù„Ø§Ø¦Û’ Ø¬Ø§ØªÛ’ ÛÛŒÚºÛ” Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ø¢Ù Ù„Ø§Ø¦Ù† Ø§ÛŒØ¬ Ù¾Ø± Ù…Ø¨Ù†ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ù† Ø°ÛÛŒÙ† Ù†Ø¸Ø§Ù…ÙˆÚº Ú©Ùˆ Ø¨Ù†Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© Ø§ÛÙ… Ø­Ù„ Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ø§Ø¨Ú¾Ø±ØªØ§ ÛÛ’Û”

## Ø³ÛŒÚ©Ú¾Ù†Û’ Ú©Û’ Ù…Ù‚Ø§ØµØ¯

Ø§Ø³ Ù¹ÛŒÙˆÙ¹ÙˆØ±ÛŒÙ„ Ú©Û’ Ø§Ø®ØªØªØ§Ù… ØªÚ©ØŒ Ø¢Ù¾ Ù‚Ø§Ø¨Ù„ ÛÙˆÚº Ú¯Û’:

- ðŸ¤– Ø§Û’ Ø¢Ø¦ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ø§ÙˆØ± Ø§ÛŒØ¬Ù†Ù¹Ú© Ø³Ø³Ù¹Ù…Ø² Ú©Û’ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØªØµÙˆØ±Ø§Øª Ú©Ùˆ Ø³Ù…Ø¬Ú¾ÛŒÚº
- ðŸ”¬ Ø§ÛŒØ¬Ù†Ù¹Ú© Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ù…ÛŒÚº Ø¨Ú‘Û’ Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚº Ú†Ú¾ÙˆÙ¹Û’ Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ù…Ø§ÚˆÙ„Ø² Ú©Û’ ÙÙˆØ§Ø¦Ø¯ Ú©ÛŒ Ø´Ù†Ø§Ø®Øª Ú©Ø±ÛŒÚº
- ðŸš€ Ø§ÛŒØ¬ Ú©Ù…Ù¾ÛŒÙˆÙ¹Ù†Ú¯ Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ù„ÛŒÛ’ SLM Ú©ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ Ø¬Ø¯ÛŒØ¯ Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒÙˆÚº Ú©Ùˆ Ø³ÛŒÚ©Ú¾ÛŒÚº
- ðŸ“± Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ú©ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ SLM Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Û’ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ùˆ Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº
- ðŸ—ï¸ Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± Ø§ÛŒØ¬Ù†Ù¹Ø³ Ø¨Ù†Ø§Ø¦ÛŒÚº
- ðŸŒ Ù…Ù‚Ø§Ù…ÛŒ LLM Ø§ÙˆØ± SLM Ø§Ù†Ø¶Ù…Ø§Ù… Ú©Û’ Ø³Ø§ØªÚ¾ Ø¢Ù Ù„Ø§Ø¦Ù† Ø§ÛŒØ¬ Ù¾Ø± Ù…Ø¨Ù†ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ùˆ ØªØ¹ÛŒÙ†Ø§Øª Ú©Ø±ÛŒÚº
- ðŸ”§ Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ú©Ùˆ Foundry Local Ú©Û’ Ø³Ø§ØªÚ¾ Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø±Ø¨ÙˆØ· Ú©Ø±ÛŒÚº

## Ø§Û’ Ø¢Ø¦ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ùˆ Ø³Ù…Ø¬Ú¾Ù†Ø§: Ø¨Ù†ÛŒØ§Ø¯ÛŒÚº Ø§ÙˆØ± Ø¯Ø±Ø¬Û Ø¨Ù†Ø¯ÛŒ

### ØªØ¹Ø±ÛŒÙ Ø§ÙˆØ± Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØªØµÙˆØ±Ø§Øª

Ù…ØµÙ†ÙˆØ¹ÛŒ Ø°ÛØ§Ù†Øª (AI) Ø§ÛŒØ¬Ù†Ù¹ Ø§ÛŒÚ© Ø§ÛŒØ³Ø§ Ù†Ø¸Ø§Ù… ÛŒØ§ Ù¾Ø±ÙˆÚ¯Ø±Ø§Ù… ÛÛ’ Ø¬Ùˆ ØµØ§Ø±Ù ÛŒØ§ Ú©Ø³ÛŒ Ø¯ÙˆØ³Ø±Û’ Ù†Ø¸Ø§Ù… Ú©ÛŒ Ø¬Ø§Ù†Ø¨ Ø³Û’ Ø®ÙˆØ¯ Ù…Ø®ØªØ§Ø± Ø·ÙˆØ± Ù¾Ø± Ú©Ø§Ù… Ø§Ù†Ø¬Ø§Ù… Ø¯ÛŒÙ†Û’ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØª Ø±Ú©Ú¾ØªØ§ ÛÛ’ØŒ Ø§Ù¾Ù†Û’ ÙˆØ±Ú© ÙÙ„Ùˆ Ú©Ùˆ ÚˆÛŒØ²Ø§Ø¦Ù† Ú©Ø±ØªØ§ ÛÛ’ Ø§ÙˆØ± Ø¯Ø³ØªÛŒØ§Ø¨ Ù¹ÙˆÙ„Ø² Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’Û” Ø±ÙˆØ§ÛŒØªÛŒ Ø§Û’ Ø¢Ø¦ÛŒ Ú©Û’ Ø¨Ø±Ø¹Ú©Ø³ Ø¬Ùˆ ØµØ±Ù Ø¢Ù¾ Ú©Û’ Ø³ÙˆØ§Ù„Ø§Øª Ú©Ø§ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒØªØ§ ÛÛ’ØŒ Ø§ÛŒÚ© Ø§ÛŒØ¬Ù†Ù¹ Ø¢Ø²Ø§Ø¯Ø§Ù†Û Ø·ÙˆØ± Ù¾Ø± Ø§ÛØ¯Ø§Ù Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ú©Ø§Ù… Ú©Ø± Ø³Ú©ØªØ§ ÛÛ’Û”

### Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ Ø¯Ø±Ø¬Û Ø¨Ù†Ø¯ÛŒ Ú©Ø§ ÙØ±ÛŒÙ… ÙˆØ±Ú©

Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ Ø­Ø¯ÙˆØ¯ Ú©Ùˆ Ø³Ù…Ø¬Ú¾Ù†Ø§ Ù…Ø®ØªÙ„Ù Ú©Ù…Ù¾ÛŒÙˆÙ¹Ù†Ú¯ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ù…Ù†Ø§Ø³Ø¨ Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ Ø§Ù‚Ø³Ø§Ù… Ú©Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ù…Ø¯Ø¯ Ú©Ø±ØªØ§ ÛÛ’:

- **ðŸ”¬ Ø³Ø§Ø¯Û Ø±ÛŒÙÙ„ÛŒÚ©Ø³ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ø§ØµÙˆÙ„ Ù¾Ø± Ù…Ø¨Ù†ÛŒ Ù†Ø¸Ø§Ù… Ø¬Ùˆ ÙÙˆØ±ÛŒ ØªØ§Ø«Ø±Ø§Øª Ù¾Ø± Ø±Ø¯Ø¹Ù…Ù„ Ø¸Ø§ÛØ± Ú©Ø±ØªÛ’ ÛÛŒÚº (ØªÚ¾Ø±Ù…ÙˆØ³Ù¹ÛŒÙ¹Ø³ØŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø¢Ù¹ÙˆÙ…ÛŒØ´Ù†)
- **ðŸ“± Ù…Ø§ÚˆÙ„ Ù¾Ø± Ù…Ø¨Ù†ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ù†Ø¸Ø§Ù… Ø¬Ùˆ Ø§Ù†Ø¯Ø±ÙˆÙ†ÛŒ Ø­Ø§Ù„Øª Ø§ÙˆØ± ÛŒØ§Ø¯Ø¯Ø§Ø´Øª Ú©Ùˆ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚº (Ø±ÙˆØ¨ÙˆÙ¹ ÙˆÛŒÚ©ÛŒÙˆÙ…Ø²ØŒ Ù†ÛŒÙˆÛŒÚ¯ÛŒØ´Ù† Ø³Ø³Ù¹Ù…Ø²)
- **âš–ï¸ Ù…Ù‚ØµØ¯ Ù¾Ø± Ù…Ø¨Ù†ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ù†Ø¸Ø§Ù… Ø¬Ùˆ Ù…Ù‚Ø§ØµØ¯ Ø­Ø§ØµÙ„ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ØªØ±ØªÛŒØ¨ ÙˆØ§Ø± Ù…Ù†ØµÙˆØ¨Û Ø¨Ù†Ø¯ÛŒ Ø§ÙˆØ± Ø¹Ù…Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº (Ø±ÙˆÙ¹ Ù¾Ù„Ø§Ù†Ø±Ø²ØŒ Ù¹Ø§Ø³Ú© Ø´ÛŒÚˆÙˆÙ„Ø±Ø²)
- **ðŸ§  Ø³ÛŒÚ©Ú¾Ù†Û’ ÙˆØ§Ù„Û’ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ù…ÙˆØ§ÙÙ‚Øª Ù¾Ø°ÛŒØ± Ù†Ø¸Ø§Ù… Ø¬Ùˆ ÙˆÙ‚Øª Ú©Û’ Ø³Ø§ØªÚ¾ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Ùˆ Ø¨ÛØªØ± Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚº (ØªØ¬ÙˆÛŒØ² Ú©Ø±Ø¯Û Ù†Ø¸Ø§Ù…ØŒ Ø°Ø§ØªÛŒ Ù…Ø¹Ø§ÙˆÙ†ÛŒÙ†)

### Ø§Û’ Ø¢Ø¦ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ú©Ù„ÛŒØ¯ÛŒ ÙÙˆØ§Ø¦Ø¯

Ø§Û’ Ø¢Ø¦ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ø¦ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙÙˆØ§Ø¦Ø¯ Ù¾ÛŒØ´ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬Ùˆ Ø§Ù†ÛÛŒÚº Ø§ÛŒØ¬ Ú©Ù…Ù¾ÛŒÙˆÙ¹Ù†Ú¯ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ù…Ø«Ø§Ù„ÛŒ Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚº:

**Ø¹Ù…Ù„ÛŒ Ø®ÙˆØ¯ Ù…Ø®ØªØ§Ø±ÛŒ**: Ø§ÛŒØ¬Ù†Ù¹Ø³ Ø¢Ø²Ø§Ø¯Ø§Ù†Û Ø·ÙˆØ± Ù¾Ø± Ú©Ø§Ù… Ø§Ù†Ø¬Ø§Ù… Ø¯ÛŒØªÛ’ ÛÛŒÚº Ø¨ØºÛŒØ± Ù…Ø³Ù„Ø³Ù„ Ø§Ù†Ø³Ø§Ù†ÛŒ Ù†Ú¯Ø±Ø§Ù†ÛŒ Ú©Û’ØŒ Ø¬Ùˆ Ø§Ù†ÛÛŒÚº Ø­Ù‚ÛŒÙ‚ÛŒ ÙˆÙ‚Øª Ú©ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ù…Ø«Ø§Ù„ÛŒ Ø¨Ù†Ø§ØªØ§ ÛÛ’Û” ÙˆÛ Ú©Ù… Ù†Ú¯Ø±Ø§Ù†ÛŒ Ú©Û’ Ø³Ø§ØªÚ¾ Ù…ÙˆØ§ÙÙ‚Øª Ù¾Ø°ÛŒØ± Ø±ÙˆÛŒÛ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾ØªÛ’ ÛÛŒÚºØŒ Ø¬Ø³ Ø³Û’ ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ù…Ø­Ø¯ÙˆØ¯ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ù¾Ø± ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ù…Ù…Ú©Ù† ÛÙˆØªÛŒ ÛÛ’Û”

**ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ Ù„Ú†Ú©**: ÛŒÛ Ù†Ø¸Ø§Ù… Ø§Ù†Ù¹Ø±Ù†ÛŒÙ¹ Ú©Ù†ÛŒÚ©Ù¹ÛŒÙˆÛŒÙ¹ÛŒ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ú©Û’ Ø¨ØºÛŒØ± ÚˆÛŒÙˆØ§Ø¦Ø³ Ù¾Ø± Ø§Û’ Ø¢Ø¦ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ù…Ù‚Ø§Ù…ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒØ³ÛŒ Ø§ÙˆØ± Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ú©Ùˆ Ø¨Ú‘Ú¾Ø§ØªÛ’ ÛÛŒÚºØŒ ÚˆÙˆÙ…ÛŒÙ† Ù…Ø®ØµÙˆØµ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ø­Ø³Ø¨ Ø¶Ø±ÙˆØ±Øª Ø¨Ù†Ø§Ø¦Û’ Ø¬Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± Ù…Ø®ØªÙ„Ù Ø§ÛŒØ¬ Ú©Ù…Ù¾ÛŒÙˆÙ¹Ù†Ú¯ Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ù…ÙˆØ²ÙˆÚº ÛÛŒÚºÛ”

**Ù„Ø§Ú¯Øª Ú©ÛŒ Ù…Ø¤Ø«Ø±ÛŒØª**: Ø§ÛŒØ¬Ù†Ù¹ Ø³Ø³Ù¹Ù…Ø² Ú©Ù„Ø§Ø¤Úˆ Ù¾Ø± Ù…Ø¨Ù†ÛŒ Ø­Ù„ Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚº Ù„Ø§Ú¯Øª Ù…Ø¤Ø«Ø± ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ù¾ÛŒØ´ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø§ÛŒØ¬ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ú©Ù… Ø¢Ù¾Ø±ÛŒÙ¹Ù†Ú¯ Ø§Ø®Ø±Ø§Ø¬Ø§Øª Ø§ÙˆØ± Ú©Ù… Ø¨ÛŒÙ†ÚˆÙˆÚˆØªÚ¾ Ú©ÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ú©Û’ Ø³Ø§ØªÚ¾Û”

## Ú†Ú¾ÙˆÙ¹Û’ Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒ Ø¬Ø¯ÛŒØ¯ Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒ

### SLM (Ú†Ú¾ÙˆÙ¹Û’ Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ù…Ø§ÚˆÙ„) Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ÛŒÚº

Ø§ÛŒÚ© Ú†Ú¾ÙˆÙ¹Ø§ Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ù…Ø§ÚˆÙ„ (SLM) Ø§ÛŒÚ© Ù„ÛŒÙ†Ú¯ÙˆÛŒØ¬ Ù…Ø§ÚˆÙ„ ÛÛ’ Ø¬Ùˆ Ø¹Ø§Ù… ØµØ§Ø±Ù Ø§Ù„ÛŒÚ©Ù¹Ø±Ø§Ù†Ú© ÚˆÛŒÙˆØ§Ø¦Ø³ Ù¾Ø± ÙÙ¹ ÛÙˆ Ø³Ú©ØªØ§ ÛÛ’ Ø§ÙˆØ± Ø§ÛŒÚ© ØµØ§Ø±Ù Ú©ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ú© Ø¯Ø±Ø®ÙˆØ§Ø³ØªÙˆÚº Ú©Ùˆ Ù¾ÙˆØ±Ø§ Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¹Ù…Ù„ÛŒ Ø·ÙˆØ± Ù¾Ø± Ú©Ù… ØªØ§Ø®ÛŒØ± Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ù†ÙØ±ÛŒÙ†Ø³ Ø§Ù†Ø¬Ø§Ù… Ø¯Û’ Ø³Ú©ØªØ§ ÛÛ’Û” Ø¹Ù…Ù„ÛŒ Ø·ÙˆØ± Ù¾Ø±ØŒ SLMs Ø¹Ø§Ù… Ø·ÙˆØ± Ù¾Ø± 10 Ø§Ø±Ø¨ Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø² Ø³Û’ Ú©Ù… Ù…Ø§ÚˆÙ„Ø² ÛÙˆØªÛ’ ÛÛŒÚºÛ”

**ÙØ§Ø±Ù…ÛŒÙ¹ Ø¯Ø±ÛŒØ§ÙØª Ú©ÛŒ Ø®ØµÙˆØµÛŒØ§Øª**: SLMs Ù…Ø®ØªÙ„Ù Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù† Ù„ÛŒÙˆÙ„Ø²ØŒ Ú©Ø±Ø§Ø³ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù… Ù…Ø·Ø§Ø¨Ù‚ØªØŒ Ø­Ù‚ÛŒÙ‚ÛŒ ÙˆÙ‚Øª Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ø§ØµÙ„Ø§Ø­ØŒ Ø§ÙˆØ± Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø¬Ø¯ÛŒØ¯ Ø³Ù¾ÙˆØ±Ù¹ Ù¾ÛŒØ´ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ” ØµØ§Ø±ÙÛŒÙ† Ù…Ù‚Ø§Ù…ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ø§ÙˆØ± WebGPU Ø³Ù¾ÙˆØ±Ù¹ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø¨Ø±Ø§Ø¤Ø²Ø± Ù¾Ø± Ù…Ø¨Ù†ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ø°Ø±ÛŒØ¹Û’ Ø¨ÛØªØ± Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒØ³ÛŒ ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

**Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù† Ù„ÛŒÙˆÙ„ Ú©Ù„ÛŒÚ©Ø´Ù†Ø²**: Ù…Ù‚Ø¨ÙˆÙ„ SLM ÙØ§Ø±Ù…ÛŒÙ¹Ø³ Ù…ÛŒÚº Q4_K_M Ø´Ø§Ù…Ù„ ÛÛŒÚº Ø¬Ùˆ Ù…ÙˆØ¨Ø§Ø¦Ù„ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ù…ÛŒÚº Ù…ØªÙˆØ§Ø²Ù† Ú©Ù…Ù¾Ø±ÛŒØ´Ù† Ú©Û’ Ù„ÛŒÛ’ØŒ Q5_K_S Ø³ÛŒØ±ÛŒØ² Ø¬Ùˆ Ù…Ø¹ÛŒØ§Ø± Ù¾Ø± Ù…Ø±Ú©ÙˆØ² Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ØŒ Q8_0 Ø¬Ùˆ Ø·Ø§Ù‚ØªÙˆØ± Ø§ÛŒØ¬ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ù¾Ø± Ù‚Ø±ÛŒØ¨ Ø§ØµÙ„ Ø¯Ø±Ø³ØªÚ¯ÛŒ Ú©Û’ Ù„ÛŒÛ’ØŒ Ø§ÙˆØ± ØªØ¬Ø±Ø¨Ø§ØªÛŒ ÙØ§Ø±Ù…ÛŒÙ¹Ø³ Ø¬ÛŒØ³Û’ Q2_K Ø§Ù†ØªÛØ§Ø¦ÛŒ Ú©Ù… ÙˆØ³Ø§Ø¦Ù„ ÙˆØ§Ù„Û’ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’Û”

### GGUF (Ø¬Ù†Ø±Ù„ GGML ÛŒÙˆÙ†ÛŒÙˆØ±Ø³Ù„ ÙØ§Ø±Ù…ÛŒÙ¹) SLM ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’

GGUF Ø§ÛŒØ¬Ù†Ù¹Ú© Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø± Ø¨ÛØªØ± SLMs Ú©Ùˆ CPU Ø§ÙˆØ± Ø§ÛŒØ¬ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ù¾Ø± ØªØ¹ÛŒÙ†Ø§Øª Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙØ§Ø±Ù…ÛŒÙ¹ Ú©Û’ Ø·ÙˆØ± Ù¾Ø± Ú©Ø§Ù… Ú©Ø±ØªØ§ ÛÛ’:

**Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± Ø®ØµÙˆØµÛŒØ§Øª**: ÙØ§Ø±Ù…ÛŒÙ¹ SLM Ú©Ù†ÙˆØ±Ú˜Ù† Ø§ÙˆØ± ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ø¬Ø§Ù…Ø¹ ÙˆØ³Ø§Ø¦Ù„ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ØŒ Ù¹ÙˆÙ„ Ú©Ø§Ù„Ù†Ú¯ØŒ Ø³Ø§Ø®ØªÛŒ Ø¢Ø¤Ù¹ Ù¾Ù¹ Ø¬Ù†Ø±ÛŒØ´Ù†ØŒ Ø§ÙˆØ± Ù…Ù„Ù¹ÛŒ Ù¹Ø±Ù† Ú¯ÙØªÚ¯Ùˆ Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± Ø³Ù¾ÙˆØ±Ù¹ Ú©Û’ Ø³Ø§ØªÚ¾Û” Ú©Ø±Ø§Ø³ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù… Ù…Ø·Ø§Ø¨Ù‚Øª Ù…Ø®ØªÙ„Ù Ø§ÛŒØ¬ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ù¾Ø± Ù…Ø³ØªÙ‚Ù„ Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ Ø±ÙˆÛŒÛ’ Ú©Ùˆ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§ØªÛŒ ÛÛ’Û”

**Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ø§ØµÙ„Ø§Ø­**: GGUF Ø§ÛŒØ¬Ù†Ù¹ ÙˆØ±Ú© ÙÙ„Ùˆ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø¤Ø«Ø± Ù…ÛŒÙ…ÙˆØ±ÛŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ù…Ù„Ù¹ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ø³Ø³Ù¹Ù…Ø² Ú©Û’ Ù„ÛŒÛ’ Ù…ØªØ­Ø±Ú© Ù…Ø§ÚˆÙ„ Ù„ÙˆÚˆÙ†Ú¯ Ú©Ùˆ Ø³Ù¾ÙˆØ±Ù¹ Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ø­Ù‚ÛŒÙ‚ÛŒ ÙˆÙ‚Øª Ú©Û’ Ø§ÛŒØ¬Ù†Ù¹ ØªØ¹Ø§Ù…Ù„Ø§Øª Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± Ø§Ù†ÙØ±ÛŒÙ†Ø³ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’Û”

### Ø§ÛŒØ¬ Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± SLM ÙØ±ÛŒÙ… ÙˆØ±Ú©

#### Llama.cpp Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ø§ØµÙ„Ø§Ø­

Llama.cpp Ø§ÛŒØ¬Ù†Ù¹Ú© SLM ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø± Ø¨ÛØªØ± Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù† ØªÚ©Ù†ÛŒÚ© ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’:

**Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø®ØµÙˆØµ Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù†**: ÙØ±ÛŒÙ… ÙˆØ±Ú© Q4_0 (Ù…ÙˆØ¨Ø§Ø¦Ù„ Ø§ÛŒØ¬Ù†Ù¹ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ 75% Ø³Ø§Ø¦Ø² Ú©ÛŒ Ú©Ù…ÛŒ Ú©Û’ Ø³Ø§ØªÚ¾ Ø¨ÛØªØ±ÛŒÙ†)ØŒ Q5_1 (Ø§ÛŒØ¬ Ø§Ù†ÙØ±ÛŒÙ†Ø³ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø¹ÛŒØ§Ø±-Ú©Ù…Ù¾Ø±ÛŒØ´Ù† Ú©Ø§ ØªÙˆØ§Ø²Ù†)ØŒ Ø§ÙˆØ± Q8_0 (Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ø§ÛŒØ¬Ù†Ù¹ Ø³Ø³Ù¹Ù…Ø² Ú©Û’ Ù„ÛŒÛ’ Ù‚Ø±ÛŒØ¨ Ø§ØµÙ„ Ù…Ø¹ÛŒØ§Ø±) Ú©Ùˆ Ø³Ù¾ÙˆØ±Ù¹ Ú©Ø±ØªØ§ ÛÛ’Û” Ø¬Ø¯ÛŒØ¯ ÙØ§Ø±Ù…ÛŒÙ¹Ø³ Ø§Ù†ØªÛØ§Ø¦ÛŒ Ú©Ù…Ù¾Ø±ÛŒØ³Úˆ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ùˆ Ø§Ù†ØªÛØ§Ø¦ÛŒ Ø§ÛŒØ¬ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ ÙØ¹Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

**Ù†ÙØ§Ø° Ú©Û’ ÙÙˆØ§Ø¦Ø¯**: SIMD Ø§ÛŒÚ©Ø³ÛŒÙ„Ø±ÛŒØ´Ù† Ú©Û’ Ø³Ø§ØªÚ¾ CPU Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± Ø§Ù†ÙØ±ÛŒÙ†Ø³ Ù…ÛŒÙ…ÙˆØ±ÛŒ Ù…Ø¤Ø«Ø± Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ Ù†ÙØ§Ø° Ú©Ùˆ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’Û” x86ØŒ ARMØŒ Ø§ÙˆØ± Apple Silicon Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±Ø² Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ú©Ø±Ø§Ø³ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù… Ù…Ø·Ø§Ø¨Ù‚Øª Ø¹Ø§Ù„Ù…ÛŒ Ø§ÛŒØ¬Ù†Ù¹ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±ØªÛŒ ÛÛ’Û”

#### Apple MLX ÙØ±ÛŒÙ… ÙˆØ±Ú© SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’

Apple MLX Ø§ÛŒÙ¾Ù„ Ø³Ù„ÛŒÚ©ÙˆÙ† ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ù¾Ø± SLM Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Û’ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø± Ø¨ÛØªØ± Ù…Ù‚Ø§Ù…ÛŒ Ø§ØµÙ„Ø§Ø­ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’:

**Ø§ÛŒÙ¾Ù„ Ø³Ù„ÛŒÚ©ÙˆÙ† Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ Ø§ØµÙ„Ø§Ø­**: ÙØ±ÛŒÙ… ÙˆØ±Ú© Ù…ØªØ­Ø¯ Ù…ÛŒÙ…ÙˆØ±ÛŒ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø± Ú©Û’ Ø³Ø§ØªÚ¾ Ù…ÛŒÙ¹Ù„ Ù¾Ø±ÙØ§Ø±Ù…Ù†Ø³ Ø´ÛŒÚˆØ±Ø² Ø§Ù†Ø¶Ù…Ø§Ù…ØŒ Ø§ÛŒØ¬Ù†Ù¹ Ø§Ù†ÙØ±ÛŒÙ†Ø³ Ú©Û’ Ù„ÛŒÛ’ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ú©Ø³Úˆ Ù¾Ø±ÛŒØ³ÛŒØ´Ù†ØŒ Ø§ÙˆØ± Ù…Ù„Ù¹ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ø³Ø³Ù¹Ù…Ø² Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± Ù…ÛŒÙ…ÙˆØ±ÛŒ Ø¨ÛŒÙ†ÚˆÙˆÚˆØªÚ¾ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’Û” SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ M Ø³ÛŒØ±ÛŒØ² Ú†Ù¾Ø³ Ù¾Ø± ØºÛŒØ± Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø¯Ú©Ú¾Ø§ØªÛ’ ÛÛŒÚºÛ”

**ØªØ±Ù‚ÛŒ Ú©ÛŒ Ø®ØµÙˆØµÛŒØ§Øª**: Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø®ØµÙˆØµ Ø§ØµÙ„Ø§Ø­Ø§Øª Ú©Û’ Ø³Ø§ØªÚ¾ Python Ø§ÙˆØ± Swift API Ø³Ù¾ÙˆØ±Ù¹ØŒ Ø§ÛŒØ¬Ù†Ù¹ Ù„Ø±Ù†Ù†Ú¯ Ú©Û’ Ù„ÛŒÛ’ Ø®ÙˆØ¯Ú©Ø§Ø± ØªÙØ±ÛŒÙ‚ØŒ Ø§ÙˆØ± Ø§ÛŒÙ¾Ù„ ÚˆÛŒÙˆÙ„Ù¾Ù…Ù†Ù¹ Ù¹ÙˆÙ„Ø² Ú©Û’ Ø³Ø§ØªÚ¾ ÛÙ…ÙˆØ§Ø± Ø§Ù†Ø¶Ù…Ø§Ù… Ø¬Ø§Ù…Ø¹ Ø§ÛŒØ¬Ù†Ù¹ ÚˆÛŒÙˆÙ„Ù¾Ù…Ù†Ù¹ Ù…Ø§Ø­ÙˆÙ„ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

#### ONNX Runtime Ú©Ø±Ø§Ø³ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù… SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’

ONNX Runtime Ø§ÛŒÚ© ÛŒÙˆÙ†ÛŒÙˆØ±Ø³Ù„ Ø§Ù†ÙØ±ÛŒÙ†Ø³ Ø§Ù†Ø¬Ù† ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ùˆ Ù…Ø®ØªÙ„Ù ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù…Ø² Ø§ÙˆØ± Ø¢Ù¾Ø±ÛŒÙ¹Ù†Ú¯ Ø³Ø³Ù¹Ù…Ø² Ù¾Ø± Ù…Ø³ØªÙ‚Ù„ Ø·ÙˆØ± Ù¾Ø± Ú†Ù„Ø§Ù†Û’ Ú©Û’ Ù‚Ø§Ø¨Ù„ Ø¨Ù†Ø§ØªØ§ ÛÛ’:

**ÛŒÙˆÙ†ÛŒÙˆØ±Ø³Ù„ ØªØ¹ÛŒÙ†Ø§ØªÛŒ**: ONNX Runtime Ù…Ø®ØªÙ„Ù Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù…Ø² Ø¬ÛŒØ³Û’ ÙˆÙ†ÚˆÙˆØ²ØŒ Ù„ÛŒÙ†Ú©Ø³ØŒ Ù…ÛŒÚ© Ø§Ùˆ Ø§ÛŒØ³ØŒ Ø¢Ø¦ÛŒ Ø§Ùˆ Ø§ÛŒØ³ØŒ Ø§ÙˆØ± Ø§ÛŒÙ†ÚˆØ±Ø§Ø¦ÛŒÚˆ Ù¾Ø± SLM Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ Ø±ÙˆÛŒÛ’ Ú©Ùˆ Ù…Ø³ØªÙ‚Ù„ Ø·ÙˆØ± Ù¾Ø± ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§ØªØ§ ÛÛ’Û” ÛŒÛ Ú©Ø±Ø§Ø³ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù… Ù…Ø·Ø§Ø¨Ù‚Øª ÚˆÙˆÛŒÙ„Ù¾Ø±Ø² Ú©Ùˆ Ø§ÛŒÚ© Ø¨Ø§Ø± Ù„Ú©Ú¾Ù†Û’ Ø§ÙˆØ± ÛØ± Ø¬Ú¯Û ØªØ¹ÛŒÙ†Ø§Øª Ú©Ø±Ù†Û’ Ú©Û’ Ù‚Ø§Ø¨Ù„ Ø¨Ù†Ø§ØªÛŒ ÛÛ’ØŒ Ù…Ù„Ù¹ÛŒ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù… Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ ØªØ±Ù‚ÛŒ Ø§ÙˆØ± Ø¯ÛŒÚ©Ú¾ Ø¨Ú¾Ø§Ù„ Ú©Û’ Ø§Ø®Ø±Ø§Ø¬Ø§Øª Ú©Ùˆ Ù†Ù…Ø§ÛŒØ§Úº Ø·ÙˆØ± Ù¾Ø± Ú©Ù… Ú©Ø±ØªÛŒ ÛÛ’Û”

**ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ø§ÛŒÚ©Ø³ÛŒÙ„Ø±ÛŒØ´Ù† Ú©Û’ Ø§Ø®ØªÛŒØ§Ø±Ø§Øª**: ÙØ±ÛŒÙ… ÙˆØ±Ú© Ù…Ø®ØªÙ„Ù ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± Ù†ÙØ§Ø° ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ù† Ù…ÛŒÚº CPU (Intel, AMD, ARM)ØŒ GPU (NVIDIA CUDA, AMD ROCm)ØŒ Ø§ÙˆØ± Ø®ØµÙˆØµÛŒ Ø§ÛŒÚ©Ø³ÛŒÙ„Ø±ÛŒÙ¹Ø±Ø² (Intel VPU, Qualcomm NPU) Ø´Ø§Ù…Ù„ ÛÛŒÚºÛ” SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©ÙˆÚˆ Ù…ÛŒÚº ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©Û’ Ø¨ØºÛŒØ± Ø¨ÛØªØ±ÛŒÙ† Ø¯Ø³ØªÛŒØ§Ø¨ ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ú©Ø§ Ø®ÙˆØ¯ Ø¨Ø®ÙˆØ¯ ÙØ§Ø¦Ø¯Û Ø§Ù¹Ú¾Ø§ Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”

**Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± Ø®ØµÙˆØµÛŒØ§Øª**: ONNX Runtime Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ø§ÛŒØ¬Ù†Ù¹ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ø¶Ø±ÙˆØ±ÛŒ Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ú¯Ø±ÛŒÚˆ Ø®ØµÙˆØµÛŒØ§Øª Ù¾ÛŒØ´ Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ù† Ù…ÛŒÚº ØªÛŒØ² ØªØ± Ø§Ù†ÙØ±ÛŒÙ†Ø³ Ú©Û’ Ù„ÛŒÛ’ Ú¯Ø±Ø§Ù Ú©ÛŒ Ø§ØµÙ„Ø§Ø­ØŒ ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ù…Ø­Ø¯ÙˆØ¯ Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ù…ÛŒÙ…ÙˆØ±ÛŒ Ù…ÛŒÙ†Ø¬Ù…Ù†Ù¹ØŒ Ø§ÙˆØ± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ ØªØ¬Ø²ÛŒÛ’ Ú©Û’ Ù„ÛŒÛ’ Ø¬Ø§Ù…Ø¹ Ù¾Ø±ÙˆÙØ§Ø¦Ù„Ù†Ú¯ Ù¹ÙˆÙ„Ø² Ø´Ø§Ù…Ù„ ÛÛŒÚºÛ” ÙØ±ÛŒÙ… ÙˆØ±Ú© Python Ø§ÙˆØ± C++ APIs Ø¯ÙˆÙ†ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ù„Ú†Ú©Ø¯Ø§Ø± Ø§Ù†Ø¶Ù…Ø§Ù… Ú©Ùˆ Ø³Ù¾ÙˆØ±Ù¹ Ú©Ø±ØªØ§ ÛÛ’Û”
- Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ø§Ù†Ø¶Ù…Ø§Ù… Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Ø±ÛŒÚº  
- Ø¢Ù Ù„Ø§Ø¦Ù† Ø¢Ù¾Ø±ÛŒØ´Ù† Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©ÛŒ ØªØµØ¯ÛŒÙ‚ Ú©Ø±ÛŒÚº  
- ÙÛŒÙ„ Ø§ÙˆÙˆØ± Ù…Ù†Ø¸Ø±Ù†Ø§Ù…Û’ Ø§ÙˆØ± ØºÙ„Ø·ÛŒ Ø³Û’ Ù†Ù…Ù¹Ù†Û’ Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Ø±ÛŒÚº  
- Ø§ÛŒØ¬Ù†Ù¹ ÙˆØ±Ú© ÙÙ„Ùˆ Ú©ÛŒ Ù…Ú©Ù…Ù„ ØªÙˆØ«ÛŒÙ‚ Ú©Ø±ÛŒÚº  

**ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ú©Û’ Ø³Ø§ØªÚ¾ Ù…ÙˆØ§Ø²Ù†Û**:

| Ø®ØµÙˆØµÛŒØª | ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ | Ø§ÙˆÙ„Ø§Ù…Ø§ |
|---------|---------------|--------|
| **ÛØ¯Ù Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø§ Ú©ÛŒØ³** | Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† | ØªØ±Ù‚ÛŒ Ø§ÙˆØ± Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ |
| **Ù…Ø§ÚˆÙ„ Ø§ÛŒÚ©Ùˆ Ø³Ø³Ù¹Ù…** | Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ú©ÛŒÙˆØ±ÛŒÙ¹Úˆ | ÙˆØ³ÛŒØ¹ Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ |
| **ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†** | Ø®ÙˆØ¯Ú©Ø§Ø± (CUDA/NPU/CPU) | Ø¯Ø³ØªÛŒ ØªØ±ØªÛŒØ¨ |
| **Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ø®ØµÙˆØµÛŒØ§Øª** | Ø¨Ù„Ù¹ Ø§Ù† Ù…Ø§Ù†ÛŒÙ¹Ø±Ù†Ú¯ØŒ Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ | Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ù¹ÙˆÙ„Ø² |
| **ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ** | Ø³Ø§Ø¯Û (winget Ø§Ù†Ø³Ù¹Ø§Ù„) | Ø³Ø§Ø¯Û (curl Ø§Ù†Ø³Ù¹Ø§Ù„) |
| **API Ù…Ø·Ø§Ø¨Ù‚Øª** | OpenAI + ØªÙˆØ³ÛŒØ¹Ø§Øª | OpenAI Ù…Ø¹ÛŒØ§Ø±ÛŒ |
| **Ø³Ù¾ÙˆØ±Ù¹** | Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø¢ÙÛŒØ´Ù„ | Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ ÚˆØ±Ø§Ø¦ÛŒÙˆÙ† |
| **Ø¨ÛØªØ±ÛŒÙ† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’** | Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ø§ÛŒØ¬Ù†Ù¹Ø³ | Ù¾Ø±ÙˆÙ¹ÙˆÙ¹Ø§Ø¦Ù¾Ù†Ú¯ØŒ ØªØ­Ù‚ÛŒÙ‚ |

**Ø§ÙˆÙ„Ø§Ù…Ø§ Ú©Ùˆ Ú©Ø¨ Ù…Ù†ØªØ®Ø¨ Ú©Ø±ÛŒÚº**:  
- **ØªØ±Ù‚ÛŒ Ø§ÙˆØ± Ù¾Ø±ÙˆÙ¹ÙˆÙ¹Ø§Ø¦Ù¾Ù†Ú¯**: Ù…Ø®ØªÙ„Ù Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø³Ø§ØªÚ¾ ØªÛŒØ² ØªØ¬Ø±Ø¨Ø§Øª  
- **Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ù…Ø§ÚˆÙ„Ø²**: ØªØ§Ø²Û ØªØ±ÛŒÙ† Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ ØªØ¹Ø§ÙˆÙ† ÛŒØ§ÙØªÛ Ù…Ø§ÚˆÙ„Ø² ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ  
- **ØªØ¹Ù„ÛŒÙ…ÛŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„**: AI Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØªØ±Ù‚ÛŒ Ø³ÛŒÚ©Ú¾Ù†Û’ Ø§ÙˆØ± Ø³Ú©Ú¾Ø§Ù†Û’ Ú©Û’ Ù„ÛŒÛ’  
- **ØªØ­Ù‚ÛŒÙ‚ÛŒ Ù…Ù†ØµÙˆØ¨Û’**: Ù…ØªÙ†ÙˆØ¹ Ù…Ø§ÚˆÙ„Ø² ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÙˆØ§Ù„Û’ ØªØ¹Ù„ÛŒÙ…ÛŒ ØªØ­Ù‚ÛŒÙ‚  
- **Ø­Ø³Ø¨ Ø¶Ø±ÙˆØ±Øª Ù…Ø§ÚˆÙ„Ø²**: Ø§Ù¾Ù†ÛŒ Ù…Ø±Ø¶ÛŒ Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ†Úˆ Ù…Ø§ÚˆÙ„Ø² Ø¨Ù†Ø§Ù†Ø§ Ø§ÙˆØ± Ø¬Ø§Ù†Ú†Ù†Ø§  

### VLLM: Ø§Ø¹Ù„ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ SLM Ø§ÛŒØ¬Ù†Ù¹ Ø§Ù†ÙØ±Ù†Ø³  

VLLM (Ø¨ÛØª Ø¨Ú‘Û’ Ø²Ø¨Ø§Ù† Ù…Ø§ÚˆÙ„ Ø§Ù†ÙØ±Ù†Ø³) Ø§ÛŒÚ© Ø§Ø¹Ù„ÛŒ ØªÚ¾Ø±ÙˆÙ¾Ù¹ØŒ Ù…ÛŒÙ…ÙˆØ±ÛŒ Ù…ÙˆØ«Ø± Ø§Ù†ÙØ±Ù†Ø³ Ø§Ù†Ø¬Ù† ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø± Ø¨Ú‘Û’ Ù¾ÛŒÙ…Ø§Ù†Û’ Ù¾Ø± Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† SLM ØªØ¹ÛŒÙ†Ø§ØªÛŒÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± Ø¨Ù†Ø§ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’Û” Ø¬Ø¨Ú©Û ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ù…ÛŒÚº Ø¢Ø³Ø§Ù†ÛŒ Ù¾Ø± ØªÙˆØ¬Û Ù…Ø±Ú©ÙˆØ² Ú©Ø±ØªØ§ ÛÛ’ Ø§ÙˆØ± Ø§ÙˆÙ„Ø§Ù…Ø§ Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ù…Ø§ÚˆÙ„Ø² Ù¾Ø± Ø²ÙˆØ± Ø¯ÛŒØªØ§ ÛÛ’ØŒ VLLM Ø§Ù† Ø§Ø¹Ù„ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ ÙˆØ§Ù„Û’ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ù…ÛŒÚº Ø¨ÛØªØ±ÛŒÙ† ÛÛ’ Ø¬Ù† Ù…ÛŒÚº Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û ØªÚ¾Ø±ÙˆÙ¾Ù¹ Ø§ÙˆØ± ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ù…ÙˆØ«Ø± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆØªÛŒ ÛÛ’Û”  

**Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙÙ† ØªØ¹Ù…ÛŒØ± Ø§ÙˆØ± Ø®ØµÙˆØµÛŒØ§Øª**:  
- **PagedAttention**: Ù…ÙˆØ«Ø± ØªÙˆØ¬Û Ú©ÛŒ Ú¯Ù†ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ø§Ù†Ù‚Ù„Ø§Ø¨ÛŒ Ù…ÛŒÙ…ÙˆØ±ÛŒ Ù…ÛŒÙ†Ø¬Ù…Ù†Ù¹  
- **Dynamic Batching**: Ø¨ÛØªØ±ÛŒÙ† ØªÚ¾Ø±ÙˆÙ¾Ù¹ Ú©Û’ Ù„ÛŒÛ’ Ø°ÛÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø¨ÛŒÚ†Ù†Ú¯  
- **GPU Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†**: Ø¬Ø¯ÛŒØ¯ CUDA Ú©Ø±Ù†Ù„Ø² Ø§ÙˆØ± Ù¹ÛŒÙ†Ø³Ø± Ù¾ÛŒØ±Ø§Ù„Ù„Ø²Ù… Ø³Ù¾ÙˆØ±Ù¹  
- **OpenAI Ù…Ø·Ø§Ø¨Ù‚Øª**: ÛÙ…ÙˆØ§Ø± Ø§Ù†Ø¶Ù…Ø§Ù… Ú©Û’ Ù„ÛŒÛ’ Ù…Ú©Ù…Ù„ API Ù…Ø·Ø§Ø¨Ù‚Øª  
- **Speculative Decoding**: Ø¬Ø¯ÛŒØ¯ Ø§Ù†ÙØ±Ù†Ø³ Ø§ÛŒÚ©Ø³Ù„Ø±ÛŒØ´Ù† ØªÚ©Ù†ÛŒÚ©  
- **Quantization Ø³Ù¾ÙˆØ±Ù¹**: INT4ØŒ INT8ØŒ Ø§ÙˆØ± FP16 Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù† Ù…ÛŒÙ…ÙˆØ±ÛŒ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ù„ÛŒÛ’  

#### Ø§Ù†Ø³Ù¹Ø§Ù„ÛŒØ´Ù† Ø§ÙˆØ± Ø³ÛŒÙ¹ Ø§Ù¾  

**Ø§Ù†Ø³Ù¹Ø§Ù„ÛŒØ´Ù† Ú©Û’ Ø§Ø®ØªÛŒØ§Ø±Ø§Øª**:  
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```
  
**Ø§ÛŒØ¬Ù†Ù¹ ØªØ±Ù‚ÛŒ Ú©Û’ Ù„ÛŒÛ’ ÙÙˆØ±ÛŒ Ø¢ØºØ§Ø²**:  
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```
  

#### Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ø§Ù†Ø¶Ù…Ø§Ù…  

**VLLM Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ú©Û’ Ø³Ø§ØªÚ¾**:  
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```
  
**Ø§Ø¹Ù„ÛŒ ØªÚ¾Ø±ÙˆÙ¾Ù¹ Ù…Ù„Ù¹ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ø³ÛŒÙ¹ Ø§Ù¾**:  
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```
  

#### Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù†Ù…ÙˆÙ†Û’  

**Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² VLLM Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ø³Ø±ÙˆØ³**:  
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```
  

#### Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ø®ØµÙˆØµÛŒØ§Øª Ø§ÙˆØ± Ù…Ø§Ù†ÛŒÙ¹Ø±Ù†Ú¯  

**Ø¬Ø¯ÛŒØ¯ VLLM Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ù…Ø§Ù†ÛŒÙ¹Ø±Ù†Ú¯**:  
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```
  

#### Ø¬Ø¯ÛŒØ¯ ØªØ±ØªÛŒØ¨ Ø§ÙˆØ± Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†  

**Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† VLLM Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù† Ù¹ÛŒÙ…Ù¾Ù„ÛŒÙ¹Ø³**:  
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```
  
**Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú†ÛŒÚ© Ù„Ø³Ù¹ VLLM Ú©Û’ Ù„ÛŒÛ’**:  

âœ… **ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†**:  
- Ù…Ù„Ù¹ÛŒ-GPU Ø³ÛŒÙ¹ Ø§Ù¾ Ú©Û’ Ù„ÛŒÛ’ Ù¹ÛŒÙ†Ø³Ø± Ù¾ÛŒØ±Ø§Ù„Ù„Ø²Ù… ØªØ±ØªÛŒØ¨ Ø¯ÛŒÚº  
- Ù…ÛŒÙ…ÙˆØ±ÛŒ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù† (AWQ/GPTQ) ÙØ¹Ø§Ù„ Ú©Ø±ÛŒÚº  
- GPU Ù…ÛŒÙ…ÙˆØ±ÛŒ Ú©Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ùˆ Ø¨ÛØªØ± Ø¨Ù†Ø§Ø¦ÛŒÚº (85-95%)  
- ØªÚ¾Ø±ÙˆÙ¾Ù¹ Ú©Û’ Ù„ÛŒÛ’ Ù…Ù†Ø§Ø³Ø¨ Ø¨ÛŒÚ† Ø³Ø§Ø¦Ø² ØªØ±ØªÛŒØ¨ Ø¯ÛŒÚº  

âœ… **Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ù¹ÛŒÙˆÙ†Ù†Ú¯**:  
- Ø¨Ø§Ø± Ø¨Ø§Ø± Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ù„ÛŒÛ’ Ù¾Ø±ÛŒ ÙÚ©Ø³ Ú©ÛŒØ´Ù†Ú¯ ÙØ¹Ø§Ù„ Ú©Ø±ÛŒÚº  
- Ø·ÙˆÛŒÙ„ Ø³Ù„Ø³Ù„ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ú†Ù†Ú©Úˆ Ù¾Ø±ÛŒ ÙÙ„ ØªØ±ØªÛŒØ¨ Ø¯ÛŒÚº  
- ØªÛŒØ² Ø§Ù†ÙØ±Ù†Ø³ Ú©Û’ Ù„ÛŒÛ’ speculative decoding Ø³ÛŒÙ¹ Ú©Ø±ÛŒÚº  
- ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ max_num_seqs Ú©Ùˆ Ø¨ÛØªØ± Ø¨Ù†Ø§Ø¦ÛŒÚº  

âœ… **Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ø®ØµÙˆØµÛŒØ§Øª**:  
- ØµØ­Øª Ú©ÛŒ Ù†Ú¯Ø±Ø§Ù†ÛŒ Ø§ÙˆØ± Ù…ÛŒÙ¹Ø±Ú©Ø³ Ø¬Ù…Ø¹ Ú©Ø±Ù†Û’ Ú©Ø§ Ø³ÛŒÙ¹ Ø§Ù¾ Ú©Ø±ÛŒÚº  
- Ø®ÙˆØ¯Ú©Ø§Ø± Ø±ÛŒ Ø§Ø³Ù¹Ø§Ø±Ù¹ Ø§ÙˆØ± ÙÛŒÙ„ Ø§ÙˆÙˆØ± ØªØ±ØªÛŒØ¨ Ø¯ÛŒÚº  
- Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©ÛŒ Ù‚Ø·Ø§Ø± Ø¨Ù†Ø¯ÛŒ Ø§ÙˆØ± Ù„ÙˆÚˆ Ø¨ÛŒÙ„Ù†Ø³Ù†Ú¯ Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº  
- Ø¬Ø§Ù…Ø¹ Ù„Ø§Ú¯Ù†Ú¯ Ø§ÙˆØ± Ø§Ù„Ø±Ù¹Ù†Ú¯ Ø³ÛŒÙ¹ Ø§Ù¾ Ú©Ø±ÛŒÚº  

âœ… **Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ø§ÙˆØ± Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯**:  
- ÙØ§Ø¦Ø± ÙˆØ§Ù„ Ø±ÙˆÙ„Ø² Ø§ÙˆØ± Ø±Ø³Ø§Ø¦ÛŒ Ú©Ù†Ù¹Ø±ÙˆÙ„ ØªØ±ØªÛŒØ¨ Ø¯ÛŒÚº  
- API Ø±ÛŒÙ¹ Ù„Ù…ÛŒÙ¹Ù†Ú¯ Ø§ÙˆØ± ØªØµØ¯ÛŒÙ‚ Ø³ÛŒÙ¹ Ø§Ù¾ Ú©Ø±ÛŒÚº  
- Ú¯Ø±ÛŒØ³ÙÙ„ Ø´Ù¹ ÚˆØ§Ø¤Ù† Ø§ÙˆØ± ØµÙØ§Ø¦ÛŒ Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº  
- Ø¨ÛŒÚ© Ø§Ù¾ Ø§ÙˆØ± Ø¢ÙØª Ø³Û’ Ø¨Ø­Ø§Ù„ÛŒ ØªØ±ØªÛŒØ¨ Ø¯ÛŒÚº  

âœ… **Ø§Ù†Ø¶Ù…Ø§Ù… Ú©ÛŒ Ø¬Ø§Ù†Ú†**:  
- Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ø§Ù†Ø¶Ù…Ø§Ù… Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Ø±ÛŒÚº  
- Ø§Ø¹Ù„ÛŒ ØªÚ¾Ø±ÙˆÙ¾Ù¹ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©ÛŒ ØªÙˆØ«ÛŒÙ‚ Ú©Ø±ÛŒÚº  
- ÙÛŒÙ„ Ø§ÙˆÙˆØ± Ø§ÙˆØ± Ø¨Ø­Ø§Ù„ÛŒ Ú©Û’ Ø·Ø±ÛŒÙ‚Û Ú©Ø§Ø± Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Ø±ÛŒÚº  
- Ù„ÙˆÚˆ Ú©Û’ ØªØ­Øª Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Ø§ Ø¨ÛŒÙ†Ú† Ù…Ø§Ø±Ú© Ú©Ø±ÛŒÚº  

**Ø¯ÛŒÚ¯Ø± Ø­Ù„ÙˆÚº Ú©Û’ Ø³Ø§ØªÚ¾ Ù…ÙˆØ§Ø²Ù†Û**:

| Ø®ØµÙˆØµÛŒØª | VLLM | ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ | Ø§ÙˆÙ„Ø§Ù…Ø§ |
|---------|------|---------------|--------|
| **ÛØ¯Ù Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø§ Ú©ÛŒØ³** | Ø§Ø¹Ù„ÛŒ ØªÚ¾Ø±ÙˆÙ¾Ù¹ Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† | Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ù…ÛŒÚº Ø¢Ø³Ø§Ù†ÛŒ | ØªØ±Ù‚ÛŒ Ø§ÙˆØ± Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ |
| **Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ** | Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û ØªÚ¾Ø±ÙˆÙ¾Ù¹ | Ù…ØªÙˆØ§Ø²Ù† | Ø§Ú†Ú¾Ø§ |
| **Ù…ÛŒÙ…ÙˆØ±ÛŒ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ** | PagedAttention Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù† | Ø®ÙˆØ¯Ú©Ø§Ø± Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù† | Ù…Ø¹ÛŒØ§Ø±ÛŒ |
| **Ø³ÛŒÙ¹ Ø§Ù¾ Ú©ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ** | Ø²ÛŒØ§Ø¯Û (Ú©Ø¦ÛŒ Ù¾ÛŒØ±Ø§Ù…ÛŒÙ¹Ø±Ø²) | Ú©Ù… (Ø®ÙˆØ¯Ú©Ø§Ø±) | Ú©Ù… (Ø³Ø§Ø¯Û) |
| **Ø§Ø³Ú©ÛŒÙ„ Ø§ÛŒØ¨Ù„Ù¹ÛŒ** | Ø¨ÛØªØ±ÛŒÙ† (Ù¹ÛŒÙ†Ø³Ø±/Ù¾Ø§Ø¦Ù¾ Ù„Ø§Ø¦Ù† Ù¾ÛŒØ±Ø§Ù„Ù„) | Ø§Ú†Ú¾Ø§ | Ù…Ø­Ø¯ÙˆØ¯ |
| **Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù†** | Ø¬Ø¯ÛŒØ¯ (AWQØŒ GPTQØŒ FP8) | Ø®ÙˆØ¯Ú©Ø§Ø± | Ù…Ø¹ÛŒØ§Ø±ÛŒ GGUF |
| **Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ø®ØµÙˆØµÛŒØ§Øª** | Ø­Ø³Ø¨ Ø¶Ø±ÙˆØ±Øª Ù†ÙØ§Ø° Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª | Ø¨Ù„Ù¹ Ø§Ù† | Ú©Ù…ÛŒÙˆÙ†Ù¹ÛŒ Ù¹ÙˆÙ„Ø² |
| **Ø¨ÛØªØ±ÛŒÙ† Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’** | Ø¨Ú‘Û’ Ù¾ÛŒÙ…Ø§Ù†Û’ Ù¾Ø± Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ø§ÛŒØ¬Ù†Ù¹Ø³ | Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† | ØªØ±Ù‚ÛŒ |

**VLLM Ú©Ùˆ Ú©Ø¨ Ù…Ù†ØªØ®Ø¨ Ú©Ø±ÛŒÚº**:  
- **Ø§Ø¹Ù„ÛŒ ØªÚ¾Ø±ÙˆÙ¾Ù¹ Ú©ÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª**: Ø³ÛŒÚ©Ù†Úˆ Ù…ÛŒÚº Ø³ÛŒÙ†Ú©Ú‘ÙˆÚº Ø¯Ø±Ø®ÙˆØ§Ø³ØªÙˆÚº Ú©ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯  
- **Ø¨Ú‘Û’ Ù¾ÛŒÙ…Ø§Ù†Û’ Ù¾Ø± ØªØ¹ÛŒÙ†Ø§ØªÛŒ**: Ù…Ù„Ù¹ÛŒ-GPUØŒ Ù…Ù„Ù¹ÛŒ-Ù†ÙˆÚˆ ØªØ¹ÛŒÙ†Ø§ØªÛŒ  
- **Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø§ÛÙ…**: Ø¨Ú‘Û’ Ù¾ÛŒÙ…Ø§Ù†Û’ Ù¾Ø± Ø³Ø¨ Ø³ÛŒÚ©Ù†Úˆ Ø±Ø³Ù¾Ø§Ù†Ø³ Ù¹Ø§Ø¦Ù…Ø²  
- **Ø¬Ø¯ÛŒØ¯ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†**: Ø­Ø³Ø¨ Ø¶Ø±ÙˆØ±Øª Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù† Ø§ÙˆØ± Ø¨ÛŒÚ†Ù†Ú¯ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª  
- **ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ**: Ù…ÛÙ†Ú¯Û’ GPU ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ú©Ø§ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ø§Ø³ØªØ¹Ù…Ø§Ù„  

## Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ú©Û’ SLM Ø§ÛŒØ¬Ù†Ù¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø²  

### Ú©Ø³Ù¹Ù…Ø± Ø³Ø±ÙˆØ³ SLM Ø§ÛŒØ¬Ù†Ù¹Ø³  
- **SLM ØµÙ„Ø§Ø­ÛŒØªÛŒÚº**: Ø§Ú©Ø§Ø¤Ù†Ù¹ Ú©ÛŒ ØªÙ„Ø§Ø´ØŒ Ù¾Ø§Ø³ ÙˆØ±Úˆ Ø±ÛŒ Ø³ÛŒÙ¹ØŒ Ø¢Ø±ÚˆØ± Ú©ÛŒ Ø­ÛŒØ«ÛŒØª Ú©ÛŒ Ø¬Ø§Ù†Ú†  
- **Ù„Ø§Ú¯Øª Ú©Û’ ÙÙˆØ§Ø¦Ø¯**: LLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù…Ù‚Ø§Ø¨Ù„Û’ Ù…ÛŒÚº Ø§Ù†ÙØ±Ù†Ø³ Ù„Ø§Ú¯Øª Ù…ÛŒÚº 10x Ú©Ù…ÛŒ  
- **Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ**: Ù…Ø¹Ù…ÙˆÙ„ Ú©Û’ Ø³ÙˆØ§Ù„Ø§Øª Ú©Û’ Ù„ÛŒÛ’ Ù…Ø³ØªÙ‚Ù„ Ù…Ø¹ÛŒØ§Ø± Ú©Û’ Ø³Ø§ØªÚ¾ ØªÛŒØ² Ø±Ø¯Ø¹Ù…Ù„ Ú©Ø§ ÙˆÙ‚Øª  

### Ø¨Ø²Ù†Ø³ Ù¾Ø±ÙˆØ³ÛŒØ³ SLM Ø§ÛŒØ¬Ù†Ù¹Ø³  
- **Ø§Ù†ÙˆØ§Ø¦Ø³ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ø§ÛŒØ¬Ù†Ù¹Ø³**: ÚˆÛŒÙ¹Ø§ Ù†Ú©Ø§Ù„ÛŒÚºØŒ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©ÛŒ ØªÙˆØ«ÛŒÙ‚ Ú©Ø±ÛŒÚºØŒ Ù…Ù†Ø¸ÙˆØ±ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ø±ÙˆÙ¹ Ú©Ø±ÛŒÚº  
- **Ø§ÛŒ Ù…ÛŒÙ„ Ù…ÛŒÙ†Ø¬Ù…Ù†Ù¹ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ø®ÙˆØ¯ Ø¨Ø®ÙˆØ¯ Ø²Ù…Ø±Û Ø¨Ù†Ø¯ÛŒ Ú©Ø±ÛŒÚºØŒ ØªØ±Ø¬ÛŒØ­ Ø¯ÛŒÚºØŒ Ø¬ÙˆØ§Ø¨Ø§Øª Ú©Ø§ Ù…Ø³ÙˆØ¯Û ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº  
- **Ø´ÛŒÚˆÙˆÙ„Ù†Ú¯ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ù…Ù„Ø§Ù‚Ø§ØªÙˆÚº Ú©Ùˆ Ù…Ø±Ø¨ÙˆØ· Ú©Ø±ÛŒÚºØŒ Ú©ÛŒÙ„Ù†ÚˆØ±Ø² Ú©Ø§ Ø§Ù†ØªØ¸Ø§Ù… Ú©Ø±ÛŒÚºØŒ ÛŒØ§Ø¯ Ø¯ÛØ§Ù†ÛŒØ§Úº Ø¨Ú¾ÛŒØ¬ÛŒÚº  

### Ø°Ø§ØªÛŒ SLM ÚˆÛŒØ¬ÛŒÙ¹Ù„ Ø§Ø³Ø³Ù¹Ù†Ù¹Ø³  
- **Ù¹Ø§Ø³Ú© Ù…ÛŒÙ†Ø¬Ù…Ù†Ù¹ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ù…Ø¤Ø«Ø± Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ù¹Ùˆ ÚˆÙˆ Ù„Ø³Ù¹ Ø¨Ù†Ø§Ø¦ÛŒÚºØŒ Ø§Ù¾ ÚˆÛŒÙ¹ Ú©Ø±ÛŒÚºØŒ Ù…Ù†Ø¸Ù… Ú©Ø±ÛŒÚº  
- **Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¬Ù…Ø¹ Ú©Ø±Ù†Û’ ÙˆØ§Ù„Û’ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù¾Ø± ØªØ­Ù‚ÛŒÙ‚ Ú©Ø±ÛŒÚºØŒ Ù…Ù‚Ø§Ù…ÛŒ Ø·ÙˆØ± Ù¾Ø± Ù†ØªØ§Ø¦Ø¬ Ú©Ø§ Ø®Ù„Ø§ØµÛ Ú©Ø±ÛŒÚº  
- **Ú©Ù…ÛŒÙˆÙ†ÛŒÚ©ÛŒØ´Ù† Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ø§ÛŒ Ù…ÛŒÙ„Ø²ØŒ Ù¾ÛŒØºØ§Ù…Ø§ØªØŒ Ø³ÙˆØ´Ù„ Ù…ÛŒÚˆÛŒØ§ Ù¾ÙˆØ³Ù¹Ø³ Ù†Ø¬ÛŒ Ø·ÙˆØ± Ù¾Ø± ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº  

### Ù¹Ø±ÛŒÚˆÙ†Ú¯ Ø§ÙˆØ± Ù…Ø§Ù„ÛŒØ§ØªÛŒ SLM Ø§ÛŒØ¬Ù†Ù¹Ø³  
- **Ù…Ø§Ø±Ú©ÛŒÙ¹ Ù…Ø§Ù†ÛŒÙ¹Ø±Ù†Ú¯ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ù‚ÛŒÙ…ØªÙˆÚº Ú©Ùˆ Ù¹Ø±ÛŒÚ© Ú©Ø±ÛŒÚºØŒ Ø­Ù‚ÛŒÙ‚ÛŒ ÙˆÙ‚Øª Ù…ÛŒÚº Ø±Ø¬Ø­Ø§Ù†Ø§Øª Ú©ÛŒ Ø´Ù†Ø§Ø®Øª Ú©Ø±ÛŒÚº  
- **Ø±Ù¾ÙˆØ±Ù¹ Ø¬Ù†Ø±ÛŒØ´Ù† Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ø®ÙˆØ¯ Ø¨Ø®ÙˆØ¯ Ø±ÙˆØ²Ø§Ù†Û/ÛÙØªÛ ÙˆØ§Ø± Ø®Ù„Ø§ØµÛ’ Ø¨Ù†Ø§Ø¦ÛŒÚº  
- **Ø±Ø³Ú© Ø§Ø³ÛŒØ³Ù…Ù†Ù¹ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ù…Ù‚Ø§Ù…ÛŒ ÚˆÛŒÙ¹Ø§ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù¾ÙˆØ±Ù¹ ÙÙˆÙ„ÛŒÙˆ Ù¾ÙˆØ²ÛŒØ´Ù†Ø² Ú©Ø§ Ø¬Ø§Ø¦Ø²Û Ù„ÛŒÚº  

### ØµØ­Øª Ú©ÛŒ Ø¯ÛŒÚ©Ú¾ Ø¨Ú¾Ø§Ù„ Ú©Û’ Ù…Ø¹Ø§ÙˆÙ† SLM Ø§ÛŒØ¬Ù†Ù¹Ø³  
- **Ù…Ø±ÛŒØ¶ Ø´ÛŒÚˆÙˆÙ„Ù†Ú¯ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ù…Ù„Ø§Ù‚Ø§ØªÙˆÚº Ú©Ùˆ Ù…Ø±Ø¨ÙˆØ· Ú©Ø±ÛŒÚºØŒ Ø®ÙˆØ¯Ú©Ø§Ø± ÛŒØ§Ø¯ Ø¯ÛØ§Ù†ÛŒØ§Úº Ø¨Ú¾ÛŒØ¬ÛŒÚº  
- **Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ù…Ù‚Ø§Ù…ÛŒ Ø·ÙˆØ± Ù¾Ø± Ø·Ø¨ÛŒ Ø®Ù„Ø§ØµÛ’ØŒ Ø±Ù¾ÙˆØ±Ù¹Ø³ ØªÛŒØ§Ø± Ú©Ø±ÛŒÚº  
- **Ù†Ø³Ø®Û Ù…ÛŒÙ†Ø¬Ù…Ù†Ù¹ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ø±ÛŒÙÙ„Ø² Ú©Ùˆ Ù¹Ø±ÛŒÚ© Ú©Ø±ÛŒÚºØŒ Ù†Ø¬ÛŒ Ø·ÙˆØ± Ù¾Ø± ØªØ¹Ø§Ù…Ù„Ø§Øª Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Ø±ÛŒÚº  

## Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú©: Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ú©Û’ Ù„ÛŒÛ’ ØªÛŒØ§Ø± Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØªØ±Ù‚ÛŒ  

### Ø¬Ø§Ø¦Ø²Û Ø§ÙˆØ± ÙÙ† ØªØ¹Ù…ÛŒØ±  

Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ø§ÛŒÚ© Ø¬Ø§Ù…Ø¹ØŒ Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ú¯Ø±ÛŒÚˆ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù… ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ùˆ AI Ø§ÛŒØ¬Ù†Ù¹Ø³ Ø¨Ù†Ø§Ù†Û’ØŒ ØªØ¹ÛŒÙ†Ø§Øª Ú©Ø±Ù†Û’ØŒ Ø§ÙˆØ± Ø§Ù† Ú©Ø§ Ø§Ù†ØªØ¸Ø§Ù… Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ÛÛ’ Ø¬Ùˆ Ú©Ù„Ø§Ø¤Úˆ Ø§ÙˆØ± Ø¢Ù Ù„Ø§Ø¦Ù† Ø§ÛŒØ¬ Ù…Ø§Ø­ÙˆÙ„ Ù…ÛŒÚº Ú©Ø§Ù… Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ” ÙØ±ÛŒÙ… ÙˆØ±Ú© Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø± Ú†Ú¾ÙˆÙ¹Û’ Ø²Ø¨Ø§Ù† Ù…Ø§ÚˆÙ„Ø² Ø§ÙˆØ± Ø§ÛŒØ¬ Ú©Ù…Ù¾ÛŒÙˆÙ¹Ù†Ú¯ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©Û’ Ø³Ø§ØªÚ¾ ÛÙ…ÙˆØ§Ø± Ú©Ø§Ù… Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ ÚˆÛŒØ²Ø§Ø¦Ù† Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ØŒ Ø¬Ùˆ Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒØ³ÛŒ Ø­Ø³Ø§Ø³ Ø§ÙˆØ± ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ù…Ø­Ø¯ÙˆØ¯ ØªØ¹ÛŒÙ†Ø§ØªÛŒÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ù…Ø«Ø§Ù„ÛŒ ÛÛ’Û”  

**Ø¨Ù†ÛŒØ§Ø¯ÛŒ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ú©Û’ Ø§Ø¬Ø²Ø§Ø¡**:  
- **Ø§ÛŒØ¬Ù†Ù¹ Ø±Ù† Ù¹Ø§Ø¦Ù…**: Ø§ÛŒØ¬ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± ÛÙ„Ú©Ø§ Ù¾Ú¾Ù„Ú©Ø§ Ø§ÛŒÚ¯Ø²ÛŒÚ©ÛŒÙˆØ´Ù† Ù…Ø§Ø­ÙˆÙ„  
- **Ù¹ÙˆÙ„ Ø§Ù†Ù¹ÛŒÚ¯Ø±ÛŒØ´Ù† Ø³Ø³Ù¹Ù…**: Ø¨ÛŒØ±ÙˆÙ†ÛŒ Ø®Ø¯Ù…Ø§Øª Ø§ÙˆØ± APIs Ú©Ùˆ Ù…Ø±Ø¨ÙˆØ· Ú©Ø±Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù‚Ø§Ø¨Ù„ ØªÙˆØ³ÛŒØ¹ Ù¾Ù„Ú¯ Ø§Ù† ÙÙ† ØªØ¹Ù…ÛŒØ±  
- **Ø§Ø³Ù¹ÛŒÙ¹ Ù…ÛŒÙ†Ø¬Ù…Ù†Ù¹**: Ø³ÛŒØ´Ù†Ø² Ú©Û’ Ø¯ÙˆØ±Ø§Ù† Ù…Ø³ØªÙ‚Ù„ Ø§ÛŒØ¬Ù†Ù¹ Ù…ÛŒÙ…ÙˆØ±ÛŒ Ø§ÙˆØ± Ø³ÛŒØ§Ù‚ Ùˆ Ø³Ø¨Ø§Ù‚ Ú©Ùˆ Ø³Ù†Ø¨Ú¾Ø§Ù„Ù†Ø§  
- **Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ù„ÛŒØ¦Ø±**: Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ø¨Ù„Ù¹ Ø§Ù† Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ú©Ù†Ù¹Ø±ÙˆÙ„Ø²  
- **Ø¢Ø±Ú©Ø³Ù¹Ø±ÛŒØ´Ù† Ø§Ù†Ø¬Ù†**: Ù…Ù„Ù¹ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ú©ÙˆØ¢Ø±ÚˆÛŒÙ†ÛŒØ´Ù† Ø§ÙˆØ± ÙˆØ±Ú© ÙÙ„Ùˆ Ù…ÛŒÙ†Ø¬Ù…Ù†Ù¹  

### Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ú©Ù„ÛŒØ¯ÛŒ Ø®ØµÙˆØµÛŒØ§Øª  

**Ø¢Ù Ù„Ø§Ø¦Ù† ÙØ±Ø³Ù¹ ÙÙ† ØªØ¹Ù…ÛŒØ±**: Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ø¢Ù Ù„Ø§Ø¦Ù† ÙØ±Ø³Ù¹ Ø§ØµÙˆÙ„ÙˆÚº Ú©Û’ Ø³Ø§ØªÚ¾ ÚˆÛŒØ²Ø§Ø¦Ù† Ú©ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’ØŒ Ø¬Ùˆ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ùˆ Ù…Ø³ØªÙ‚Ù„ Ø§Ù†Ù¹Ø±Ù†ÛŒÙ¹ Ú©Ù†ÛŒÚ©Ù¹ÛŒÙˆÛŒÙ¹ÛŒ Ú©Û’ Ø¨ØºÛŒØ± Ù…Ø¤Ø«Ø± Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ú©Ø§Ù… Ú©Ø±Ù†Û’ Ú©Û’ Ù‚Ø§Ø¨Ù„ Ø¨Ù†Ø§ØªØ§ ÛÛ’Û” Ø§Ø³ Ù…ÛŒÚº Ù…Ù‚Ø§Ù…ÛŒ Ù…Ø§ÚˆÙ„ Ø§Ù†ÙØ±Ù†Ø³ØŒ Ú©ÛŒØ´Úˆ Ù†Ø§Ù„Ø¬ Ø¨ÛŒØ³Ø²ØŒ Ø¢Ù Ù„Ø§Ø¦Ù† Ù¹ÙˆÙ„ Ø§ÛŒÚ¯Ø²ÛŒÚ©ÛŒÙˆØ´Ù†ØŒ Ø§ÙˆØ± Ú©Ù„Ø§Ø¤Úˆ Ø³Ø±ÙˆØ³Ø² Ú©ÛŒ Ø¹Ø¯Ù… Ø¯Ø³ØªÛŒØ§Ø¨ÛŒ Ú©Û’ ÙˆÙ‚Øª Ú¯Ø±ÛŒØ³ÙÙ„ ÚˆÛŒÚ¯Ø±ÛŒÚˆÛŒØ´Ù† Ø´Ø§Ù…Ù„ ÛÛ’Û”  

**ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†**: ÙØ±ÛŒÙ… ÙˆØ±Ú© Ø°ÛÛŒÙ† ÙˆØ³Ø§Ø¦Ù„ Ú©Û’ Ø§Ù†ØªØ¸Ø§Ù… ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ Ø¬Ø³ Ù…ÛŒÚº SLMs Ú©Û’ Ù„ÛŒÛ’ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…ÛŒÙ…ÙˆØ±ÛŒ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†ØŒ Ø§ÛŒØ¬ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ú©Û’ Ù„ÛŒÛ’ CPU/GPU Ù„ÙˆÚˆ Ø¨ÛŒÙ„Ù†Ø³Ù†Ú¯ØŒ Ø¯Ø³ØªÛŒØ§Ø¨ ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ù…ÙˆØ§ÙÙ‚Øª Ù¾Ø°ÛŒØ± Ù…Ø§ÚˆÙ„ Ú©Ø§ Ø§Ù†ØªØ®Ø§Ø¨ØŒ Ø§ÙˆØ± Ù…ÙˆØ¨Ø§Ø¦Ù„ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ù¾Ø§ÙˆØ± Ù…ÙˆØ«Ø± Ø§Ù†ÙØ±Ù†Ø³ Ù¾ÛŒÙ¹Ø±Ù†Ø² Ø´Ø§Ù…Ù„ ÛÛŒÚºÛ”  

**Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ø§ÙˆØ± Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒØ³ÛŒ**: Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ú¯Ø±ÛŒÚˆ Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ø®ØµÙˆØµÛŒØ§Øª Ù…ÛŒÚº Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒØ³ÛŒ Ú©Ùˆ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù…Ù‚Ø§Ù…ÛŒ ÚˆÛŒÙ¹Ø§ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ØŒ Ø§ÛŒØ¬Ù†Ù¹ Ú©Ù…ÛŒÙˆÙ†ÛŒÚ©ÛŒØ´Ù† Ú†ÛŒÙ†Ù„Ø² Ú©ÛŒ Ø§Ù†Ú©Ø±Ù¾Ø´Ù†ØŒ Ø§ÛŒØ¬Ù†Ù¹ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø±ÙˆÙ„ Ø¨ÛŒØ³Úˆ Ø§ÛŒÚ©Ø³ÛŒØ³ Ú©Ù†Ù¹Ø±ÙˆÙ„Ø²ØŒ Ø§ÙˆØ± ØªØ¹Ù…ÛŒÙ„ Ú©ÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ú©Û’ Ù„ÛŒÛ’ Ø¢ÚˆÙ¹ Ù„Ø§Ú¯Ù†Ú¯ Ø´Ø§Ù…Ù„ ÛÛŒÚºÛ”  

### ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ù†Ø¶Ù…Ø§Ù…  

Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ú©Û’ Ø³Ø§ØªÚ¾ ÛÙ…ÙˆØ§Ø± Ø§Ù†Ø¶Ù…Ø§Ù… ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ ØªØ§Ú©Û Ù…Ú©Ù…Ù„ Ø§ÛŒØ¬ AI Ø­Ù„ ÙØ±Ø§ÛÙ… Ú©ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’:  

**Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø§ÚˆÙ„ Ø¯Ø±ÛŒØ§ÙØª**: ÙØ±ÛŒÙ… ÙˆØ±Ú© Ø®ÙˆØ¯ Ø¨Ø®ÙˆØ¯ ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ø§Ù†Ø³Ù¹ÛŒÙ†Ø³Ø² Ú©Ø§ Ù¾ØªÛ Ù„Ú¯Ø§ØªØ§ ÛÛ’ØŒ Ø¯Ø³ØªÛŒØ§Ø¨ SLM Ù…Ø§ÚˆÙ„Ø² Ø³Û’ Ø¬Ú‘ØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ø§ÙˆØ± ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ø¨ÛØªØ±ÛŒÙ† Ù…Ø§ÚˆÙ„Ø² Ú©Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±ØªØ§ ÛÛ’Û”  

**ÚˆØ§Ø¦Ù†Ø§Ù…Ú© Ù…Ø§ÚˆÙ„ Ù„ÙˆÚˆÙ†Ú¯**: Ø§ÛŒØ¬Ù†Ù¹Ø³ Ù…Ø®ØµÙˆØµ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ù…Ø®ØªÙ„Ù SLMs Ú©Ùˆ Ù…ØªØ­Ø±Ú© Ø·ÙˆØ± Ù¾Ø± Ù„ÙˆÚˆ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ù…Ù„Ù¹ÛŒ Ù…Ø§ÚˆÙ„ Ø§ÛŒØ¬Ù†Ù¹ Ø³Ø³Ù¹Ù…Ø² Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±ØªÛ’ ÛÛŒÚº Ø¬ÛØ§Úº Ù…Ø®ØªÙ„Ù Ù…Ø§ÚˆÙ„Ø² Ù…Ø®ØªÙ„Ù Ù‚Ø³Ù… Ú©ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³ØªÙˆÚº Ú©Ùˆ Ø³Ù†Ø¨Ú¾Ø§Ù„ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± Ø¯Ø³ØªÛŒØ§Ø¨ÛŒ Ø§ÙˆØ± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ø®ÙˆØ¯Ú©Ø§Ø± ÙÛŒÙ„ Ø§ÙˆÙˆØ±Û”  

**Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†**: Ù…Ø±Ø¨ÙˆØ· Ú©ÛŒØ´Ù†Ú¯ Ù…ÛŒÚ©Ø§Ù†Ø²Ù… Ù…Ø§ÚˆÙ„ Ù„ÙˆÚˆÙ†Ú¯ Ú©Û’ ÙˆÙ‚Øª Ú©Ùˆ Ú©Ù… Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ú©Ù†Ú©Ø´Ù† Ù¾ÙˆÙ„Ù†Ú¯ ÙØ§Ø¤Ù†ÚˆØ±ÛŒ Ù„ÙˆÚ©Ù„ Ú©Û’ API Ú©Ø§Ù„Ø² Ú©Ùˆ Ø¨ÛØªØ± Ø¨Ù†Ø§ØªÛŒ ÛÛ’ØŒ Ø§ÙˆØ± Ø°ÛÛŒÙ† Ø¨ÛŒÚ†Ù†Ú¯ Ù…ØªØ¹Ø¯Ø¯ Ø§ÛŒØ¬Ù†Ù¹ Ø¯Ø±Ø®ÙˆØ§Ø³ØªÙˆÚº Ú©Û’ Ù„ÛŒÛ’ ØªÚ¾Ø±ÙˆÙ¾Ù¹ Ú©Ùˆ Ø¨ÛØªØ± Ø¨Ù†Ø§ØªÛŒ ÛÛ’Û”  

### Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ú©Û’ Ø³Ø§ØªÚ¾ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ø¨Ù†Ø§Ù†Ø§  

#### Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØªØ¹Ø±ÛŒÙ Ø§ÙˆØ± ØªØ±ØªÛŒØ¨  

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```
  
#### Ø§ÛŒØ¬ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ù¹ÙˆÙ„ Ø§Ù†Ø¶Ù…Ø§Ù…  

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```
  
#### Ù…Ù„Ù¹ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ø¢Ø±Ú©Ø³Ù¹Ø±ÛŒØ´Ù†  

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```
  

### Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ø¬Ø¯ÛŒØ¯ Ù†Ù…ÙˆÙ†Û’  

#### Ø¯Ø±Ø¬Û Ø¨Ù†Ø¯ÛŒ Ø§ÛŒØ¬Ù†Ù¹ ÙÙ† ØªØ¹Ù…ÛŒØ±  

**Ù…Ù‚Ø§Ù…ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ú©Ù„Ø³Ù¹Ø±Ø²**: Ø§ÛŒØ¬ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ù¾Ø± Ù…ØªØ¹Ø¯Ø¯ Ø®ØµÙˆØµÛŒ SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ùˆ ØªØ¹ÛŒÙ†Ø§Øª Ú©Ø±ÛŒÚºØŒ ÛØ± Ø§ÛŒÚ© Ù…Ø®ØµÙˆØµ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± Ø¨Ù†Ø§ÛŒØ§ Ú¯ÛŒØ§ ÛÛ’Û” Ø³Ø§Ø¯Û Ø±ÙˆÙ¹Ù†Ú¯ Ø§ÙˆØ± Ø´ÛŒÚˆÙˆÙ„Ù†Ú¯ Ú©Û’ Ù„ÛŒÛ’ Qwen2.5-0.5B Ø¬ÛŒØ³Û’ ÛÙ„Ú©Û’ Ù…Ø§ÚˆÙ„Ø² Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚºØŒ Ú©Ø³Ù¹Ù…Ø± Ø³Ø±ÙˆØ³ Ø§ÙˆØ± Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ú©Û’ Ù„ÛŒÛ’ Ø¯Ø±Ù…ÛŒØ§Ù†Û’ Ù…Ø§ÚˆÙ„Ø² Ø¬ÛŒØ³Û’ Phi-4-MiniØŒ Ø§ÙˆØ± ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ø§Ø¬Ø§Ø²Øª Ú©Û’ ÙˆÙ‚Øª Ù¾ÛŒÚ†ÛŒØ¯Û Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ Ø¨Ú‘Û’ Ù…Ø§ÚˆÙ„Ø²Û”  

**Ø§ÛŒØ¬ Ø³Û’ Ú©Ù„Ø§Ø¤Úˆ Ú©ÙˆØ¢Ø±ÚˆÛŒÙ†ÛŒØ´Ù†**: Ø°ÛÛŒÙ† Ø§Ø³Ú©ÛŒÙ„ÛŒØ´Ù† Ù¾ÛŒÙ¹Ø±Ù†Ø² Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº Ø¬ÛØ§Úº Ù…Ù‚Ø§Ù…ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ù…Ø¹Ù…ÙˆÙ„ Ú©Û’ Ú©Ø§Ù…ÙˆÚº Ú©Ùˆ Ø³Ù†Ø¨Ú¾Ø§Ù„ØªÛ’ ÛÛŒÚºØŒ Ú©Ù„Ø§Ø¤Úˆ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ù†ÛŒÚ©Ù¹ÛŒÙˆÛŒÙ¹ÛŒ Ú©ÛŒ Ø§Ø¬Ø§Ø²Øª Ú©Û’ ÙˆÙ‚Øª Ù¾ÛŒÚ†ÛŒØ¯Û Ø§Ø³ØªØ¯Ù„Ø§Ù„ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ø§ÙˆØ± Ø§ÛŒØ¬ Ø§ÙˆØ± Ú©Ù„Ø§Ø¤Úˆ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† ÛÙ…ÙˆØ§Ø± ÛÛŒÙ†Úˆ Ø¢Ù ØªØ³Ù„Ø³Ù„ Ú©Ùˆ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø±Ú©Ú¾ØªØ§ ÛÛ’Û”  

#### ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ ØªØ±ØªÛŒØ¨  

**Ø³Ù†Ú¯Ù„ ÚˆÛŒÙˆØ§Ø¦Ø³ ØªØ¹ÛŒÙ†Ø§ØªÛŒ**:  
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```
  
**ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯Û Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ**:  
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```
  

### Ø§ÛŒØ¬ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†  

#### Ù…Ø§ÚˆÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ú©ÛŒ Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒ  

**Ù¹Ø§Ø³Ú© Ù¾Ø± Ù…Ø¨Ù†ÛŒ Ù…Ø§ÚˆÙ„ Ø§Ø³Ø§Ø¦Ù†Ù…Ù†Ù¹**: Ù…Ø§Ø¦ÛŒÚ©Ø±ÙˆØ³Ø§ÙÙ¹ Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ú©Ø§Ù… Ú©ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø§ÙˆØ± Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ø°ÛÛŒÙ† Ù…Ø§ÚˆÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ùˆ ÙØ¹Ø§Ù„ Ú©Ø±ØªØ§ ÛÛ’:  

- **Ø³Ø§Ø¯Û Ú©Ø§Ù…** (Q&AØŒ Ø±ÙˆÙ¹Ù†Ú¯): Qwen2.5-0.5B (500MBØŒ <100ms Ø±Ø³Ù¾Ø§Ù†Ø³)  
- **Ø¯Ø±Ù…ÛŒØ§Ù†Û’ Ú©Ø§Ù…** (Ú©Ø³Ù¹Ù…Ø± Ø³Ø±ÙˆØ³ØŒ Ø´ÛŒÚˆÙˆÙ„Ù†Ú¯): Phi-4-Mini (2.4GBØŒ 200-500ms Ø±Ø³Ù¾Ø§Ù†Ø³)  
- **Ù¾ÛŒÚ†ÛŒØ¯Û Ú©Ø§Ù…** (ØªÚ©Ù†ÛŒÚ©ÛŒ ØªØ¬Ø²ÛŒÛØŒ Ù…Ù†ØµÙˆØ¨Û Ø¨Ù†Ø¯ÛŒ): Phi-4 (7GBØŒ 1-3s Ø±Ø³Ù¾Ø§Ù†Ø³ Ø¬Ø¨ ÙˆØ³Ø§Ø¦Ù„ Ø§Ø¬Ø§Ø²Øª Ø¯ÛŒÚº)  

**ÚˆØ§Ø¦Ù†Ø§Ù…Ú© Ù…Ø§ÚˆÙ„ Ø³ÙˆØ¦Ú†Ù†Ú¯**: Ø§ÛŒØ¬Ù†Ù¹Ø³ Ù…ÙˆØ¬ÙˆØ¯Û Ø³Ø³Ù¹Ù… Ù„ÙˆÚˆØŒ Ú©Ø§Ù… Ú©ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ú©ÛŒ ØªØ´Ø®ÛŒØµØŒ ØµØ§Ø±Ù Ú©ÛŒ ØªØ±Ø¬ÛŒØ­ÛŒ Ø³Ø·Ø­ØŒ Ø§ÙˆØ± Ø¯Ø³ØªÛŒØ§Ø¨ ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ø¨Ù†ÛŒØ§Ø¯ Ù¾Ø± Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ø³ÙˆØ¦Ú† Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºÛ”  

#### Ù…ÛŒÙ…ÙˆØ±ÛŒ Ø§ÙˆØ± ÙˆØ³Ø§Ø¦Ù„ Ú©Ø§ Ø§Ù†ØªØ¸Ø§Ù…  

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```
  

### Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ø§Ù†Ø¶Ù…Ø§Ù… Ú©Û’ Ù†Ù…ÙˆÙ†Û’  

#### Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ø§ÙˆØ± ØªØ¹Ù…ÛŒÙ„  

**Ù…Ù‚Ø§Ù…ÛŒ ÚˆÛŒÙ¹Ø§ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯**: ØªÙ…Ø§Ù… Ø§ÛŒØ¬Ù†Ù¹ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ù…Ù‚Ø§Ù…ÛŒ Ø·ÙˆØ± Ù¾Ø± ÛÙˆØªÛŒ ÛÛ’ØŒ Ø§Ø³ Ø¨Ø§Øª Ú©Ùˆ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§ØªÛ’ ÛÙˆØ¦Û’ Ú©Û Ø­Ø³Ø§Ø³ ÚˆÛŒÙ¹Ø§ Ú©Ø¨Ú¾ÛŒ Ø¨Ú¾ÛŒ Ø§ÛŒØ¬ ÚˆÛŒÙˆØ§Ø¦Ø³ Ø³Û’ Ø¨Ø§ÛØ± Ù†Û Ø¬Ø§Ø¦Û’Û” Ø§Ø³ Ù…ÛŒÚº Ú©Ø³Ù¹Ù…Ø± Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©Ø§ ØªØ­ÙØ¸ØŒ ØµØ­Øª Ú©ÛŒ Ø¯ÛŒÚ©Ú¾ Ø¨Ú¾Ø§Ù„ Ú©Û’ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ HIPAA ØªØ¹Ù…ÛŒÙ„ØŒ Ø¨ÛŒÙ†Ú©Ù†Ú¯ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø§Ù„ÛŒØ§ØªÛŒ ÚˆÛŒÙ¹Ø§ Ú©ÛŒ Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒØŒ Ø§ÙˆØ± ÛŒÙˆØ±Ù¾ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒÙˆÚº Ú©Û’ Ù„ÛŒÛ’ GDPR ØªØ¹Ù…ÛŒÙ„ Ø´Ø§Ù…Ù„ ÛÛ’Û”  

**Ø±Ø³Ø§Ø¦ÛŒ Ú©Ù†Ù¹Ø±ÙˆÙ„**: Ø±ÙˆÙ„ Ù¾Ø± Ù…Ø¨Ù†ÛŒ Ø§Ø¬Ø§Ø²ØªÛŒÚº Ú©Ù†Ù¹Ø±ÙˆÙ„ Ú©Ø±ØªÛŒ ÛÛŒÚº Ú©Û Ú©ÙˆÙ† Ø³Û’ Ù¹ÙˆÙ„Ø² Ø§ÛŒØ¬Ù†Ù¹Ø³ ØªÚ© Ø±Ø³Ø§Ø¦ÛŒ Ø­Ø§ØµÙ„ Ú©Ø± Ø³Ú©ØªÛ’ ÛÛŒÚºØŒ Ø§ÛŒØ¬Ù†Ù¹ ØªØ¹Ø§Ù…Ù„Ø§Øª
**Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ ÙØ±ÛŒÙ… ÙˆØ±Ú© Ú©Ø§ Ø§Ù†ØªØ®Ø§Ø¨**: ÛØ¯Ù ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ø§ÙˆØ± Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù† ÙØ±ÛŒÙ… ÙˆØ±Ú© Ú©Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±ÛŒÚºÛ” CPU-optimized Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Llama.cpp Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚºØŒ Apple Silicon Ø§ÛŒØ¬Ù†Ù¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Apple MLXØŒ Ø§ÙˆØ± Ú©Ø±Ø§Ø³ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù… Ø§ÛŒØ¬Ù†Ù¹ Ù…Ø·Ø§Ø¨Ù‚Øª Ú©Û’ Ù„ÛŒÛ’ ONNXÛ”

## Ø¹Ù…Ù„ÛŒ SLM Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØªØ¨Ø¯ÛŒÙ„ÛŒ Ø§ÙˆØ± Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ú©ÛŒØ³Ø²

### Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ú©Û’ Ø§ÛŒØ¬Ù†Ù¹ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…Û’

**Ù…ÙˆØ¨Ø§Ø¦Ù„ Ø§ÛŒØ¬Ù†Ù¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø²**: Q4_K ÙØ§Ø±Ù…ÛŒÙ¹Ø³ Ø§Ø³Ù…Ø§Ø±Ù¹ ÙÙˆÙ† Ø§ÛŒØ¬Ù†Ù¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ù…ÛŒÚº Ú©Ù… Ù…ÛŒÙ…ÙˆØ±ÛŒ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Û’ Ø³Ø§ØªÚ¾ Ø¨ÛØªØ±ÛŒÙ† ÛÛŒÚºØŒ Ø¬Ø¨Ú©Û Q8_0 Ù¹ÛŒØ¨Ù„Ù¹ Ù¾Ø± Ù…Ø¨Ù†ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ø³Ø³Ù¹Ù…Ø² Ú©Û’ Ù„ÛŒÛ’ Ù…ØªÙˆØ§Ø²Ù† Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’Û” Q5_K ÙØ§Ø±Ù…ÛŒÙ¹Ø³ Ù…ÙˆØ¨Ø§Ø¦Ù„ Ù¾Ø±ÙˆÚˆÚ©Ù¹ÛŒÙˆÛŒÙ¹ÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø¹Ù„ÛŒÙ° Ù…Ø¹ÛŒØ§Ø± ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

**ÚˆÛŒØ³Ú© Ù¹Ø§Ù¾ Ø§ÙˆØ± Ø§ÛŒØ¬ Ø§ÛŒØ¬Ù†Ù¹ Ú©Ù…Ù¾ÛŒÙˆÙ¹Ù†Ú¯**: Q5_K ÚˆÛŒØ³Ú© Ù¹Ø§Ù¾ Ø§ÛŒØ¬Ù†Ù¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ±ÛŒÙ† Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ØŒ Q8_0 ÙˆØ±Ú© Ø³Ù¹ÛŒØ´Ù† Ø§ÛŒØ¬Ù†Ù¹ Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ø§Ø¹Ù„ÛŒÙ° Ù…Ø¹ÛŒØ§Ø± Ú©ÛŒ Ø§Ù†ÙØ±Ù†Ø³ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± Q4_K Ø§ÛŒØ¬ Ø§ÛŒØ¬Ù†Ù¹ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ù¾Ø± Ù…Ø¤Ø«Ø± Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ú©Ùˆ Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªØ§ ÛÛ’Û”

**ØªØ­Ù‚ÛŒÙ‚ Ø§ÙˆØ± ØªØ¬Ø±Ø¨Ø§ØªÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ø¬Ø¯ÛŒØ¯ Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù† ÙØ§Ø±Ù…ÛŒÙ¹Ø³ Ø§Ù†ØªÛØ§Ø¦ÛŒ Ú©Ù… Ù¾Ø±ÛŒØ³ÛŒÚ˜Ù† Ø§ÛŒØ¬Ù†Ù¹ Ø§Ù†ÙØ±Ù†Ø³ Ú©ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ø§ÙˆØ± Ù¾Ø±ÙˆÙ Ø¢Ù Ú©Ø§Ù†Ø³ÛŒÙ¾Ù¹ Ø§ÛŒØ¬Ù†Ù¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ø¬Ùˆ Ø§Ù†ØªÛØ§Ø¦ÛŒ ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ù¾Ø§Ø¨Ù†Ø¯ÛŒÙˆÚº Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆØªÛŒ ÛÛ’ØŒ Ú©Ùˆ Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚºÛ”

### SLM Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ø¨ÛŒÙ†Ú† Ù…Ø§Ø±Ú©Ø³

**Ø§ÛŒØ¬Ù†Ù¹ Ø§Ù†ÙØ±Ù†Ø³ Ú©ÛŒ Ø±ÙØªØ§Ø±**: Q4_K Ù…ÙˆØ¨Ø§Ø¦Ù„ CPUs Ù¾Ø± Ø³Ø¨ Ø³Û’ ØªÛŒØ² Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ Ø±Ø¯Ø¹Ù…Ù„ Ú©Ø§ ÙˆÙ‚Øª Ø­Ø§ØµÙ„ Ú©Ø±ØªØ§ ÛÛ’ØŒ Q5_K Ø¹Ù…ÙˆÙ…ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ø±ÙØªØ§Ø± Ø§ÙˆØ± Ù…Ø¹ÛŒØ§Ø± Ú©Ø§ Ù…ØªÙˆØ§Ø²Ù† ØªÙ†Ø§Ø³Ø¨ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ØŒ Q8_0 Ù¾ÛŒÚ†ÛŒØ¯Û Ø§ÛŒØ¬Ù†Ù¹ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø§Ø¹Ù„ÛŒÙ° Ù…Ø¹ÛŒØ§Ø± ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ØŒ Ø§ÙˆØ± ØªØ¬Ø±Ø¨Ø§ØªÛŒ ÙØ§Ø±Ù…ÛŒÙ¹Ø³ Ø®ØµÙˆØµÛŒ Ø§ÛŒØ¬Ù†Ù¹ ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ú©Û’ Ù„ÛŒÛ’ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û ØªÚ¾Ø±ÙˆÙ¾Ù¹ ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

**Ø§ÛŒØ¬Ù†Ù¹ Ù…ÛŒÙ…ÙˆØ±ÛŒ Ú©ÛŒ Ø¶Ø±ÙˆØ±ÛŒØ§Øª**: Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù† Ù„ÛŒÙˆÙ„Ø² Q2_K (Ú†Ú¾ÙˆÙ¹Û’ Ø§ÛŒØ¬Ù†Ù¹ Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ù„ÛŒÛ’ 500MB Ø³Û’ Ú©Ù…) Ø³Û’ Ù„Û’ Ú©Ø± Q8_0 (Ø§ØµÙ„ Ø³Ø§Ø¦Ø² Ú©Ø§ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ 50%) ØªÚ© ÛÙˆØªÛ’ ÛÛŒÚºØŒ Ø¬Ø¨Ú©Û ØªØ¬Ø±Ø¨Ø§ØªÛŒ Ú©Ù†ÙÛŒÚ¯Ø±ÛŒØ´Ù†Ø² ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ù¾Ø§Ø¨Ù†Ø¯ÛŒ ÙˆØ§Ù„Û’ Ø§ÛŒØ¬Ù†Ù¹ Ù…Ø§Ø­ÙˆÙ„ Ú©Û’ Ù„ÛŒÛ’ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û Ú©Ù…Ù¾Ø±ÛŒØ´Ù† Ø­Ø§ØµÙ„ Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

## SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ú†ÛŒÙ„Ù†Ø¬Ø² Ø§ÙˆØ± ØºÙˆØ± Ùˆ ÙÚ©Ø±

### Ø§ÛŒØ¬Ù†Ù¹ Ø³Ø³Ù¹Ù…Ø² Ù…ÛŒÚº Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ø³Ù…Ø¬Ú¾ÙˆØªÛ’

SLM Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ù…ÛŒÚº Ù…Ø§ÚˆÙ„ Ú©Û’ Ø³Ø§Ø¦Ø²ØŒ Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ Ø±Ø¯Ø¹Ù…Ù„ Ú©ÛŒ Ø±ÙØªØ§Ø±ØŒ Ø§ÙˆØ± Ø¢Ø¤Ù¹ Ù¾Ù¹ Ú©Û’ Ù…Ø¹ÛŒØ§Ø± Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ø³Ù…Ø¬Ú¾ÙˆØªÛ’ Ù¾Ø± ØºÙˆØ± Ú©Ø±Ù†Ø§ Ø´Ø§Ù…Ù„ ÛÛ’Û” Ø¬ÛØ§Úº Q4_K Ù…ÙˆØ¨Ø§Ø¦Ù„ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ ØºÛŒØ± Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø±ÙØªØ§Ø± Ø§ÙˆØ± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’ØŒ ÙˆÛÛŒÚº Q8_0 Ù¾ÛŒÚ†ÛŒØ¯Û Ø§ÛŒØ¬Ù†Ù¹ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø§Ø¹Ù„ÛŒÙ° Ù…Ø¹ÛŒØ§Ø± ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’Û” Q5_K Ø²ÛŒØ§Ø¯Û ØªØ± Ø¹Ù…ÙˆÙ…ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© Ø¯Ø±Ù…ÛŒØ§Ù†ÛŒ Ø±Ø§Ø³ØªÛ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’Û”

### SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ù…Ø·Ø§Ø¨Ù‚Øª

Ù…Ø®ØªÙ„Ù Ø§ÛŒØ¬ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² SLM Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø®ØªÙ„Ù ØµÙ„Ø§Ø­ÛŒØªÛŒÚº Ø±Ú©Ú¾ØªÛŒ ÛÛŒÚºÛ” Q4_K Ø³Ø§Ø¯Û Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ø±Ø² Ù¾Ø± Ù…Ø¤Ø«Ø± Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ú†Ù„ØªØ§ ÛÛ’ØŒ Q5_K Ù…ØªÙˆØ§Ø²Ù† Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø¹ØªØ¯Ù„ Ú©Ù…Ù¾ÛŒÙˆÙ¹ÛŒØ´Ù†Ù„ ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª ÛÙˆØªÛŒ ÛÛ’ØŒ Ø§ÙˆØ± Q8_0 Ø§Ø¹Ù„ÛŒÙ° Ø¯Ø±Ø¬Û’ Ú©Û’ ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ø³Û’ ÙØ§Ø¦Ø¯Û Ø§Ù¹Ú¾Ø§ØªØ§ ÛÛ’ ØªØ§Ú©Û Ø¬Ø¯ÛŒØ¯ Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ú©Ùˆ Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§ Ø³Ú©Û’Û”

### SLM Ø§ÛŒØ¬Ù†Ù¹ Ø³Ø³Ù¹Ù…Ø² Ù…ÛŒÚº Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ø§ÙˆØ± Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒØ³ÛŒ

Ø¬ÛØ§Úº SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ø¨ÛØªØ± Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒØ³ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ù…Ù‚Ø§Ù…ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ú©Ùˆ Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚºØŒ ÙˆÛØ§Úº Ø§ÛŒØ¬Ù†Ù¹ Ù…Ø§ÚˆÙ„Ø² Ø§ÙˆØ± ÚˆÛŒÙ¹Ø§ Ú©Ùˆ Ø§ÛŒØ¬ Ù…Ø§Ø­ÙˆÙ„ Ù…ÛŒÚº Ù…Ø­ÙÙˆØ¸ Ø±Ú©Ú¾Ù†Û’ Ú©Û’ Ù„ÛŒÛ’ Ù…Ù†Ø§Ø³Ø¨ Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª Ù†Ø§ÙØ° Ú©Ø±Ù†Ø§ Ø¶Ø±ÙˆØ±ÛŒ ÛÛ’Û” ÛŒÛ Ø®Ø§Øµ Ø·ÙˆØ± Ù¾Ø± Ø§ÛÙ… ÛÛ’ Ø¬Ø¨ Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ù…Ø§Ø­ÙˆÙ„ Ù…ÛŒÚº Ø§Ø¹Ù„ÛŒÙ° Ù¾Ø±ÛŒØ³ÛŒÚ˜Ù† Ø§ÛŒØ¬Ù†Ù¹ ÙØ§Ø±Ù…ÛŒÙ¹Ø³ ÛŒØ§ Ø­Ø³Ø§Ø³ ÚˆÛŒÙ¹Ø§ Ú©Ùˆ ÛÛŒÙ†ÚˆÙ„ Ú©Ø±Ù†Û’ ÙˆØ§Ù„ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ù…ÛŒÚº Ú©Ù…Ù¾Ø±ÛŒØ³Úˆ Ø§ÛŒØ¬Ù†Ù¹ ÙØ§Ø±Ù…ÛŒÙ¹Ø³ Ú©Ùˆ ØªØ¹ÛŒÙ†Ø§Øª Ú©ÛŒØ§ Ø¬Ø§Ø¦Û’Û”

## SLM Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØªØ±Ù‚ÛŒ Ù…ÛŒÚº Ù…Ø³ØªÙ‚Ø¨Ù„ Ú©Û’ Ø±Ø¬Ø­Ø§Ù†Ø§Øª

SLM Ø§ÛŒØ¬Ù†Ù¹ Ú©Ø§ Ù…Ù†Ø¸Ø± Ù†Ø§Ù…Û Ú©Ù…Ù¾Ø±ÛŒØ´Ù† ØªÚ©Ù†ÛŒÚ©ØŒ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù† Ú©Û’ Ø·Ø±ÛŒÙ‚Û’ØŒ Ø§ÙˆØ± Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒÙˆÚº Ù…ÛŒÚº ØªØ±Ù‚ÛŒ Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ø³Ù„Ø³Ù„ ØªØ±Ù‚ÛŒ Ú©Ø± Ø±ÛØ§ ÛÛ’Û” Ù…Ø³ØªÙ‚Ø¨Ù„ Ú©ÛŒ ØªØ±Ù‚ÛŒ Ù…ÛŒÚº Ø§ÛŒØ¬Ù†Ù¹ Ù…Ø§ÚˆÙ„Ø² Ú©Û’ Ù„ÛŒÛ’ Ø²ÛŒØ§Ø¯Û Ù…Ø¤Ø«Ø± Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²ÛŒØ´Ù† Ø§Ù„Ú¯ÙˆØ±ØªÚ¾Ù…Ø²ØŒ Ø§ÛŒØ¬Ù†Ù¹ ÙˆØ±Ú© ÙÙ„Ùˆ Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ± Ú©Ù…Ù¾Ø±ÛŒØ´Ù† Ú©Û’ Ø·Ø±ÛŒÙ‚Û’ØŒ Ø§ÙˆØ± Ø§ÛŒØ¬Ù†Ù¹ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒØ¬ ÛØ§Ø±ÚˆÙˆÛŒØ¦Ø± Ø§ÛŒÚ©Ø³ÛŒÙ„ÛŒØ±ÛŒÙ¹Ø±Ø² Ú©Û’ Ø³Ø§ØªÚ¾ Ø¨ÛØªØ± Ø§Ù†Ø¶Ù…Ø§Ù… Ø´Ø§Ù…Ù„ ÛÛŒÚºÛ”

**SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø§Ø±Ú©ÛŒÙ¹ Ú©ÛŒ Ù¾ÛŒØ´ Ú¯ÙˆØ¦ÛŒØ§Úº**: Ø­Ø§Ù„ÛŒÛ ØªØ­Ù‚ÛŒÙ‚ Ú©Û’ Ù…Ø·Ø§Ø¨Ù‚ØŒ Ø§ÛŒØ¬Ù†Ù¹ Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„ÛŒ Ø¢Ù¹ÙˆÙ…ÛŒØ´Ù† 2027 ØªÚ© Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² ÙˆØ±Ú© ÙÙ„Ùˆ Ù…ÛŒÚº 40â€“60% Ø¯ÛØ±Ø§Ø¦Û’ Ø¬Ø§Ù†Û’ ÙˆØ§Ù„Û’ Ø¹Ù„Ù…ÛŒ Ú©Ø§Ù…ÙˆÚº Ú©Ùˆ Ø®ØªÙ… Ú©Ø± Ø³Ú©ØªÛŒ ÛÛ’ØŒ SLMs Ø§Ø³ ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©ÛŒ Ù‚ÛŒØ§Ø¯Øª Ú©Ø±ÛŒÚº Ú¯Û’ Ú©ÛŒÙˆÙ†Ú©Û ÙˆÛ Ù„Ø§Ú¯Øª Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø§ÙˆØ± ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ Ù„Ú†Ú© ÙØ±Ø§ÛÙ… Ú©Ø±ØªÛ’ ÛÛŒÚºÛ”

**SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ù…ÛŒÚº Ù¹ÛŒÚ©Ù†Ø§Ù„ÙˆØ¬ÛŒ Ú©Û’ Ø±Ø¬Ø­Ø§Ù†Ø§Øª**:
- **Ø®ØµÙˆØµÛŒ SLM Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ù…Ø®ØµÙˆØµ Ø§ÛŒØ¬Ù†Ù¹ Ú©Ø§Ù…ÙˆÚº Ø§ÙˆØ± ØµÙ†Ø¹ØªÙˆÚº Ú©Û’ Ù„ÛŒÛ’ ØªØ±Ø¨ÛŒØª ÛŒØ§ÙØªÛ ÚˆÙˆÙ…ÛŒÙ† Ù…Ø®ØµÙˆØµ Ù…Ø§ÚˆÙ„Ø²
- **Ø§ÛŒØ¬ Ø§ÛŒØ¬Ù†Ù¹ Ú©Ù…Ù¾ÛŒÙˆÙ¹Ù†Ú¯**: Ø¨ÛØªØ± Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒØ³ÛŒ Ø§ÙˆØ± Ú©Ù… Ù„ÛŒÙ¹Ù†Ø³ÛŒ Ú©Û’ Ø³Ø§ØªÚ¾ Ø¢Ù† ÚˆÛŒÙˆØ§Ø¦Ø³ Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÛŒÚº
- **Ø§ÛŒØ¬Ù†Ù¹ Ø¢Ø±Ú©Ø³Ù¹Ø±ÛŒØ´Ù†**: Ù…ØªØ¹Ø¯Ø¯ SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† Ø¨ÛØªØ± ÛÙ… Ø¢ÛÙ†Ú¯ÛŒØŒ Ù…ØªØ­Ø±Ú© Ø±ÙˆÙ¹Ù†Ú¯ Ø§ÙˆØ± Ù„ÙˆÚˆ Ø¨ÛŒÙ„Ù†Ø³Ù†Ú¯ Ú©Û’ Ø³Ø§ØªÚ¾
- **Ø¬Ù…ÛÙˆØ±ÛŒØª Ù¾Ø³Ù†Ø¯ÛŒ**: SLM Ú©ÛŒ Ù„Ú†Ú© ØªÙ†Ø¸ÛŒÙ…ÙˆÚº Ù…ÛŒÚº Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØªØ±Ù‚ÛŒ Ù…ÛŒÚº ÙˆØ³ÛŒØ¹ Ø´Ø±Ú©Øª Ú©Ùˆ Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªÛŒ ÛÛ’

## SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ø³Ø§ØªÚ¾ Ø´Ø±ÙˆØ¹Ø§Øª

### Ù…Ø±Ø­Ù„Û 1: Microsoft Agent Framework Ù…Ø§Ø­ÙˆÙ„ ØªØ±ØªÛŒØ¨ Ø¯ÛŒÚº

**Dependencies Ø§Ù†Ø³Ù¹Ø§Ù„ Ú©Ø±ÛŒÚº**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Foundry Local Ú©Ùˆ Ø§Ù†ÛŒØ´ÛŒØ¦Ù„Ø§Ø¦Ø² Ú©Ø±ÛŒÚº**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Ù…Ø±Ø­Ù„Û 2: Ø§ÛŒØ¬Ù†Ù¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ø§Ù¾Ù†Ø§ SLM Ù…Ù†ØªØ®Ø¨ Ú©Ø±ÛŒÚº
Microsoft Agent Framework Ú©Û’ Ù„ÛŒÛ’ Ù…Ø´ÛÙˆØ± Ø§Ø®ØªÛŒØ§Ø±Ø§Øª:
- **Microsoft Phi-4 Mini (3.8B)**: Ø¹Ù…ÙˆÙ…ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø¨ÛØªØ±ÛŒÙ†ØŒ Ù…ØªÙˆØ§Ø²Ù† Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ø³Ø§ØªÚ¾
- **Qwen2.5-0.5B (0.5B)**: Ø³Ø§Ø¯Û Ø±ÙˆÙ¹Ù†Ú¯ Ø§ÙˆØ± Ú©Ù„Ø§Ø³ÛŒÙÛŒÚ©ÛŒØ´Ù† Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ø§Ù†ØªÛØ§Ø¦ÛŒ Ù…Ø¤Ø«Ø±
- **Qwen2.5-Coder-0.5B (0.5B)**: Ú©ÙˆÚˆ Ø³Û’ Ù…ØªØ¹Ù„Ù‚ Ø§ÛŒØ¬Ù†Ù¹ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø®ØµÙˆØµÛŒ
- **Phi-4 (7B)**: ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ Ø§Ø¬Ø§Ø²Øª ÛÙˆÙ†Û’ Ù¾Ø± Ù¾ÛŒÚ†ÛŒØ¯Û Ø§ÛŒØ¬ Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ Ø§Ø¹Ù„ÛŒÙ° Ø§Ø³ØªØ¯Ù„Ø§Ù„

### Ù…Ø±Ø­Ù„Û 3: Microsoft Agent Framework Ú©Û’ Ø³Ø§ØªÚ¾ Ø§Ù¾Ù†Ø§ Ù¾ÛÙ„Ø§ Ø§ÛŒØ¬Ù†Ù¹ Ø¨Ù†Ø§Ø¦ÛŒÚº

**Ø¨Ù†ÛŒØ§Ø¯ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ø³ÛŒÙ¹ Ø§Ù¾**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Ù…Ø±Ø­Ù„Û 4: Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ Ø¯Ø§Ø¦Ø±Û Ú©Ø§Ø± Ø§ÙˆØ± Ø¶Ø±ÙˆØ±ÛŒØ§Øª Ú©ÛŒ ÙˆØ¶Ø§Ø­Øª Ú©Ø±ÛŒÚº
Microsoft Agent Framework Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù…Ø±Ú©ÙˆØ²ØŒ Ø§Ú†Ú¾ÛŒ Ø·Ø±Ø­ Ø³Û’ ÙˆØ¶Ø§Ø­Øª Ø´Ø¯Û Ø§ÛŒØ¬Ù†Ù¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ø³Ø§ØªÚ¾ Ø´Ø±ÙˆØ¹ Ú©Ø±ÛŒÚº:
- **Ø³Ù†Ú¯Ù„ ÚˆÙˆÙ…ÛŒÙ† Ø§ÛŒØ¬Ù†Ù¹Ø³**: Ú©Ø³Ù¹Ù…Ø± Ø³Ø±ÙˆØ³ ÛŒØ§ Ø´ÛŒÚˆÙˆÙ„Ù†Ú¯ ÛŒØ§ ØªØ­Ù‚ÛŒÙ‚
- **Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ ÙˆØ§Ø¶Ø­ Ù…Ù‚Ø§ØµØ¯**: Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø®ØµÙˆØµØŒ Ù‚Ø§Ø¨Ù„ Ù¾ÛŒÙ…Ø§Ø¦Ø´ Ø§ÛØ¯Ø§Ù
- **Ù…Ø­Ø¯ÙˆØ¯ Ù¹ÙˆÙ„ Ø§Ù†Ø¶Ù…Ø§Ù…**: Ø§Ø¨ØªØ¯Ø§Ø¦ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ø²ÛŒØ§Ø¯Û Ø³Û’ Ø²ÛŒØ§Ø¯Û 3-5 Ù¹ÙˆÙ„Ø²
- **Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ÙˆØ§Ø¶Ø­ Ø­Ø¯ÙˆØ¯**: Ù¾ÛŒÚ†ÛŒØ¯Û Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ ÙˆØ§Ø¶Ø­ Ø§Ø³Ú©ÛŒÙ„ÛŒØ´Ù† Ø±Ø§Ø³ØªÛ’
- **Ø§ÛŒØ¬-ÙØ±Ø³Ù¹ ÚˆÛŒØ²Ø§Ø¦Ù†**: Ø¢Ù Ù„Ø§Ø¦Ù† ÙØ¹Ø§Ù„ÛŒØª Ø§ÙˆØ± Ù…Ù‚Ø§Ù…ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ú©Ùˆ ØªØ±Ø¬ÛŒØ­ Ø¯ÛŒÚº

### Ù…Ø±Ø­Ù„Û 5: Microsoft Agent Framework Ú©Û’ Ø³Ø§ØªÚ¾ Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº

**ÙˆØ³Ø§Ø¦Ù„ Ú©ÛŒ ØªØ±ØªÛŒØ¨**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Ø§ÛŒØ¬ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ Ø­ÙØ§Ø¸ØªÛŒ Ø§Ù‚Ø¯Ø§Ù…Ø§Øª ØªØ¹ÛŒÙ†Ø§Øª Ú©Ø±ÛŒÚº**:
- **Ù…Ù‚Ø§Ù…ÛŒ Ø§Ù† Ù¾Ù¹ Ú©ÛŒ ØªÙˆØ«ÛŒÙ‚**: Ú©Ù„Ø§Ø¤Úˆ Ù¾Ø± Ø§Ù†Ø­ØµØ§Ø± Ú©ÛŒÛ’ Ø¨ØºÛŒØ± Ø¯Ø±Ø®ÙˆØ§Ø³ØªÙˆÚº Ú©ÛŒ Ø¬Ø§Ù†Ú† Ú©Ø±ÛŒÚº
- **Ø¢Ù Ù„Ø§Ø¦Ù† Ø¢Ø¤Ù¹ Ù¾Ù¹ ÙÙ„Ù¹Ø±Ù†Ú¯**: ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§Ø¦ÛŒÚº Ú©Û Ø¬ÙˆØ§Ø¨Ø§Øª Ù…Ù‚Ø§Ù…ÛŒ Ø·ÙˆØ± Ù¾Ø± Ù…Ø¹ÛŒØ§Ø± Ú©Û’ Ù…Ø¹ÛŒØ§Ø± Ù¾Ø± Ù¾ÙˆØ±Ø§ Ø§ØªØ±ØªÛ’ ÛÛŒÚº
- **Ø§ÛŒØ¬ Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ú©Ù†Ù¹Ø±ÙˆÙ„Ø²**: Ø§Ù†Ù¹Ø±Ù†ÛŒÙ¹ Ú©Ù†ÛŒÚ©Ù¹ÛŒÙˆÛŒÙ¹ÛŒ Ú©ÛŒ Ø¶Ø±ÙˆØ±Øª Ú©Û’ Ø¨ØºÛŒØ± Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ù†Ø§ÙØ° Ú©Ø±ÛŒÚº
- **Ù…Ù‚Ø§Ù…ÛŒ Ù†Ú¯Ø±Ø§Ù†ÛŒ**: Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Ùˆ Ù¹Ø±ÛŒÚ© Ú©Ø±ÛŒÚº Ø§ÙˆØ± Ø§ÛŒØ¬ Ù¹ÛŒÙ„ÛŒÙ…ÛŒÙ¹Ø±ÛŒ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ Ù…Ø³Ø§Ø¦Ù„ Ú©Ùˆ Ù†Ø´Ø§Ù† Ø²Ø¯ Ú©Ø±ÛŒÚº

### Ù…Ø±Ø­Ù„Û 6: Ø§ÛŒØ¬ Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ù¾ÛŒÙ…Ø§Ø¦Ø´ Ø§ÙˆØ± Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø² Ú©Ø±ÛŒÚº
- **Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ Ú©Ø§Ù… Ù…Ú©Ù…Ù„ Ú©Ø±Ù†Û’ Ú©ÛŒ Ø´Ø±Ø­ÛŒÚº**: Ø¢Ù Ù„Ø§Ø¦Ù† Ù…Ù†Ø¸Ø±Ù†Ø§Ù…ÙˆÚº Ù…ÛŒÚº Ú©Ø§Ù…ÛŒØ§Ø¨ÛŒ Ú©ÛŒ Ø´Ø±Ø­ÙˆÚº Ú©ÛŒ Ù†Ú¯Ø±Ø§Ù†ÛŒ Ú©Ø±ÛŒÚº
- **Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ Ø±Ø¯Ø¹Ù…Ù„ Ú©Û’ Ø§ÙˆÙ‚Ø§Øª**: Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ø³Ø¨ Ø³ÛŒÚ©Ù†Úˆ Ø±Ø¯Ø¹Ù…Ù„ Ú©Û’ Ø§ÙˆÙ‚Ø§Øª Ú©Ùˆ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§Ø¦ÛŒÚº
- **ÙˆØ³Ø§Ø¦Ù„ Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„**: Ø§ÛŒØ¬ ÚˆÛŒÙˆØ§Ø¦Ø³Ø² Ù¾Ø± Ù…ÛŒÙ…ÙˆØ±ÛŒØŒ CPUØŒ Ø§ÙˆØ± Ø¨ÛŒÙ¹Ø±ÛŒ Ú©Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ùˆ Ù¹Ø±ÛŒÚ© Ú©Ø±ÛŒÚº
- **Ù„Ø§Ú¯Øª Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ**: Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ø§Ø®Ø±Ø§Ø¬Ø§Øª Ú©Ùˆ Ú©Ù„Ø§Ø¤Úˆ Ù¾Ø± Ù…Ø¨Ù†ÛŒ Ù…ØªØ¨Ø§Ø¯Ù„Ø§Øª Ø³Û’ Ù…ÙˆØ§Ø²Ù†Û Ú©Ø±ÛŒÚº
- **Ø¢Ù Ù„Ø§Ø¦Ù† Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯**: Ù†ÛŒÙ¹ ÙˆØ±Ú© Ú©ÛŒ Ø®Ø±Ø§Ø¨ÛŒ Ú©Û’ Ø¯ÙˆØ±Ø§Ù† Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ù¾ÛŒÙ…Ø§Ø¦Ø´ Ú©Ø±ÛŒÚº

## SLM Ø§ÛŒØ¬Ù†Ù¹ Ú©Û’ Ù†ÙØ§Ø° Ú©Û’ Ù„ÛŒÛ’ Ø§ÛÙ… Ù†Ú©Ø§Øª

1. **Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ SLM Ú©Ø§ÙÛŒ ÛÛŒÚº**: Ø²ÛŒØ§Ø¯Û ØªØ± Ø§ÛŒØ¬Ù†Ù¹ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ØŒ Ú†Ú¾ÙˆÙ¹Û’ Ù…Ø§ÚˆÙ„Ø² Ø¨Ú‘Û’ Ù…Ø§ÚˆÙ„Ø² Ú©ÛŒ Ø·Ø±Ø­ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø¯Ú©Ú¾Ø§ØªÛ’ ÛÛŒÚº Ø¬Ø¨Ú©Û Ø§ÛÙ… ÙÙˆØ§Ø¦Ø¯ Ù¾ÛŒØ´ Ú©Ø±ØªÛ’ ÛÛŒÚº
2. **Ø§ÛŒØ¬Ù†Ù¹Ø³ Ù…ÛŒÚº Ù„Ø§Ú¯Øª Ú©ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ**: SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ùˆ Ú†Ù„Ø§Ù†Ø§ 10-30x Ø³Ø³ØªØ§ ÛÛ’ØŒ Ø¬Ùˆ Ø§Ù†ÛÛŒÚº ÙˆØ³ÛŒØ¹ Ù¾ÛŒÙ…Ø§Ù†Û’ Ù¾Ø± ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø·ÙˆØ± Ù¾Ø± Ù‚Ø§Ø¨Ù„ Ø¹Ù…Ù„ Ø¨Ù†Ø§ØªØ§ ÛÛ’
3. **Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ Ù„ÛŒÛ’ ØªØ®ØµØµ Ú©Ø§Ù… Ú©Ø±ØªØ§ ÛÛ’**: Ù…Ø®ØµÙˆØµ Ø§ÛŒØ¬Ù†Ù¹ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ù…ÛŒÚº ÙØ§Ø¦Ù† Ù¹ÛŒÙˆÙ†Úˆ SLMs Ø§Ú©Ø«Ø± Ø¹Ù…ÙˆÙ…ÛŒ Ù…Ù‚ØµØ¯ Ú©Û’ LLMs Ø³Û’ Ø¨ÛØªØ± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ø¯Ú©Ú¾Ø§ØªÛ’ ÛÛŒÚº
4. **ÛØ§Ø¦Ø¨Ø±Úˆ Ø§ÛŒØ¬Ù†Ù¹ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø±**: Ù…Ø¹Ù…ÙˆÙ„ Ú©Û’ Ø§ÛŒØ¬Ù†Ù¹ Ú©Ø§Ù…ÙˆÚº Ú©Û’ Ù„ÛŒÛ’ SLMs Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ÛŒÚºØŒ Ù¾ÛŒÚ†ÛŒØ¯Û Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ú©Û’ Ù„ÛŒÛ’ LLMs Ø¬Ø¨ Ø¶Ø±ÙˆØ±ÛŒ ÛÙˆ
5. **Microsoft Agent Framework Ù¾ÛŒØ¯Ø§ÙˆØ§Ø± Ú©ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Ùˆ Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªØ§ ÛÛ’**: Ø§ÛŒØ¬ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©ÛŒ ØªØ¹Ù…ÛŒØ±ØŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒØŒ Ø§ÙˆØ± Ø§Ù†ØªØ¸Ø§Ù… Ú©Û’ Ù„ÛŒÛ’ Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ú¯Ø±ÛŒÚˆ Ù¹ÙˆÙ„Ø² ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’
6. **Ø§ÛŒØ¬-ÙØ±Ø³Ù¹ ÚˆÛŒØ²Ø§Ø¦Ù† Ø§ØµÙˆÙ„**: Ø¢Ù Ù„Ø§Ø¦Ù† Ù‚Ø§Ø¨Ù„ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ù…Ù‚Ø§Ù…ÛŒ Ù¾Ø±ÙˆØ³ÛŒØ³Ù†Ú¯ Ú©Û’ Ø³Ø§ØªÚ¾ Ù¾Ø±Ø§Ø¦ÛŒÙˆÛŒØ³ÛŒ Ø§ÙˆØ± Ù‚Ø§Ø¨Ù„ Ø§Ø¹ØªÙ…Ø§Ø¯ Ú©Ùˆ ÛŒÙ‚ÛŒÙ†ÛŒ Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚº
7. **Foundry Local Ø§Ù†Ø¶Ù…Ø§Ù…**: Microsoft Agent Framework Ø§ÙˆØ± Ù…Ù‚Ø§Ù…ÛŒ Ù…Ø§ÚˆÙ„ Ø§Ù†ÙØ±Ù†Ø³ Ú©Û’ Ø¯Ø±Ù…ÛŒØ§Ù† ÛÙ…ÙˆØ§Ø± Ú©Ù†Ú©Ø´Ù†
8. **Ù…Ø³ØªÙ‚Ø¨Ù„ SLM Ø§ÛŒØ¬Ù†Ù¹Ø³ ÛÛŒÚº**: Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† ÙØ±ÛŒÙ… ÙˆØ±Ú© Ú©Û’ Ø³Ø§ØªÚ¾ Ú†Ú¾ÙˆÙ¹Û’ Ø²Ø¨Ø§Ù† Ú©Û’ Ù…Ø§ÚˆÙ„Ø² Ø§ÛŒØ¬Ù†Ù¹Ú© AI Ú©Ø§ Ù…Ø³ØªÙ‚Ø¨Ù„ ÛÛŒÚºØŒ Ø¬Ùˆ Ø¬Ù…ÛÙˆØ±ÛŒ Ø§ÙˆØ± Ù…Ø¤Ø«Ø± Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Ùˆ Ù…Ù…Ú©Ù† Ø¨Ù†Ø§ØªÛ’ ÛÛŒÚº

## Ø­ÙˆØ§Ù„Û Ø¬Ø§Øª Ø§ÙˆØ± Ù…Ø²ÛŒØ¯ Ù…Ø·Ø§Ù„Ø¹Û

### Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØªØ­Ù‚ÛŒÙ‚ Ú©Û’ Ù…Ù‚Ø§Ù„Û’ Ø§ÙˆØ± Ø§Ø´Ø§Ø¹ØªÛŒÚº

#### AI Ø§ÛŒØ¬Ù†Ù¹Ø³ Ø§ÙˆØ± Ø§ÛŒØ¬Ù†Ù¹Ú© Ø³Ø³Ù¹Ù…Ø²
- **"Language Agents as Optimizable Graphs"** (2024) - Ø§ÛŒØ¬Ù†Ù¹ Ø¢Ø±Ú©ÛŒÙ¹ÛŒÚ©Ú†Ø± Ø§ÙˆØ± Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù† Ù¾Ø± Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØªØ­Ù‚ÛŒÙ‚
  - Ù…ØµÙ†ÙÛŒÙ†: Wenyue Hua, Lishan Yang, ÙˆØºÛŒØ±Û
  - Ù„Ù†Ú©: https://arxiv.org/abs/2402.16823
  - Ø§ÛÙ… Ù†Ú©Ø§Øª: Ú¯Ø±Ø§Ù Ù¾Ø± Ù…Ø¨Ù†ÛŒ Ø§ÛŒØ¬Ù†Ù¹ ÚˆÛŒØ²Ø§Ø¦Ù† Ø§ÙˆØ± Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù† Ú©ÛŒ Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒ

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Ù…ØµÙ†ÙÛŒÙ†: Zhiheng Xi, Wenxiang Chen, ÙˆØºÛŒØ±Û
  - Ù„Ù†Ú©: https://arxiv.org/abs/2309.07864
  - Ø§ÛÙ… Ù†Ú©Ø§Øª: LLM Ù¾Ø± Ù…Ø¨Ù†ÛŒ Ø§ÛŒØ¬Ù†Ù¹ Ú©ÛŒ ØµÙ„Ø§Ø­ÛŒØªÙˆÚº Ø§ÙˆØ± Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Ø§ Ø¬Ø§Ù…Ø¹ Ø³Ø±ÙˆÛ’

- **"Cognitive Architectures for Language Agents"** (2024)
  - Ù…ØµÙ†ÙÛŒÙ†: Theodore Sumers, Shunyu Yao, ÙˆØºÛŒØ±Û
  - Ù„Ù†Ú©: https://arxiv.org/abs/2309.02427
  - Ø§ÛÙ… Ù†Ú©Ø§Øª: Ø°ÛÛŒÙ† Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Û’ ÚˆÛŒØ²Ø§Ø¦Ù† Ú©Û’ Ù„ÛŒÛ’ Ø¹Ù„Ù…ÛŒ ÙØ±ÛŒÙ… ÙˆØ±Ú©

#### Ú†Ú¾ÙˆÙ¹Û’ Ø²Ø¨Ø§Ù† Ú©Û’ Ù…Ø§ÚˆÙ„Ø² Ø§ÙˆØ± Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Ù…ØµÙ†ÙÛŒÙ†: Microsoft Research Team
  - Ù„Ù†Ú©: https://arxiv.org/abs/2404.14219
  - Ø§ÛÙ… Ù†Ú©Ø§Øª: SLM ÚˆÛŒØ²Ø§Ø¦Ù† Ø§ØµÙˆÙ„ Ø§ÙˆØ± Ù…ÙˆØ¨Ø§Ø¦Ù„ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©ÛŒ Ø­Ú©Ù…Øª Ø¹Ù…Ù„ÛŒ

- **"Qwen2.5 Technical Report"** (2024)
  - Ù…ØµÙ†ÙÛŒÙ†: Alibaba Cloud Team
  - Ù„Ù†Ú©: https://arxiv.org/abs/2407.10671
  - Ø§ÛÙ… Ù†Ú©Ø§Øª: Ø¬Ø¯ÛŒØ¯ SLM ØªØ±Ø¨ÛŒØªÛŒ ØªÚ©Ù†ÛŒÚ© Ø§ÙˆØ± Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©ÛŒ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù†

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Ù…ØµÙ†ÙÛŒÙ†: Peiyuan Zhang, Guangtao Zeng, ÙˆØºÛŒØ±Û
  - Ù„Ù†Ú©: https://arxiv.org/abs/2401.02385
  - Ø§ÛÙ… Ù†Ú©Ø§Øª: Ø§Ù†ØªÛØ§Ø¦ÛŒ Ú©Ù…Ù¾ÛŒÚ©Ù¹ Ù…Ø§ÚˆÙ„ ÚˆÛŒØ²Ø§Ø¦Ù† Ø§ÙˆØ± ØªØ±Ø¨ÛŒØªÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ

### Ø¢ÙÛŒØ´Ù„ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª Ø§ÙˆØ± ÙØ±ÛŒÙ… ÙˆØ±Ú©

#### Microsoft Agent Framework
- **Ø¢ÙÛŒØ´Ù„ Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub Ø±ÛŒÙ¾ÙˆØ²ÛŒÙ¹Ø±ÛŒ**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Ù¾Ø±Ø§Ø¦Ù…Ø±ÛŒ Ø±ÛŒÙ¾ÙˆØ²ÛŒÙ¹Ø±ÛŒ**: https://github.com/microsoft/foundry-local
- **Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Ù…ÛŒÙ† Ø±ÛŒÙ¾ÙˆØ²ÛŒÙ¹Ø±ÛŒ**: https://github.com/vllm-project/vllm
- **Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª**: https://docs.vllm.ai/


#### Ollama
- **Ø¢ÙÛŒØ´Ù„ ÙˆÛŒØ¨ Ø³Ø§Ø¦Ù¹**: https://ollama.ai/
- **GitHub Ø±ÛŒÙ¾ÙˆØ²ÛŒÙ¹Ø±ÛŒ**: https://github.com/ollama/ollama

### Ù…Ø§ÚˆÙ„ Ø¢Ù¾Ù¹ÛŒÙ…Ø§Ø¦Ø²ÛŒØ´Ù† ÙØ±ÛŒÙ… ÙˆØ±Ú©

#### Llama.cpp
- **Ø±ÛŒÙ¾ÙˆØ²ÛŒÙ¹Ø±ÛŒ**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Ø¯Ø³ØªØ§ÙˆÛŒØ²Ø§Øª**: https://microsoft.github.io/Olive/
- **GitHub Ø±ÛŒÙ¾ÙˆØ²ÛŒÙ¹Ø±ÛŒ**: https://github.com/microsoft/Olive

#### OpenVINO
- **Ø¢ÙÛŒØ´Ù„ Ø³Ø§Ø¦Ù¹**: https://docs.openvino.ai/

#### Apple MLX
- **Ø±ÛŒÙ¾ÙˆØ²ÛŒÙ¹Ø±ÛŒ**: https://github.com/ml-explore/mlx

### Ø§Ù†ÚˆØ³Ù¹Ø±ÛŒ Ø±Ù¾ÙˆØ±Ù¹Ø³ Ø§ÙˆØ± Ù…Ø§Ø±Ú©ÛŒÙ¹ ØªØ¬Ø²ÛŒÛ

#### AI Ø§ÛŒØ¬Ù†Ù¹ Ù…Ø§Ø±Ú©ÛŒÙ¹ Ø±ÛŒØ³Ø±Ú†
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Ù„Ù†Ú©: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - Ø§ÛÙ… Ù†Ú©Ø§Øª: Ù…Ø§Ø±Ú©ÛŒÙ¹ Ú©Û’ Ø±Ø¬Ø­Ø§Ù†Ø§Øª Ø§ÙˆØ± Ø§Ù†Ù¹Ø±Ù¾Ø±Ø§Ø¦Ø² Ø§Ù¾Ù†Ø§Ù†Û’ Ú©Û’ Ù†Ù…ÙˆÙ†Û’

#### ØªÚ©Ù†ÛŒÚ©ÛŒ Ø¨ÛŒÙ†Ú† Ù…Ø§Ø±Ú©Ø³

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Ù„Ù†Ú©: https://mlcommons.org/en/inference-edge/
  - Ø§ÛÙ… Ù†Ú©Ø§Øª: Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ù„ÛŒÛ’ Ù…Ø¹ÛŒØ§Ø±ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒ Ú©Û’ Ù…ÛŒÙ¹Ø±Ú©Ø³

### Ù…Ø¹ÛŒØ§Ø±Ø§Øª Ø§ÙˆØ± ÙˆØ¶Ø§Ø­ØªÛŒÚº

#### Ù…Ø§ÚˆÙ„ ÙØ§Ø±Ù…ÛŒÙ¹Ø³ Ø§ÙˆØ± Ù…Ø¹ÛŒØ§Ø±Ø§Øª
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Ø§Ù†Ù¹Ø±Ø¢Ù¾Ø±ÛŒØ¨Ù„Ù¹ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ú©Ø±Ø§Ø³ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù… Ù…Ø§ÚˆÙ„ ÙØ§Ø±Ù…ÛŒÙ¹
- **GGUF Specification**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - CPU Ø§Ù†ÙØ±Ù†Ø³ Ú©Û’ Ù„ÛŒÛ’ Ú©ÙˆØ§Ù†Ù¹Ø§Ø¦Ø²Úˆ Ù…Ø§ÚˆÙ„ ÙØ§Ø±Ù…ÛŒÙ¹
- **OpenAI API Specification**: https://platform.openai.com/docs/api-reference
  - Ø²Ø¨Ø§Ù† Ù…Ø§ÚˆÙ„ Ø§Ù†Ø¶Ù…Ø§Ù… Ú©Û’ Ù„ÛŒÛ’ Ù…Ø¹ÛŒØ§Ø±ÛŒ API ÙØ§Ø±Ù…ÛŒÙ¹

#### Ø³ÛŒÚ©ÛŒÙˆØ±Ù¹ÛŒ Ø§ÙˆØ± ØªØ¹Ù…ÛŒÙ„
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI Systems**: AI Ø³Ø³Ù¹Ù…Ø² Ø§ÙˆØ± Ø­ÙØ§Ø¸Øª Ú©Û’ Ù„ÛŒÛ’ ÙØ±ÛŒÙ… ÙˆØ±Ú©
- **IEEE Standards for AI**: https://standards.ieee.org/industry-connections/ai/

SLM Ø³Û’ Ú†Ù„Ù†Û’ ÙˆØ§Ù„Û’ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©ÛŒ Ø·Ø±Ù Ù…Ù†ØªÙ‚Ù„ÛŒ AI ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ú©Û’ Ø·Ø±ÛŒÙ‚Û’ Ù…ÛŒÚº Ø§ÛŒÚ© Ø¨Ù†ÛŒØ§Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ÛŒ Ú©ÛŒ Ù†Ù…Ø§Ø¦Ù†Ø¯Ú¯ÛŒ Ú©Ø±ØªÛŒ ÛÛ’Û” Microsoft Agent FrameworkØŒ Ù…Ù‚Ø§Ù…ÛŒ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù…Ø² Ø§ÙˆØ± Ù…Ø¤Ø«Ø± Small Language Models Ú©Û’ Ø³Ø§ØªÚ¾ Ù…Ù„ Ú©Ø±ØŒ Ø§ÛŒØ¬ Ù…Ø§Ø­ÙˆÙ„ Ù…ÛŒÚº Ù…Ø¤Ø«Ø± Ø·Ø±ÛŒÙ‚Û’ Ø³Û’ Ú©Ø§Ù… Ú©Ø±Ù†Û’ ÙˆØ§Ù„Û’ Ù¾Ø±ÙˆÚˆÚ©Ø´Ù† Ø±ÛŒÚˆÛŒ Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©ÛŒ ØªØ¹Ù…ÛŒØ± Ú©Û’ Ù„ÛŒÛ’ Ø§ÛŒÚ© Ù…Ú©Ù…Ù„ Ø­Ù„ ÙØ±Ø§ÛÙ… Ú©Ø±ØªØ§ ÛÛ’Û” Ú©Ø§Ø±Ú©Ø±Ø¯Ú¯ÛŒØŒ ØªØ®ØµØµØŒ Ø§ÙˆØ± Ø¹Ù…Ù„ÛŒ Ø§ÙØ§Ø¯ÛŒØª Ù¾Ø± ØªÙˆØ¬Û Ù…Ø±Ú©ÙˆØ² Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ØŒ ÛŒÛ Ù¹ÛŒÚ©Ù†Ø§Ù„ÙˆØ¬ÛŒ Ø§Ø³Ù¹ÛŒÚ© AI Ø§ÛŒØ¬Ù†Ù¹Ø³ Ú©Ùˆ ÛØ± ØµÙ†Ø¹Øª Ø§ÙˆØ± Ø§ÛŒØ¬ Ú©Ù…Ù¾ÛŒÙˆÙ¹Ù†Ú¯ Ù…Ø§Ø­ÙˆÙ„ Ù…ÛŒÚº Ø­Ù‚ÛŒÙ‚ÛŒ Ø¯Ù†ÛŒØ§ Ú©ÛŒ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù†Ø² Ú©Û’ Ù„ÛŒÛ’ Ø²ÛŒØ§Ø¯Û Ù‚Ø§Ø¨Ù„ Ø±Ø³Ø§Ø¦ÛŒØŒ Ø³Ø³ØªÛŒØŒ Ø§ÙˆØ± Ù…Ø¤Ø«Ø± Ø¨Ù†Ø§ØªØ§ ÛÛ’Û”

2025 ØªÚ©ØŒ Ú†Ú¾ÙˆÙ¹Û’ Ù…Ø§ÚˆÙ„Ø²ØŒ Microsoft Agent Framework Ø¬ÛŒØ³Û’ Ù¾ÛŒÚ†ÛŒØ¯Û Ø§ÛŒØ¬Ù†Ù¹ ÙØ±ÛŒÙ… ÙˆØ±Ú©ØŒ Ø§ÙˆØ± Ù…Ø¶Ø¨ÙˆØ· Ø§ÛŒØ¬ ØªØ¹ÛŒÙ†Ø§ØªÛŒ Ù¾Ù„ÛŒÙ¹ ÙØ§Ø±Ù…Ø² Ú©Û’ Ø§Ù…ØªØ²Ø§Ø¬ Ú©Û’ Ø³Ø§ØªÚ¾ØŒ Ø®ÙˆØ¯ Ù…Ø®ØªØ§Ø± Ø³Ø³Ù¹Ù…Ø² Ú©Û’ Ù„ÛŒÛ’ Ù†Ø¦Û’ Ø§Ù…Ú©Ø§Ù†Ø§Øª Ú©Ú¾

---

**ÚˆØ³Ú©Ù„ÛŒÙ…Ø±**:  
ÛŒÛ Ø¯Ø³ØªØ§ÙˆÛŒØ² AI ØªØ±Ø¬Ù…Û Ø³Ø±ÙˆØ³ [Co-op Translator](https://github.com/Azure/co-op-translator) Ú©Ø§ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ú©Ø±ØªÛ’ ÛÙˆØ¦Û’ ØªØ±Ø¬Ù…Û Ú©ÛŒ Ú¯Ø¦ÛŒ ÛÛ’Û” ÛÙ… Ø¯Ø±Ø³ØªÚ¯ÛŒ Ú©Û’ Ù„ÛŒÛ’ Ú©ÙˆØ´Ø´ Ú©Ø±ØªÛ’ ÛÛŒÚºØŒ Ù„ÛŒÚ©Ù† Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø¢Ú¯Ø§Û Ø±ÛÛŒÚº Ú©Û Ø®ÙˆØ¯Ú©Ø§Ø± ØªØ±Ø¬Ù…Û’ Ù…ÛŒÚº ØºÙ„Ø·ÛŒØ§Úº ÛŒØ§ ØºÛŒØ± Ø¯Ø±Ø³ØªÛŒØ§Úº ÛÙˆ Ø³Ú©ØªÛŒ ÛÛŒÚºÛ” Ø§ØµÙ„ Ø¯Ø³ØªØ§ÙˆÛŒØ² Ú©Ùˆ Ø§Ø³ Ú©ÛŒ Ø§ØµÙ„ Ø²Ø¨Ø§Ù† Ù…ÛŒÚº Ù…Ø³ØªÙ†Ø¯ Ø°Ø±ÛŒØ¹Û Ø³Ù…Ø¬Ú¾Ø§ Ø¬Ø§Ù†Ø§ Ú†Ø§ÛÛŒÛ’Û” Ø§ÛÙ… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ú©Û’ Ù„ÛŒÛ’ØŒ Ù¾ÛŒØ´Û ÙˆØ± Ø§Ù†Ø³Ø§Ù†ÛŒ ØªØ±Ø¬Ù…Û Ú©ÛŒ Ø³ÙØ§Ø±Ø´ Ú©ÛŒ Ø¬Ø§ØªÛŒ ÛÛ’Û” ÛÙ… Ø§Ø³ ØªØ±Ø¬Ù…Û’ Ú©Û’ Ø§Ø³ØªØ¹Ù…Ø§Ù„ Ø³Û’ Ù¾ÛŒØ¯Ø§ ÛÙˆÙ†Û’ ÙˆØ§Ù„ÛŒ Ú©Ø³ÛŒ Ø¨Ú¾ÛŒ ØºÙ„Ø· ÙÛÙ…ÛŒ ÛŒØ§ ØºÙ„Ø· ØªØ´Ø±ÛŒØ­ Ú©Û’ Ø°Ù…Û Ø¯Ø§Ø± Ù†ÛÛŒÚº ÛÛŒÚºÛ”