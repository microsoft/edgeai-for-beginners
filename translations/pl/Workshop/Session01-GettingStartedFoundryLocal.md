<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8c30436578b1bd604c48233ecdd39701",
  "translation_date": "2025-11-11T22:54:18+00:00",
  "source_file": "Workshop/Session01-GettingStartedFoundryLocal.md",
  "language_code": "pl"
}
-->
# Sesja 1: RozpoczÄ™cie pracy z Foundry Local

## Streszczenie

Naucz siÄ™ instalowaÄ‡, konfigurowaÄ‡ i uruchamiaÄ‡ swoje pierwsze modele AI za pomocÄ… Microsoft Foundry Local. Ta praktyczna sesja oferuje krok po kroku wprowadzenie do lokalnego wnioskowania, od instalacji po stworzenie pierwszej aplikacji czatu z wykorzystaniem modeli takich jak Phi-4, Qwen i DeepSeek.

## Cele nauki

Po zakoÅ„czeniu tej sesji bÄ™dziesz w stanie:

- **ZainstalowaÄ‡ i skonfigurowaÄ‡**: SkonfigurowaÄ‡ Foundry Local z odpowiedniÄ… weryfikacjÄ… instalacji
- **OpanowaÄ‡ operacje CLI**: KorzystaÄ‡ z Foundry Local CLI do zarzÄ…dzania modelami i ich wdraÅ¼ania
- **UruchomiÄ‡ swÃ³j pierwszy model**: PomyÅ›lnie wdroÅ¼yÄ‡ i interaktywnie korzystaÄ‡ z lokalnego modelu AI
- **ZbudowaÄ‡ aplikacjÄ™ czatu**: StworzyÄ‡ podstawowÄ… aplikacjÄ™ czatu za pomocÄ… Foundry Local Python SDK
- **ZrozumieÄ‡ lokalne AI**: PoznaÄ‡ podstawy lokalnego wnioskowania i zarzÄ…dzania modelami

## Wymagania wstÄ™pne

### Wymagania systemowe

- **Windows**: Windows 11 (22H2 lub nowszy) LUB **macOS**: macOS 11+ (ograniczone wsparcie)
- **RAM**: Minimum 8GB, zalecane 16GB+
- **PamiÄ™Ä‡**: Minimum 10GB wolnego miejsca na modele
- **Python**: Zainstalowany Python 3.10 lub nowszy
- **DostÄ™p administracyjny**: Uprawnienia administratora do instalacji

### Åšrodowisko programistyczne

- Visual Studio Code z rozszerzeniem Python (zalecane)
- DostÄ™p do wiersza poleceÅ„ (PowerShell na Windows, Terminal na macOS)
- Git do klonowania repozytoriÃ³w (opcjonalnie)

## Przebieg warsztatu (30 minut)

### Krok 1: Instalacja Foundry Local (5 minut)

#### Instalacja na Windows

Zainstaluj Foundry Local za pomocÄ… menedÅ¼era pakietÃ³w Windows:

```powershell
# Install via winget (recommended)
winget install Microsoft.FoundryLocal
```

Alternatywnie: Pobierz bezpoÅ›rednio z [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)

#### Instalacja na macOS (ograniczone wsparcie)

> [!NOTE]  
> Wsparcie dla macOS jest obecnie w wersji preview. SprawdÅº oficjalnÄ… dokumentacjÄ™, aby uzyskaÄ‡ najnowsze informacje.

JeÅ›li dostÄ™pne, zainstaluj za pomocÄ… Homebrew:

```bash
# If Homebrew formula is available
brew update
brew install foundry-local

# Or manual download (check official docs for latest)
curl -L -o foundry-local.tar.gz "https://download.microsoft.com/foundry-local/latest/macos/foundry-local.tar.gz"
tar -xzf foundry-local.tar.gz
sudo ./install.sh
```

**Alternatywa dla uÅ¼ytkownikÃ³w macOS:**
- UÅ¼yj maszyny wirtualnej Windows 11 (Parallels/UTM) i postÄ™puj zgodnie z krokami dla Windows
- Uruchom za pomocÄ… kontenera, jeÅ›li dostÄ™pne, i skonfiguruj `FOUNDRY_LOCAL_ENDPOINT`

### Krok 2: Weryfikacja instalacji (3 minuty)

Po instalacji, uruchom ponownie terminal i sprawdÅº, czy Foundry Local dziaÅ‚a:

```powershell
# Check if Foundry Local is installed correctly
foundry --version

# View available commands
foundry --help
```

Oczekiwany wynik powinien pokazaÄ‡ informacje o wersji i dostÄ™pne polecenia.

### Krok 3: Konfiguracja Å›rodowiska Python (5 minut)

UtwÃ³rz dedykowane Å›rodowisko Python dla tego warsztatu:

**Windows:**
```powershell
# Create virtual environment
py -m venv .venv

# Activate environment
.\.venv\Scripts\Activate.ps1

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

**macOS/Linux:**
```bash
# Create virtual environment
python3 -m venv .venv

# Activate environment
source .venv/bin/activate

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```


### Krok 4: Uruchom swÃ³j pierwszy model (7 minut)

Teraz uruchomimy nasz pierwszy model AI lokalnie!

#### Rozpocznij od Phi-4 Mini (zalecany pierwszy model)

```powershell
# Download and start phi-4-mini (lightweight, fast)
foundry model run phi-4-mini

# Test the model with a simple prompt
foundry model run phi-4-mini --prompt "Hello, introduce yourself in one sentence"
```

> [!TIP]  
> To polecenie pobiera model (pierwszy raz) i automatycznie uruchamia usÅ‚ugÄ™ Foundry Local.

#### SprawdÅº, co dziaÅ‚a

```powershell
# List available models (shows downloaded models)
foundry model list

# Check service status
foundry service status

# See what models are cached locally
foundry cache list
```


#### WyprÃ³buj rÃ³Å¼ne modele

Gdy phi-4-mini dziaÅ‚a, eksperymentuj z innymi modelami:

```powershell
# Larger model with better capabilities
foundry model run gpt-oss-20b --prompt "Explain edge AI in simple terms"

# Fast, efficient model
foundry model run qwen2.5-0.5b --prompt "What are the benefits of local AI inference?"
```


### Krok 5: Zbuduj swojÄ… pierwszÄ… aplikacjÄ™ czatu (10 minut)

Teraz stworzymy aplikacjÄ™ Python, ktÃ³ra korzysta z modeli, ktÃ³re wÅ‚aÅ›nie uruchomiliÅ›my.

#### UtwÃ³rz skrypt czatu

UtwÃ³rz nowy plik o nazwie `my_first_chat.py` (lub uÅ¼yj dostarczonego przykÅ‚adu):

```python
#!/usr/bin/env python3
"""
My First Foundry Local Chat Application
Using FoundryLocalManager for automatic service management
"""

import os
from foundry_local import FoundryLocalManager
from openai import OpenAI

def main():
    # Get model alias from environment or use default
    alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-4-mini")
    
    try:
        # Initialize Foundry Local Manager (auto-starts service, downloads model)
        manager = FoundryLocalManager(alias)
        
        # Create OpenAI client pointing to local endpoint
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-needed"
        )
        
        # Get the actual model ID for this alias
        model_id = manager.get_model_info(alias).id
        
        print("ğŸ¤– Welcome to your first local AI chat!")
        print(f"ï¿½ Using model: {alias} -> {model_id}")
        print(f"ğŸŒ Endpoint: {manager.endpoint}")
        print("ï¿½ğŸ’¡ Type 'quit' to exit\n")
        
    except Exception as e:
        print(f"âŒ Failed to initialize Foundry Local: {e}")
        print("ğŸ’¡ Make sure Foundry Local is installed: foundry --version")
        return
    
    while True:
        # Get user input
        user_message = input("You: ").strip()
        
        if user_message.lower() in ['quit', 'exit', 'bye']:
            print("ğŸ‘‹ Goodbye!")
            break
            
        if not user_message:
            continue
            
        try:
            # Send message to local AI model
            response = client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": "You are a helpful AI assistant running locally."},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=200,
                temperature=0.7
            )
            
            # Display the response
            ai_response = response.choices[0].message.content
            print(f"ğŸ¤– AI: {ai_response}\n")
            
        except Exception as e:
            print(f"âŒ Error: {e}")
            print("ğŸ’¡ Check service status: foundry service status\n")

if __name__ == "__main__":
    main()
```

> [!TIP]  
> **PowiÄ…zane przykÅ‚ady**: Aby uzyskaÄ‡ bardziej zaawansowane zastosowania, zobacz:
>
> - **PrzykÅ‚ad Python**: `Workshop/samples/session01/chat_bootstrap.py` - Zawiera odpowiedzi strumieniowe i obsÅ‚ugÄ™ bÅ‚Ä™dÃ³w
> - **Notebook Jupyter**: `Workshop/notebooks/session01_chat_bootstrap.ipynb` - Interaktywna wersja z szczegÃ³Å‚owymi wyjaÅ›nieniami

#### Przetestuj swojÄ… aplikacjÄ™ czatu

```powershell
# No need to manually start models - FoundryLocalManager handles this!
# Just run your chat application
python my_first_chat.py
```

Alternatywnie: UÅ¼yj bezpoÅ›rednio dostarczonych przykÅ‚adÃ³w

```powershell
# Try the complete sample with streaming support
cd Workshop/samples
python -m session01.chat_bootstrap "Your question here"
```

Lub eksploruj interaktywny notebook  
OtwÃ³rz Workshop/notebooks/session01_chat_bootstrap.ipynb w VS Code

WyprÃ³buj te przykÅ‚adowe rozmowy:

- "Co to jest Microsoft Foundry Local?"
- "WymieÅ„ 3 korzyÅ›ci z uruchamiania modeli AI lokalnie"
- "PomÃ³Å¼ mi zrozumieÄ‡ edge AI"

## Co osiÄ…gnÄ…Å‚eÅ›

Gratulacje! PomyÅ›lnie:

1. âœ… **ZainstalowaÅ‚eÅ› Foundry Local** i zweryfikowaÅ‚eÅ› jego dziaÅ‚anie
2. âœ… **UruchomiÅ‚eÅ› swÃ³j pierwszy model AI** (phi-4-mini) lokalnie
3. âœ… **PrzetestowaÅ‚eÅ› rÃ³Å¼ne modele** za pomocÄ… wiersza poleceÅ„
4. âœ… **ZbudowaÅ‚eÅ› aplikacjÄ™ czatu**, ktÃ³ra Å‚Ä…czy siÄ™ z lokalnym AI
5. âœ… **DoÅ›wiadczyÅ‚eÅ› lokalnego wnioskowania AI** bez zaleÅ¼noÅ›ci od chmury

## Zrozumienie, co siÄ™ wydarzyÅ‚o

### Lokalne wnioskowanie AI

- Twoje modele AI dziaÅ‚ajÄ… caÅ‚kowicie na Twoim komputerze
- Å»adne dane nie sÄ… wysyÅ‚ane do chmury
- Odpowiedzi sÄ… generowane lokalnie za pomocÄ… Twojego CPU/GPU
- PrywatnoÅ›Ä‡ i bezpieczeÅ„stwo sÄ… zachowane

### ZarzÄ…dzanie modelami

- `foundry model run` pobiera i uruchamia modele
- **FoundryLocalManager SDK** automatycznie obsÅ‚uguje uruchamianie usÅ‚ug i Å‚adowanie modeli
- Modele sÄ… przechowywane lokalnie do przyszÅ‚ego uÅ¼ytku
- MoÅ¼na pobraÄ‡ wiele modeli, ale zazwyczaj dziaÅ‚a jeden na raz
- UsÅ‚uga automatycznie zarzÄ…dza cyklem Å¼ycia modelu

### PodejÅ›cie SDK vs CLI

- **PodejÅ›cie CLI**: RÄ™czne zarzÄ…dzanie modelami za pomocÄ… `foundry model run <model>`
- **PodejÅ›cie SDK**: Automatyczne zarzÄ…dzanie usÅ‚ugami i modelami za pomocÄ… `FoundryLocalManager(alias)`
- **Rekomendacja**: UÅ¼ywaj SDK do aplikacji, CLI do testowania i eksploracji

## Referencje do poleceÅ„

### Podstawowe polecenia CLI

```powershell
# Installation & Setup
foundry --version              # Check installation
foundry --help                 # View all commands

# Model Management
foundry model list             # List available models
foundry model run <model>      # Download and start a model
foundry model run <model> --prompt "text"  # One-shot prompt
foundry cache list             # Show downloaded models

# Service Management
foundry service status         # Check if service is running
foundry service start          # Start the service manually
foundry service stop           # Stop the service
```


### Rekomendacje modeli

- **phi-4-mini**: Najlepszy model startowy - szybki, lekki, dobra jakoÅ›Ä‡
- **qwen2.5-0.5b**: Najszybsze wnioskowanie, minimalne zuÅ¼ycie pamiÄ™ci
- **gpt-oss-20b**: WyÅ¼sza jakoÅ›Ä‡ odpowiedzi, wymaga wiÄ™cej zasobÃ³w
- **deepseek-coder-1.3b**: Optymalizowany do programowania i zadaÅ„ zwiÄ…zanych z kodem

## RozwiÄ…zywanie problemÃ³w

### "Polecenie Foundry nie zostaÅ‚o znalezione"

**RozwiÄ…zanie:**

```powershell
# Restart your terminal after installation
# Or manually add to PATH (Windows)
$env:PATH += ";C:\Program Files\Microsoft\FoundryLocal"
```


### "Model nie zaÅ‚adowaÅ‚ siÄ™"

**RozwiÄ…zanie:**

```powershell
# Check available system memory
foundry service status

# Try a smaller model first
foundry model run phi-4-mini

# Check disk space for model downloads
# Models are stored in: %USERPROFILE%\.foundry\models (Windows)
```


### "PoÅ‚Ä…czenie odrzucone na localhost"

**RozwiÄ…zanie:**

```powershell
# Check if service is running
foundry service status

# Start service if needed
foundry service start

# Verify the port (default is 5273)
# Check for port conflicts with: netstat -an | findstr 5273
```


## Kolejne kroki

### Natychmiastowe dziaÅ‚ania

1. **Eksperymentuj** z rÃ³Å¼nymi modelami i zapytaniami
2. **Modyfikuj** swojÄ… aplikacjÄ™ czatu, aby wyprÃ³bowaÄ‡ rÃ³Å¼ne modele
3. **TwÃ³rz** wÅ‚asne zapytania i testuj odpowiedzi
4. **Eksploruj** SesjÄ™ 2: Budowanie aplikacji RAG

### Zaawansowana Å›cieÅ¼ka nauki

1. **Sesja 2**: Budowanie rozwiÄ…zaÅ„ AI z RAG (Retrieval-Augmented Generation)
2. **Sesja 3**: PorÃ³wnanie rÃ³Å¼nych modeli open-source
3. **Sesja 4**: Praca z najnowoczeÅ›niejszymi modelami
4. **Sesja 5**: Budowanie systemÃ³w AI z wieloma agentami

## Zmienne Å›rodowiskowe (opcjonalne)

Dla bardziej zaawansowanego uÅ¼ycia moÅ¼esz ustawiÄ‡ te zmienne Å›rodowiskowe:

| Zmienna | Cel | PrzykÅ‚ad |
|---------|-----|---------|
| `FOUNDRY_LOCAL_ALIAS` | DomyÅ›lny model do uÅ¼ycia | `phi-4-mini` |
| `FOUNDRY_LOCAL_ENDPOINT` | Nadpisanie URL punktu koÅ„cowego | `http://localhost:5273/v1` |

UtwÃ³rz plik `.env` w katalogu projektu:
```
FOUNDRY_LOCAL_ALIAS=phi-4-mini
FOUNDRY_LOCAL_ENDPOINT=auto
```


## Dodatkowe zasoby

### Dokumentacja

- [Foundry Local Python SDK Reference](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk?pivots=programming-language-python)
- [Foundry Local Installation Guide](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)
- [Model Catalog](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/models)

### PrzykÅ‚adowy kod

- **PrzykÅ‚ad Python Sesja01**: `Workshop/samples/session01/chat_bootstrap.py` - Kompletny czat z odpowiedziami strumieniowymi
- **Notebook Sesja01**: `Workshop/notebooks/session01_chat_bootstrap.ipynb` - Interaktywny tutorial  
- [PrzykÅ‚ad ModuÅ‚08 01](../Module08/samples/01/README.md) - Szybki start REST Chat
- [PrzykÅ‚ad ModuÅ‚08 02](../Module08/samples/02/README.md) - Integracja OpenAI SDK
- [PrzykÅ‚ad ModuÅ‚08 03](../Module08/samples/03/README.md) - Odkrywanie modeli i benchmarking

### SpoÅ‚ecznoÅ›Ä‡

- [Foundry Local GitHub Discussions](https://github.com/microsoft/Foundry-Local/discussions)
- [Azure AI Community](https://techcommunity.microsoft.com/category/artificialintelligence)

---

**Czas trwania sesji**: 30 minut praktyki + 15 minut Q&A  
**Poziom trudnoÅ›ci**: PoczÄ…tkujÄ…cy  
**Wymagania wstÄ™pne**: Windows 11/macOS 11+, Python 3.10+, dostÄ™p administracyjny

## PrzykÅ‚adowy scenariusz warsztatu

### Kontekst rzeczywisty

**Scenariusz**: ZespÃ³Å‚ IT w przedsiÄ™biorstwie musi oceniÄ‡ wnioskowanie AI na urzÄ…dzeniu w celu przetwarzania poufnych opinii pracownikÃ³w bez wysyÅ‚ania danych do zewnÄ™trznych usÅ‚ug.

**TwÃ³j cel**: ZademonstrowaÄ‡, Å¼e lokalne modele AI mogÄ… dostarczaÄ‡ odpowiedzi wysokiej jakoÅ›ci z opÃ³Åºnieniem poniÅ¼ej sekundy, jednoczeÅ›nie zachowujÄ…c peÅ‚nÄ… prywatnoÅ›Ä‡ danych.

### PrzykÅ‚adowe zapytania

UÅ¼yj tych zapytaÅ„, aby zweryfikowaÄ‡ swojÄ… konfiguracjÄ™:

```json
[
    "List two benefits of local inference.",
    "Summarize why keeping data on device improves privacy.",
    "Give one trade-off when choosing a small model over a large model."
]
```


### Kryteria sukcesu

- âœ… Wszystkie zapytania otrzymujÄ… odpowiedzi w czasie poniÅ¼ej 2 sekund
- âœ… Å»adne dane nie opuszczajÄ… Twojego lokalnego komputera
- âœ… Odpowiedzi sÄ… trafne i pomocne
- âœ… Twoja aplikacja czatu dziaÅ‚a pÅ‚ynnie

Ta weryfikacja zapewnia, Å¼e Twoja konfiguracja Foundry Local jest gotowa na zaawansowane warsztaty w Sesjach 2-6.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ZastrzeÅ¼enie**:  
Ten dokument zostaÅ‚ przetÅ‚umaczony za pomocÄ… usÅ‚ugi tÅ‚umaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). ChociaÅ¼ staramy siÄ™ zapewniÄ‡ dokÅ‚adnoÅ›Ä‡, prosimy pamiÄ™taÄ‡, Å¼e automatyczne tÅ‚umaczenia mogÄ… zawieraÄ‡ bÅ‚Ä™dy lub nieÅ›cisÅ‚oÅ›ci. Oryginalny dokument w jego rodzimym jÄ™zyku powinien byÄ‡ uznawany za wiarygodne ÅºrÃ³dÅ‚o. W przypadku informacji krytycznych zaleca siÄ™ skorzystanie z profesjonalnego tÅ‚umaczenia przez czÅ‚owieka. Nie ponosimy odpowiedzialnoÅ›ci za jakiekolwiek nieporozumienia lub bÅ‚Ä™dne interpretacje wynikajÄ…ce z uÅ¼ycia tego tÅ‚umaczenia.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->