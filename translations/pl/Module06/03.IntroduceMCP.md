# Sekcja 03 - Integracja z protokołem Model Context Protocol (MCP)

## Wprowadzenie do MCP (Model Context Protocol)

Model Context Protocol (MCP) to otwarty standard umożliwiający połączenie aplikacji AI z systemami zewnętrznymi. Dzięki MCP aplikacje AI, takie jak Claude czy ChatGPT, mogą łączyć się z źródłami danych (np. lokalnymi plikami, bazami danych), narzędziami (np. wyszukiwarkami, kalkulatorami) oraz przepływami pracy (np. specjalistycznymi podpowiedziami), co pozwala im na dostęp do kluczowych informacji i wykonywanie zadań.

MCP można porównać do **portu USB-C dla aplikacji AI**. Tak jak USB-C zapewnia standardowy sposób łączenia urządzeń elektronicznych, MCP oferuje standardowy sposób łączenia aplikacji AI z systemami zewnętrznymi.

### Co umożliwia MCP?

MCP odblokowuje potężne możliwości dla aplikacji AI:

- **Spersonalizowani asystenci AI**: Agenci mogą uzyskać dostęp do Twojego Google Kalendarza i Notion, działając jako bardziej spersonalizowany asystent AI
- **Zaawansowane generowanie kodu**: Claude Code może stworzyć całą aplikację webową na podstawie projektu z Figma
- **Integracja danych w przedsiębiorstwie**: Chatboty dla firm mogą łączyć się z wieloma bazami danych w organizacji, umożliwiając użytkownikom analizę danych za pomocą rozmowy
- **Kreatywne przepływy pracy**: Modele AI mogą tworzyć projekty 3D w Blenderze i drukować je na drukarce 3D
- **Dostęp do informacji w czasie rzeczywistym**: Połączenie z zewnętrznymi źródłami danych w celu uzyskania aktualnych informacji
- **Złożone operacje wieloetapowe**: Wykonywanie zaawansowanych przepływów pracy łączących wiele narzędzi i systemów

### Dlaczego MCP jest ważny?

MCP przynosi korzyści w całym ekosystemie:

**Dla deweloperów**: MCP skraca czas i zmniejsza złożoność podczas tworzenia lub integracji aplikacji AI lub agenta.

**Dla aplikacji AI**: MCP zapewnia dostęp do ekosystemu źródeł danych, narzędzi i aplikacji, które zwiększają możliwości i poprawiają doświadczenie użytkownika końcowego.

**Dla użytkowników końcowych**: MCP prowadzi do bardziej zaawansowanych aplikacji AI lub agentów, które mogą uzyskać dostęp do Twoich danych i podejmować działania w Twoim imieniu, gdy jest to konieczne.

## Małe modele językowe (SLM) w MCP

Małe modele językowe (SLM) reprezentują efektywne podejście do wdrażania AI, oferując kilka korzyści:

### Korzyści z SLM
- **Efektywność zasobów**: Mniejsze wymagania obliczeniowe
- **Szybsze czasy odpowiedzi**: Zredukowana latencja dla aplikacji w czasie rzeczywistym  
- **Kosztowność**: Minimalne potrzeby infrastrukturalne
- **Prywatność**: Możliwość działania lokalnie bez przesyłania danych
- **Dostosowanie**: Łatwiejsze dostosowanie do konkretnych dziedzin

### Dlaczego SLM dobrze współpracują z MCP

Połączenie SLM z MCP tworzy potężną kombinację, w której zdolności rozumowania modelu są wspomagane przez narzędzia zewnętrzne, kompensując mniejszą liczbę parametrów modelu dzięki rozszerzonej funkcjonalności.

## Przegląd Python MCP SDK

Python MCP SDK stanowi podstawę do budowy aplikacji obsługujących MCP. SDK zawiera:

- **Biblioteki klienckie**: Do łączenia się z serwerami MCP
- **Framework serwera**: Do tworzenia niestandardowych serwerów MCP
- **Obsługę protokołu**: Do zarządzania komunikacją
- **Integrację narzędzi**: Do wykonywania funkcji zewnętrznych

## Praktyczna implementacja: Klient Phi-4 MCP

Przyjrzyjmy się rzeczywistej implementacji z wykorzystaniem mini modelu Phi-4 firmy Microsoft z funkcjami MCP.

### Przegląd architektury MCP

MCP opiera się na **architekturze klient-serwer**, gdzie host MCP (aplikacja AI, taka jak Claude Code lub Claude Desktop) nawiązuje połączenia z jednym lub kilkoma serwerami MCP. Host MCP realizuje to, tworząc jednego klienta MCP dla każdego serwera MCP.

#### Kluczowi uczestnicy

- **Host MCP**: Aplikacja AI, która koordynuje i zarządza jednym lub wieloma klientami MCP
- **Klient MCP**: Komponent, który utrzymuje połączenie z serwerem MCP i uzyskuje kontekst od serwera MCP do wykorzystania przez hosta MCP
- **Serwer MCP**: Program, który dostarcza kontekst klientom MCP

#### Dwuwarstwowa architektura

MCP składa się z dwóch odrębnych warstw:

**Warstwa danych**: Definiuje protokół oparty na JSON-RPC dla komunikacji klient-serwer, w tym:
- Zarządzanie cyklem życia (inicjalizacja połączenia, negocjacja możliwości)
- Podstawowe prymitywy (narzędzia, zasoby, podpowiedzi)
- Funkcje klienta (próbkowanie, pozyskiwanie, logowanie)
- Funkcje użytkowe (powiadomienia, śledzenie postępów)

**Warstwa transportowa**: Definiuje mechanizmy i kanały komunikacji:
- **Transport STDIO**: Wykorzystuje standardowe strumienie wejścia/wyjścia dla procesów lokalnych (optymalna wydajność, brak obciążenia sieciowego)
- **Transport HTTP Streamable**: Wykorzystuje HTTP POST z opcjonalnymi zdarzeniami Server-Sent Events dla serwerów zdalnych (obsługuje standardowe uwierzytelnianie HTTP)

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### Podstawowe prymitywy MCP

MCP definiuje prymitywy określające rodzaje informacji kontekstowych, które mogą być udostępniane aplikacjom AI, oraz zakres działań, które mogą być wykonywane.

#### Prymitywy serwera

MCP definiuje trzy podstawowe prymitywy, które serwery mogą udostępniać:

**Narzędzia**: Funkcje wykonywalne, które aplikacje AI mogą wywoływać w celu wykonania działań
- Przykłady: operacje na plikach, wywołania API, zapytania do bazy danych
- Metody: `tools/list`, `tools/call`
- Obsługa dynamicznego odkrywania i wykonywania

**Zasoby**: Źródła danych dostarczające informacji kontekstowych aplikacjom AI
- Przykłady: zawartość plików, rekordy bazy danych, odpowiedzi API
- Metody: `resources/list`, `resources/read`
- Umożliwiają dostęp do danych strukturalnych

**Podpowiedzi**: Wielokrotnego użytku szablony pomagające w strukturze interakcji z modelami językowymi
- Przykłady: podpowiedzi systemowe, przykłady few-shot
- Metody: `prompts/list`, `prompts/get`
- Standaryzują wzorce interakcji AI

#### Prymitywy klienta

MCP definiuje również prymitywy, które klienci mogą udostępniać, aby umożliwić bogatsze interakcje:

**Próbkowanie**: Pozwala serwerom na żądanie uzupełnień modelu językowego od aplikacji AI klienta
- Metoda: `sampling/complete`
- Umożliwia niezależne od modelu rozwijanie serwera
- Zapewnia dostęp do modelu językowego hosta

**Pozyskiwanie**: Pozwala serwerom na żądanie dodatkowych informacji od użytkowników
- Metoda: `elicitation/request`
- Umożliwia interakcję z użytkownikiem i potwierdzenie
- Obsługuje dynamiczne zbieranie informacji

**Logowanie**: Umożliwia serwerom wysyłanie komunikatów logowania do klientów
- Wykorzystywane do debugowania i monitorowania
- Zapewnia wgląd w operacje serwera

### Cykl życia protokołu MCP

#### Inicjalizacja i negocjacja możliwości

MCP to protokół stanowy, który wymaga zarządzania cyklem życia. Proces inicjalizacji pełni kilka kluczowych funkcji:

1. **Negocjacja wersji protokołu**: Zapewnia, że zarówno klient, jak i serwer używają kompatybilnych wersji protokołu (np. "2025-06-18")
2. **Odkrywanie możliwości**: Każda strona deklaruje obsługiwane funkcje i prymitywy
3. **Wymiana tożsamości**: Dostarcza informacje identyfikacyjne i wersjonowanie

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### Odkrywanie i wykonywanie narzędzi

Po inicjalizacji klienci mogą odkrywać i wykonywać narzędzia:

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### Powiadomienia w czasie rzeczywistym

MCP obsługuje powiadomienia w czasie rzeczywistym dla dynamicznych aktualizacji:

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## Pierwsze kroki: Przewodnik krok po kroku

### Krok 1: Konfiguracja środowiska

Zainstaluj wymagane zależności:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Krok 2: Podstawowa konfiguracja

Skonfiguruj zmienne środowiskowe:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Krok 3: Uruchomienie pierwszego klienta MCP

**Podstawowa konfiguracja Ollama:**
```bash
python ghmodel_mcp_demo.py
```

**Użycie backendu vLLM:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Połączenie Server-Sent Events:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Niestandardowy serwer MCP:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Krok 4: Użycie programowe

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Zaawansowane funkcje

### Obsługa wielu backendów

Implementacja obsługuje zarówno backendy Ollama, jak i vLLM, pozwalając na wybór w zależności od potrzeb:

- **Ollama**: Lepszy do lokalnego rozwoju i testowania
- **vLLM**: Optymalizowany pod kątem produkcji i scenariuszy wymagających dużej przepustowości

### Elastyczne protokoły połączeń

Obsługiwane są dwa tryby połączeń:

**Tryb STDIO**: Bezpośrednia komunikacja procesów
- Niższa latencja
- Odpowiedni dla lokalnych narzędzi
- Prosta konfiguracja

**Tryb SSE**: Streaming oparty na HTTP
- Możliwość pracy w sieci
- Lepszy dla systemów rozproszonych
- Aktualizacje w czasie rzeczywistym

### Możliwości integracji narzędzi

System może integrować się z różnymi narzędziami:
- Automatyzacja webowa (Playwright)
- Operacje na plikach
- Interakcje z API
- Polecenia systemowe
- Funkcje niestandardowe

## Obsługa błędów i najlepsze praktyki

### Kompleksowe zarządzanie błędami

Implementacja obejmuje solidne zarządzanie błędami dla:

**Błędy połączenia:**
- Awaria serwera MCP
- Przekroczenie czasu oczekiwania sieci
- Problemy z łącznością

**Błędy wykonywania narzędzi:**
- Brak narzędzi
- Walidacja parametrów
- Awaria wykonania

**Błędy przetwarzania odpowiedzi:**
- Problemy z parsowaniem JSON
- Niespójności formatów
- Anomalie odpowiedzi LLM

### Najlepsze praktyki

1. **Zarządzanie zasobami**: Używaj asynchronicznych menedżerów kontekstu
2. **Obsługa błędów**: Implementuj kompleksowe bloki try-catch
3. **Logowanie**: Włącz odpowiednie poziomy logowania
4. **Bezpieczeństwo**: Waliduj dane wejściowe i oczyszczaj dane wyjściowe
5. **Wydajność**: Używaj puli połączeń i pamięci podręcznej

## Zastosowania w rzeczywistości

### Automatyzacja webowa
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Przetwarzanie danych
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### Integracja API
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Optymalizacja wydajności

### Zarządzanie pamięcią
- Efektywne zarządzanie historią wiadomości
- Odpowiednie czyszczenie zasobów
- Pula połączeń

### Optymalizacja sieci
- Asynchroniczne operacje HTTP
- Konfigurowalne limity czasu
- Łagodne odzyskiwanie po błędach

### Przetwarzanie równoległe
- Nieblokujące operacje I/O
- Równoległe wykonywanie narzędzi
- Efektywne wzorce asynchroniczne

## Rozważania dotyczące bezpieczeństwa

### Ochrona danych
- Bezpieczne zarządzanie kluczami API
- Walidacja danych wejściowych
- Oczyszczanie danych wyjściowych

### Bezpieczeństwo sieci
- Obsługa HTTPS
- Domyślne lokalne punkty końcowe
- Bezpieczne zarządzanie tokenami

### Bezpieczeństwo wykonania
- Filtrowanie narzędzi
- Środowiska sandbox
- Logowanie audytowe

## Ekosystem MCP i rozwój

### Zakres projektu MCP

Ekosystem Model Context Protocol obejmuje kilka kluczowych komponentów:

- **[Specyfikacja MCP](https://modelcontextprotocol.io/specification/latest)**: Oficjalna specyfikacja określająca wymagania implementacyjne dla klientów i serwerów
- **[SDK MCP](https://modelcontextprotocol.io/docs/sdk)**: SDK dla różnych języków programowania implementujących MCP
- **Narzędzia rozwojowe MCP**: Narzędzia do tworzenia serwerów i klientów MCP, w tym [MCP Inspector](https://github.com/modelcontextprotocol/inspector)
- **[Referencyjne implementacje serwerów MCP](https://github.com/modelcontextprotocol/servers)**: Referencyjne implementacje serwerów MCP

### Rozpoczęcie pracy z rozwojem MCP

Aby rozpocząć budowanie z MCP:

**Tworzenie serwerów**: [Twórz serwery MCP](https://modelcontextprotocol.io/docs/develop/build-server), aby udostępniać swoje dane i narzędzia

**Tworzenie klientów**: [Rozwijaj aplikacje](https://modelcontextprotocol.io/docs/develop/build-client), które łączą się z serwerami MCP

**Poznawanie koncepcji**: [Zrozum podstawowe koncepcje](https://modelcontextprotocol.io/docs/learn/architecture) i architekturę MCP

## Podsumowanie

SLM zintegrowane z MCP reprezentują zmianę paradygmatu w rozwoju aplikacji AI. Łącząc efektywność małych modeli z mocą narzędzi zewnętrznych, deweloperzy mogą tworzyć inteligentne systemy, które są zarówno zasobooszczędne, jak i bardzo funkcjonalne.

Model Context Protocol zapewnia standardowy sposób łączenia aplikacji AI z systemami zewnętrznymi, podobnie jak USB-C zapewnia uniwersalny standard połączeń dla urządzeń elektronicznych. Ta standaryzacja umożliwia:

- **Bezproblemową integrację**: Łączenie modeli AI z różnorodnymi źródłami danych i narzędziami
- **Rozwój ekosystemu**: Twórz raz, używaj w wielu aplikacjach AI
- **Zwiększone możliwości**: Rozszerz funkcjonalność SLM dzięki narzędziom zewnętrznym
- **Aktualizacje w czasie rzeczywistym**: Obsługa dynamicznych, responsywnych aplikacji AI

Kluczowe wnioski:
- MCP to otwarty standard łączący aplikacje AI z systemami zewnętrznymi
- Protokół obsługuje narzędzia, zasoby i podpowiedzi jako podstawowe prymitywy
- Powiadomienia w czasie rzeczywistym umożliwiają dynamiczne, responsywne aplikacje
- Odpowiednie zarządzanie cyklem życia i obsługa błędów są kluczowe dla użycia produkcyjnego
- Ekosystem oferuje kompleksowe SDK i narzędzia rozwojowe

## Źródła i dalsza lektura

### Oficjalna dokumentacja MCP

- **[Oficjalna strona Model Context Protocol](https://modelcontextprotocol.io/)** - Kompleksowa dokumentacja i specyfikacje
- **[Przewodnik wprowadzający do MCP](https://modelcontextprotocol.io/docs/getting-started/intro)** - Wprowadzenie i podstawowe koncepcje
- **[Przegląd architektury MCP](https://modelcontextprotocol.io/docs/learn/architecture)** - Szczegółowa architektura techniczna
- **[Specyfikacja MCP](https://modelcontextprotocol.io/specification/latest)** - Oficjalna specyfikacja protokołu
- **[Dokumentacja Ollama](https://ollama.ai/docs)** - Platforma do lokalnego wdrażania LLM
- **[Dokumentacja vLLM](https://docs.vllm.ai/)** - Wysokowydajne serwowanie LLM

### Standardy techniczne i protokoły

- **[Specyfikacja JSON-RPC 2.0](https://www.jsonrpc.org/)** - Podstawowy protokół RPC używany przez MCP
- **[JSON Schema](https://json-schema.org/)** - Standard definiowania schematów dla narzędzi MCP
- **[Specyfikacja OpenAPI](https://swagger.io/specification/)** - Standard dokumentacji API
- **[Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - Standard webowy dla aktualizacji w czasie rzeczywistym

### Rozwój agentów AI

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - Gotowe do produkcji narzędzie do tworzenia agentów
- **[Dokumentacja LangChain](https://docs.langchain.com/)** - Framework do integracji agentów i narzędzi
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - SDK Microsoftu do orkiestracji AI

### Raporty branżowe i badania

- **[Ogłoszenie protokołu Model Context Protocol od Anthropic](https://www.anthropic.com/news/model-context-protocol)** - Wprowadzenie MCP
- **[Przegląd małych modeli językowych](https://arxiv.org/abs/2410.20011)** - Akademicki przegląd badań nad SLM
- **[Analiza rynku Edge AI](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - Trendy i prognozy branżowe
- **[Najlepsze praktyki w rozwoju agentów AI](https://arxiv.org/abs/2309.02427)** - Badania nad architekturami agentów

Ta sekcja stanowi podstawę do budowania własnych aplikacji MCP opartych na SLM, otwierając możliwości automatyzacji, przetwarzania danych i integracji inteligentnych systemów.

## ➡️ Co dalej

- [Moduł 7. Przykłady Edge AI](../Module07/README.md)

---

**Zastrzeżenie**:  
Ten dokument został przetłumaczony za pomocą usługi tłumaczenia AI [Co-op Translator](https://github.com/Azure/co-op-translator). Chociaż staramy się zapewnić dokładność, należy pamiętać, że automatyczne tłumaczenia mogą zawierać błędy lub nieścisłości. Oryginalny dokument w jego rodzimym języku powinien być uznawany za autorytatywne źródło. W przypadku informacji krytycznych zaleca się skorzystanie z profesjonalnego tłumaczenia przez człowieka. Nie ponosimy odpowiedzialności za jakiekolwiek nieporozumienia lub błędne interpretacje wynikające z użycia tego tłumaczenia.