<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T13:44:54+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "ms"
}
-->
# Ejen AI dan Model Bahasa Kecil: Panduan Komprehensif

## Pengenalan

Dalam tutorial ini, kita akan meneroka Ejen AI dan Model Bahasa Kecil (SLM) serta strategi pelaksanaan maju mereka untuk persekitaran pengkomputeran tepi. Kita akan membincangkan konsep asas AI ejen, teknik pengoptimuman SLM, strategi pelaksanaan praktikal untuk peranti dengan sumber terhad, dan Microsoft Agent Framework untuk membina sistem ejen yang sedia untuk pengeluaran.

Landskap kecerdasan buatan sedang mengalami perubahan paradigma pada tahun 2025. Walaupun tahun 2023 adalah tahun chatbot dan tahun 2024 menyaksikan ledakan dalam pembantu pintar, tahun 2025 adalah milik ejen AI â€” sistem pintar yang berfikir, membuat keputusan, merancang, menggunakan alat, dan melaksanakan tugas dengan input manusia yang minimum, dikuasakan oleh Model Bahasa Kecil yang semakin cekap. Microsoft Agent Framework muncul sebagai penyelesaian utama untuk membina sistem pintar ini dengan keupayaan tepi luar talian.

## Objektif Pembelajaran

Pada akhir tutorial ini, anda akan dapat:

- ğŸ¤– Memahami konsep asas ejen AI dan sistem ejen
- ğŸ”¬ Mengenal pasti kelebihan Model Bahasa Kecil berbanding Model Bahasa Besar dalam aplikasi ejen
- ğŸš€ Mempelajari strategi pelaksanaan SLM maju untuk persekitaran pengkomputeran tepi
- ğŸ“± Melaksanakan ejen berkuasa SLM untuk aplikasi dunia sebenar
- ğŸ—ï¸ Membina ejen sedia pengeluaran menggunakan Microsoft Agent Framework
- ğŸŒ Melaksanakan ejen tepi luar talian dengan integrasi LLM dan SLM tempatan
- ğŸ”§ Mengintegrasikan Microsoft Agent Framework dengan Foundry Local untuk pelaksanaan tepi

## Memahami Ejen AI: Asas dan Pengelasan

### Definisi dan Konsep Asas

Ejen kecerdasan buatan (AI) merujuk kepada sistem atau program yang mampu melaksanakan tugas secara autonomi bagi pihak pengguna atau sistem lain dengan merancang aliran kerjanya dan menggunakan alat yang tersedia. Tidak seperti AI tradisional yang hanya menjawab soalan anda, ejen boleh bertindak secara bebas untuk mencapai matlamat.

### Kerangka Pengelasan Ejen

Memahami sempadan ejen membantu dalam memilih jenis ejen yang sesuai untuk senario pengkomputeran yang berbeza:

- **ğŸ”¬ Ejen Refleks Mudah**: Sistem berasaskan peraturan yang bertindak balas kepada persepsi segera (termostat, automasi asas)
- **ğŸ“± Ejen Berasaskan Model**: Sistem yang mengekalkan keadaan dalaman dan memori (robot vakum, sistem navigasi)
- **âš–ï¸ Ejen Berasaskan Matlamat**: Sistem yang merancang dan melaksanakan urutan untuk mencapai objektif (perancang laluan, penjadual tugas)
- **ğŸ§  Ejen Pembelajaran**: Sistem adaptif yang meningkatkan prestasi dari masa ke masa (sistem cadangan, pembantu peribadi)

### Kelebihan Utama Ejen AI

Ejen AI menawarkan beberapa kelebihan asas yang menjadikannya ideal untuk aplikasi pengkomputeran tepi:

**Autonomi Operasi**: Ejen menyediakan pelaksanaan tugas secara bebas tanpa pengawasan manusia yang berterusan, menjadikannya ideal untuk aplikasi masa nyata. Mereka memerlukan pengawasan minimum sambil mengekalkan tingkah laku adaptif, membolehkan pelaksanaan pada peranti dengan sumber terhad dengan pengurangan beban operasi.

**Fleksibiliti Pelaksanaan**: Sistem ini membolehkan keupayaan AI pada peranti tanpa keperluan sambungan internet, meningkatkan privasi dan keselamatan melalui pemprosesan tempatan, boleh disesuaikan untuk aplikasi khusus domain, dan sesuai untuk pelbagai persekitaran pengkomputeran tepi.

**Keberkesanan Kos**: Sistem ejen menawarkan pelaksanaan yang menjimatkan kos berbanding penyelesaian berasaskan awan, dengan pengurangan kos operasi dan keperluan jalur lebar yang lebih rendah untuk aplikasi tepi.

## Strategi Model Bahasa Kecil Maju

### Asas SLM (Model Bahasa Kecil)

Model Bahasa Kecil (SLM) ialah model bahasa yang boleh dimuatkan pada peranti elektronik pengguna biasa dan melaksanakan inferens dengan latensi yang cukup rendah untuk praktikal apabila memenuhi permintaan ejen pengguna. Dalam istilah praktikal, SLM biasanya model dengan kurang daripada 10 bilion parameter.

**Ciri Penemuan Format**: SLM menawarkan sokongan maju untuk pelbagai tahap kuantisasi, keserasian merentas platform, pengoptimuman prestasi masa nyata, dan keupayaan pelaksanaan tepi. Pengguna boleh mengakses privasi yang dipertingkatkan melalui pemprosesan tempatan dan sokongan WebGPU untuk pelaksanaan berasaskan pelayar.

**Koleksi Tahap Kuantisasi**: Format SLM popular termasuk Q4_K_M untuk pemampatan seimbang dalam aplikasi mudah alih, siri Q5_K_S untuk pelaksanaan tepi yang berfokuskan kualiti, Q8_0 untuk ketepatan hampir asal pada peranti tepi yang berkuasa, dan format eksperimen seperti Q2_K untuk senario sumber ultra rendah.

### GGUF (Format GGML Universal Umum) untuk Pelaksanaan SLM

GGUF berfungsi sebagai format utama untuk melaksanakan SLM yang dikuantisasi pada CPU dan peranti tepi, dioptimumkan khusus untuk aplikasi ejen:

**Ciri Dioptimumkan Ejen**: Format ini menyediakan sumber komprehensif untuk penukaran dan pelaksanaan SLM dengan sokongan yang dipertingkatkan untuk panggilan alat, penjanaan output berstruktur, dan perbualan berbilang giliran. Keserasian merentas platform memastikan tingkah laku ejen yang konsisten di pelbagai peranti tepi.

**Pengoptimuman Prestasi**: GGUF membolehkan penggunaan memori yang cekap untuk aliran kerja ejen, menyokong pemuatan model dinamik untuk sistem berbilang ejen, dan menyediakan inferens yang dioptimumkan untuk interaksi ejen masa nyata.

### Kerangka SLM Dioptimumkan Tepi

#### Pengoptimuman Llama.cpp untuk Ejen

Llama.cpp menyediakan teknik kuantisasi terkini yang dioptimumkan khusus untuk pelaksanaan SLM ejen:

**Kuantisasi Khusus Ejen**: Kerangka ini menyokong Q4_0 (optimum untuk pelaksanaan ejen mudah alih dengan pengurangan saiz 75%), Q5_1 (kualiti-pemampatan seimbang untuk ejen inferens tepi), dan Q8_0 (kualiti hampir asal untuk sistem ejen pengeluaran). Format maju membolehkan ejen ultra mampat untuk senario tepi ekstrem.

**Manfaat Pelaksanaan**: Inferens dioptimumkan CPU dengan pecutan SIMD menyediakan pelaksanaan ejen yang cekap memori. Keserasian merentas platform di seluruh seni bina x86, ARM, dan Apple Silicon membolehkan keupayaan pelaksanaan ejen sejagat.

#### Kerangka Apple MLX untuk Ejen SLM

Apple MLX menyediakan pengoptimuman asli yang direka khusus untuk ejen berkuasa SLM pada peranti Apple Silicon:

**Pengoptimuman Ejen Apple Silicon**: Kerangka ini menggunakan seni bina memori bersatu dengan integrasi Metal Performance Shaders, ketepatan campuran automatik untuk inferens ejen, dan lebar jalur memori yang dioptimumkan untuk sistem berbilang ejen. Ejen SLM menunjukkan prestasi yang luar biasa pada cip siri M.

**Ciri Pembangunan**: Sokongan API Python dan Swift dengan pengoptimuman khusus ejen, pembezaan automatik untuk pembelajaran ejen, dan integrasi lancar dengan alat pembangunan Apple menyediakan persekitaran pembangunan ejen yang komprehensif.

#### ONNX Runtime untuk Ejen SLM Merentas Platform

ONNX Runtime menyediakan enjin inferens sejagat yang membolehkan ejen SLM berjalan secara konsisten di pelbagai platform perkakasan dan sistem operasi:

**Pelaksanaan Sejagat**: ONNX Runtime memastikan tingkah laku ejen SLM yang konsisten di seluruh platform Windows, Linux, macOS, iOS, dan Android. Keserasian merentas platform ini membolehkan pembangun menulis sekali dan melaksanakan di mana-mana, mengurangkan kos pembangunan dan penyelenggaraan untuk aplikasi berbilang platform.

**Pilihan Pecutan Perkakasan**: Kerangka ini menyediakan penyedia pelaksanaan yang dioptimumkan untuk pelbagai konfigurasi perkakasan termasuk CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm), dan pemecut khusus (Intel VPU, Qualcomm NPU). Ejen SLM boleh secara automatik memanfaatkan perkakasan terbaik yang tersedia tanpa perubahan kod.

**Ciri Sedia Pengeluaran**: ONNX Runtime menawarkan ciri-ciri gred perusahaan yang penting untuk pelaksanaan ejen pengeluaran termasuk pengoptimuman graf untuk inferens yang lebih pantas, pengurusan memori untuk persekitaran dengan sumber terhad, dan alat profil komprehensif untuk analisis prestasi. Kerangka ini menyokong kedua-dua API Python dan C++ untuk integrasi yang fleksibel.

## SLM vs LLM dalam Sistem Ejen: Perbandingan Maju

### Kelebihan SLM dalam Aplikasi Ejen

**Kecekapan Operasi**: SLM menyediakan pengurangan kos 10-30Ã— berbanding LLM untuk tugas ejen, membolehkan respons ejen masa nyata pada skala. Mereka menawarkan masa inferens yang lebih pantas kerana kerumitan pengiraan yang dikurangkan, menjadikannya ideal untuk aplikasi ejen interaktif.

**Keupayaan Pelaksanaan Tepi**: SLM membolehkan pelaksanaan ejen pada peranti tanpa kebergantungan internet, privasi yang dipertingkatkan melalui pemprosesan ejen tempatan, dan penyesuaian untuk aplikasi ejen khusus domain yang sesuai untuk pelbagai persekitaran pengkomputeran tepi.

**Pengoptimuman Khusus Ejen**: SLM cemerlang dalam panggilan alat, penjanaan output berstruktur, dan aliran kerja membuat keputusan rutin yang merangkumi 70-80% tugas ejen biasa.

### Bila Menggunakan SLM vs LLM dalam Sistem Ejen

**Sesuai untuk SLM**:
- **Tugas ejen berulang**: Kemasukan data, pengisian borang, panggilan API rutin
- **Integrasi alat**: Pertanyaan pangkalan data, operasi fail, interaksi sistem
- **Aliran kerja berstruktur**: Mengikuti proses ejen yang telah ditetapkan
- **Ejen khusus domain**: Perkhidmatan pelanggan, penjadualan, analisis asas
- **Pemprosesan tempatan**: Operasi ejen yang sensitif terhadap privasi

**Lebih baik untuk LLM**:
- **Penyelesaian masalah kompleks**: Penyelesaian masalah baru, perancangan strategik
- **Perbualan terbuka**: Sembang umum, perbincangan kreatif
- **Tugas pengetahuan luas**: Penyelidikan yang memerlukan pengetahuan umum yang luas
- **Situasi baru**: Mengendalikan senario ejen yang benar-benar baru

### Seni Bina Ejen Hibrid

Pendekatan optimum menggabungkan SLM dan LLM dalam sistem ejen heterogen:

**Orkestrasi Ejen Pintar**:
1. **SLM sebagai utama**: Menangani 70-80% tugas ejen rutin secara tempatan
2. **LLM bila diperlukan**: Mengarahkan pertanyaan kompleks kepada model yang lebih besar berasaskan awan
3. **SLM khusus**: Model kecil yang berbeza untuk domain ejen yang berbeza
4. **Pengoptimuman kos**: Meminimumkan panggilan LLM yang mahal melalui penghalaan pintar

## Strategi Pelaksanaan Ejen SLM Pengeluaran

### Foundry Local: Runtime AI Tepi Gred Perusahaan

Foundry Local (https://github.com/microsoft/foundry-local) berfungsi sebagai penyelesaian utama Microsoft untuk melaksanakan Model Bahasa Kecil dalam persekitaran tepi pengeluaran. Ia menyediakan persekitaran runtime lengkap yang direka khusus untuk ejen berkuasa SLM dengan ciri gred perusahaan dan keupayaan integrasi yang lancar.

**Seni Bina dan Ciri Teras**:
- **API Serasi OpenAI**: Keserasian penuh dengan integrasi SDK OpenAI dan Agent Framework
- **Pengoptimuman Perkakasan Automatik**: Pemilihan pintar varian model berdasarkan perkakasan yang tersedia (CUDA GPU, Qualcomm NPU, CPU)
- **Pengurusan Model**: Muat turun, caching, dan pengurusan kitaran hayat model SLM secara automatik
- **Penemuan Perkhidmatan**: Pengesanan perkhidmatan tanpa konfigurasi untuk kerangka ejen
- **Pengoptimuman Sumber**: Pengurusan memori pintar dan kecekapan kuasa untuk pelaksanaan tepi

#### Pemasangan dan Persediaan

**Pemasangan Merentas Platform**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Permulaan Pantas untuk Pembangunan Ejen**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Integrasi Kerangka Ejen

**Integrasi SDK Foundry Local**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Pemilihan Model Automatik dan Pengoptimuman Perkakasan**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Corak Pelaksanaan Pengeluaran

**Persediaan Pengeluaran Ejen Tunggal**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Orkestrasi Pengeluaran Berbilang Ejen**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### Ciri Perusahaan dan Pemantauan

**Pemantauan Kesihatan dan Kebolehlihatan**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Pengurusan Sumber dan Auto-scaling**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### Konfigurasi dan Pengoptimuman Lanjutan

**Konfigurasi Model Tersuai**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Senarai Semak Pelaksanaan Pengeluaran**:

âœ… **Konfigurasi Perkhidmatan**:
- Konfigurasikan alias model yang sesuai untuk kes penggunaan
- Tetapkan had sumber dan ambang pemantauan
- Aktifkan pemeriksaan kesihatan dan pengumpulan metrik
- Konfigurasikan permulaan semula automatik dan failover

âœ… **Persediaan Keselamatan**:
- Aktifkan akses API tempatan sahaja (tiada pendedahan luaran)
- Konfigurasikan pengurusan kunci API yang sesuai
- Tetapkan log audit untuk interaksi ejen
- Laksanakan had kadar untuk penggunaan pengeluaran

âœ… **Pengoptimuman Prestasi**:
- Uji prestasi model di bawah beban yang dijangkakan
- Konfigurasikan tahap kuantisasi yang sesuai
- Tetapkan strategi caching dan pemanasan model
- Pantau corak penggunaan memori dan CPU

âœ… **Ujian Integrasi**:
- Uji integrasi kerangka ejen
- Sahkan keupayaan operasi luar talian
- Uji senario failover dan pemulihan
- Sahkan aliran kerja ejen hujung ke hujung

### Ollama: Pelaksanaan Ejen SLM yang Dipermudahkan

### Ollama: Pelaksanaan Ejen SLM Berfokuskan Komuniti

Ollama menyediakan pendekatan yang didorong oleh komuniti untuk pelaksanaan ejen SLM dengan penekanan pada kesederhanaan, ekosistem model yang luas, dan aliran kerja mesra pembangun. Walaupun Foundry Local memberi tumpuan kepada ciri gred perusahaan, Ollama cemerlang dalam prototaip pantas, akses model komuniti, dan senario pelaksanaan yang dipermudahkan.

**Seni Bina dan Ciri Teras**:
- **API Serasi OpenAI**: Keserasian penuh REST API untuk integrasi kerangka ejen yang lancar
- **Perpustakaan Model yang Luas**: Akses kepada ratusan model yang disumbangkan oleh komuniti dan rasmi
- **Pengurusan Model Mudah**: Pemasangan dan penukaran model dengan satu arahan
- **Sokongan Merentas Platform**: Sokongan asli di seluruh Windows, macOS, dan Linux
- **Pengoptimuman Sumber**: Kuantisasi automatik dan pengesanan perkakasan

#### Pemasangan dan Persediaan

**Pemasangan Merentas Platform**:
```bash
# Windows
winget install Ollama.Ollama

# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**Permulaan Pantas untuk Pembangunan Ejen**:
```bash
# Start Ollama service
ollama serve

# Pull and run models for agent development
ollama pull phi3.5:3.8b-mini-instruct-q4_K_M    # Microsoft Phi-3.5 Mini
ollama pull qwen2.5:0.5b-instruct-q4_K_M        # Qwen2.5 0.5B
ollama pull llama3.2:1b-instruct-q4_K_M         # Llama 3.2 1B

# Test model availability
ollama list

# Test API endpoint
curl http://localhost:11434/api/generate -d '{
  "model": "phi3.5:3.8b-mini-instruct-q4_K_M",
  "prompt": "Hello, how can I help you today?"
}'
```

#### Integrasi Kerangka Ejen

**Ollama dengan Microsoft Agent Framework**:
```python
from microsoft_agent_framework import Agent, Config
import openai
import requests
import json

class OllamaManager:
    def __init__(self, model_name: str, base_url: str = "http://localhost:11434"):
        self.model_name = model_name
        self.base_url = base_url
        self.api_url = f"{base_url}/api"
        self.openai_url = f"{base_url}/v1"
        
    def ensure_model_available(self) -> bool:
        """Ensure the model is pulled and available."""
        try:
            response = requests.post(f"{self.api_url}/pull", 
                json={"name": self.model_name})
            return response.status_code == 200
        except Exception as e:
            print(f"Failed to pull model {self.model_name}: {e}")
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for Ollama."""
        return openai.OpenAI(
            base_url=self.openai_url,
            api_key="ollama",  # Ollama doesn't require real API key
        )
    
    def health_check(self) -> bool:
        """Check if Ollama service is running."""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            return response.status_code == 200
        except:
            return False

# Initialize Ollama for agent development
ollama_manager = OllamaManager("phi3.5:3.8b-mini-instruct-q4_K_M")
ollama_manager.ensure_model_available()

# Configure agent with Ollama backend
agent_config = Config(
    name="ollama-agent",
    model_provider="ollama",
    model_id="phi3.5:3.8b-mini-instruct-q4_K_M",
    endpoint=ollama_manager.openai_url,
    api_key="ollama"
)

agent = Agent(config=agent_config)
```

**Persediaan Ejen Berbilang Model dengan Ollama**:
```python
class OllamaMultiModelManager:
    def __init__(self):
        self.models = {
            "lightweight": "qwen2.5:0.5b-instruct-q4_K_M",      # 350MB
            "balanced": "phi3.5:3.8b-mini-instruct-q4_K_M",     # 2.3GB  
            "capable": "llama3.2:3b-instruct-q4_K_M",           # 1.9GB
            "coding": "codellama:7b-code-q4_K_M"                # 4.1GB
        }
        self.base_url = "http://localhost:11434"
        self.clients = {}
        self._initialize_models()
    
    def _initialize_models(self):
        """Pull all required models and create clients."""
        for category, model_name in self.models.items():
            # Pull model if not available
            self._pull_model(model_name)
            
            # Create OpenAI client for each model
            self.clients[category] = openai.OpenAI(
                base_url=f"{self.base_url}/v1",
                api_key="ollama"
            )
    
    def _pull_model(self, model_name: str):
        """Pull model if not already available."""
        try:
            response = requests.post(f"{self.base_url}/api/pull", 
                json={"name": model_name})
            if response.status_code == 200:
                print(f"Model {model_name} ready")
        except Exception as e:
            print(f"Failed to pull {model_name}: {e}")
    
    def get_agent_for_task(self, task_type: str) -> Agent:
        """Get appropriate agent based on task complexity."""
        model_category = self._classify_task(task_type)
        model_name = self.models[model_category]
        
        config = Config(
            name=f"ollama-{model_category}-agent",
            model_provider="ollama",
            model_id=model_name,
            endpoint=f"{self.base_url}/v1",
            api_key="ollama"
        )
        
        return Agent(config=config)
    
    def _classify_task(self, task_type: str) -> str:
        """Classify task to appropriate model category."""
        if any(keyword in task_type.lower() for keyword in ["simple", "route", "classify"]):
            return "lightweight"
        elif any(keyword in task_type.lower() for keyword in ["code", "programming", "debug"]):
            return "coding"
        elif any(keyword in task_type.lower() for keyword in ["complex", "analysis", "research"]):
            return "capable"
        else:
            return "balanced"

# Usage example
manager = OllamaMultiModelManager()

# Get appropriate agents for different tasks
routing_agent = manager.get_agent_for_task("simple routing")
coding_agent = manager.get_agent_for_task("code debugging")
analysis_agent = manager.get_agent_for_task("complex analysis")
```

#### Corak Pelaksanaan Pengeluaran

**Perkhidmatan Pengeluaran dengan Ollama**:
```python
import asyncio
import logging
from typing import Dict, Optional
from microsoft_agent_framework import Agent, Config
import requests
import openai

class OllamaProductionService:
    def __init__(self, models_config: Dict[str, str]):
        self.models_config = models_config
        self.base_url = "http://localhost:11434"
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "errors": 0,
            "model_usage": {model: 0 for model in models_config.keys()}
        }
        self._initialize_production_agents()
    
    def _initialize_production_agents(self):
        """Initialize production agents with health checks."""
        for agent_type, model_name in self.models_config.items():
            try:
                # Ensure model is available
                self._ensure_model_ready(model_name)
                
                # Create production agent
                config = Config(
                    name=f"production-{agent_type}",
                    model_provider="ollama",
                    model_id=model_name,
                    endpoint=f"{self.base_url}/v1",
                    api_key="ollama",
                    max_tokens=512,
                    temperature=0.1,
                    timeout=30.0
                )
                
                agent = Agent(config=config)
                
                # Add production tools based on agent type
                self._add_production_tools(agent, agent_type)
                
                self.agents[agent_type] = agent
                logging.info(f"Initialized {agent_type} agent with model {model_name}")
                
            except Exception as e:
                logging.error(f"Failed to initialize {agent_type} agent: {e}")
    
    def _ensure_model_ready(self, model_name: str):
        """Ensure model is pulled and ready for use."""
        try:
            # Check if model exists
            response = requests.get(f"{self.base_url}/api/tags")
            models = response.json().get('models', [])
            
            model_exists = any(model['name'] == model_name for model in models)
            
            if not model_exists:
                logging.info(f"Pulling model {model_name}...")
                pull_response = requests.post(f"{self.base_url}/api/pull", 
                    json={"name": model_name})
                
                if pull_response.status_code != 200:
                    raise Exception(f"Failed to pull model {model_name}")
                    
        except Exception as e:
            raise Exception(f"Model setup failed for {model_name}: {e}")
    
    def _add_production_tools(self, agent: Agent, agent_type: str):
        """Add tools based on agent type."""
        if agent_type == "customer_service":
            @agent.tool
            def lookup_customer(customer_id: str) -> dict:
                """Look up customer information."""
                # Simulate database lookup
                return {"customer_id": customer_id, "status": "active", "tier": "premium"}
            
            @agent.tool
            def create_support_ticket(issue: str, priority: str = "medium") -> str:
                """Create a support ticket."""
                ticket_id = f"TICK-{hash(issue) % 10000:04d}"
                return f"Created ticket {ticket_id} with priority {priority}"
        
        elif agent_type == "technical_support":
            @agent.tool
            def run_diagnostics(system_info: str) -> dict:
                """Run system diagnostics."""
                return {"status": "healthy", "issues": [], "recommendations": []}
            
            @agent.tool
            def access_knowledge_base(query: str) -> str:
                """Search technical knowledge base."""
                return f"Knowledge base results for: {query}"
    
    async def process_request(self, request: str, agent_type: str = "customer_service") -> dict:
        """Process user request with monitoring and error handling."""
        start_time = time.time()
        
        try:
            if agent_type not in self.agents:
                raise ValueError(f"Agent type {agent_type} not available")
            
            agent = self.agents[agent_type]
            response = await agent.chat_async(request)
            
            # Update metrics
            self.metrics["requests_processed"] += 1
            self.metrics["model_usage"][agent_type] += 1
            
            processing_time = time.time() - start_time
            
            self._log_interaction(request, response, "success", processing_time, agent_type)
            
            return {
                "response": response,
                "status": "success",
                "processing_time": processing_time,
                "agent_type": agent_type
            }
            
        except Exception as e:
            self.metrics["errors"] += 1
            processing_time = time.time() - start_time
            
            self._log_interaction(request, str(e), "error", processing_time, agent_type)
            
            return {
                "response": "I'm experiencing technical difficulties. Please try again.",
                "status": "error",
                "error": str(e),
                "processing_time": processing_time
            }
    
    def _log_interaction(self, request: str, response: str, status: str, 
                        processing_time: float, agent_type: str):
        """Log interaction for monitoring and analysis."""
        logging.info(f"Agent: {agent_type}, Status: {status}, Time: {processing_time:.2f}s")
        
        # In production, this would write to a proper logging system
        log_entry = {
            "timestamp": time.time(),
            "agent_type": agent_type,
            "request_length": len(request),
            "response_length": len(response),
            "status": status,
            "processing_time": processing_time
        }
    
    def get_health_status(self) -> dict:
        """Get service health status."""
        try:
            # Check Ollama service health
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            ollama_healthy = response.status_code == 200
            
            # Check model availability
            available_models = []
            if ollama_healthy:
                models = response.json().get('models', [])
                available_models = [model['name'] for model in models]
            
            return {
                "service_status": "healthy" if ollama_healthy else "unhealthy",
                "ollama_endpoint": self.base_url,
                "available_models": available_models,
                "active_agents": list(self.agents.keys()),
                "metrics": self.metrics,
                "timestamp": time.time()
            }
            
        except Exception as e:
            return {
                "service_status": "error",
                "error": str(e),
                "timestamp": time.time()
            }

# Production deployment example
production_models = {
    "customer_service": "phi3.5:3.8b-mini-instruct-q4_K_M",
    "technical_support": "llama3.2:3b-instruct-q4_K_M",
    "routing": "qwen2.5:0.5b-instruct-q4_K_M"
}

service = OllamaProductionService(production_models)

# Process requests
result = await service.process_request(
    "I need help with my account settings", 
    "customer_service"
)
print(result)
```

#### Ciri Perusahaan dan Pemantauan

**Pemantauan dan Kebolehlihatan Ollama**:
```python
import time
import asyncio
import requests
from typing import Dict, List

class OllamaMonitoringService:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.metrics_history = []
        self.alert_thresholds = {
            "response_time_ms": 2000,
            "error_rate_percent": 5,
            "memory_usage_percent": 85
        }
    
    async def collect_metrics(self) -> dict:
        """Collect comprehensive metrics from Ollama service."""
        metrics = {
            "timestamp": time.time(),
            "service_status": "unknown",
            "models": {},
            "performance": {},
            "resources": {}
        }
        
        try:
            # Check service health
            health_response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            metrics["service_status"] = "healthy" if health_response.status_code == 200 else "unhealthy"
            
            if metrics["service_status"] == "healthy":
                # Get model information
                models_data = health_response.json().get('models', [])
                for model in models_data:
                    model_name = model['name']
                    metrics["models"][model_name] = {
                        "size_gb": model.get('size', 0) / (1024**3),
                        "modified": model.get('modified_at', ''),
                        "digest": model.get('digest', '')[:12]  # Short digest
                    }
                
                # Test inference performance
                start_time = time.time()
                test_response = requests.post(f"{self.base_url}/api/generate", 
                    json={
                        "model": list(metrics["models"].keys())[0] if metrics["models"] else "",
                        "prompt": "Hello",
                        "stream": False
                    }, timeout=10)
                
                if test_response.status_code == 200:
                    inference_time = (time.time() - start_time) * 1000
                    metrics["performance"] = {
                        "inference_time_ms": inference_time,
                        "tokens_per_second": self._calculate_tokens_per_second(test_response.json()),
                        "last_successful_inference": time.time()
                    }
            
        except Exception as e:
            metrics["service_status"] = "error"
            metrics["error"] = str(e)
        
        self.metrics_history.append(metrics)
        
        # Keep only last 100 metrics entries
        if len(self.metrics_history) > 100:
            self.metrics_history = self.metrics_history[-100:]
        
        return metrics
    
    def _calculate_tokens_per_second(self, response_data: dict) -> float:
        """Calculate approximate tokens per second from response."""
        try:
            # Estimate tokens (rough approximation)
            response_text = response_data.get('response', '')
            estimated_tokens = len(response_text.split())
            
            # Get timing info if available
            eval_duration = response_data.get('eval_duration', 0)
            if eval_duration > 0:
                # Convert nanoseconds to seconds
                duration_seconds = eval_duration / 1e9
                return estimated_tokens / duration_seconds if duration_seconds > 0 else 0
        except:
            pass
        return 0
    
    def check_alerts(self, current_metrics: dict) -> List[dict]:
        """Check current metrics against alert thresholds."""
        alerts = []
        
        # Check response time
        if current_metrics.get('performance', {}).get('inference_time_ms', 0) > self.alert_thresholds['response_time_ms']:
            alerts.append({
                "type": "performance",
                "message": f"High response time: {current_metrics['performance']['inference_time_ms']:.0f}ms",
                "severity": "warning"
            })
        
        # Check service status
        if current_metrics.get('service_status') != 'healthy':
            alerts.append({
                "type": "availability",
                "message": f"Service unhealthy: {current_metrics.get('error', 'Unknown error')}",
                "severity": "critical"
            })
        
        return alerts
    
    def get_performance_summary(self, minutes: int = 60) -> dict:
        """Get performance summary for the last N minutes."""
        cutoff_time = time.time() - (minutes * 60)
        recent_metrics = [m for m in self.metrics_history if m['timestamp'] > cutoff_time]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        # Calculate averages
        response_times = [m.get('performance', {}).get('inference_time_ms', 0) 
                         for m in recent_metrics if m.get('performance')]
        
        healthy_checks = sum(1 for m in recent_metrics if m.get('service_status') == 'healthy')
        uptime_percent = (healthy_checks / len(recent_metrics)) * 100 if recent_metrics else 0
        
        return {
            "period_minutes": minutes,
            "total_checks": len(recent_metrics),
            "uptime_percent": uptime_percent,
            "avg_response_time_ms": sum(response_times) / len(response_times) if response_times else 0,
            "max_response_time_ms": max(response_times) if response_times else 0,
            "min_response_time_ms": min(response_times) if response_times else 0
        }

# Production monitoring setup
monitor = OllamaMonitoringService()

async def monitoring_loop():
    """Continuous monitoring loop."""
    while True:
        try:
            metrics = await monitor.collect_metrics()
            alerts = monitor.check_alerts(metrics)
            
            if alerts:
                for alert in alerts:
                    logging.warning(f"ALERT: {alert['message']} (Severity: {alert['severity']})")
            
            # Log performance summary every 10 minutes
            if int(time.time()) % 600 == 0:  # Every 10 minutes
                summary = monitor.get_performance_summary(10)
                logging.info(f"Performance Summary: {summary}")
            
        except Exception as e:
            logging.error(f"Monitoring error: {e}")
        
        await asyncio.sleep(30)  # Check every 30 seconds

# Start monitoring
# asyncio.create_task(monitoring_loop())
```

#### Konfigurasi dan Pengoptimuman Lanjutan

**Pengurusan Model Tersuai dengan Ollama**:
```python
class OllamaModelManager:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.model_catalog = {
            # Lightweight models for fast responses
            "ultra_light": [
                "qwen2.5:0.5b-instruct-q4_K_M",
                "tinyllama:1.1b-chat-q4_K_M"
            ],
            # Balanced models for general use
            "balanced": [
                "phi3.5:3.8b-mini-instruct-q4_K_M",
                "llama3.2:3b-instruct-q4_K_M"
            ],
            # Specialized models for specific tasks
            "code_specialist": [
                "codellama:7b-code-q4_K_M",
                "codegemma:7b-code-q4_K_M"
            ],
            # High capability models
            "high_capability": [
                "llama3.1:8b-instruct-q4_K_M",
                "qwen2.5:7b-instruct-q4_K_M"
            ]
        }
    
    def setup_production_models(self, categories: List[str]) -> dict:
        """Set up models for production use."""
        setup_results = {}
        
        for category in categories:
            if category not in self.model_catalog:
                setup_results[category] = {"status": "error", "message": "Unknown category"}
                continue
            
            models = self.model_catalog[category]
            category_results = []
            
            for model in models:
                try:
                    # Pull model
                    response = requests.post(f"{self.base_url}/api/pull", 
                        json={"name": model})
                    
                    if response.status_code == 200:
                        category_results.append({"model": model, "status": "ready"})
                    else:
                        category_results.append({"model": model, "status": "failed"})
                        
                except Exception as e:
                    category_results.append({"model": model, "status": "error", "error": str(e)})
            
            setup_results[category] = category_results
        
        return setup_results
    
    def optimize_for_hardware(self) -> dict:
        """Recommend optimal models based on available hardware."""
        # This would typically check actual hardware specs
        # For demo purposes, we'll simulate hardware detection
        
        recommendations = {
            "low_resource": {
                "models": ["qwen2.5:0.5b-instruct-q4_K_M"],
                "max_concurrent": 1,
                "memory_usage": "< 1GB"
            },
            "medium_resource": {
                "models": ["phi3.5:3.8b-mini-instruct-q4_K_M", "llama3.2:3b-instruct-q4_K_M"],
                "max_concurrent": 2,
                "memory_usage": "2-4GB"
            },
            "high_resource": {
                "models": ["llama3.1:8b-instruct-q4_K_M", "codellama:7b-code-q4_K_M"],
                "max_concurrent": 3,
                "memory_usage": "6-12GB"
            }
        }
        
        return recommendations

# Production model setup
model_manager = OllamaModelManager()
setup_results = model_manager.setup_production_models(["balanced", "ultra_light"])
print(f"Model setup results: {setup_results}")
```

**Senarai Semak Pelaksanaan Pengeluaran untuk Ollama**:

âœ… **Konfigurasi Perkhidmatan**:
- Pasang perkhidmatan Ollama dengan integrasi sistem yang betul
- Konfigurasikan model untuk kes penggunaan ejen tertentu
- Tetapkan skrip permulaan dan pengurusan perkhidmatan yang betul
- Uji pemuatan model dan ketersediaan API

âœ… **Pengurusan Model**:
- Muat turun model yang diperlukan dan sahkan integriti
- Tetapkan prosedur kemas kini dan putaran model
- Konfigurasikan caching model dan pengoptimuman storan
- Uji prestasi model di bawah beban yang dijangkakan

âœ… **Persediaan Keselamatan**:
- Konfigurasikan peraturan firewall untuk akses tempatan sahaja
- Tetapkan kawalan akses API dan had kadar
- Laksanakan log audit untuk interaksi ejen
- Konfigurasikan storan model yang selamat dan akses

âœ… **Pengoptimuman Prestasi**:
- Penanda aras model untuk kes penggunaan yang dijangkakan
- Konfigurasikan pecutan perkakasan yang sesuai
- Tetapkan strategi pemanasan dan caching model
- Pantau penggunaan sumber dan metrik prestasi

âœ… **Ujian Integrasi**:
- Uji integrasi Microsoft Agent Framework
- Sahkan keupayaan operasi luar talian
- Uji senario failover dan pengendalian ralat
- Sahkan aliran kerja ejen hujung ke hujung

**Perbandingan dengan Foundry Local**:

| Ciri | Foundry Local | Ollama |
|------|---------------|--------|
| **Kes Penggunaan Sasaran** | Pengeluaran perusahaan | Pembangunan & komuniti |
| **Ekosistem Model** | Kurasi Microsoft | Komuniti yang luas |
| **Pengoptimuman Perkakasan** | Automatik (CUDA/NPU/CPU) | Konfigurasi manual |
| **Ciri Perusahaan** | Pemantauan terbina dalam, keselamatan | Alat komuniti |
| **Kerumitan Penggunaan** | Mudah (pemasangan winget) | Mudah (pemasangan curl) |
| **Keserasian API** | OpenAI + sambungan | Standard OpenAI |
| **Sokongan** | Rasmi Microsoft | Dipacu komuniti |
| **Terbaik Untuk** | Ejen pengeluaran | Prototip, penyelidikan |

**Bila Memilih Ollama**:
- **Pembangunan dan Prototip**: Eksperimen pantas dengan model yang berbeza
- **Model Komuniti**: Akses kepada model terkini yang disumbangkan oleh komuniti
- **Penggunaan Pendidikan**: Pembelajaran dan pengajaran pembangunan ejen AI
- **Projek Penyelidikan**: Penyelidikan akademik yang memerlukan akses kepada model yang pelbagai
- **Model Tersuai**: Membina dan menguji model yang disesuaikan

### VLLM: Inferens Ejen SLM Berprestasi Tinggi

VLLM (Inferens Model Bahasa Sangat Besar) menyediakan enjin inferens throughput tinggi dan cekap memori yang dioptimumkan khusus untuk penggunaan SLM pengeluaran pada skala besar. Walaupun Foundry Local menumpukan pada kemudahan penggunaan dan Ollama menekankan model komuniti, VLLM cemerlang dalam senario berprestasi tinggi yang memerlukan throughput maksimum dan penggunaan sumber yang cekap.

**Ciri dan Seni Bina Teras**:
- **PagedAttention**: Pengurusan memori revolusioner untuk pengiraan perhatian yang cekap
- **Dynamic Batching**: Pengelompokan permintaan pintar untuk throughput optimum
- **Pengoptimuman GPU**: Sokongan kernel CUDA maju dan paralelisme tensor
- **Keserasian OpenAI**: Keserasian API penuh untuk integrasi lancar
- **Speculative Decoding**: Teknik percepatan inferens maju
- **Sokongan Kuantisasi**: Kuantisasi INT4, INT8, dan FP16 untuk kecekapan memori

#### Pemasangan dan Persediaan

**Pilihan Pemasangan**:
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

**Permulaan Cepat untuk Pembangunan Ejen**:
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```

#### Integrasi Kerangka Ejen

**VLLM dengan Microsoft Agent Framework**:
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```

**Persediaan Multi-Ejen Berkapasiti Tinggi**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```

#### Corak Penggunaan Pengeluaran

**Perkhidmatan Pengeluaran VLLM Perusahaan**:
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```

#### Ciri Perusahaan dan Pemantauan

**Pemantauan Prestasi VLLM Lanjutan**:
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```

#### Konfigurasi dan Pengoptimuman Lanjutan

**Templat Konfigurasi Pengeluaran VLLM**:
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```

**Senarai Semak Penggunaan Pengeluaran untuk VLLM**:

âœ… **Pengoptimuman Perkakasan**:
- Konfigurasi paralelisme tensor untuk persediaan multi-GPU
- Aktifkan kuantisasi (AWQ/GPTQ) untuk kecekapan memori
- Tetapkan penggunaan memori GPU optimum (85-95%)
- Konfigurasi saiz batch yang sesuai untuk throughput

âœ… **Penalaan Prestasi**:
- Aktifkan caching awalan untuk pertanyaan berulang
- Konfigurasi prefill berpecah untuk urutan panjang
- Persediaan decoding spekulatif untuk inferens lebih pantas
- Optimumkan max_num_seqs berdasarkan perkakasan

âœ… **Ciri Pengeluaran**:
- Persediaan pemantauan kesihatan dan pengumpulan metrik
- Konfigurasi restart automatik dan failover
- Laksanakan penjadualan permintaan dan pengimbangan beban
- Persediaan log dan amaran yang komprehensif

âœ… **Keselamatan dan Kebolehpercayaan**:
- Konfigurasi peraturan firewall dan kawalan akses
- Persediaan had kadar API dan pengesahan
- Laksanakan penutupan dan pembersihan yang teratur
- Konfigurasi sandaran dan pemulihan bencana

âœ… **Ujian Integrasi**:
- Uji integrasi Microsoft Agent Framework
- Sahkan senario throughput tinggi
- Uji prosedur failover dan pemulihan
- Penanda aras prestasi di bawah beban

**Perbandingan dengan Penyelesaian Lain**:

| Ciri | VLLM | Foundry Local | Ollama |
|------|------|---------------|--------|
| **Kes Penggunaan Sasaran** | Pengeluaran throughput tinggi | Kemudahan penggunaan perusahaan | Pembangunan & komuniti |
| **Prestasi** | Throughput maksimum | Seimbang | Baik |
| **Kecekapan Memori** | Pengoptimuman PagedAttention | Pengoptimuman automatik | Standard |
| **Kerumitan Persediaan** | Tinggi (banyak parameter) | Rendah (automatik) | Rendah (mudah) |
| **Kebolehskalaan** | Cemerlang (tensor/pipeline paralel) | Baik | Terhad |
| **Kuantisasi** | Lanjutan (AWQ, GPTQ, FP8) | Automatik | GGUF standard |
| **Ciri Perusahaan** | Perlukan pelaksanaan tersuai | Terbina dalam | Alat komuniti |
| **Terbaik Untuk** | Ejen pengeluaran skala besar | Pengeluaran perusahaan | Pembangunan |

**Bila Memilih VLLM**:
- **Keperluan Throughput Tinggi**: Memproses ratusan permintaan sesaat
- **Penggunaan Skala Besar**: Penggunaan multi-GPU, multi-node
- **Kritikal Prestasi**: Masa tindak balas sub-saat pada skala besar
- **Pengoptimuman Lanjutan**: Keperluan untuk kuantisasi dan pengelompokan tersuai
- **Kecekapan Sumber**: Penggunaan maksimum perkakasan GPU yang mahal

## Aplikasi Ejen SLM Dunia Nyata

### Ejen SLM Khidmat Pelanggan
- **Keupayaan SLM**: Carian akaun, tetapan semula kata laluan, semakan status pesanan
- **Manfaat kos**: Pengurangan kos inferens 10x berbanding ejen LLM
- **Prestasi**: Masa tindak balas lebih pantas dengan kualiti konsisten untuk pertanyaan rutin

### Ejen Proses Perniagaan SLM
- **Ejen pemprosesan invois**: Ekstrak data, sahkan maklumat, laluan untuk kelulusan
- **Ejen pengurusan e-mel**: Kategorikan, utamakan, draf balasan secara automatik
- **Ejen penjadualan**: Koordinasi mesyuarat, uruskan kalendar, hantar peringatan

### Pembantu Digital SLM Peribadi
- **Ejen pengurusan tugas**: Cipta, kemas kini, susun senarai tugasan dengan cekap
- **Ejen pengumpulan maklumat**: Penyelidikan topik, ringkaskan penemuan secara tempatan
- **Ejen komunikasi**: Draf e-mel, mesej, siaran media sosial secara peribadi

### Ejen SLM Perdagangan dan Kewangan
- **Ejen pemantauan pasaran**: Jejak harga, kenal pasti trend secara masa nyata
- **Ejen penjanaan laporan**: Cipta ringkasan harian/mingguan secara automatik
- **Ejen penilaian risiko**: Menilai kedudukan portfolio menggunakan data tempatan

### Ejen Sokongan Penjagaan Kesihatan SLM
- **Ejen penjadualan pesakit**: Koordinasi temu janji, hantar peringatan automatik
- **Ejen dokumentasi**: Hasilkan ringkasan perubatan, laporan secara tempatan
- **Ejen pengurusan preskripsi**: Jejak pengisian semula, semak interaksi secara peribadi

## Microsoft Agent Framework: Pembangunan Ejen Sedia Pengeluaran

### Gambaran Keseluruhan dan Seni Bina

Microsoft Agent Framework menyediakan platform komprehensif dan bertaraf perusahaan untuk membina, menggunakan, dan mengurus ejen AI yang boleh beroperasi di awan dan persekitaran tepi luar talian. Kerangka ini direka khusus untuk berfungsi dengan Model Bahasa Kecil dan senario pengkomputeran tepi, menjadikannya ideal untuk penggunaan yang sensitif terhadap privasi dan sumber terhad.

**Komponen Kerangka Teras**:
- **Agent Runtime**: Persekitaran pelaksanaan ringan yang dioptimumkan untuk peranti tepi
- **Sistem Integrasi Alat**: Seni bina plugin yang boleh diperluas untuk menyambungkan perkhidmatan dan API luaran
- **Pengurusan Keadaan**: Memori ejen yang berterusan dan pengendalian konteks merentasi sesi
- **Lapisan Keselamatan**: Kawalan keselamatan terbina dalam untuk penggunaan perusahaan
- **Enjin Orkestrasi**: Koordinasi multi-ejen dan pengurusan aliran kerja

### Ciri Utama untuk Penggunaan Tepi

**Seni Bina Offline-First**: Microsoft Agent Framework direka dengan prinsip offline-first, membolehkan ejen beroperasi dengan berkesan tanpa sambungan internet yang berterusan. Ini termasuk inferens model tempatan, pangkalan pengetahuan yang di-cache, pelaksanaan alat luar talian, dan degradasi yang teratur apabila perkhidmatan awan tidak tersedia.

**Pengoptimuman Sumber**: Kerangka ini menyediakan pengurusan sumber pintar dengan pengoptimuman memori automatik untuk SLM, pengimbangan beban CPU/GPU untuk peranti tepi, pemilihan model adaptif berdasarkan sumber yang tersedia, dan corak inferens cekap kuasa untuk penggunaan mudah alih.

**Keselamatan dan Privasi**: Ciri keselamatan bertaraf perusahaan termasuk pemprosesan data tempatan untuk mengekalkan privasi, saluran komunikasi ejen yang disulitkan, kawalan akses berdasarkan peranan untuk keupayaan ejen, dan log audit untuk keperluan pematuhan.

### Integrasi dengan Foundry Local

Microsoft Agent Framework berintegrasi dengan lancar dengan Foundry Local untuk menyediakan penyelesaian AI tepi yang lengkap:

**Penemuan Model Automatik**: Kerangka ini secara automatik mengesan dan menyambung ke instans Foundry Local, menemui model SLM yang tersedia, dan memilih model optimum berdasarkan keperluan ejen dan keupayaan perkakasan.

**Pemuatan Model Dinamik**: Ejen boleh memuatkan model SLM yang berbeza secara dinamik untuk tugas tertentu, membolehkan sistem ejen multi-model di mana model yang berbeza mengendalikan jenis permintaan yang berbeza, dan failover automatik antara model berdasarkan ketersediaan dan prestasi.

**Pengoptimuman Prestasi**: Mekanisme caching bersepadu mengurangkan masa pemuatan model, pengumpulan sambungan mengoptimumkan panggilan API ke Foundry Local, dan pengelompokan pintar meningkatkan throughput untuk permintaan ejen berganda.

### Membina Ejen dengan Microsoft Agent Framework

#### Definisi dan Konfigurasi Ejen

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```

#### Integrasi Alat untuk Senario Tepi

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```

#### Orkestrasi Multi-Ejen

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```

### Corak Penggunaan Tepi Lanjutan

#### Seni Bina Ejen Hierarki

**Kluster Ejen Tempatan**: Gunakan pelbagai ejen SLM khusus pada peranti tepi, setiap satu dioptimumkan untuk tugas tertentu. Gunakan model ringan seperti Qwen2.5-0.5B untuk penghalaan dan penjadualan mudah, model sederhana seperti Phi-4-Mini untuk khidmat pelanggan dan dokumentasi, dan model yang lebih besar untuk penaakulan kompleks apabila sumber membenarkan.

**Koordinasi Tepi-ke-Awan**: Laksanakan corak eskalasi pintar di mana ejen tempatan mengendalikan tugas rutin, ejen awan menyediakan penaakulan kompleks apabila sambungan membenarkan, dan penyerahan lancar antara pemprosesan tepi dan awan mengekalkan kesinambungan.

#### Konfigurasi Penggunaan

**Penggunaan Peranti Tunggal**:
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```

**Penggunaan Tepi Teragih**:
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```

### Pengoptimuman Prestasi untuk Ejen Tepi

#### Strategi Pemilihan Model

**Penugasan Model Berdasarkan Tugas**: Microsoft Agent Framework membolehkan pemilihan model pintar berdasarkan kerumitan tugas dan keperluan:

- **Tugas Mudah** (Q&A, penghalaan): Qwen2.5-0.5B (500MB, <100ms tindak balas)
- **Tugas Sederhana** (khidmat pelanggan, penjadualan): Phi-4-Mini (2.4GB, 200-500ms tindak balas)
- **Tugas Kompleks** (analisis teknikal, perancangan): Phi-4 (7GB, 1-3s tindak balas apabila sumber membenarkan)

**Penukaran Model Dinamik**: Ejen boleh bertukar antara model berdasarkan beban sistem semasa, penilaian kerumitan tugas, tahap keutamaan pengguna, dan sumber perkakasan yang tersedia.

#### Pengurusan Memori dan Sumber

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

### Corak Integrasi Perusahaan

#### Keselamatan dan Pematuhan

**Pemprosesan Data Tempatan**: Semua pemprosesan ejen berlaku secara tempatan, memastikan data sensitif tidak pernah meninggalkan peranti tepi. Ini termasuk perlindungan maklumat pelanggan, pematuhan HIPAA untuk ejen penjagaan kesihatan, keselamatan data kewangan untuk ejen perbankan, dan pematuhan GDPR untuk penggunaan di Eropah.

**Kawalan Akses**: Kebenaran berdasarkan peranan mengawal alat yang boleh diakses oleh ejen, pengesahan pengguna untuk interaksi ejen, dan jejak audit untuk semua tindakan dan keputusan ejen.

#### Pemantauan dan Kebolehlihatan

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```

### Contoh Pelaksanaan Dunia Nyata

#### Sistem Ejen Tepi Runcit

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```

#### Ejen Sokongan Penjagaan Kesihatan

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```

### Amalan Terbaik untuk Microsoft Agent Framework

#### Garis Panduan Pembangunan

1. **Mulakan dengan Mudah**: Mulakan dengan senario ejen tunggal sebelum membina sistem multi-ejen yang kompleks
2. **Saiz Model yang Tepat**: Pilih model terkecil yang memenuhi keperluan ketepatan anda
3. **Reka Bentuk Alat**: Cipta alat yang fokus dan berfungsi tunggal daripada alat pelbagai fungsi yang kompleks
4. **Pengendalian Ralat**: Laksanakan degradasi yang teratur untuk senario luar talian dan kegagalan model
5. **Ujian**: Uji ejen secara meluas dalam keadaan luar talian dan persekitaran yang terhad sumber

#### Amalan Terbaik Penggunaan

1. **Pelaksanaan Beransur-ansur**: Gunakan kepada kumpulan pengguna kecil pada mulanya, pantau metrik prestasi dengan teliti
2. **Pemantauan Sumber**: Persediaan amaran untuk ambang memori, CPU, dan masa tindak balas
3. **Strategi Sandaran**: Sentiasa ada rancangan sandaran untuk kegagalan model atau keletihan sumber
4. **Keselamatan Dahulu**: Laksanakan kawalan keselamatan dari awal, bukan sebagai pemikiran selepas
5. **Dokumentasi**: Kekalkan dokumentasi yang jelas tentang keupayaan dan batasan ejen

### Peta Jalan dan Integrasi Masa Depan

Microsoft Agent Framework terus berkembang dengan pengoptimuman SLM yang dipertingkatkan, alat penggunaan tepi yang lebih baik, pengurusan sumber yang lebih baik untuk persekitaran terhad, dan ekosistem alat yang diperluas untuk senario perusahaan biasa.

**Ciri Akan Datang**:
- **AutoML untuk Pengoptimuman Ejen**: Penalaan automatik SLM untuk tugas ejen tertentu
- **Rangkaian Mesh Tepi**: Koordinasi antara pelbagai penggunaan ejen tepi
- **Telemetri Lanjutan**: Pemantauan dan analitik yang dipertingkatkan untuk prestasi ejen
- **Pembina Ejen Visual**: Alat pembangunan ejen kod rendah/tanpa kod

## Amalan Terbaik untuk Pelaksanaan Ejen SLM

### Garis Panduan Pemilihan SLM untuk Ejen

Apabila memilih SLM untuk penggunaan ejen, pertimbangkan faktor berikut:

**Pertimbangan Saiz Model**: Pilih model ultra-kompres seperti Q2_K untuk aplikasi ejen mudah alih yang sangat kecil, model seimbang seperti Q4_K_M untuk senario ejen umum, dan model ketepatan tinggi seperti Q8_0 untuk aplikasi ejen yang kritikal kualiti.

**Penjajaran Kes Penggunaan Ejen**: Padankan keupayaan SLM dengan keperluan ejen tertentu, dengan mempertimbangkan faktor seperti pemeliharaan ketepatan untuk keputusan ejen, kelajuan inferens untuk interaksi ejen masa nyata, kekangan memori untuk penggunaan ejen tepi, dan keperluan operasi luar talian untuk ejen yang fokus pada privasi.

### Pemilihan Strategi Pengoptimuman untuk Ejen SLM

**Pendekatan Kuantisasi untuk Ejen**: Pilih tahap kuantisasi yang sesuai berdasarkan keperluan kualiti ejen dan kekangan perkakasan. Pertimbangkan Q4_0 untuk pemampatan maksimum
**Pemilihan Kerangka untuk Penggunaan Ejen**: Pilih kerangka pengoptimuman berdasarkan perkakasan sasaran dan keperluan ejen. Gunakan Llama.cpp untuk penggunaan ejen yang dioptimumkan CPU, Apple MLX untuk aplikasi ejen Apple Silicon, dan ONNX untuk keserasian ejen merentas platform.

## Penukaran Ejen SLM Praktikal dan Kes Penggunaan

### Senario Penggunaan Ejen Dunia Nyata

**Aplikasi Ejen Mudah Alih**: Format Q4_K sangat sesuai untuk aplikasi ejen telefon pintar dengan penggunaan memori yang minimum, manakala Q8_0 memberikan prestasi seimbang untuk sistem ejen berasaskan tablet. Format Q5_K menawarkan kualiti unggul untuk ejen produktiviti mudah alih.

**Pengkomputeran Ejen Desktop dan Edge**: Q5_K memberikan prestasi optimum untuk aplikasi ejen desktop, Q8_0 menyediakan inferens berkualiti tinggi untuk persekitaran ejen workstation, dan Q4_K membolehkan pemprosesan cekap pada peranti ejen edge.

**Ejen Penyelidikan dan Eksperimen**: Format kuantisasi maju membolehkan penerokaan inferens ejen ultra-rendah ketepatan untuk penyelidikan akademik dan aplikasi ejen bukti konsep yang memerlukan kekangan sumber yang ekstrem.

### Penanda Aras Prestasi Ejen SLM

**Kelajuan Inferens Ejen**: Q4_K mencapai masa tindak balas ejen terpantas pada CPU mudah alih, Q5_K memberikan nisbah kelajuan-kualiti yang seimbang untuk aplikasi ejen umum, Q8_0 menawarkan kualiti unggul untuk tugas ejen kompleks, dan format eksperimen memberikan throughput maksimum untuk perkakasan ejen khusus.

**Keperluan Memori Ejen**: Tahap kuantisasi untuk ejen berkisar dari Q2_K (kurang daripada 500MB untuk model ejen kecil) hingga Q8_0 (kira-kira 50% daripada saiz asal), dengan konfigurasi eksperimen mencapai pemampatan maksimum untuk persekitaran ejen yang terhad sumber.

## Cabaran dan Pertimbangan untuk Ejen SLM

### Pertukaran Prestasi dalam Sistem Ejen

Penggunaan ejen SLM melibatkan pertimbangan teliti terhadap pertukaran antara saiz model, kelajuan tindak balas ejen, dan kualiti output. Walaupun Q4_K menawarkan kelajuan dan kecekapan yang luar biasa untuk ejen mudah alih, Q8_0 memberikan kualiti unggul untuk tugas ejen kompleks. Q5_K mencapai keseimbangan yang sesuai untuk kebanyakan aplikasi ejen umum.

### Keserasian Perkakasan untuk Ejen SLM

Peranti edge yang berbeza mempunyai keupayaan yang berbeza untuk penggunaan ejen SLM. Q4_K berjalan dengan cekap pada pemproses asas untuk ejen mudah, Q5_K memerlukan sumber pengkomputeran sederhana untuk prestasi ejen yang seimbang, dan Q8_0 mendapat manfaat daripada perkakasan kelas tinggi untuk keupayaan ejen maju.

### Keselamatan dan Privasi dalam Sistem Ejen SLM

Walaupun ejen SLM membolehkan pemprosesan tempatan untuk privasi yang lebih baik, langkah keselamatan yang sesuai mesti dilaksanakan untuk melindungi model ejen dan data dalam persekitaran edge. Ini amat penting apabila menggunakan format ejen ketepatan tinggi dalam persekitaran perusahaan atau format ejen yang dimampatkan dalam aplikasi yang mengendalikan data sensitif.

## Trend Masa Depan dalam Pembangunan Ejen SLM

Landskap ejen SLM terus berkembang dengan kemajuan dalam teknik pemampatan, kaedah pengoptimuman, dan strategi penggunaan edge. Perkembangan masa depan termasuk algoritma kuantisasi yang lebih cekap untuk model ejen, kaedah pemampatan yang lebih baik untuk aliran kerja ejen, dan integrasi yang lebih baik dengan pemecut perkakasan edge untuk pemprosesan ejen.

**Ramalan Pasaran untuk Ejen SLM**: Menurut penyelidikan terkini, automasi berkuasa ejen boleh menghapuskan 40â€“60% tugas kognitif berulang dalam aliran kerja perusahaan menjelang 2027, dengan SLM memimpin transformasi ini kerana kecekapan kos dan fleksibiliti penggunaannya.

**Trend Teknologi dalam Ejen SLM**:
- **Ejen SLM Khusus**: Model khusus domain yang dilatih untuk tugas ejen tertentu dan industri
- **Pengkomputeran Ejen Edge**: Keupayaan ejen pada peranti yang dipertingkatkan dengan privasi yang lebih baik dan latensi yang dikurangkan
- **Orkestrasi Ejen**: Penyelarasan yang lebih baik antara pelbagai ejen SLM dengan penghalaan dinamik dan pengimbangan beban
- **Demokratisasi**: Fleksibiliti SLM membolehkan penyertaan yang lebih luas dalam pembangunan ejen merentasi organisasi

## Memulakan dengan Ejen SLM

### Langkah 1: Sediakan Persekitaran Kerangka Ejen Microsoft

**Pasang Kebergantungan**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inisialisasi Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Langkah 2: Pilih SLM Anda untuk Aplikasi Ejen
Pilihan popular untuk Kerangka Ejen Microsoft:
- **Microsoft Phi-4 Mini (3.8B)**: Sangat baik untuk tugas ejen umum dengan prestasi seimbang
- **Qwen2.5-0.5B (0.5B)**: Sangat cekap untuk ejen penghalaan dan klasifikasi mudah
- **Qwen2.5-Coder-0.5B (0.5B)**: Khusus untuk tugas ejen berkaitan kod
- **Phi-4 (7B)**: Penalaran maju untuk senario edge kompleks apabila sumber mencukupi

### Langkah 3: Cipta Ejen Pertama Anda dengan Kerangka Ejen Microsoft

**Persediaan Ejen Asas**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Langkah 4: Tentukan Skop dan Keperluan Ejen
Mulakan dengan aplikasi ejen yang fokus dan terdefinisi dengan baik menggunakan Kerangka Ejen Microsoft:
- **Ejen domain tunggal**: Perkhidmatan pelanggan ATAU penjadualan ATAU penyelidikan
- **Objektif ejen yang jelas**: Matlamat yang spesifik dan boleh diukur untuk prestasi ejen
- **Integrasi alat yang terhad**: Maksimum 3-5 alat untuk penggunaan ejen awal
- **Batasan ejen yang ditentukan**: Laluan eskalasi yang jelas untuk senario kompleks
- **Reka bentuk edge-first**: Utamakan fungsi luar talian dan pemprosesan tempatan

### Langkah 5: Laksanakan Penggunaan Edge dengan Kerangka Ejen Microsoft

**Konfigurasi Sumber**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Laksanakan Langkah Keselamatan untuk Ejen Edge**:
- **Pengesahan input tempatan**: Semak permintaan tanpa pergantungan awan
- **Penapisan output luar talian**: Pastikan respons memenuhi piawaian kualiti secara tempatan
- **Kawalan keselamatan edge**: Laksanakan keselamatan tanpa memerlukan sambungan internet
- **Pemantauan tempatan**: Jejak prestasi dan tandakan isu menggunakan telemetri edge

### Langkah 6: Ukur dan Optimumkan Prestasi Ejen Edge
- **Kadar penyelesaian tugas ejen**: Pantau kadar kejayaan dalam senario luar talian
- **Masa tindak balas ejen**: Pastikan masa tindak balas sub-saat untuk penggunaan edge
- **Penggunaan sumber**: Jejak penggunaan memori, CPU, dan bateri pada peranti edge
- **Kecekapan kos**: Bandingkan kos penggunaan edge dengan alternatif berasaskan awan
- **Kebolehpercayaan luar talian**: Ukur prestasi ejen semasa gangguan rangkaian

## Pengajaran Utama untuk Pelaksanaan Ejen SLM

1. **SLM mencukupi untuk ejen**: Untuk kebanyakan tugas ejen, model kecil berprestasi sama baik dengan model besar sambil menawarkan kelebihan yang ketara
2. **Kecekapan kos dalam ejen**: 10-30x lebih murah untuk menjalankan ejen SLM, menjadikannya berdaya ekonomi untuk penggunaan yang meluas
3. **Pengkhususan berkesan untuk ejen**: SLM yang disesuaikan sering mengatasi LLM umum dalam aplikasi ejen tertentu
4. **Seni bina ejen hibrid**: Gunakan SLM untuk tugas ejen rutin, LLM untuk penalaran kompleks apabila perlu
5. **Kerangka Ejen Microsoft membolehkan penggunaan pengeluaran**: Menyediakan alat kelas perusahaan untuk membina, menggunakan, dan mengurus ejen edge
6. **Prinsip reka bentuk edge-first**: Ejen yang mampu berfungsi luar talian dengan pemprosesan tempatan memastikan privasi dan kebolehpercayaan
7. **Integrasi Foundry Local**: Sambungan lancar antara Kerangka Ejen Microsoft dan inferens model tempatan
8. **Masa depan adalah ejen SLM**: Model bahasa kecil dengan kerangka pengeluaran adalah masa depan AI ejen, membolehkan penggunaan ejen yang demokratik dan cekap

## Rujukan dan Bacaan Lanjut

### Kertas Penyelidikan dan Penerbitan Teras

#### Ejen AI dan Sistem Ejenik
- **"Language Agents as Optimizable Graphs"** (2024) - Penyelidikan asas tentang seni bina ejen dan strategi pengoptimuman
  - Penulis: Wenyue Hua, Lishan Yang, et al.
  - Pautan: https://arxiv.org/abs/2402.16823
  - Wawasan Utama: Reka bentuk ejen berasaskan graf dan strategi pengoptimuman

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Penulis: Zhiheng Xi, Wenxiang Chen, et al.
  - Pautan: https://arxiv.org/abs/2309.07864
  - Wawasan Utama: Tinjauan komprehensif tentang keupayaan dan aplikasi ejen berasaskan LLM

- **"Cognitive Architectures for Language Agents"** (2024)
  - Penulis: Theodore Sumers, Shunyu Yao, et al.
  - Pautan: https://arxiv.org/abs/2309.02427
  - Wawasan Utama: Kerangka kognitif untuk mereka bentuk ejen pintar

#### Model Bahasa Kecil dan Pengoptimuman
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Penulis: Pasukan Penyelidikan Microsoft
  - Pautan: https://arxiv.org/abs/2404.14219
  - Wawasan Utama: Prinsip reka bentuk SLM dan strategi penggunaan mudah alih

- **"Qwen2.5 Technical Report"** (2024)
  - Penulis: Pasukan Alibaba Cloud
  - Pautan: https://arxiv.org/abs/2407.10671
  - Wawasan Utama: Teknik latihan SLM maju dan pengoptimuman prestasi

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Penulis: Peiyuan Zhang, Guangtao Zeng, et al.
  - Pautan: https://arxiv.org/abs/2401.02385
  - Wawasan Utama: Reka bentuk model ultra-kompak dan kecekapan latihan

### Dokumentasi Rasmi dan Kerangka

#### Kerangka Ejen Microsoft
- **Dokumentasi Rasmi**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **Repositori GitHub**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Repositori Utama**: https://github.com/microsoft/foundry-local
- **Dokumentasi**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Repositori Utama**: https://github.com/vllm-project/vllm
- **Dokumentasi**: https://docs.vllm.ai/


#### Ollama
- **Laman Web Rasmi**: https://ollama.ai/
- **Repositori GitHub**: https://github.com/ollama/ollama

### Kerangka Pengoptimuman Model

#### Llama.cpp
- **Repositori**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentasi**: https://microsoft.github.io/Olive/
- **Repositori GitHub**: https://github.com/microsoft/Olive

#### OpenVINO
- **Laman Rasmi**: https://docs.openvino.ai/

#### Apple MLX
- **Repositori**: https://github.com/ml-explore/mlx

### Laporan Industri dan Analisis Pasaran

#### Penyelidikan Pasaran Ejen AI
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Pautan: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - Wawasan Utama: Trend pasaran dan pola penggunaan perusahaan

#### Penanda Aras Teknikal

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Pautan: https://mlcommons.org/en/inference-edge/
  - Wawasan Utama: Metrik prestasi standard untuk penggunaan edge

### Piawaian dan Spesifikasi

#### Format Model dan Piawaian
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Format model merentas platform untuk interoperabiliti
- **Spesifikasi GGUF**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Format model kuantisasi untuk inferens CPU
- **Spesifikasi API OpenAI**: https://platform.openai.com/docs/api-reference
  - Format API standard untuk integrasi model bahasa

#### Keselamatan dan Pematuhan
- **Kerangka Pengurusan Risiko AI NIST**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - Sistem AI**: Kerangka untuk sistem AI dan keselamatan
- **Piawaian IEEE untuk AI**: https://standards.ieee.org/industry-connections/ai/

Peralihan ke arah ejen berkuasa SLM mewakili perubahan asas dalam cara kita mendekati penggunaan AI. Kerangka Ejen Microsoft, digabungkan dengan platform tempatan dan Model Bahasa Kecil yang cekap, menyediakan penyelesaian lengkap untuk membina ejen siap pengeluaran yang beroperasi dengan berkesan dalam persekitaran edge. Dengan memberi tumpuan kepada kecekapan, pengkhususan, dan utiliti praktikal, timbunan teknologi ini menjadikan ejen AI lebih mudah diakses, mampu dimiliki, dan berkesan untuk aplikasi dunia nyata merentasi setiap industri dan persekitaran pengkomputeran edge.

Semasa kita maju melalui 2025, gabungan model kecil yang semakin mampu, kerangka ejen yang canggih seperti Kerangka Ejen Microsoft, dan platform penggunaan edge yang kukuh akan membuka kemungkinan baharu untuk sistem autonomi yang boleh beroperasi dengan cekap pada peranti edge sambil mengekalkan privasi, mengurangkan kos, dan memberikan pengalaman pengguna yang luar biasa.

**Langkah Seterusnya untuk Pelaksanaan**:
1. **Terokai Pemanggilan Fungsi**: Pelajari cara SLM mengendalikan integrasi alat dan output berstruktur
2. **Kuasi Protokol Konteks Model (MCP)**: Fahami pola komunikasi ejen maju
3. **Bina Ejen Pengeluaran**: Gunakan Kerangka Ejen Microsoft untuk penggunaan kelas perusahaan
4. **Optimumkan untuk Edge**: Terapkan teknik pengoptimuman maju untuk persekitaran yang terhad sumber


## â¡ï¸ Apa yang seterusnya

- [02: Pemanggilan Fungsi dalam Model Bahasa Kecil (SLM)](./02.FunctionCalling.md)

---

**Penafian**:  
Dokumen ini telah diterjemahkan menggunakan perkhidmatan terjemahan AI [Co-op Translator](https://github.com/Azure/co-op-translator). Walaupun kami berusaha untuk ketepatan, sila ambil perhatian bahawa terjemahan automatik mungkin mengandungi kesilapan atau ketidaktepatan. Dokumen asal dalam bahasa asalnya harus dianggap sebagai sumber yang berwibawa. Untuk maklumat penting, terjemahan manusia profesional adalah disyorkan. Kami tidak bertanggungjawab atas sebarang salah faham atau salah tafsir yang timbul daripada penggunaan terjemahan ini.