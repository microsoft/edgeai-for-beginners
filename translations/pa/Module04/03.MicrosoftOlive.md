# ਸੈਕਸ਼ਨ 3 : ਮਾਈਕਰੋਸਾਫਟ ਓਲਿਵ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਸੂਟ

## ਸੂਚੀ
1. [ਜਾਣ ਪਛਾਣ](../../../Module04)
2. [ਮਾਈਕਰੋਸਾਫਟ ਓਲਿਵ ਕੀ ਹੈ?](../../../Module04)
3. [ਇੰਸਟਾਲੇਸ਼ਨ](../../../Module04)
4. [ਤੁਰੰਤ ਸ਼ੁਰੂਆਤ ਗਾਈਡ](../../../Module04)
5. [ਉਦਾਹਰਨ: Qwen3 ਨੂੰ ONNX INT4 ਵਿੱਚ ਬਦਲਣਾ](../../../Module04)
6. [ਤਕਨੀਕੀ ਵਰਤੋਂ](../../../Module04)
7. [ਓਲਿਵ ਰੇਸਿਪੀਜ਼ ਰਿਪੋਜ਼ਟਰੀ](../../../Module04)
8. [ਸਰਵੋਤਮ ਅਭਿਆਸ](../../../Module04)
9. [ਮੁਸ਼ਕਲਾਂ ਦਾ ਹੱਲ](../../../Module04)
10. [ਵਾਧੂ ਸਰੋਤ](../../../Module04)

## ਜਾਣ ਪਛਾਣ

ਮਾਈਕਰੋਸਾਫਟ ਓਲਿਵ ਇੱਕ ਸ਼ਕਤੀਸ਼ਾਲੀ, ਆਸਾਨ-ਵਰਤੋਂ ਯੋਗ ਹਾਰਡਵੇਅਰ-ਅਵੇਅਰ ਮਾਡਲ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਟੂਲਕਿਟ ਹੈ ਜੋ ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਪਲੇਟਫਾਰਮਾਂ 'ਤੇ ਮਸ਼ੀਨ ਲਰਨਿੰਗ ਮਾਡਲਾਂ ਨੂੰ ਡਿਪਲੌਇ ਕਰਨ ਦੀ ਪ੍ਰਕਿਰਿਆ ਨੂੰ ਸਧਾਰਨ ਬਣਾਉਂਦਾ ਹੈ। ਚਾਹੇ ਤੁਸੀਂ CPU, GPU ਜਾਂ ਵਿਸ਼ੇਸ਼ AI ਐਕਸੈਲੇਰੇਟਰਾਂ ਨੂੰ ਟਾਰਗੇਟ ਕਰ ਰਹੇ ਹੋਵੋ, ਓਲਿਵ ਤੁਹਾਨੂੰ ਮਾਡਲ ਦੀ ਸ਼ੁੱਧਤਾ ਨੂੰ ਬਰਕਰਾਰ ਰੱਖਦੇ ਹੋਏ ਵਧੀਆ ਪ੍ਰਦਰਸ਼ਨ ਪ੍ਰਾਪਤ ਕਰਨ ਵਿੱਚ ਮਦਦ ਕਰਦਾ ਹੈ।

## ਮਾਈਕਰੋਸਾਫਟ ਓਲਿਵ ਕੀ ਹੈ?

ਓਲਿਵ ਇੱਕ ਆਸਾਨ-ਵਰਤੋਂ ਯੋਗ ਹਾਰਡਵੇਅਰ-ਅਵੇਅਰ ਮਾਡਲ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਟੂਲ ਹੈ ਜੋ ਮਾਡਲ ਕੰਪ੍ਰੈਸ਼ਨ, ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਅਤੇ ਕੰਪਾਇਲੇਸ਼ਨ ਵਿੱਚ ਉਦਯੋਗ-ਅਗੇਤਕ ਤਕਨੀਕਾਂ ਨੂੰ ਜੋੜਦਾ ਹੈ। ਇਹ ONNX Runtime ਨਾਲ ਇੱਕ E2E ਇੰਫਰੈਂਸ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਹੱਲ ਵਜੋਂ ਕੰਮ ਕਰਦਾ ਹੈ।

### ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ

- **ਹਾਰਡਵੇਅਰ-ਅਵੇਅਰ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ**: ਤੁਹਾਡੇ ਟਾਰਗੇਟ ਹਾਰਡਵੇਅਰ ਲਈ ਸਵੈਚਾਲਕ ਤੌਰ 'ਤੇ ਵਧੀਆ ਢੰਗ ਚੁਣਦਾ ਹੈ
- **40+ ਬਿਲਟ-ਇਨ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਕੰਪੋਨੈਂਟਸ**: ਮਾਡਲ ਕੰਪ੍ਰੈਸ਼ਨ, ਕੁਆਂਟੀਜ਼ੇਸ਼ਨ, ਗ੍ਰਾਫ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਅਤੇ ਹੋਰ ਕਵਰ ਕਰਦਾ ਹੈ
- **ਆਸਾਨ CLI ਇੰਟਰਫੇਸ**: ਆਮ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਕੰਮਾਂ ਲਈ ਸਧਾਰਨ ਕਮਾਂਡ
- **ਮਲਟੀ-ਫਰੇਮਵਰਕ ਸਹਾਇਤਾ**: PyTorch, Hugging Face ਮਾਡਲ ਅਤੇ ONNX ਨਾਲ ਕੰਮ ਕਰਦਾ ਹੈ
- **ਪ੍ਰਸਿੱਧ ਮਾਡਲ ਸਹਾਇਤਾ**: ਓਲਿਵ ਆਟੋਮੈਟਿਕ ਤੌਰ 'ਤੇ Llama, Phi, Qwen, Gemma ਆਦਿ ਵਰਗੇ ਪ੍ਰਸਿੱਧ ਮਾਡਲ ਆਰਕੀਟੈਕਚਰਾਂ ਨੂੰ ਬਾਹਰ-ਬਾਕਸ ਓਪਟੀਮਾਈਜ਼ ਕਰ ਸਕਦਾ ਹੈ

### ਫਾਇਦੇ

- **ਵਿਕਾਸ ਸਮੇਂ ਵਿੱਚ ਕਮੀ**: ਵੱਖ-ਵੱਖ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਤਕਨੀਕਾਂ ਨਾਲ ਹੱਥੋਂ-ਹੱਥ ਤਜਰਬਾ ਕਰਨ ਦੀ ਲੋੜ ਨਹੀਂ
- **ਪ੍ਰਦਰਸ਼ਨ ਵਿੱਚ ਸੁਧਾਰ**: ਮਹੱਤਵਪੂਰਨ ਗਤੀਵਿਧੀਆਂ ਵਿੱਚ ਸੁਧਾਰ (ਕਈ ਮਾਮਲਿਆਂ ਵਿੱਚ 6x ਤੱਕ)
- **ਕਰਾਸ-ਪਲੇਟਫਾਰਮ ਡਿਪਲੌਇਮੈਂਟ**: ਓਪਟੀਮਾਈਜ਼ ਕੀਤੇ ਮਾਡਲ ਵੱਖ-ਵੱਖ ਹਾਰਡਵੇਅਰ ਅਤੇ ਓਪਰੇਟਿੰਗ ਸਿਸਟਮਾਂ 'ਤੇ ਕੰਮ ਕਰਦੇ ਹਨ
- **ਸ਼ੁੱਧਤਾ ਬਰਕਰਾਰ**: ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਮਾਡਲ ਦੀ ਗੁਣਵੱਤਾ ਨੂੰ ਬਰਕਰਾਰ ਰੱਖਦੇ ਹੋਏ ਪ੍ਰਦਰਸ਼ਨ ਵਿੱਚ ਸੁਧਾਰ ਕਰਦੇ ਹਨ

## ਇੰਸਟਾਲੇਸ਼ਨ

### ਪੂਰਵ ਸ਼ਰਤਾਂ

- Python 3.8 ਜਾਂ ਇਸ ਤੋਂ ਉੱਚਾ
- pip ਪੈਕੇਜ ਮੈਨੇਜਰ
- ਵਰਚੁਅਲ ਵਾਤਾਵਰਣ (ਸਿਫਾਰਸ਼ੀ)

### ਬੁਨਿਆਦੀ ਇੰਸਟਾਲੇਸ਼ਨ

ਵਰਚੁਅਲ ਵਾਤਾਵਰਣ ਬਣਾਓ ਅਤੇ ਐਕਟੀਵੇਟ ਕਰੋ:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

ਓਲਿਵ ਨੂੰ ਆਟੋ-ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਨਾਲ ਇੰਸਟਾਲ ਕਰੋ:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### ਵਿਕਲਪਿਕ ਡਿਪੈਂਡੈਂਸੀਜ਼

ਓਲਿਵ ਵਾਧੂ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ ਲਈ ਵੱਖ-ਵੱਖ ਵਿਕਲਪਿਕ ਡਿਪੈਂਡੈਂਸੀਜ਼ ਪੇਸ਼ ਕਰਦਾ ਹੈ:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### ਇੰਸਟਾਲੇਸ਼ਨ ਦੀ ਪੁਸ਼ਟੀ ਕਰੋ

```bash
olive --help
```

ਜੇ ਸਫਲ ਹੋਵੇ, ਤਾਂ ਤੁਹਾਨੂੰ ਓਲਿਵ CLI ਮਦਦ ਸੁਨੇਹਾ ਦਿਖਾਈ ਦੇਵੇਗਾ।

## ਤੁਰੰਤ ਸ਼ੁਰੂਆਤ ਗਾਈਡ

### ਤੁਹਾਡਾ ਪਹਿਲਾ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ

ਆਓ ਓਲਿਵ ਦੇ ਆਟੋ-ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਵਿਸ਼ੇਸ਼ਤਾ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਇੱਕ ਛੋਟੇ ਭਾਸ਼ਾ ਮਾਡਲ ਨੂੰ ਓਪਟੀਮਾਈਜ਼ ਕਰੀਏ:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ਇਹ ਕਮਾਂਡ ਕੀ ਕਰਦੀ ਹੈ

ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਪ੍ਰਕਿਰਿਆ ਵਿੱਚ ਸ਼ਾਮਲ ਹੈ: ਮਾਡਲ ਨੂੰ ਸਥਾਨਕ ਕੈਸ਼ ਤੋਂ ਪ੍ਰਾਪਤ ਕਰਨਾ, ONNX ਗ੍ਰਾਫ ਨੂੰ ਕੈਪਚਰ ਕਰਨਾ ਅਤੇ ONNX ਡਾਟਾ ਫਾਈਲ ਵਿੱਚ ਵਜ਼ਨ ਸਟੋਰ ਕਰਨਾ, ONNX ਗ੍ਰਾਫ ਨੂੰ ਓਪਟੀਮਾਈਜ਼ ਕਰਨਾ, ਅਤੇ RTN ਵਿਧੀ ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਮਾਡਲ ਨੂੰ int4 ਵਿੱਚ ਕੁਆਂਟੀਜ਼ ਕਰਨਾ।

### ਕਮਾਂਡ ਪੈਰਾਮੀਟਰਾਂ ਦੀ ਵਿਆਖਿਆ

- `--model_name_or_path`: Hugging Face ਮਾਡਲ ਆਈਡੈਂਟੀਫਾਇਰ ਜਾਂ ਸਥਾਨਕ ਪਾਥ
- `--output_path`: ਡਾਇਰੈਕਟਰੀ ਜਿੱਥੇ ਓਪਟੀਮਾਈਜ਼ ਕੀਤੇ ਮਾਡਲ ਨੂੰ ਸੇਵ ਕੀਤਾ ਜਾਵੇਗਾ
- `--device`: ਟਾਰਗੇਟ ਡਿਵਾਈਸ (cpu, gpu)
- `--provider`: ਐਗਜ਼ਿਕਿਊਸ਼ਨ ਪ੍ਰੋਵਾਈਡਰ (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ONNX Runtime Generate AI ਨੂੰ ਇੰਫਰੈਂਸ ਲਈ ਵਰਤੋ
- `--precision`: ਕੁਆਂਟੀਜ਼ੇਸ਼ਨ ਸ਼ੁੱਧਤਾ (int4, int8, fp16)
- `--log_level`: ਲੌਗਿੰਗ ਦੀ ਗਤੀ (0=minimal, 1=verbose)

## ਉਦਾਹਰਨ: Qwen3 ਨੂੰ ONNX INT4 ਵਿੱਚ ਬਦਲਣਾ

Hugging Face ਦੇ ਦਿੱਤੇ ਉਦਾਹਰਨ ਦੇ ਅਧਾਰ 'ਤੇ [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), ਇੱਥੇ Qwen3 ਮਾਡਲ ਨੂੰ ਓਪਟੀਮਾਈਜ਼ ਕਰਨ ਦਾ ਤਰੀਕਾ ਦਿੱਤਾ ਗਿਆ ਹੈ:

### ਕਦਮ 1: ਮਾਡਲ ਡਾਊਨਲੋਡ ਕਰੋ (ਵਿਕਲਪਿਕ)

ਡਾਊਨਲੋਡ ਸਮੇਂ ਨੂੰ ਘਟਾਉਣ ਲਈ, ਸਿਰਫ਼ ਜ਼ਰੂਰੀ ਫਾਈਲਾਂ ਕੈਸ਼ ਕਰੋ:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### ਕਦਮ 2: Qwen3 ਮਾਡਲ ਨੂੰ ਓਪਟੀਮਾਈਜ਼ ਕਰੋ

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ਕਦਮ 3: ਓਪਟੀਮਾਈਜ਼ ਕੀਤੇ ਮਾਡਲ ਦੀ ਜਾਂਚ ਕਰੋ

ਤੁਹਾਡੇ ਓਪਟੀਮਾਈਜ਼ ਕੀਤੇ ਮਾਡਲ ਦੀ ਜਾਂਚ ਕਰਨ ਲਈ ਇੱਕ ਸਧਾਰਨ Python ਸਕ੍ਰਿਪਟ ਬਣਾਓ:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### ਆਉਟਪੁੱਟ ਸਟ੍ਰਕਚਰ

ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਤੋਂ ਬਾਅਦ, ਤੁਹਾਡੀ ਆਉਟਪੁੱਟ ਡਾਇਰੈਕਟਰੀ ਵਿੱਚ ਇਹ ਸ਼ਾਮਲ ਹੋਵੇਗਾ:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## ਤਕਨੀਕੀ ਵਰਤੋਂ

### ਕਨਫਿਗਰੇਸ਼ਨ ਫਾਈਲਾਂ

ਵਧੇਰੇ ਜਟਿਲ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਵਰਕਫਲੋਜ਼ ਲਈ, ਤੁਸੀਂ JSON ਕਨਫਿਗਰੇਸ਼ਨ ਫਾਈਲਾਂ ਦੀ ਵਰਤੋਂ ਕਰ ਸਕਦੇ ਹੋ:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

ਕਨਫਿਗਰੇਸ਼ਨ ਨਾਲ ਚਲਾਓ:

```bash
olive run --config config.json
```

### GPU ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ

CUDA GPU ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਲਈ:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (Windows) ਲਈ:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ਓਲਿਵ ਨਾਲ ਫਾਈਨ-ਟਿਊਨਿੰਗ

ਓਲਿਵ ਮਾਡਲਾਂ ਨੂੰ ਫਾਈਨ-ਟਿਊਨ ਕਰਨ ਦਾ ਸਮਰਥਨ ਕਰਦਾ ਹੈ:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## ਸਰਵੋਤਮ ਅਭਿਆਸ

### 1. ਮਾਡਲ ਚੋਣ
- ਟੈਸਟਿੰਗ ਲਈ ਛੋਟੇ ਮਾਡਲਾਂ ਨਾਲ ਸ਼ੁਰੂ ਕਰੋ (ਜਿਵੇਂ ਕਿ 0.5B-7B ਪੈਰਾਮੀਟਰ)
- ਯਕੀਨੀ ਬਣਾਓ ਕਿ ਤੁਹਾਡਾ ਟਾਰਗੇਟ ਮਾਡਲ ਆਰਕੀਟੈਕਚਰ ਓਲਿਵ ਦੁਆਰਾ ਸਹਾਇਕ ਹੈ

### 2. ਹਾਰਡਵੇਅਰ ਵਿਚਾਰ
- ਆਪਣੇ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਟਾਰਗੇਟ ਨੂੰ ਆਪਣੇ ਡਿਪਲੌਇਮੈਂਟ ਹਾਰਡਵੇਅਰ ਨਾਲ ਮੇਲ ਕਰੋ
- ਜੇ ਤੁਹਾਡੇ ਕੋਲ CUDA-ਅਨੁਕੂਲ ਹਾਰਡਵੇਅਰ ਹੈ ਤਾਂ GPU ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਦੀ ਵਰਤੋਂ ਕਰੋ
- Windows ਮਸ਼ੀਨਾਂ ਲਈ DirectML 'ਤੇ ਵਿਚਾਰ ਕਰੋ

### 3. ਸ਼ੁੱਧਤਾ ਦੀ ਚੋਣ
- **INT4**: ਵੱਧ ਤੋਂ ਵੱਧ ਕੰਪ੍ਰੈਸ਼ਨ, ਥੋੜ੍ਹਾ ਜਿਹਾ ਸ਼ੁੱਧਤਾ ਘਟਾਉਣਾ
- **INT8**: ਆਕਾਰ ਅਤੇ ਸ਼ੁੱਧਤਾ ਦਾ ਵਧੀਆ ਸੰਤੁਲਨ
- **FP16**: ਘੱਟੋ-ਘੱਟ ਸ਼ੁੱਧਤਾ ਘਟਾਉਣਾ, ਮੋਡਰੇਟ ਆਕਾਰ ਘਟਾਉਣਾ

### 4. ਟੈਸਟਿੰਗ ਅਤੇ ਵੈਰੀਫਿਕੇਸ਼ਨ
- ਹਮੇਸ਼ਾ ਆਪਣੇ ਵਿਸ਼ੇਸ਼ ਵਰਤੋਂ ਦੇ ਕੇਸਾਂ ਨਾਲ ਓਪਟੀਮਾਈਜ਼ ਕੀਤੇ ਮਾਡਲਾਂ ਦੀ ਜਾਂਚ ਕਰੋ
- ਪ੍ਰਦਰਸ਼ਨ ਮਾਪ (ਲੇਟੈਂਸੀ, ਥਰੂਪੁੱਟ, ਸ਼ੁੱਧਤਾ) ਦੀ ਤੁਲਨਾ ਕਰੋ
- ਮੁਲਾਂਕਣ ਲਈ ਪ੍ਰਤੀਨਿਧੀ ਇਨਪੁਟ ਡਾਟਾ ਦੀ ਵਰਤੋਂ ਕਰੋ

### 5. ਦੁਹਰਾਈ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ
- ਤੇਜ਼ ਨਤੀਜਿਆਂ ਲਈ ਆਟੋ-ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਨਾਲ ਸ਼ੁਰੂ ਕਰੋ
- ਸੁਖਾਲੇ ਕੰਟਰੋਲ ਲਈ ਕਨਫਿਗਰੇਸ਼ਨ ਫਾਈਲਾਂ ਦੀ ਵਰਤੋਂ ਕਰੋ
- ਵੱਖ-ਵੱਖ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਪਾਸਾਂ ਨਾਲ ਤਜਰਬਾ ਕਰੋ

## ਮੁਸ਼ਕਲਾਂ ਦਾ ਹੱਲ

### ਆਮ ਸਮੱਸਿਆਵਾਂ

#### 1. ਇੰਸਟਾਲੇਸ਼ਨ ਸਮੱਸਿਆਵਾਂ
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU ਸਮੱਸਿਆਵਾਂ
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. ਮੈਮੋਰੀ ਸਮੱਸਿਆਵਾਂ
- ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਦੌਰਾਨ ਛੋਟੇ ਬੈਚ ਆਕਾਰ ਦੀ ਵਰਤੋਂ ਕਰੋ
- ਪਹਿਲਾਂ ਉੱਚ ਸ਼ੁੱਧਤਾ ਨਾਲ ਕੁਆਂਟੀਜ਼ੇਸ਼ਨ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰੋ (int8 ਬਜਾਏ int4)
- ਮਾਡਲ ਕੈਸ਼ਿੰਗ ਲਈ ਕਾਫ਼ੀ ਡਿਸਕ ਸਪੇਸ ਯਕੀਨੀ ਬਣਾਓ

#### 4. ਮਾਡਲ ਲੋਡਿੰਗ ਗਲਤੀਆਂ
- ਮਾਡਲ ਪਾਥ ਅਤੇ ਐਕਸੈਸ ਅਧਿਕਾਰਾਂ ਦੀ ਪੁਸ਼ਟੀ ਕਰੋ
- ਜਾਂਚੋ ਕਿ ਮਾਡਲ ਨੂੰ `trust_remote_code=True` ਦੀ ਲੋੜ ਹੈ
- ਯਕੀਨੀ ਬਣਾਓ ਕਿ ਸਾਰੇ ਜ਼ਰੂਰੀ ਮਾਡਲ ਫਾਈਲਾਂ ਡਾਊਨਲੋਡ ਕੀਤੀਆਂ ਗਈਆਂ ਹਨ

### ਮਦਦ ਪ੍ਰਾਪਤ ਕਰਨਾ

- **ਡਾਕੂਮੈਂਟੇਸ਼ਨ**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Examples**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## ਓਲਿਵ ਰੇਸਿਪੀਜ਼ ਰਿਪੋਜ਼ਟਰੀ

### ਓਲਿਵ ਰੇਸਿਪੀਜ਼ ਦਾ ਪਰੀਚੇ

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) ਰਿਪੋਜ਼ਟਰੀ ਮੁੱਖ ਓਲਿਵ ਟੂਲਕਿਟ ਨੂੰ ਪੂਰਾ ਕਰਦਾ ਹੈ ਅਤੇ ਪ੍ਰਸਿੱਧ AI ਮਾਡਲਾਂ ਲਈ ਤਿਆਰ-ਵਰਤੋਂ ਯੋਗ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਰੇਸਿਪੀਜ਼ ਦਾ ਵਿਸਤ੍ਰਿਤ ਸੰਗ੍ਰਹਿ ਪ੍ਰਦਾਨ ਕਰਦਾ ਹੈ। ਇਹ ਰਿਪੋਜ਼ਟਰੀ ਜਨਤਕ ਤੌਰ 'ਤੇ ਉਪਲਬਧ ਮਾਡਲਾਂ ਨੂੰ ਓਪਟੀਮਾਈਜ਼ ਕਰਨ ਅਤੇ ਮਾਲਕਾਨਾ ਮਾਡਲਾਂ ਲਈ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਵਰਕਫਲੋਜ਼ ਬਣਾਉਣ ਲਈ ਇੱਕ ਵਿਵਹਾਰਕ ਸੰਦਰਭ ਵਜੋਂ ਕੰਮ ਕਰਦਾ ਹੈ।

### ਮੁੱਖ ਵਿਸ਼ੇਸ਼ਤਾਵਾਂ

- **100+ ਪ੍ਰੀ-ਬਿਲਟ ਰੇਸਿਪੀਜ਼**: ਪ੍ਰਸਿੱਧ ਮਾਡਲਾਂ ਲਈ ਤਿਆਰ-ਵਰਤੋਂ ਯੋਗ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਕਨਫਿਗਰੇਸ਼ਨ
- **ਮਲਟੀ-ਆਰਕੀਟੈਕਚਰ ਸਹਾਇਤਾ**: ਟ੍ਰਾਂਸਫਾਰਮਰ ਮਾਡਲਾਂ, ਵਿਜ਼ਨ ਮਾਡਲਾਂ ਅਤੇ ਮਲਟੀਮੋਡਲ ਆਰਕੀਟੈਕਚਰਾਂ ਨੂੰ ਕਵਰ ਕਰਦਾ ਹੈ
- **ਹਾਰਡਵੇਅਰ-ਵਿਸ਼ੇਸ਼ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ**: CPU, GPU ਅਤੇ ਵਿਸ਼ੇਸ਼ ਐਕਸੈਲੇਰੇਟਰਾਂ ਲਈ ਰੇਸਿਪੀਜ਼
- **ਪ੍ਰਸਿੱਧ ਮਾਡਲ ਪਰਿਵਾਰ**: Phi, Llama, Qwen, Gemma, Mistral ਅਤੇ ਹੋਰ ਬਹੁਤ ਕੁਝ ਸ਼ਾਮਲ ਹਨ

### ਸਹਾਇਕ ਮਾਡਲ ਪਰਿਵਾਰ

ਰਿਪੋਜ਼ਟਰੀ ਵਿੱਚ ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ ਰੇਸਿਪੀਜ਼ ਸ਼ਾਮਲ ਹਨ:

#### ਭਾਸ਼ਾ ਮਾਡਲ
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 series (0.5B to 14B)
- **Google Gemma**: ਵੱਖ-ਵੱਖ Gemma ਮਾਡਲ ਕਨਫਿਗਰੇਸ਼ਨ
- **Mistral AI**: Mistral-7B series
- **DeepSeek**: R1-Distill series ਮਾਡਲ

#### ਵਿਜ਼ਨ ਅਤੇ ਮਲਟੀਮੋਡਲ ਮਾਡਲ
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP ਮਾਡਲ**: ਵੱਖ-ਵੱਖ CLIP-ViT ਕਨਫਿਗਰੇਸ਼ਨ
- **ResNet**: ResNet-50 ਓਪਟੀਮਾਈਜ਼ੇਸ਼ਨ
- **ਵਿਜ਼ਨ ਟ੍ਰਾਂਸਫਾਰਮਰ**: ViT-base-patch16-224

#### ਵਿਸ਼ੇਸ਼ ਮਾਡਲ
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: ਬੇਸ ਅਤੇ ਮਲਟੀਲਿੰਗੁਅਲ ਵੈਰੀਐਂਟਸ
- **Sentence Transformers**: all-MiniLM-L6-v2

### ਓਲਿਵ ਰੇਸਿਪੀਜ਼ ਦੀ ਵਰਤੋਂ



---

**ਅਸਵੀਕਰਤਾ**:  
ਇਹ ਦਸਤਾਵੇਜ਼ AI ਅਨੁਵਾਦ ਸੇਵਾ [Co-op Translator](https://github.com/Azure/co-op-translator) ਦੀ ਵਰਤੋਂ ਕਰਕੇ ਅਨੁਵਾਦ ਕੀਤਾ ਗਿਆ ਹੈ। ਜਦੋਂ ਕਿ ਅਸੀਂ ਸਹੀ ਹੋਣ ਦੀ ਕੋਸ਼ਿਸ਼ ਕਰਦੇ ਹਾਂ, ਕਿਰਪਾ ਕਰਕੇ ਧਿਆਨ ਦਿਓ ਕਿ ਸਵੈਚਾਲਿਤ ਅਨੁਵਾਦਾਂ ਵਿੱਚ ਗਲਤੀਆਂ ਜਾਂ ਅਸੁੱਤੀਆਂ ਹੋ ਸਕਦੀਆਂ ਹਨ। ਇਸ ਦੀ ਮੂਲ ਭਾਸ਼ਾ ਵਿੱਚ ਮੂਲ ਦਸਤਾਵੇਜ਼ ਨੂੰ ਅਧਿਕਾਰਤ ਸਰੋਤ ਮੰਨਿਆ ਜਾਣਾ ਚਾਹੀਦਾ ਹੈ। ਮਹੱਤਵਪੂਰਨ ਜਾਣਕਾਰੀ ਲਈ, ਪੇਸ਼ੇਵਰ ਮਨੁੱਖੀ ਅਨੁਵਾਦ ਦੀ ਸਿਫਾਰਸ਼ ਕੀਤੀ ਜਾਂਦੀ ਹੈ। ਇਸ ਅਨੁਵਾਦ ਦੀ ਵਰਤੋਂ ਤੋਂ ਪੈਦਾ ਹੋਣ ਵਾਲੇ ਕਿਸੇ ਵੀ ਗਲਤਫਹਿਮੀ ਜਾਂ ਗਲਤ ਵਿਆਖਿਆ ਲਈ ਅਸੀਂ ਜ਼ਿੰਮੇਵਾਰ ਨਹੀਂ ਹਾਂ।