<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "44bc28c27b993c5fb988791b7a115705",
  "translation_date": "2025-10-30T12:08:55+00:00",
  "source_file": "Module06/01.IntroduceAgent.md",
  "language_code": "pa"
}
-->
# AI à¨à¨œà©°à¨Ÿ à¨…à¨¤à©‡ à¨›à©‹à¨Ÿà©‡ à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨²: à¨‡à©±à¨• à¨µà¨¿à¨¸à¨¤à©à¨°à¨¿à¨¤ à¨—à¨¾à¨ˆà¨¡

## à¨ªà¨°à¨¿à¨šà¨¯

à¨‡à¨¸ à¨Ÿà¨¿à¨Šà¨Ÿà©‹à¨°à¨¿à¨…à¨² à¨µà¨¿à©±à¨š, à¨…à¨¸à©€à¨‚ AI à¨à¨œà©°à¨Ÿ à¨…à¨¤à©‡ à¨›à©‹à¨Ÿà©‡ à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨² (SLMs) à¨…à¨¤à©‡ à¨‰à¨¨à©à¨¹à¨¾à¨‚ à¨¦à©‡ à¨‰à©±à¨š-à¨¤à¨•à¨¨à©€à¨•à©€ à¨²à¨¾à¨—à©‚ à¨•à¨°à¨¨ à¨¦à©‡ à¨¤à¨°à©€à¨•à¨¿à¨†à¨‚ à¨¦à©€ à¨ªà©œà¨šà©‹à¨² à¨•à¨°à¨¾à¨‚à¨—à©‡, à¨–à¨¾à¨¸ à¨•à¨°à¨•à©‡ à¨à¨œ à¨•à©°à¨ªà¨¿à¨Šà¨Ÿà¨¿à©°à¨— à¨µà¨¾à¨¤à¨¾à¨µà¨°à¨£à¨¾à¨‚ à¨²à¨ˆà¥¤ à¨…à¨¸à©€à¨‚ à¨à¨œà©°à¨Ÿà¨¿à¨• AI à¨¦à©‡ à¨®à©‚à¨² à¨§à¨¾à¨°à¨¨à¨¾, SLM à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨ à¨¤à¨•à¨¨à©€à¨•à¨¾à¨‚, à¨¸à©°à¨¸à¨¾à¨§à¨¨-à¨¸à©€à¨®à¨¿à¨¤ à¨¡à¨¿à¨µà¨¾à¨ˆà¨¸à¨¾à¨‚ à¨²à¨ˆ à¨µà¨¿à¨¹à¨¾à¨°à¨• à¨¤à¨°à©€à¨•à©‡, à¨…à¨¤à©‡ à¨‰à¨¤à¨ªà¨¾à¨¦à¨¨-à¨¤à¨¿à¨†à¨° à¨à¨œà©°à¨Ÿ à¨¸à¨¿à¨¸à¨Ÿà¨® à¨¬à¨£à¨¾à¨‰à¨£ à¨²à¨ˆ Microsoft Agent Framework à¨¨à©‚à©° à¨•à¨µà¨° à¨•à¨°à¨¾à¨‚à¨—à©‡à¥¤

à¨•à©à¨°à¨¿à¨¤à©à¨°à¨¿à¨® à¨¬à©à©±à¨§à©€ à¨¦à¨¾ à¨–à©‡à¨¤à¨° 2025 à¨µà¨¿à©±à¨š à¨‡à©±à¨• à¨¨à¨µà©€à¨‚ à¨¦à¨¿à¨¸à¨¼à¨¾ à¨µà©±à¨² à¨µà¨§ à¨°à¨¿à¨¹à¨¾ à¨¹à©ˆà¥¤ à¨œà¨¿à©±à¨¥à©‡ 2023 à¨šà©ˆà¨Ÿà¨¬à©‹à¨Ÿà¨¸ à¨¦à¨¾ à¨¸à¨¾à¨² à¨¸à©€ à¨…à¨¤à©‡ 2024 à¨µà¨¿à©±à¨š à¨•à©‹à¨ªà¨¾à¨‡à¨²à¨Ÿà¨¸ à¨¦à¨¾ à¨¬à©‚à¨® à¨¸à©€, 2025 AI à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¦à¨¾ à¨¸à¨¾à¨² à¨¹à©ˆ â€” à¨¬à©à©±à¨§à©€à¨®à¨¾à¨¨ à¨¸à¨¿à¨¸à¨Ÿà¨® à¨œà©‹ à¨¸à©‹à¨šà¨¦à©‡ à¨¹à¨¨, à¨¯à©‹à¨œà¨¨à¨¾ à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à©‡ à¨¹à¨¨, à¨¸à©°à¨¦à¨¾à¨‚ à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨•à¨°à¨¦à©‡ à¨¹à¨¨, à¨…à¨¤à©‡ à¨˜à©±à¨Ÿ à¨¤à©‹à¨‚ à¨˜à©±à¨Ÿ à¨®à¨¨à©à©±à¨–à©€ à¨¦à¨–à¨² à¨¨à¨¾à¨² à¨•à©°à¨® à¨•à¨°à¨¦à©‡ à¨¹à¨¨à¥¤ à¨‡à¨¹ à¨¸à¨¿à¨¸à¨Ÿà¨® à¨¬à¨¹à©à¨¤ à¨¹à©±à¨¦ à¨¤à©±à¨• à¨•à©à¨¸à¨¼à¨² à¨›à©‹à¨Ÿà©‡ à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨²à¨¾à¨‚ à¨¦à©à¨†à¨°à¨¾ à¨¸à©°à¨šà¨¾à¨²à¨¿à¨¤ à¨¹à©à©°à¨¦à©‡ à¨¹à¨¨à¥¤ Microsoft Agent Framework à¨à¨œ-à¨…à¨§à¨¾à¨°à¨¿à¨¤ à¨¸à¨®à¨°à©±à¨¥à¨¾à¨µà¨¾à¨‚ à¨¨à¨¾à¨² à¨‡à¨¹ à¨¬à©à©±à¨§à©€à¨®à¨¾à¨¨ à¨¸à¨¿à¨¸à¨Ÿà¨® à¨¬à¨£à¨¾à¨‰à¨£ à¨²à¨ˆ à¨‡à©±à¨• à¨…à¨—à©‡à¨¤à¨°à©€ à¨¹à©±à¨² à¨µà¨œà©‹à¨‚ à¨‰à¨­à¨° à¨°à¨¿à¨¹à¨¾ à¨¹à©ˆà¥¤

## à¨¸à¨¿à©±à¨–à¨£ à¨¦à©‡ à¨‰à¨¦à©‡à¨¸à¨¼

à¨‡à¨¸ à¨Ÿà¨¿à¨Šà¨Ÿà©‹à¨°à¨¿à¨…à¨² à¨¦à©‡ à¨…à©°à¨¤ à¨¤à©±à¨•, à¨¤à©à¨¸à©€à¨‚ à¨‡à¨¹ à¨¸à¨®à¨à¨£ à¨¦à©‡ à¨¯à©‹à¨— à¨¹à©‹à¨µà©‹à¨—à©‡:

- ðŸ¤– AI à¨à¨œà©°à¨Ÿ à¨…à¨¤à©‡ à¨à¨œà©°à¨Ÿà¨¿à¨• à¨¸à¨¿à¨¸à¨Ÿà¨®à¨¾à¨‚ à¨¦à©€à¨†à¨‚ à¨®à©‚à¨² à¨§à¨¾à¨°à¨¨à¨¾à¨µà¨¾à¨‚ à¨¨à©‚à©° à¨¸à¨®à¨à©‹
- ðŸ”¬ à¨à¨œà©°à¨Ÿà¨¿à¨• à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨›à©‹à¨Ÿà©‡ à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨²à¨¾à¨‚ à¨¦à©‡ à¨µà©±à¨¡à©‡ à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨²à¨¾à¨‚ à¨‰à©±à¨¤à©‡ à¨«à¨¾à¨‡à¨¦à©‡ à¨ªà¨›à¨¾à¨£à©‹
- ðŸš€ à¨à¨œ à¨•à©°à¨ªà¨¿à¨Šà¨Ÿà¨¿à©°à¨— à¨µà¨¾à¨¤à¨¾à¨µà¨°à¨£à¨¾à¨‚ à¨²à¨ˆ à¨‰à©±à¨š-à¨¤à¨•à¨¨à©€à¨•à©€ SLM à¨¤à¨°à©€à¨•à¨¿à¨†à¨‚ à¨¨à©‚à©° à¨¸à¨¿à©±à¨–à©‹
- ðŸ“± à¨…à¨¸à¨² à¨¦à©à¨¨à©€à¨† à¨¦à©‡ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨²à¨ˆ à¨µà¨¿à¨¹à¨¾à¨°à¨• SLM-à¨¸à©°à¨šà¨¾à¨²à¨¿à¨¤ à¨à¨œà©°à¨Ÿ à¨²à¨¾à¨—à©‚ à¨•à¨°à©‹
- ðŸ—ï¸ Microsoft Agent Framework à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨•à¨°à¨•à©‡ à¨‰à¨¤à¨ªà¨¾à¨¦à¨¨-à¨¤à¨¿à¨†à¨° à¨à¨œà©°à¨Ÿ à¨¬à¨£à¨¾à¨“
- ðŸŒ à¨¸à¨¥à¨¾à¨¨à¨• LLM à¨…à¨¤à©‡ SLM à¨‡à©°à¨Ÿà©€à¨—à©à¨°à©‡à¨¸à¨¼à¨¨ à¨¨à¨¾à¨² à¨à¨œ-à¨…à¨§à¨¾à¨°à¨¿à¨¤ à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¨à©‚à©° à¨¡à¨¿à¨ªà¨²à©Œà¨‡ à¨•à¨°à©‹
- ðŸ”§ Microsoft Agent Framework à¨¨à©‚à©° Foundry Local à¨¨à¨¾à¨² à¨à¨œ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨²à¨ˆ à¨‡à©°à¨Ÿà©€à¨—à©à¨°à©‡à¨Ÿ à¨•à¨°à©‹

## AI à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¨à©‚à©° à¨¸à¨®à¨à¨£à¨¾: à¨¬à©à¨¨à¨¿à¨†à¨¦à¨¾à¨‚ à¨…à¨¤à©‡ à¨µà¨°à¨—à©€à¨•à¨°à¨¨

### à¨ªà¨°à¨¿à¨­à¨¾à¨¸à¨¼à¨¾ à¨…à¨¤à©‡ à¨®à©à©±à¨– à¨§à¨¾à¨°à¨¨à¨¾à¨µà¨¾à¨‚

à¨•à©à¨°à¨¿à¨¤à©à¨°à¨¿à¨® à¨¬à©à©±à¨§à©€ (AI) à¨à¨œà©°à¨Ÿ à¨‡à©±à¨• à¨¸à¨¿à¨¸à¨Ÿà¨® à¨œà¨¾à¨‚ à¨ªà©à¨°à©‹à¨—à¨°à¨¾à¨® à¨¨à©‚à©° à¨¦à¨°à¨¸à¨¾à¨‰à¨‚à¨¦à¨¾ à¨¹à©ˆ à¨œà©‹ à¨•à¨¿à¨¸à©‡ à¨‰à¨ªà¨­à©‹à¨—à¨¤à¨¾ à¨œà¨¾à¨‚ à¨•à¨¿à¨¸à©‡ à¨¹à©‹à¨° à¨¸à¨¿à¨¸à¨Ÿà¨® à¨¦à©€ à¨¥à¨¾à¨‚ à¨•à©°à¨® à¨•à¨°à¨¨ à¨¦à©‡ à¨¯à©‹à¨— à¨¹à©à©°à¨¦à¨¾ à¨¹à©ˆà¥¤ à¨‡à¨¹ à¨†à¨ªà¨£à¨¾ à¨•à©°à¨®à¨•à¨¾à¨œ à¨¡à¨¿à¨œà¨¼à¨¾à¨ˆà¨¨ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ à¨…à¨¤à©‡ à¨‰à¨ªà¨²à¨¬à¨§ à¨¸à©°à¨¦à¨¾à¨‚ à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆà¥¤ à¨°à¨µà¨¾à¨‡à¨¤à©€ AI à¨¦à©‡ à¨µà¨¿à¨°à©à©±à¨§, à¨œà©‹ à¨¸à¨¿à¨°à¨« à¨¤à©à¨¹à¨¾à¨¡à©‡ à¨¸à¨µà¨¾à¨²à¨¾à¨‚ à¨¦à¨¾ à¨œà¨µà¨¾à¨¬ à¨¦à¨¿à©°à¨¦à¨¾ à¨¹à©ˆ, à¨‡à©±à¨• à¨à¨œà©°à¨Ÿ à¨†à¨ªà¨£à©‡ à¨†à¨ª à¨•à©°à¨® à¨•à¨°à¨¨ à¨¦à©‡ à¨¯à©‹à¨— à¨¹à©à©°à¨¦à¨¾ à¨¹à©ˆà¥¤

### à¨à¨œà©°à¨Ÿ à¨µà¨°à¨—à©€à¨•à¨°à¨¨ à¨«à¨°à©‡à¨®à¨µà¨°à¨•

à¨à¨œà©°à¨Ÿ à¨¦à©€à¨†à¨‚ à¨¹à©±à¨¦à¨¾à¨‚ à¨¨à©‚à©° à¨¸à¨®à¨à¨£à¨¾ à¨µà©±à¨–-à¨µà©±à¨– à¨•à©°à¨ªà¨¿à¨Šà¨Ÿà¨¿à©°à¨— à¨¸à¨¥à¨¿à¨¤à©€à¨†à¨‚ à¨²à¨ˆ à¨¸à¨¹à©€ à¨à¨œà©°à¨Ÿ à¨¦à©€ à¨šà©‹à¨£ à¨µà¨¿à©±à¨š à¨®à¨¦à¨¦ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ:

- **ðŸ”¬ à¨¸à¨§à¨¾à¨°à¨¨ à¨°à¨¿à¨«à¨²à©ˆà¨•à¨¸ à¨à¨œà©°à¨Ÿ**: à¨¨à¨¿à¨¯à¨®-à¨…à¨§à¨¾à¨°à¨¿à¨¤ à¨¸à¨¿à¨¸à¨Ÿà¨® à¨œà©‹ à¨¤à©à¨°à©°à¨¤ à¨§à¨¾à¨°à¨¨à¨¾à¨µà¨¾à¨‚ à¨¦à¨¾ à¨œà¨µà¨¾à¨¬ à¨¦à¨¿à©°à¨¦à©‡ à¨¹à¨¨ (à¨¥à¨°à¨®à©‹à¨¸à¨Ÿà©ˆà¨Ÿ, à¨¬à©à¨¨à¨¿à¨†à¨¦à©€ à¨†à¨Ÿà©‹à¨®à©‡à¨¸à¨¼à¨¨)
- **ðŸ“± à¨®à¨¾à¨¡à¨²-à¨…à¨§à¨¾à¨°à¨¿à¨¤ à¨à¨œà©°à¨Ÿ**: à¨¸à¨¿à¨¸à¨Ÿà¨® à¨œà©‹ à¨…à©°à¨¦à¨°à©‚à¨¨à©€ à¨¸à¨¥à¨¿à¨¤à©€ à¨…à¨¤à©‡ à¨¯à¨¾à¨¦ à¨¨à©‚à©° à¨¬à¨£à¨¾à¨ˆ à¨°à©±à¨–à¨¦à©‡ à¨¹à¨¨ (à¨°à©‹à¨¬à©‹à¨Ÿ à¨µà©ˆà¨•à©‚à¨®, à¨¨à©ˆà¨µà©€à¨—à©‡à¨¸à¨¼à¨¨ à¨¸à¨¿à¨¸à¨Ÿà¨®)
- **âš–ï¸ à¨—à©‹à¨²-à¨…à¨§à¨¾à¨°à¨¿à¨¤ à¨à¨œà©°à¨Ÿ**: à¨¸à¨¿à¨¸à¨Ÿà¨® à¨œà©‹ à¨‰à¨¦à©‡à¨¸à¨¼à¨¾à¨‚ à¨¨à©‚à©° à¨ªà©‚à¨°à¨¾ à¨•à¨°à¨¨ à¨²à¨ˆ à¨¯à©‹à¨œà¨¨à¨¾ à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à©‡ à¨¹à¨¨ à¨…à¨¤à©‡ à¨•à©à¨°à¨®à¨¬à©±à¨§ à¨¤à¨°à©€à¨•à©‡ à¨¨à¨¾à¨² à¨•à©°à¨® à¨•à¨°à¨¦à©‡ à¨¹à¨¨ (à¨°à©‚à¨Ÿ à¨ªà¨²à©ˆà¨¨à¨°, à¨Ÿà¨¾à¨¸à¨• à¨¸à¨¼à©ˆà¨¡à¨¿à¨Šà¨²à¨°)
- **ðŸ§  à¨¸à¨¿à©±à¨–à¨£ à¨µà¨¾à¨²à©‡ à¨à¨œà©°à¨Ÿ**: à¨…à¨¨à©à¨•à©‚à¨² à¨¸à¨¿à¨¸à¨Ÿà¨® à¨œà©‹ à¨¸à¨®à©‡à¨‚ à¨¦à©‡ à¨¨à¨¾à¨² à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨µà¨¿à©±à¨š à¨¸à©à¨§à¨¾à¨° à¨•à¨°à¨¦à©‡ à¨¹à¨¨ (à¨¸à¨¿à¨«à¨¾à¨°à¨¸à¨¼à©€ à¨¸à¨¿à¨¸à¨Ÿà¨®, à¨¨à¨¿à©±à¨œà©€ à¨¸à¨¹à¨¾à¨‡à¨•)

### AI à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¦à©‡ à¨®à©à©±à¨– à¨«à¨¾à¨‡à¨¦à©‡

AI à¨à¨œà©°à¨Ÿ à¨•à¨ˆ à¨®à©à©±à¨– à¨«à¨¾à¨‡à¨¦à©‡ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à©‡ à¨¹à¨¨ à¨œà©‹ à¨‰à¨¨à©à¨¹à¨¾à¨‚ à¨¨à©‚à©° à¨à¨œ à¨•à©°à¨ªà¨¿à¨Šà¨Ÿà¨¿à©°à¨— à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨²à¨ˆ à¨†à¨¦à¨°à¨¸à¨¼ à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à©‡ à¨¹à¨¨:

**à¨¸à¨µà©ˆ-à¨šà¨¾à¨²à¨¿à¨¤ à¨•à©°à¨®à¨•à¨¾à¨œ**: à¨à¨œà©°à¨Ÿ à¨°à©€à¨…à¨²-à¨Ÿà¨¾à¨ˆà¨® à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨²à¨ˆ à¨˜à©±à¨Ÿ à¨®à¨¨à©à©±à¨–à©€ à¨¨à¨¿à¨—à¨°à¨¾à¨¨à©€ à¨¨à¨¾à¨² à¨¸à¨µà©ˆ-à¨šà¨¾à¨²à¨¿à¨¤ à¨•à©°à¨®à¨•à¨¾à¨œ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à©‡ à¨¹à¨¨à¥¤ à¨‡à¨¹ à¨¸à©°à¨¸à¨¾à¨§à¨¨-à¨¸à©€à¨®à¨¿à¨¤ à¨¡à¨¿à¨µà¨¾à¨ˆà¨¸à¨¾à¨‚ 'à¨¤à©‡ à¨˜à©±à¨Ÿ à¨“à¨ªà¨°à©‡à¨¸à¨¼à¨¨à¨² à¨“à¨µà¨°à¨¹à©ˆà©±à¨¡ à¨¨à¨¾à¨² à¨¤à©ˆà¨¨à¨¾à¨¤à©€ à¨¯à©‹à¨— à¨¹à¨¨à¥¤

**à¨¤à©ˆà¨¨à¨¾à¨¤à©€ à¨¦à©€ à¨²à¨šà©€à¨²à¨¤à¨¾**: à¨‡à¨¹ à¨¸à¨¿à¨¸à¨Ÿà¨® à¨‡à©°à¨Ÿà¨°à¨¨à©ˆà¨Ÿ à¨•à¨¨à©ˆà¨•à¨Ÿà¨¿à¨µà¨¿à¨Ÿà©€ à¨¦à©€ à¨²à©‹à©œ à¨¬à¨¿à¨¨à¨¾à¨‚ à¨¡à¨¿à¨µà¨¾à¨ˆà¨¸-à¨…à¨§à¨¾à¨°à¨¿à¨¤ AI à¨¸à¨®à¨°à©±à¨¥à¨¾à¨µà¨¾à¨‚ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à©‡ à¨¹à¨¨, à¨¸à¨¥à¨¾à¨¨à¨• à¨ªà©à¨°à©‹à¨¸à©ˆà¨¸à¨¿à©°à¨— à¨¦à©à¨†à¨°à¨¾ à¨—à©‹à¨ªà¨¨à©€à¨¯à¨¤à¨¾ à¨…à¨¤à©‡ à¨¸à©à¨°à©±à¨–à¨¿à¨† à¨¨à©‚à©° à¨µà¨§à¨¾à¨‰à¨‚à¨¦à©‡ à¨¹à¨¨, à¨…à¨¤à©‡ à¨µà©±à¨–-à¨µà©±à¨– à¨à¨œ à¨•à©°à¨ªà¨¿à¨Šà¨Ÿà¨¿à©°à¨— à¨µà¨¾à¨¤à¨¾à¨µà¨°à¨£à¨¾à¨‚ à¨²à¨ˆ à¨¯à©‹à¨— à¨¹à¨¨à¥¤

**à¨²à¨¾à¨—à¨¤ à¨¦à©€ à¨•à©à¨¸à¨¼à¨²à¨¤à¨¾**: à¨à¨œà©°à¨Ÿ à¨¸à¨¿à¨¸à¨Ÿà¨® à¨•à¨²à¨¾à¨‰à¨¡-à¨…à¨§à¨¾à¨°à¨¿à¨¤ à¨¹à©±à¨²à¨¾à¨‚ à¨¦à©‡ à¨®à©à¨•à¨¾à¨¬à¨²à©‡ à¨²à¨¾à¨—à¨¤-à¨•à©à¨¸à¨¼à¨² à¨¤à©ˆà¨¨à¨¾à¨¤à©€ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à©‡ à¨¹à¨¨, à¨˜à©±à¨Ÿ à¨“à¨ªà¨°à©‡à¨¸à¨¼à¨¨à¨² à¨²à¨¾à¨—à¨¤ à¨…à¨¤à©‡ à¨à¨œ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨²à¨ˆ à¨˜à©±à¨Ÿ à¨¬à©ˆà¨‚à¨¡à¨µà¨¿à¨¡à¨¥ à¨¦à©€ à¨²à©‹à©œà¥¤

## à¨‰à©±à¨š-à¨¤à¨•à¨¨à©€à¨•à©€ à¨›à©‹à¨Ÿà©‡ à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨² à¨¤à¨°à©€à¨•à©‡

### SLM (à¨›à©‹à¨Ÿà¨¾ à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨²) à¨¬à©à¨¨à¨¿à¨†à¨¦à¨¾à¨‚

à¨›à©‹à¨Ÿà¨¾ à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨² (SLM) à¨‡à©±à¨• à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨² à¨¹à©ˆ à¨œà©‹ à¨†à¨® à¨‰à¨ªà¨­à©‹à¨—à¨¤à¨¾ à¨‡à¨²à©ˆà¨•à¨Ÿà©à¨°à¨¾à¨¨à¨¿à¨• à¨¡à¨¿à¨µà¨¾à¨ˆà¨¸ 'à¨¤à©‡ à¨«à¨¿à©±à¨Ÿ à¨¹à©‹ à¨¸à¨•à¨¦à¨¾ à¨¹à©ˆ à¨…à¨¤à©‡ à¨‡à©±à¨• à¨‰à¨ªà¨­à©‹à¨—à¨¤à¨¾ à¨¦à©‡ à¨à¨œà©°à¨Ÿà¨¿à¨• à¨…à¨¨à©à¨°à©‹à¨§à¨¾à¨‚ à¨¦à©€ à¨¸à©‡à¨µà¨¾ à¨•à¨°à¨¦à©‡ à¨¸à¨®à©‡à¨‚ à¨ªà©à¨°à¨¯à©‹à¨—à¨¯à©‹à¨— à¨²à©ˆà¨Ÿà©ˆà¨‚à¨¸à©€ à¨¨à¨¾à¨² à¨…à¨¨à©à¨®à¨¾à¨¨ à¨²à¨—à¨¾ à¨¸à¨•à¨¦à¨¾ à¨¹à©ˆà¥¤ à¨µà¨¿à¨¹à¨¾à¨°à¨• à¨¤à©Œà¨° 'à¨¤à©‡, SLM à¨†à¨® à¨¤à©Œà¨° 'à¨¤à©‡ 10 à¨¬à¨¿à¨²à©€à¨…à¨¨ à¨ªà©ˆà¨°à¨¾à¨®à©€à¨Ÿà¨°à¨¾à¨‚ à¨¤à©‹à¨‚ à¨˜à©±à¨Ÿ à¨®à¨¾à¨¡à¨² à¨¹à©à©°à¨¦à©‡ à¨¹à¨¨à¥¤

**à¨«à¨¾à¨°à¨®à©ˆà¨Ÿ à¨–à©‹à¨œ à¨¸à¨®à¨°à©±à¨¥à¨¾à¨µà¨¾à¨‚**: SLM à¨µà©±à¨–-à¨µà©±à¨– à¨•à©à¨†à©°à¨Ÿà¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨ à¨ªà©±à¨§à¨°à¨¾à¨‚, à¨•à©à¨°à¨¾à¨¸-à¨ªà¨²à©‡à¨Ÿà¨«à¨¾à¨°à¨® à¨…à¨¨à©à¨•à©‚à¨²à¨¤à¨¾, à¨°à©€à¨…à¨²-à¨Ÿà¨¾à¨ˆà¨® à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨, à¨…à¨¤à©‡ à¨à¨œ à¨¤à©ˆà¨¨à¨¾à¨¤à©€ à¨¸à¨®à¨°à©±à¨¥à¨¾à¨µà¨¾à¨‚ à¨²à¨ˆ à¨‰à©±à¨š-à¨¤à¨•à¨¨à©€à¨•à©€ à¨¸à¨¹à¨¾à¨‡à¨¤à¨¾ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à©‡ à¨¹à¨¨à¥¤ à¨‰à¨ªà¨­à©‹à¨—à¨¤à¨¾ à¨¸à¨¥à¨¾à¨¨à¨• à¨ªà©à¨°à©‹à¨¸à©ˆà¨¸à¨¿à©°à¨— à¨¦à©à¨†à¨°à¨¾ à¨µà¨§à©‡à¨°à©‡ à¨—à©‹à¨ªà¨¨à©€à¨¯à¨¤à¨¾ à¨…à¨¤à©‡ à¨¬à©à¨°à¨¾à¨Šà¨œà¨¼à¨°-à¨…à¨§à¨¾à¨°à¨¿à¨¤ à¨¤à©ˆà¨¨à¨¾à¨¤à©€ à¨²à¨ˆ WebGPU à¨¸à¨¹à¨¾à¨‡à¨¤à¨¾ à¨ªà©à¨°à¨¾à¨ªà¨¤ à¨•à¨° à¨¸à¨•à¨¦à©‡ à¨¹à¨¨à¥¤

**à¨•à©à¨†à©°à¨Ÿà¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨ à¨ªà©±à¨§à¨° à¨¸à©°à¨—à©à¨°à¨¹à¨¿**: à¨ªà©à¨°à¨¸à¨¿à©±à¨§ SLM à¨«à¨¾à¨°à¨®à©ˆà¨Ÿà¨¾à¨‚ à¨µà¨¿à©±à¨š Q4_K_M à¨®à©‹à¨¬à¨¾à¨ˆà¨² à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨¸à©°à¨¤à©à¨²à¨¿à¨¤ à¨•à©°à¨ªà©à¨°à©ˆà¨¸à¨¼à¨¨ à¨²à¨ˆ, Q5_K_S à¨¸à©€à¨°à©€à¨œà¨¼ à¨—à©à¨£à¨µà©±à¨¤à¨¾-à¨•à©‡à¨‚à¨¦à¨°à¨¤ à¨à¨œ à¨¤à©ˆà¨¨à¨¾à¨¤à©€ à¨²à¨ˆ, Q8_0 à¨¸à¨¼à¨•à¨¤à©€à¨¸à¨¼à¨¾à¨²à©€ à¨à¨œ à¨¡à¨¿à¨µà¨¾à¨ˆà¨¸à¨¾à¨‚ 'à¨¤à©‡ à¨²à¨—à¨­à¨—-à¨…à¨¸à¨²à©€ à¨¸à¨¹à©€à¨¤à¨¾ à¨²à¨ˆ, à¨…à¨¤à©‡ à¨ªà©à¨°à¨¯à©‹à¨—à¨¾à¨¤à¨®à¨• à¨«à¨¾à¨°à¨®à©ˆà¨Ÿ à¨œà¨¿à¨µà©‡à¨‚ Q2_K à¨…à¨¤à¨¿-à¨˜à©±à¨Ÿ à¨¸à©°à¨¸à¨¾à¨§à¨¨ à¨¸à¨¥à¨¿à¨¤à©€à¨†à¨‚ à¨²à¨ˆ à¨¸à¨¼à¨¾à¨®à¨² à¨¹à¨¨à¥¤

### GGUF (à¨œà¨¨à¨°à¨² GGML à¨¯à©‚à¨¨à©€à¨µà¨°à¨¸à¨² à¨«à¨¾à¨°à¨®à©ˆà¨Ÿ) SLM à¨¤à©ˆà¨¨à¨¾à¨¤à©€ à¨²à¨ˆ

GGUF à¨®à©à©±à¨– à¨«à¨¾à¨°à¨®à©ˆà¨Ÿ à¨µà¨œà©‹à¨‚ SLMs à¨¨à©‚à©° CPU à¨…à¨¤à©‡ à¨à¨œ à¨¡à¨¿à¨µà¨¾à¨ˆà¨¸à¨¾à¨‚ 'à¨¤à©‡ à¨¤à©ˆà¨¨à¨¾à¨¤ à¨•à¨°à¨¨ à¨²à¨ˆ à¨¸à©‡à¨µà¨¾ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ, à¨–à¨¾à¨¸ à¨¤à©Œà¨° 'à¨¤à©‡ à¨à¨œà©°à¨Ÿà¨¿à¨• à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨²à¨ˆ à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼ à¨•à©€à¨¤à¨¾ à¨—à¨¿à¨† à¨¹à©ˆ:

**à¨à¨œà©°à¨Ÿ-à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à¨¡ à¨¸à¨®à¨°à©±à¨¥à¨¾à¨µà¨¾à¨‚**: à¨«à¨¾à¨°à¨®à©ˆà¨Ÿ SLM à¨°à©‚à¨ªà¨¾à¨‚à¨¤à¨°à¨¨ à¨…à¨¤à©‡ à¨¤à©ˆà¨¨à¨¾à¨¤à©€ à¨²à¨ˆ à¨µà¨¿à¨¸à¨¤à©à¨°à¨¿à¨¤ à¨¸à©°à¨¸à¨¾à¨§à¨¨ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ, à¨¸à©°à¨¦ à¨•à¨¾à¨²à¨¿à©°à¨—, à¨¸à©°à¨°à¨šà¨¿à¨¤ à¨†à¨‰à¨Ÿà¨ªà©à©±à¨Ÿ à¨œà¨¨à¨°à©‡à¨¸à¨¼à¨¨, à¨…à¨¤à©‡ à¨®à¨²à¨Ÿà©€-à¨Ÿà¨°à¨¨ à¨—à©±à¨²à¨¬à¨¾à¨¤à¨¾à¨‚ à¨²à¨ˆ à¨µà¨§à©‡à¨°à©‡ à¨¸à¨¹à¨¾à¨‡à¨¤à¨¾ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆà¥¤ à¨•à©à¨°à¨¾à¨¸-à¨ªà¨²à©‡à¨Ÿà¨«à¨¾à¨°à¨® à¨…à¨¨à©à¨•à©‚à¨²à¨¤à¨¾ à¨µà©±à¨–-à¨µà©±à¨– à¨à¨œ à¨¡à¨¿à¨µà¨¾à¨ˆà¨¸à¨¾à¨‚ 'à¨¤à©‡ à¨¸à¨¥à¨¿à¨° à¨à¨œà©°à¨Ÿ à¨µà¨¿à¨¹à¨¾à¨° à¨¨à©‚à©° à¨¯à¨•à©€à¨¨à©€ à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à©€ à¨¹à©ˆà¥¤

**à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨**: GGUF à¨à¨œà©°à¨Ÿ à¨µà¨°à¨•à¨«à¨²à©‹à¨œà¨¼ à¨²à¨ˆ à¨•à©à¨¸à¨¼à¨² à¨®à©ˆà¨®à©‹à¨°à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨¨à©‚à©° à¨¯à¨•à©€à¨¨à©€ à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à¨¾ à¨¹à©ˆ, à¨®à¨²à¨Ÿà©€-à¨à¨œà©°à¨Ÿ à¨¸à¨¿à¨¸à¨Ÿà¨®à¨¾à¨‚ à¨²à¨ˆ à¨¡à¨¾à¨‡à¨¨à¨¾à¨®à¨¿à¨• à¨®à¨¾à¨¡à¨² à¨²à©‹à¨¡à¨¿à©°à¨— à¨¦à¨¾ à¨¸à¨®à¨°à¨¥à¨¨ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ, à¨…à¨¤à©‡ à¨°à©€à¨…à¨²-à¨Ÿà¨¾à¨ˆà¨® à¨à¨œà©°à¨Ÿ à¨…à©°à¨¤à¨°à¨•à©à¨°à¨¿à¨† à¨²à¨ˆ à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à¨¡ à¨…à¨¨à©à¨®à¨¾à¨¨ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆà¥¤

### à¨à¨œ-à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à¨¡ SLM à¨«à¨°à©‡à¨®à¨µà¨°à¨•

#### Llama.cpp à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨

Llama.cpp à¨›à©‹à¨Ÿà©‡ à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨²à¨¾à¨‚ à¨¦à©€ à¨¤à©ˆà¨¨à¨¾à¨¤à©€ à¨²à¨ˆ à¨•à©±à¨Ÿà¨¿à©°à¨—-à¨à¨œ à¨•à©à¨†à©°à¨Ÿà¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨ à¨¤à¨•à¨¨à©€à¨•à¨¾à¨‚ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ:

**à¨à¨œà©°à¨Ÿ-à¨–à¨¾à¨¸ à¨•à©à¨†à©°à¨Ÿà¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨**: à¨«à¨°à©‡à¨®à¨µà¨°à¨• Q4_0 (à¨®à©‹à¨¬à¨¾à¨ˆà¨² à¨à¨œà©°à¨Ÿ à¨¤à©ˆà¨¨à¨¾à¨¤à©€ à¨²à¨ˆ 75% à¨†à¨•à¨¾à¨° à¨˜à¨Ÿà¨¾à¨‰à¨£ à¨²à¨ˆ), Q5_1 (à¨à¨œ à¨…à¨¨à©à¨®à¨¾à¨¨ à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨—à©à¨£à¨µà©±à¨¤à¨¾-à¨•à©°à¨ªà©à¨°à©ˆà¨¸à¨¼à¨¨ à¨¸à©°à¨¤à©à¨²à¨¨), à¨…à¨¤à©‡ Q8_0 (à¨‰à¨¤à¨ªà¨¾à¨¦à¨¨ à¨à¨œà©°à¨Ÿ à¨¸à¨¿à¨¸à¨Ÿà¨®à¨¾à¨‚ à¨²à¨ˆ à¨²à¨—à¨­à¨—-à¨…à¨¸à¨²à©€ à¨—à©à¨£à¨µà©±à¨¤à¨¾) à¨¦à¨¾ à¨¸à¨®à¨°à¨¥à¨¨ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆà¥¤ à¨…à¨—à©‡à¨¤à¨°à©€ à¨«à¨¾à¨°à¨®à©ˆà¨Ÿ à¨…à¨¤à¨¿-à¨¸à©°à¨•à©à¨šà¨¿à¨¤ à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¨à©‚à©° à¨…à¨¤à¨¿-à¨à¨œ à¨¸à¨¥à¨¿à¨¤à©€à¨†à¨‚ à¨²à¨ˆ à¨¯à©‹à¨— à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à©‡ à¨¹à¨¨à¥¤

**à¨²à¨¾à¨—à©‚ à¨•à¨°à¨¨ à¨¦à©‡ à¨«à¨¾à¨‡à¨¦à©‡**: SIMD à¨à¨•à¨¸à¨²à©‡à¨°à©‡à¨¸à¨¼à¨¨ à¨¨à¨¾à¨² CPU-à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à¨¡ à¨…à¨¨à©à¨®à¨¾à¨¨ à¨®à©ˆà¨®à©‹à¨°à©€-à¨•à©à¨¸à¨¼à¨² à¨à¨œà©°à¨Ÿ à¨•à¨¾à¨°à¨œà¨•à¨¾à¨°à©€ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆà¥¤ x86, ARM, à¨…à¨¤à©‡ Apple Silicon à¨†à¨°à¨•à©€à¨Ÿà©ˆà¨•à¨šà¨°à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨•à©à¨°à¨¾à¨¸-à¨ªà¨²à©‡à¨Ÿà¨«à¨¾à¨°à¨® à¨…à¨¨à©à¨•à©‚à¨²à¨¤à¨¾ à¨µà¨¿à¨¸à¨¼à¨µà¨µà¨¿à¨†à¨ªà©€ à¨à¨œà©°à¨Ÿ à¨¤à©ˆà¨¨à¨¾à¨¤à©€ à¨¸à¨®à¨°à©±à¨¥à¨¾à¨µà¨¾à¨‚ à¨¯à¨•à©€à¨¨à©€ à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à©€ à¨¹à©ˆà¥¤

#### Apple MLX à¨«à¨°à©‡à¨®à¨µà¨°à¨• SLM à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ

Apple MLX Apple Silicon à¨¡à¨¿à¨µà¨¾à¨ˆà¨¸à¨¾à¨‚ 'à¨¤à©‡ SLM-à¨¸à©°à¨šà¨¾à¨²à¨¿à¨¤ à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨–à¨¾à¨¸ à¨¤à©Œà¨° 'à¨¤à©‡ à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ:

**Apple Silicon à¨à¨œà©°à¨Ÿ à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨**: à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨¯à©‚à¨¨à¨¿à¨«à¨¾à¨‡à¨¡ à¨®à©ˆà¨®à©‹à¨°à©€ à¨†à¨°à¨•à©€à¨Ÿà©ˆà¨•à¨šà¨° à¨¨à¨¾à¨² à¨®à©ˆà¨Ÿà¨² à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨¸à¨¼à©‡à¨¡à¨°à¨œà¨¼ à¨‡à©°à¨Ÿà©€à¨—à©à¨°à©‡à¨¸à¨¼à¨¨, à¨à¨œà©°à¨Ÿ à¨…à¨¨à©à¨®à¨¾à¨¨ à¨²à¨ˆ à¨†à¨Ÿà©‹à¨®à©ˆà¨Ÿà¨¿à¨• à¨®à¨¿à¨•à¨¸à¨¡ à¨ªà©à¨°à¨¿à¨¸à©€à¨¸à¨¼à¨¨, à¨…à¨¤à©‡ à¨®à¨²à¨Ÿà©€-à¨à¨œà©°à¨Ÿ à¨¸à¨¿à¨¸à¨Ÿà¨®à¨¾à¨‚ à¨²à¨ˆ à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à¨¡ à¨®à©ˆà¨®à©‹à¨°à©€ à¨¬à©ˆà¨‚à¨¡à¨µà¨¿à¨¡à¨¥ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆà¥¤ M-à¨¸à©€à¨°à©€à¨œà¨¼ à¨šà¨¿à¨ªà¨¸ 'à¨¤à©‡ SLM à¨à¨œà©°à¨Ÿ à¨¸à¨¼à¨¾à¨¨à¨¦à¨¾à¨° à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨¦à¨¿à¨–à¨¾à¨‰à¨‚à¨¦à©‡ à¨¹à¨¨à¥¤

**à¨¡à¨¿à¨µà©ˆà¨²à¨ªà¨®à©ˆà¨‚à¨Ÿ à¨¸à¨®à¨°à©±à¨¥à¨¾à¨µà¨¾à¨‚**: à¨ªà¨¾à¨‡à¨¥à¨¨ à¨…à¨¤à©‡ à¨¸à¨µà¨¿à¨«à¨Ÿ API à¨¸à¨®à¨°à¨¥à¨¨ à¨¨à¨¾à¨² à¨à¨œà©°à¨Ÿ-à¨–à¨¾à¨¸ à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨, à¨à¨œà©°à¨Ÿ à¨¸à¨¿à©±à¨–à¨£ à¨²à¨ˆ à¨†à¨Ÿà©‹à¨®à©ˆà¨Ÿà¨¿à¨• à¨¡à¨¿à¨«à¨°à©ˆà¨‚à¨¸à¨¼à©€à¨à¨¸à¨¼à¨¨, à¨…à¨¤à©‡ Apple à¨¡à¨¿à¨µà©ˆà¨²à¨ªà¨®à©ˆà¨‚à¨Ÿ à¨Ÿà©‚à¨²à¨¾à¨‚ à¨¨à¨¾à¨² à¨¸à¨¹à©€ à¨‡à©°à¨Ÿà©€à¨—à©à¨°à©‡à¨¸à¨¼à¨¨ à¨µà¨¿à¨¸à¨¤à©à¨°à¨¿à¨¤ à¨à¨œà©°à¨Ÿ à¨¡à¨¿à¨µà©ˆà¨²à¨ªà¨®à©ˆà¨‚à¨Ÿ à¨µà¨¾à¨¤à¨¾à¨µà¨°à¨£ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à©‡ à¨¹à¨¨à¥¤

#### ONNX Runtime à¨•à©à¨°à¨¾à¨¸-à¨ªà¨²à©‡à¨Ÿà¨«à¨¾à¨°à¨® SLM à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ

ONNX Runtime à¨‡à©±à¨• à¨µà¨¿à¨¸à¨¼à¨µà¨µà¨¿à¨†à¨ªà©€ à¨…à¨¨à©à¨®à¨¾à¨¨ à¨‡à©°à¨œà¨¨ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ à¨œà©‹ SLM à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¨à©‚à©° à¨µà©±à¨–-à¨µà©±à¨– à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨ªà¨²à©‡à¨Ÿà¨«à¨¾à¨°à¨®à¨¾à¨‚ à¨…à¨¤à©‡ à¨“à¨ªà¨°à©‡à¨Ÿà¨¿à©°à¨— à¨¸à¨¿à¨¸à¨Ÿà¨®à¨¾à¨‚ 'à¨¤à©‡ à¨¸à¨¥à¨¿à¨° à¨¤à©Œà¨° 'à¨¤à©‡ à¨šà¨²à¨¾à¨‰à¨£ à¨¯à©‹à¨— à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à¨¾ à¨¹à©ˆ:

**à¨µà¨¿à¨¸à¨¼à¨µà¨µà¨¿à¨†à¨ªà©€ à¨¤à©ˆà¨¨à¨¾à¨¤à©€**: ONNX Runtime à¨¯à¨•à©€à¨¨à©€ à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à¨¾ à¨¹à©ˆ à¨•à¨¿ SLM à¨à¨œà©°à¨Ÿ Windows, Linux, macOS, iOS, à¨…à¨¤à©‡ Android à¨ªà¨²à©‡à¨Ÿà¨«à¨¾à¨°à¨®à¨¾à¨‚ 'à¨¤à©‡ à¨¸à¨¥à¨¿à¨° à¨¤à©Œà¨° 'à¨¤à©‡ à¨•à©°à¨® à¨•à¨°à¨¦à©‡ à¨¹à¨¨à¥¤ à¨‡à¨¹ à¨•à©à¨°à¨¾à¨¸-à¨ªà¨²à©‡à¨Ÿà¨«à¨¾à¨°à¨® à¨…à¨¨à©à¨•à©‚à¨²à¨¤à¨¾ à¨¡à¨¿à¨µà©ˆà¨²à¨ªà¨°à¨¾à¨‚ à¨¨à©‚à©° à¨‡à©±à¨• à¨µà¨¾à¨° à¨²à¨¿à¨–à¨£ à¨…à¨¤à©‡ à¨¹à¨° à¨œà¨—à©à¨¹à¨¾ à¨¤à©ˆà¨¨à¨¾à¨¤ à¨•à¨°à¨¨ à¨¦à©€ à¨¯à©‹à¨—à¨¤à¨¾ à¨¦à¨¿à©°à¨¦à©€ à¨¹à©ˆ, à¨¬à¨¹à©-à¨ªà¨²à©‡à¨Ÿà¨«à¨¾à¨°à¨® à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨²à¨ˆ à¨¡à¨¿à¨µà©ˆà¨²à¨ªà¨®à©ˆà¨‚à¨Ÿ à¨…à¨¤à©‡ à¨°à©±à¨–-à¨°à¨–à¨¾à¨… à¨¦à©‡ à¨“à¨µà¨°à¨¹à©ˆà©±à¨¡ à¨¨à©‚à©° à¨˜à¨Ÿà¨¾à¨‰à¨‚à¨¦à©€ à¨¹à©ˆà¥¤

**à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨à¨•à¨¸à¨²à©‡à¨°à©‡à¨¸à¨¼à¨¨ à¨µà¨¿à¨•à¨²à¨ª**: à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨µà©±à¨–-à¨µà©±à¨– à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨¸à©°à¨°à¨šà¨¨à¨¾à¨µà¨¾à¨‚ à¨²à¨ˆ à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à¨¡ à¨•à¨¾à¨°à¨œà¨•à¨¾à¨°à©€ à¨ªà©à¨°à¨¦à¨¾à¨¤à¨¾ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ à¨œà¨¿à¨µà©‡à¨‚ à¨•à¨¿ CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm), à¨…à¨¤à©‡ à¨µà¨¿à¨¸à¨¼à©‡à¨¸à¨¼ à¨à¨•à¨¸à¨²à©‡à¨°à©‡à¨Ÿà¨° (Intel VPU, Qualcomm NPU)à¥¤ SLM à¨à¨œà©°à¨Ÿ à¨•à©‹à¨¡ à¨µà¨¿à©±à¨š à¨¬à¨¦à¨²à¨¾à¨… à¨•à©€à¨¤à©‡ à¨¬à¨¿à¨¨à¨¾à¨‚ à¨‰à¨ªà¨²à¨¬à¨§ à¨¸à¨­ à¨¤à©‹à¨‚ à¨µà¨§à©€à¨† à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨•à¨° à¨¸à¨•à¨¦à©‡ à¨¹à¨¨à¥¤

**à¨‰à¨¤à¨ªà¨¾à¨¦à¨¨-à¨¤à¨¿à¨†à¨° à¨¸à¨®à¨°à©±à¨¥à¨¾à¨µà¨¾à¨‚**: ONNX Runtime à¨‰à¨¤à¨ªà¨¾à¨¦à¨¨ à¨à¨œà©°à¨Ÿ à¨¤à©ˆà¨¨à¨¾à¨¤à©€ à¨²à¨ˆ à¨œà¨¼à¨°à©‚à¨°à©€ à¨µà¨¿à¨¸à¨¼à¨µà¨µà¨¿à¨†à¨ªà©€ à¨¸à¨®à¨°à©±à¨¥à¨¾à¨µà¨¾à¨‚ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ à¨œà¨¿à¨µà©‡à¨‚ à¨•à¨¿ à¨¤à©‡à¨œà¨¼ à¨…à¨¨à©à¨®à¨¾à¨¨ à¨²à¨ˆ à¨—à©à¨°à¨¾à¨« à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨, à¨¸à©°à¨¸à¨¾à¨§à¨¨-à¨¸à©€à¨®à¨¿à¨¤ à¨µà¨¾à¨¤à¨¾à¨µà¨°à¨£à¨¾à¨‚ à¨²à¨ˆ à¨®à©ˆà¨®à©‹à¨°à©€ à¨ªà©à¨°à¨¬à©°à¨§à¨¨, à¨…à¨¤à©‡ à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨µà¨¿à¨¸à¨¼à¨²à©‡à¨¸à¨¼à¨£ à¨²à¨ˆ à¨µà¨¿à¨¸à¨¤à©à¨°à¨¿à¨¤ à¨ªà©à¨°à©‹à¨«à¨¾à¨ˆà¨²à¨¿à©°à¨— à¨Ÿà©‚à¨²à¥¤ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨ªà¨¾à¨‡à¨¥à¨¨ à¨…à¨¤à©‡ C++ APIs à¨¦à©‹à¨µà¨¾à¨‚ à¨²à¨ˆ à¨¸à¨®à¨°à¨¥à¨¨ à¨ªà©‡à¨¸à¨¼ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆà¥¤
- à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨‡à©°à¨Ÿà©€à¨—à©à¨°à©‡à¨¸à¨¼à¨¨ à¨¦à©€ à¨œà¨¾à¨‚à¨š à¨•à¨°à©‹  
- à¨†à¨«à¨²à¨¾à¨ˆà¨¨ à¨“à¨ªà¨°à©‡à¨¸à¨¼à¨¨ à¨¸à¨®à¨°à©±à¨¥à¨¾à¨µà¨¾à¨‚ à¨¦à©€ à¨ªà©à¨¸à¨¼à¨Ÿà©€ à¨•à¨°à©‹  
- à¨«à©‡à¨²à¨“à¨µà¨° à¨¸à¨¥à¨¿à¨¤à©€à¨†à¨‚ à¨…à¨¤à©‡ à¨—à¨²à¨¤à©€ à¨¸à©°à¨­à¨¾à¨² à¨¦à©€ à¨œà¨¾à¨‚à¨š à¨•à¨°à©‹  
- à¨à¨œà©°à¨Ÿ à¨µà¨°à¨•à¨«à¨²à©‹à¨œà¨¼ à¨¦à©€ à¨…à©°à¨¤-à¨¤à©±à¨• à¨µà©ˆà¨§à¨¤à¨¾ à¨•à¨°à©‹  

**Foundry Local à¨¨à¨¾à¨² à¨¤à©à¨²à¨¨à¨¾**:

| à¨«à©€à¨šà¨° | Foundry Local | Ollama |
|---------|---------------|--------|
| **à¨Ÿà¨¾à¨°à¨—à¨Ÿ à¨¯à©‚à¨œà¨¼ à¨•à©‡à¨¸** | à¨‡à©°à¨Ÿà¨°à¨ªà©à¨°à¨¾à¨ˆà¨œà¨¼ à¨ªà©à¨°à©‹à¨¡à¨•à¨¸à¨¼à¨¨ | à¨µà¨¿à¨•à¨¾à¨¸ à¨…à¨¤à©‡ à¨•à¨®à¨¿à¨Šà¨¨à¨¿à¨Ÿà©€ |
| **à¨®à¨¾à¨¡à¨² à¨ˆà¨•à©‹à¨¸à¨¿à¨¸à¨Ÿà¨®** | à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ-à¨•à¨¿à¨Šà¨°à©‡à¨Ÿà¨¡ | à¨µà¨¿à¨†à¨ªà¨• à¨•à¨®à¨¿à¨Šà¨¨à¨¿à¨Ÿà©€ |
| **à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨…à¨ªà¨Ÿà©€à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨** | à¨†à¨Ÿà©‹à¨®à©ˆà¨Ÿà¨¿à¨• (CUDA/NPU/CPU) | à¨®à©ˆà¨¨à©‚à¨…à¨² à¨•à¨¨à¨«à¨¿à¨—à¨°à©‡à¨¸à¨¼à¨¨ |
| **à¨‡à©°à¨Ÿà¨°à¨ªà©à¨°à¨¾à¨ˆà¨œà¨¼ à¨«à©€à¨šà¨°à¨¸** | à¨¬à¨¿à¨²à¨Ÿ-à¨‡à¨¨ à¨®à¨¾à¨¨à©€à¨Ÿà¨°à¨¿à©°à¨—, à¨¸à©à¨°à©±à¨–à¨¿à¨† | à¨•à¨®à¨¿à¨Šà¨¨à¨¿à¨Ÿà©€ à¨Ÿà©‚à¨² |
| **à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨•à©Œà¨‚à¨ªà¨²à©‡à¨•à¨¸à¨¿à¨Ÿà©€** | à¨¸à¨§à¨¾à¨°à¨¨ (winget install) | à¨¸à¨§à¨¾à¨°à¨¨ (curl install) |
| **API à¨•à¨®à¨ªà©ˆà¨Ÿà¨¿à¨¬à¨¿à¨²à¨¿à¨Ÿà©€** | OpenAI + à¨µà¨¾à¨§à©‡ | OpenAI à¨¸à¨Ÿà©ˆà¨‚à¨¡à¨°à¨¡ |
| **à¨¸à¨ªà©‹à¨°à¨Ÿ** | à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨…à¨§à¨¿à¨•à¨¾à¨°à¨• | à¨•à¨®à¨¿à¨Šà¨¨à¨¿à¨Ÿà©€-à¨šà¨²à¨¾à¨‡à¨† |
| **à¨¸à¨­ à¨¤à©‹à¨‚ à¨µà¨§à©€à¨†** | à¨ªà©à¨°à©‹à¨¡à¨•à¨¸à¨¼à¨¨ à¨à¨œà©°à¨Ÿ | à¨ªà©à¨°à©‹à¨Ÿà©‹à¨Ÿà¨¾à¨ˆà¨ªà¨¿à©°à¨—, à¨°à¨¿à¨¸à¨°à¨š |

**Ollama à¨•à¨¿à¨‰à¨‚ à¨šà©à¨£à©‹**:  
- **à¨µà¨¿à¨•à¨¾à¨¸ à¨…à¨¤à©‡ à¨ªà©à¨°à©‹à¨Ÿà©‹à¨Ÿà¨¾à¨ˆà¨ªà¨¿à©°à¨—**: à¨µà©±à¨–-à¨µà©±à¨– à¨®à¨¾à¨¡à¨²à¨¾à¨‚ à¨¨à¨¾à¨² à¨¤à©‡à¨œà¨¼à©€ à¨¨à¨¾à¨² à¨ªà©à¨°à¨¯à©‹à¨—  
- **à¨•à¨®à¨¿à¨Šà¨¨à¨¿à¨Ÿà©€ à¨®à¨¾à¨¡à¨²**: à¨¨à¨µà©€à¨‚ à¨•à¨®à¨¿à¨Šà¨¨à¨¿à¨Ÿà©€-à¨¯à©‹à¨—à¨¦à¨¾à¨¨ à¨®à¨¾à¨¡à¨²à¨¾à¨‚ à¨¤à©±à¨• à¨ªà¨¹à©à©°à¨š  
- **à¨¸à¨¿à©±à¨–à¨£ à¨²à¨ˆ à¨µà¨°à¨¤à©‹à¨‚**: AI à¨à¨œà©°à¨Ÿ à¨µà¨¿à¨•à¨¾à¨¸ à¨¸à¨¿à©±à¨–à¨£ à¨…à¨¤à©‡ à¨¸à¨¿à¨–à¨¾à¨‰à¨£ à¨²à¨ˆ  
- **à¨°à¨¿à¨¸à¨°à¨š à¨ªà©à¨°à©‹à¨œà©ˆà¨•à¨Ÿ**: à¨µà©±à¨–-à¨µà©±à¨– à¨®à¨¾à¨¡à¨² à¨ªà¨¹à©à©°à¨š à¨¦à©€ à¨²à©‹à©œ à¨µà¨¾à¨²à©‡ à¨…à¨•à¨¾à¨¦à¨®à¨¿à¨• à¨°à¨¿à¨¸à¨°à¨š à¨²à¨ˆ  
- **à¨•à¨¸à¨Ÿà¨® à¨®à¨¾à¨¡à¨²**: à¨•à¨¸à¨Ÿà¨® à¨«à¨¾à¨ˆà¨¨-à¨Ÿà¨¿à¨Šà¨¨à¨¡ à¨®à¨¾à¨¡à¨² à¨¬à¨£à¨¾à¨‰à¨£ à¨…à¨¤à©‡ à¨œà¨¾à¨‚à¨šà¨£ à¨²à¨ˆ  

### VLLM: à¨‰à©±à¨š-à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ SLM à¨à¨œà©°à¨Ÿ à¨‡à©°à¨«à¨°à©ˆà¨‚à¨¸

VLLM (à¨¬à¨¹à©à¨¤ à¨µà©±à¨¡à¨¾ à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨² à¨‡à©°à¨«à¨°à©ˆà¨‚à¨¸) à¨‰à©±à¨š-à¨¥à¨°à©‚à¨ªà©à©±à¨Ÿ, à¨®à©ˆà¨®à©‹à¨°à©€-à¨•à©à¨¸à¨¼à¨² à¨‡à©°à¨«à¨°à©ˆà¨‚à¨¸ à¨‡à©°à¨œà¨¨ à¨ªà©à¨°à¨¦à¨¾à¨¨ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ à¨œà©‹ à¨µà©±à¨¡à©‡ à¨ªà©ˆà¨®à¨¾à¨¨à©‡ 'à¨¤à©‡ à¨ªà©à¨°à©‹à¨¡à¨•à¨¸à¨¼à¨¨ SLM à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨²à¨ˆ à¨µà¨¿à¨¸à¨¼à©‡à¨¸à¨¼ à¨¤à©Œà¨° 'à¨¤à©‡ à¨…à¨ªà¨Ÿà©€à¨®à¨¾à¨ˆà¨œà¨¼ à¨•à©€à¨¤à¨¾ à¨—à¨¿à¨† à¨¹à©ˆà¥¤ à¨œà¨¦à©‹à¨‚ à¨•à¨¿ Foundry Local à¨µà¨°à¨¤à©‹à¨‚ à¨¦à©€ à¨¸à¨¹à©‚à¨²à¨¤ 'à¨¤à©‡ à¨§à¨¿à¨†à¨¨ à¨•à©‡à¨‚à¨¦à¨°à¨¿à¨¤ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ à¨…à¨¤à©‡ Ollama à¨•à¨®à¨¿à¨Šà¨¨à¨¿à¨Ÿà©€ à¨®à¨¾à¨¡à¨²à¨¾à¨‚ 'à¨¤à©‡ à¨œà¨¼à©‹à¨° à¨¦à¨¿à©°à¨¦à¨¾ à¨¹à©ˆ, VLLM à¨‰à©±à¨š-à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨¸à¨¥à¨¿à¨¤à©€à¨†à¨‚ à¨µà¨¿à©±à¨š à¨¸à¨¼à¨¾à¨¨à¨¦à¨¾à¨° à¨¹à©ˆ à¨œà¨¿à©±à¨¥à©‡ à¨µà©±à¨§ à¨¤à©‹à¨‚ à¨µà©±à¨§ à¨¥à¨°à©‚à¨ªà©à©±à¨Ÿ à¨…à¨¤à©‡ à¨•à©à¨¸à¨¼à¨² à¨¸à¨°à©‹à¨¤ à¨µà¨°à¨¤à©‹à¨‚ à¨¦à©€ à¨²à©‹à©œ à¨¹à©à©°à¨¦à©€ à¨¹à©ˆà¥¤

**à¨®à©à©±à¨– à¨†à¨°à¨•à©€à¨Ÿà©ˆà¨•à¨šà¨° à¨…à¨¤à©‡ à¨«à©€à¨šà¨°à¨¸**:  
- **PagedAttention**: à¨§à¨¿à¨†à¨¨ à¨—à¨£à¨¨à¨¾ à¨²à¨ˆ à¨•à©à¨¸à¨¼à¨² à¨®à©ˆà¨®à©‹à¨°à©€ à¨ªà©à¨°à¨¬à©°à¨§à¨¨  
- **Dynamic Batching**: à¨¥à¨°à©‚à¨ªà©à©±à¨Ÿ à¨²à¨ˆ à¨¸à¨®à¨°à©±à¨¥ à¨¬à©ˆà¨šà¨¿à©°à¨—  
- **GPU Optimization**: à¨‰à©±à¨š-à¨¤à¨•à¨¨à©€à¨•à©€ CUDA à¨•à¨°à¨¨à¨² à¨…à¨¤à©‡ à¨Ÿà©ˆà¨‚à¨¸à¨° à¨ªà©ˆà¨°à¨²à¨²à¨¿à¨œà¨¼à¨® à¨¸à¨ªà©‹à¨°à¨Ÿ  
- **OpenAI Compatibility**: à¨¸à¨¹à©€ API à¨•à¨®à¨ªà©ˆà¨Ÿà¨¿à¨¬à¨¿à¨²à¨¿à¨Ÿà©€  
- **Speculative Decoding**: à¨‡à©°à¨«à¨°à©ˆà¨‚à¨¸ à¨¤à©‡à¨œà¨¼à©€ à¨²à¨ˆ à¨¤à¨•à¨¨à©€à¨•  
- **Quantization Support**: à¨®à©ˆà¨®à©‹à¨°à©€ à¨•à©à¨¸à¨¼à¨²à¨¤à¨¾ à¨²à¨ˆ INT4, INT8, à¨…à¨¤à©‡ FP16 à¨•à¨µà¨¾à¨‚à¨Ÿà¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨  

#### à¨‡à©°à¨¸à¨Ÿà¨¾à¨²à©‡à¨¸à¨¼à¨¨ à¨…à¨¤à©‡ à¨¸à©ˆà¨Ÿà¨…à©±à¨ª

**à¨‡à©°à¨¸à¨Ÿà¨¾à¨²à©‡à¨¸à¨¼à¨¨ à¨µà¨¿à¨•à¨²à¨ª**:  
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```
  
**à¨à¨œà©°à¨Ÿ à¨µà¨¿à¨•à¨¾à¨¸ à¨²à¨ˆ à¨¤à©‡à¨œà¨¼ à¨¸à¨¼à©à¨°à©‚à¨†à¨¤**:  
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```
  

#### à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨‡à©°à¨Ÿà©€à¨—à©à¨°à©‡à¨¸à¨¼à¨¨

**VLLM à¨¨à¨¾à¨² à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨•**:  
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```
  
**à¨‰à©±à¨š-à¨¥à¨°à©‚à¨ªà©à©±à¨Ÿ à¨®à¨²à¨Ÿà©€-à¨à¨œà©°à¨Ÿ à¨¸à©ˆà¨Ÿà¨…à©±à¨ª**:  
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```
  

#### à¨ªà©à¨°à©‹à¨¡à¨•à¨¸à¨¼à¨¨ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨ªà©ˆà¨Ÿà¨°à¨¨à¨¸

**à¨‡à©°à¨Ÿà¨°à¨ªà©à¨°à¨¾à¨ˆà¨œà¨¼ VLLM à¨ªà©à¨°à©‹à¨¡à¨•à¨¸à¨¼à¨¨ à¨¸à©‡à¨µà¨¾**:  
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```
  

#### à¨‡à©°à¨Ÿà¨°à¨ªà©à¨°à¨¾à¨ˆà¨œà¨¼ à¨«à©€à¨šà¨°à¨¸ à¨…à¨¤à©‡ à¨®à¨¾à¨¨à©€à¨Ÿà¨°à¨¿à©°à¨—

**à¨‰à©±à¨š-à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ VLLM à¨®à¨¾à¨¨à©€à¨Ÿà¨°à¨¿à©°à¨—**:  
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```
  

#### à¨‰à©±à¨š-à¨¤à¨•à¨¨à©€à¨•à©€ à¨•à¨¨à¨«à¨¿à¨—à¨°à©‡à¨¸à¨¼à¨¨ à¨…à¨¤à©‡ à¨…à¨ªà¨Ÿà©€à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨

**à¨ªà©à¨°à©‹à¨¡à¨•à¨¸à¨¼à¨¨ VLLM à¨•à¨¨à¨«à¨¿à¨—à¨°à©‡à¨¸à¨¼à¨¨ à¨Ÿà©ˆà¨‚à¨ªà¨²à©‡à¨Ÿà¨¸**:  
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```
  
**VLLM à¨²à¨ˆ à¨ªà©à¨°à©‹à¨¡à¨•à¨¸à¨¼à¨¨ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨šà©ˆà©±à¨•à¨²à¨¿à¨¸à¨Ÿ**:

âœ… **à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨…à¨ªà¨Ÿà©€à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨**:  
- à¨®à¨²à¨Ÿà©€-GPU à¨¸à©ˆà¨Ÿà¨…à©±à¨ª à¨²à¨ˆ à¨Ÿà©ˆà¨‚à¨¸à¨° à¨ªà©ˆà¨°à¨²à¨²à¨¿à¨œà¨¼à¨® à¨•à¨¨à¨«à¨¿à¨—à¨° à¨•à¨°à©‹  
- à¨®à©ˆà¨®à©‹à¨°à©€ à¨•à©à¨¸à¨¼à¨²à¨¤à¨¾ à¨²à¨ˆ à¨•à¨µà¨¾à¨‚à¨Ÿà¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨ (AWQ/GPTQ) à¨à¨¨à¨¬à¨² à¨•à¨°à©‹  
- GPU à¨®à©ˆà¨®à©‹à¨°à©€ à¨µà¨°à¨¤à©‹à¨‚ (85-95%) à¨²à¨ˆ à¨¸à¨¹à©€ à¨¸à©ˆà¨Ÿà¨¿à©°à¨— à¨•à¨°à©‹  
- à¨¥à¨°à©‚à¨ªà©à©±à¨Ÿ à¨²à¨ˆ à¨¸à¨¹à©€ à¨¬à©ˆà¨š à¨¸à¨¾à¨ˆà¨œà¨¼ à¨•à¨¨à¨«à¨¿à¨—à¨° à¨•à¨°à©‹  

âœ… **à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨Ÿà¨¿à¨Šà¨¨à¨¿à©°à¨—**:  
- à¨¦à©à¨¹à¨°à¨¾à¨ à¨—à¨ à¨ªà©à¨°à¨¸à¨¼à¨¨à¨¾à¨‚ à¨²à¨ˆ à¨ªà©à¨°à©€à¨«à¨¿à¨•à¨¸ à¨•à©ˆà¨¸à¨¼à¨¿à©°à¨— à¨à¨¨à¨¬à¨² à¨•à¨°à©‹  
- à¨²à©°à¨¬à©‡ à¨¸à¨¿à¨•à¨µà©°à¨¸ à¨²à¨ˆ à¨šà©°à¨•à¨¡ à¨ªà©à¨°à©€à¨«à¨¿à¨² à¨•à¨¨à¨«à¨¿à¨—à¨° à¨•à¨°à©‹  
- à¨¤à©‡à¨œà¨¼ à¨‡à©°à¨«à¨°à©ˆà¨‚à¨¸ à¨²à¨ˆ à¨¸à¨ªà©ˆà¨•à©‚à¨²à©‡à¨Ÿà¨¿à¨µ à¨¡à¨¿à¨•à©‹à¨¡à¨¿à©°à¨— à¨¸à©ˆà¨Ÿà¨…à©±à¨ª à¨•à¨°à©‹  
- à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨¦à©‡ à¨…à¨§à¨¾à¨° 'à¨¤à©‡ max_num_seqs à¨…à¨ªà¨Ÿà©€à¨®à¨¾à¨ˆà¨œà¨¼ à¨•à¨°à©‹  

âœ… **à¨ªà©à¨°à©‹à¨¡à¨•à¨¸à¨¼à¨¨ à¨«à©€à¨šà¨°à¨¸**:  
- à¨¸à¨¿à¨¹à¨¤ à¨®à¨¾à¨¨à©€à¨Ÿà¨°à¨¿à©°à¨— à¨…à¨¤à©‡ à¨®à©ˆà¨Ÿà©à¨°à¨¿à¨•à¨¸ à¨•à¨²à©ˆà¨•à¨¸à¨¼à¨¨ à¨¸à©ˆà¨Ÿà¨…à©±à¨ª à¨•à¨°à©‹  
- à¨†à¨Ÿà©‹à¨®à©ˆà¨Ÿà¨¿à¨• à¨°à©€à¨¸à¨Ÿà¨¾à¨°à¨Ÿ à¨…à¨¤à©‡ à¨«à©‡à¨²à¨“à¨µà¨° à¨•à¨¨à¨«à¨¿à¨—à¨° à¨•à¨°à©‹  
- à¨°à¨¿à¨•à¨µà©ˆà¨¸à¨Ÿ à¨•à¨¿à¨Šà¨‡à©°à¨— à¨…à¨¤à©‡ à¨²à©‹à¨¡ à¨¬à©ˆà¨²à©ˆà¨‚à¨¸à¨¿à©°à¨— à¨²à¨¾à¨—à©‚ à¨•à¨°à©‹  
- à¨µà¨¿à¨¸à¨¤à©à¨°à¨¿à¨¤ à¨²à©Œà¨—à¨¿à©°à¨— à¨…à¨¤à©‡ à¨…à¨²à¨°à¨Ÿà¨¿à©°à¨— à¨¸à©ˆà¨Ÿà¨…à©±à¨ª à¨•à¨°à©‹  

âœ… **à¨¸à©à¨°à©±à¨–à¨¿à¨† à¨…à¨¤à©‡ à¨­à¨°à©‹à¨¸à©‡à¨¯à©‹à¨—à¨¤à¨¾**:  
- à¨«à¨¾à¨‡à¨°à¨µà¨¾à¨² à¨¨à¨¿à¨¯à¨® à¨…à¨¤à©‡ à¨ªà¨¹à©à©°à¨š à¨¨à¨¿à¨¯à©°à¨¤à¨°à¨£ à¨•à¨¨à¨«à¨¿à¨—à¨° à¨•à¨°à©‹  
- API à¨°à©‡à¨Ÿ à¨²à¨¿à¨®à¨¿à¨Ÿà¨¿à©°à¨— à¨…à¨¤à©‡ à¨ªà©à¨°à¨®à¨¾à¨£à¨¿à¨•à¨¤à¨¾ à¨¸à©ˆà¨Ÿà¨…à©±à¨ª à¨•à¨°à©‹  
- à¨—à©à¨°à©‡à¨¸à¨«à©à¨² à¨¸à¨¼à¨Ÿà¨¡à¨¾à¨Šà¨¨ à¨…à¨¤à©‡ à¨•à¨²à©€à¨¨à¨…à©±à¨ª à¨²à¨¾à¨—à©‚ à¨•à¨°à©‹  
- à¨¬à©ˆà¨•à¨…à©±à¨ª à¨…à¨¤à©‡ à¨¡à¨¿à¨œà¨¾à¨¸à¨Ÿà¨° à¨°à¨¿à¨•à¨µà¨°à©€ à¨•à¨¨à¨«à¨¿à¨—à¨° à¨•à¨°à©‹  

âœ… **à¨‡à©°à¨Ÿà©€à¨—à©à¨°à©‡à¨¸à¨¼à¨¨ à¨Ÿà©ˆà¨¸à¨Ÿà¨¿à©°à¨—**:  
- à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨‡à©°à¨Ÿà©€à¨—à©à¨°à©‡à¨¸à¨¼à¨¨ à¨¦à©€ à¨œà¨¾à¨‚à¨š à¨•à¨°à©‹  
- à¨‰à©±à¨š-à¨¥à¨°à©‚à¨ªà©à©±à¨Ÿ à¨¸à¨¥à¨¿à¨¤à©€à¨†à¨‚ à¨¦à©€ à¨ªà©à¨¸à¨¼à¨Ÿà©€ à¨•à¨°à©‹  
- à¨«à©‡à¨²à¨“à¨µà¨° à¨…à¨¤à©‡ à¨°à¨¿à¨•à¨µà¨°à©€ à¨ªà©à¨°à¨•à¨¿à¨°à¨¿à¨†à¨µà¨¾à¨‚ à¨¦à©€ à¨œà¨¾à¨‚à¨š à¨•à¨°à©‹  
- à¨²à©‹à¨¡ à¨¹à©‡à¨  à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨¬à©ˆà¨‚à¨šà¨®à¨¾à¨°à¨• à¨•à¨°à©‹  

**à¨¹à©‹à¨° à¨¹à©±à¨²à¨¾à¨‚ à¨¨à¨¾à¨² à¨¤à©à¨²à¨¨à¨¾**:

| à¨«à©€à¨šà¨° | VLLM | Foundry Local | Ollama |
|---------|------|---------------|--------|
| **à¨Ÿà¨¾à¨°à¨—à¨Ÿ à¨¯à©‚à¨œà¨¼ à¨•à©‡à¨¸** | à¨‰à©±à¨š-à¨¥à¨°à©‚à¨ªà©à©±à¨Ÿ à¨ªà©à¨°à©‹à¨¡à¨•à¨¸à¨¼à¨¨ | à¨‡à©°à¨Ÿà¨°à¨ªà©à¨°à¨¾à¨ˆà¨œà¨¼ à¨¸à¨¹à©‚à¨²à¨¤ | à¨µà¨¿à¨•à¨¾à¨¸ à¨…à¨¤à©‡ à¨•à¨®à¨¿à¨Šà¨¨à¨¿à¨Ÿà©€ |
| **à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨** | à¨µà©±à¨§ à¨¤à©‹à¨‚ à¨µà©±à¨§ à¨¥à¨°à©‚à¨ªà©à©±à¨Ÿ | à¨¸à©°à¨¤à©à¨²à¨¿à¨¤ | à¨šà©°à¨—à¨¾ |
| **à¨®à©ˆà¨®à©‹à¨°à©€ à¨•à©à¨¸à¨¼à¨²à¨¤à¨¾** | PagedAttention à¨…à¨ªà¨Ÿà©€à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨ | à¨†à¨Ÿà©‹à¨®à©ˆà¨Ÿà¨¿à¨• à¨…à¨ªà¨Ÿà©€à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨ | à¨¸à¨Ÿà©ˆà¨‚à¨¡à¨°à¨¡ |
| **à¨¸à©ˆà¨Ÿà¨…à©±à¨ª à¨•à©Œà¨‚à¨ªà¨²à©‡à¨•à¨¸à¨¿à¨Ÿà©€** | à¨‰à©±à¨š (à¨•à¨ˆ à¨ªà©ˆà¨°à¨¾à¨®à©€à¨Ÿà¨°) | à¨˜à©±à¨Ÿ (à¨†à¨Ÿà©‹à¨®à©ˆà¨Ÿà¨¿à¨•) | à¨˜à©±à¨Ÿ (à¨¸à¨§à¨¾à¨°à¨¨) |
| **à¨¸à¨•à©ˆà¨²à©‡à¨¬à¨¿à¨²à¨¿à¨Ÿà©€** | à¨¸à¨¼à¨¾à¨¨à¨¦à¨¾à¨° (à¨Ÿà©ˆà¨‚à¨¸à¨°/à¨ªà¨¾à¨ˆà¨ªà¨²à¨¾à¨ˆà¨¨ à¨ªà©ˆà¨°à¨²à¨²) | à¨šà©°à¨—à¨¾ | à¨¸à©€à¨®à¨¿à¨¤ |
| **à¨•à¨µà¨¾à¨‚à¨Ÿà¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨** | à¨…à¨—à¨°à¨¸à¨° (AWQ, GPTQ, FP8) | à¨†à¨Ÿà©‹à¨®à©ˆà¨Ÿà¨¿à¨• | à¨¸à¨Ÿà©ˆà¨‚à¨¡à¨°à¨¡ GGUF |
| **à¨‡à©°à¨Ÿà¨°à¨ªà©à¨°à¨¾à¨ˆà¨œà¨¼ à¨«à©€à¨šà¨°à¨¸** | à¨•à¨¸à¨Ÿà¨® à¨‡à©°à¨ªà¨²à©€à¨®à©ˆà¨‚à¨Ÿà©‡à¨¸à¨¼à¨¨ à¨¦à©€ à¨²à©‹à©œ | à¨¬à¨¿à¨²à¨Ÿ-à¨‡à¨¨ | à¨•à¨®à¨¿à¨Šà¨¨à¨¿à¨Ÿà©€ à¨Ÿà©‚à¨² |
| **à¨¸à¨­ à¨¤à©‹à¨‚ à¨µà¨§à©€à¨†** | à¨µà©±à¨¡à©‡ à¨ªà©ˆà¨®à¨¾à¨¨à©‡ à¨¦à©‡ à¨ªà©à¨°à©‹à¨¡à¨•à¨¸à¨¼à¨¨ à¨à¨œà©°à¨Ÿ | à¨‡à©°à¨Ÿà¨°à¨ªà©à¨°à¨¾à¨ˆà¨œà¨¼ à¨ªà©à¨°à©‹à¨¡à¨•à¨¸à¨¼à¨¨ | à¨µà¨¿à¨•à¨¾à¨¸ |

**VLLM à¨•à¨¿à¨‰à¨‚ à¨šà©à¨£à©‹**:  
- **à¨‰à©±à¨š-à¨¥à¨°à©‚à¨ªà©à©±à¨Ÿ à¨¦à©€ à¨²à©‹à©œ**: à¨¸à©ˆà¨•à¨¿à©°à¨¡ à¨µà¨¿à©±à¨š à¨¸à©ˆà¨‚à¨•à©œà©‡ à¨°à¨¿à¨•à¨µà©ˆà¨¸à¨Ÿ à¨ªà©à¨°à©‹à¨¸à©ˆà¨¸ à¨•à¨°à¨¨à¨¾  
- **à¨µà©±à¨¡à©‡ à¨ªà©ˆà¨®à¨¾à¨¨à©‡ à¨¦à©‡ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ**: à¨®à¨²à¨Ÿà©€-GPU, à¨®à¨²à¨Ÿà©€-à¨¨à©‹à¨¡ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ  
- **à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨®à¨¹à©±à¨¤à¨µà¨ªà©‚à¨°à¨¨**: à¨µà©±à¨¡à©‡ à¨ªà©ˆà¨®à¨¾à¨¨à©‡ 'à¨¤à©‡ à¨¸à¨¬-à¨¸à©ˆà¨•à¨¿à©°à¨¡ à¨°à¨¿à¨¸à¨ªà¨¾à¨‚à¨¸ à¨Ÿà¨¾à¨ˆà¨®  
- **à¨…à¨—à¨°à¨¸à¨° à¨…à¨ªà¨Ÿà©€à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨**: à¨•à¨¸à¨Ÿà¨® à¨•à¨µà¨¾à¨‚à¨Ÿà¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨ à¨…à¨¤à©‡ à¨¬à©ˆà¨šà¨¿à©°à¨— à¨¦à©€ à¨²à©‹à©œ  
- **à¨¸à¨°à©‹à¨¤ à¨•à©à¨¸à¨¼à¨²à¨¤à¨¾**: à¨®à¨¹à¨¿à©°à¨—à©‡ GPU à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨¦à©€ à¨µà©±à¨§ à¨¤à©‹à¨‚ à¨µà©±à¨§ à¨µà¨°à¨¤à©‹à¨‚  

## à¨…à¨¸à¨²-à¨¦à©à¨¨à©€à¨† à¨¦à©‡ SLM à¨à¨œà©°à¨Ÿ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨

### à¨—à¨¾à¨¹à¨• à¨¸à©‡à¨µà¨¾ SLM à¨à¨œà©°à¨Ÿ  
- **SLM à¨¸à¨®à¨°à©±à¨¥à¨¾à¨µà¨¾à¨‚**: à¨–à¨¾à¨¤à©‡ à¨¦à©€ à¨œà¨¾à¨£à¨•à¨¾à¨°à©€, à¨ªà¨¾à¨¸à¨µà¨°à¨¡ à¨°à©€à¨¸à©ˆà¨Ÿ, à¨†à¨°à¨¡à¨° à¨¸à¨¥à¨¿à¨¤à©€ à¨šà©ˆà©±à¨•  
- **à¨²à¨¾à¨—à¨¤ à¨¦à©‡ à¨«à¨¾à¨‡à¨¦à©‡**: LLM à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¦à©‡ à¨®à©à¨•à¨¾à¨¬à¨²à©‡ 10x à¨˜à¨Ÿà¨¾à¨“  
- **à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨**: à¨°à©à¨Ÿà©€à¨¨ à¨ªà©à¨°à¨¸à¨¼à¨¨à¨¾à¨‚ à¨²à¨ˆ à¨¤à©‡à¨œà¨¼ à¨…à¨¤à©‡ à¨¸à¨¥à¨¿à¨° à¨—à©à¨£à¨µà©±à¨¤à¨¾  

### à¨•à¨¾à¨°à©‹à¨¬à¨¾à¨°à©€ à¨ªà©à¨°à¨•à¨¿à¨°à¨¿à¨† SLM à¨à¨œà©°à¨Ÿ  
- **à¨šà¨²à¨¾à¨¨ à¨ªà©à¨°à¨•à¨¿à¨°à¨¿à¨† à¨à¨œà©°à¨Ÿ**: à¨¡à¨¾à¨Ÿà¨¾ à¨•à©±à¨¢à©‹, à¨œà¨¾à¨£à¨•à¨¾à¨°à©€ à¨¦à©€ à¨ªà©à¨¸à¨¼à¨Ÿà©€ à¨•à¨°à©‹, à¨®à¨¨à¨œà¨¼à©‚à¨°à©€ à¨²à¨ˆ à¨°à©‚à¨Ÿ à¨•à¨°à©‹  
- **à¨ˆà¨®à©‡à¨² à¨ªà©à¨°à¨¬à©°à¨§à¨¨ à¨à¨œà©°à¨Ÿ**: à¨¸à¨¼à©à¨°à©‡à¨£à©€à¨¬à©±à¨§ à¨•à¨°à©‹, à¨¤à¨°à¨œà©€à¨¹ à¨¦à¨¿à¨“, à¨¸à¨µà©ˆ-à¨šà¨¾à¨²à¨¤ à¨œà¨µà¨¾à¨¬ à¨¡à¨°à¨¾à¨«à¨Ÿ à¨•à¨°à©‹  
- **à¨¸à¨¼à¨¡à¨¿à¨Šà¨²à¨¿à©°à¨— à¨à¨œà©°à¨Ÿ**: à¨®à©€à¨Ÿà¨¿à©°à¨—à¨¾à¨‚ à¨¦à¨¾ à¨¸à¨®à¨¨à©à¨¯à©‹, à¨•à©ˆà¨²à©°à¨¡à¨° à¨ªà©à¨°à¨¬à©°à¨§à¨¿à¨¤ à¨•à¨°à©‹, à¨¯à¨¾à¨¦ à¨¦à¨¿à¨µà¨¾à¨“  

### à¨¨à¨¿à©±à¨œà©€ SLM à¨¡à¨¿à¨œà©€à¨Ÿà¨² à¨¸à¨¹à¨¾à¨‡à¨•  
- **à¨Ÿà¨¾à¨¸à¨• à¨ªà©à¨°à¨¬à©°à¨§à¨¨ à¨à¨œà©°à¨Ÿ**: à¨•à©°à¨® à¨¬à¨£à¨¾à¨“, à¨…à¨ªà¨¡à©‡à¨Ÿ à¨•à¨°à©‹, à¨¸à¨§à¨¾à¨°à¨¨ à¨¤à¨°à©€à¨•à©‡ à¨¨à¨¾à¨² à¨¸à©°à¨—à¨ à¨¿à¨¤ à¨•à¨°à©‹  
- **à¨œà¨¾à¨£à¨•à¨¾à¨°à©€ à¨‡à¨•à©±à¨ à©€ à¨•à¨°à¨¨ à¨µà¨¾à¨²à©‡ à¨à¨œà©°à¨Ÿ**: à¨µà¨¿à¨¸à¨¼à¨¿à¨†à¨‚ à¨¦à©€ à¨–à©‹à¨œ à¨•à¨°à©‹, à¨¸à¨¥à¨¾à¨¨à¨• à¨¤à©Œà¨° 'à¨¤à©‡ à¨¨à¨¤à©€à¨œà©‡ à¨¸à©°à¨–à©‡à¨ª à¨•à¨°à©‹  
- **à¨¸à©°à¨šà¨¾à¨° à¨à¨œà©°à¨Ÿ**: à¨ˆà¨®à©‡à¨², à¨¸à©à¨¨à©‡à¨¹à©‡, à¨¸à©‹à¨¸à¨¼à¨² à¨®à©€à¨¡à©€à¨† à¨ªà©‹à¨¸à¨Ÿ à¨¸à¨µà©ˆ-à¨¨à¨¿à©±à¨œà©€ à¨¤à©Œà¨° 'à¨¤à©‡ à¨¡à¨°à¨¾à¨«à¨Ÿ à¨•à¨°à©‹  

### à¨Ÿà©à¨°à©‡à¨¡à¨¿à©°à¨— à¨…à¨¤à©‡ à¨µà¨¿à©±à¨¤à©€ SLM à¨à¨œà©°à¨Ÿ  
- **à¨®à¨¾à¨°à¨•à©€à¨Ÿ à¨®à¨¾à¨¨à©€à¨Ÿà¨°à¨¿à©°à¨— à¨à¨œà©°à¨Ÿ**: à¨•à©€à¨®à¨¤à¨¾à¨‚ à¨¨à©‚à©° à¨Ÿà©à¨°à©ˆà¨• à¨•à¨°à©‹, à¨°à©€à¨…à¨²-à¨Ÿà¨¾à¨ˆà¨® à¨µà¨¿à©±à¨š à¨°à©à¨à¨¾à¨¨ à¨ªà¨›à¨¾à¨£à©‹  
- **à¨°à¨¿à¨ªà©‹à¨°à¨Ÿ à¨œà¨¨à¨°à©‡à¨¸à¨¼à¨¨ à¨à¨œà©°à¨Ÿ**: à¨¸à¨µà©ˆ-à¨šà¨¾à¨²à¨¤ à¨¤à©Œà¨° 'à¨¤à©‡ à¨¦à¨¿à¨¨/à¨¹à¨«à¨¼à¨¤à©‡ à¨¦à©‡ à¨¸à©°à¨–à©‡à¨ª à¨¬à¨£à¨¾à¨“  
- **à¨°à¨¿à¨¸à¨• à¨…à¨¸à©ˆà¨¸à¨®à©ˆà¨‚à¨Ÿ à¨à¨œà©°à¨Ÿ**: à¨¸à¨¥à¨¾à¨¨à¨• à¨¡à¨¾à¨Ÿà¨¾ à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨•à¨°à¨•à©‡ à¨ªà©‹à¨°à¨Ÿà¨«à©‹à¨²à¨¿à¨“ à¨¸à¨¥à¨¿à¨¤à©€à¨†à¨‚ à¨¦à¨¾ à¨®à©à¨²à¨¾à¨‚à¨•à¨£ à¨•à¨°à©‹  

### à¨¸à¨¿à¨¹à¨¤ à¨¸à¨¹à¨¾à¨‡à¨¤à¨¾ SLM à¨à¨œà©°à¨Ÿ  
- **à¨®à¨°à©€à¨œà¨¼ à¨¸à¨¼à¨¡à¨¿à¨Šà¨²à¨¿à©°à¨— à¨à¨œà©°à¨Ÿ**: à¨®à©€à¨Ÿà¨¿à©°à¨—à¨¾à¨‚ à¨¦à¨¾ à¨¸à¨®à¨¨à©à¨¯à©‹, à¨¸à¨µà©ˆ-à¨šà¨¾à¨²à¨¤ à¨¯à¨¾à¨¦ à¨¦à¨¿à¨µà¨¾à¨“  
- **à¨¦à¨¸à¨¤à¨¾à¨µà©‡à¨œà¨¼à©€ à¨à¨œà©°à¨Ÿ**: à¨¸à¨¥à¨¾à¨¨à¨• à¨¤à©Œà¨° 'à¨¤à©‡ à¨®à©ˆà¨¡à©€à¨•à¨² à¨¸à©°à¨–à©‡à¨ª, à¨°à¨¿à¨ªà©‹à¨°à¨Ÿ à¨¬à¨£à¨¾à¨“  
- **à¨ªà©à¨°à¨¿à¨¸à¨•à©à¨°à¨¿à¨ªà¨¸à¨¼à¨¨ à¨ªà©à¨°à¨¬à©°à¨§à¨¨ à¨à¨œà©°à¨Ÿ**: à¨°à©€à¨«à¨¿à¨² à¨Ÿà©à¨°à©ˆà¨• à¨•à¨°à©‹, à¨¨à¨¿à©±à¨œà©€ à¨¤à©Œà¨° 'à¨¤à©‡ à¨‡à©°à¨Ÿà¨°à©ˆà¨•à¨¸à¨¼à¨¨ à¨šà©ˆà©±à¨• à¨•à¨°à©‹  

## à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨•: à¨ªà©à¨°à©‹à¨¡à¨•à¨¸à¨¼à¨¨-à¨¤à¨¿à¨†à¨° à¨à¨œà©°à¨Ÿ à¨µà¨¿à¨•à¨¾à¨¸

### à¨à¨²à¨• à¨…à¨¤à©‡ à¨†à¨°à¨•à©€à¨Ÿà©ˆà¨•à¨šà¨°

à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨‡à©±à¨• à¨µà¨¿à¨¸à¨¤à©à¨°à¨¿à¨¤, à¨‡à©°à¨Ÿà¨°à¨ªà©à¨°à¨¾à¨ˆà¨œà¨¼-à¨—à©à¨°à©‡à¨¡ à¨ªà¨²à©‡à¨Ÿà¨«à¨¾à¨°à¨® à¨ªà©à¨°à¨¦à¨¾à¨¨ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ à¨œà©‹ AI à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¨à©‚à©° à¨¬à¨£à¨¾à¨‰à¨£, à¨¡à¨¿à¨ªà¨²à©Œà¨‡ à¨•à¨°à¨¨ à¨…à¨¤à©‡ à¨ªà©à¨°à¨¬à©°à¨§à¨¿à¨¤ à¨•à¨°à¨¨ à¨²à¨ˆ à¨¹à©ˆ à¨œà©‹ à¨•à¨²à¨¾à¨‰à¨¡ à¨…à¨¤à©‡ à¨†à¨«à¨²à¨¾à¨ˆà¨¨ à¨à¨œ à¨µà¨¾à¨¤à¨¾à¨µà¨°à¨£à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨•à©°à¨® à¨•à¨° à¨¸à¨•à¨¦à©‡ à¨¹à¨¨à¥¤ à¨‡à¨¹ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨–à¨¾à¨¸ à¨¤à©Œà¨° 'à¨¤à©‡ à¨›à©‹à¨Ÿà©‡ à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨²à¨¾à¨‚ à¨…à¨¤à©‡ à¨à¨œ à¨•à©°à¨ªà¨¿à¨Šà¨Ÿà¨¿à©°à¨— à¨¸à¨¥à¨¿à¨¤à©€à¨†à¨‚ à¨¨à¨¾à¨² à¨¬à©‡à¨¹à¨¤à¨°à©€à¨¨ à¨¤à¨°à©€à¨•à©‡ à¨¨à¨¾à¨² à¨•à©°à¨® à¨•à¨°à¨¨ à¨²à¨ˆ à¨¡à¨¿à¨œà¨¼à¨¾à¨ˆà¨¨ à¨•à©€à¨¤à¨¾ à¨—à¨¿à¨† à¨¹à©ˆ, à¨œà¨¿à¨¸ à¨¨à¨¾à¨² à¨—à©‹à¨ªà¨¨à©€à¨¯à¨¤à¨¾-à¨¸à©°à¨µà©‡à¨¦à¨¨à¨¸à¨¼à©€à¨² à¨…à¨¤à©‡ à¨¸à¨°à©‹à¨¤-à¨¸à©€à¨®à¨¿à¨¤ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨²à¨ˆ à¨‡à¨¹ à¨†à¨¦à¨°à¨¸à¨¼ à¨¬à¨£à¨¦à¨¾ à¨¹à©ˆà¥¤

**à¨®à©à©±à¨– à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨˜à¨Ÿà¨•**:  
- **Agent Runtime**: à¨à¨œ à¨¡à¨¿à¨µà¨¾à¨ˆà¨¸à¨¾à¨‚ à¨²à¨ˆ à¨…à¨ªà¨Ÿà©€à¨®à¨¾à¨ˆà¨œà¨¼ à¨•à©€à¨¤à¨¾ à¨¹à¨²à¨•à¨¾ à¨à¨—à¨œà¨¼à¨¿à¨•à¨¿à¨Šà¨¸à¨¼à¨¨ à¨µà¨¾à¨¤à¨¾à¨µà¨°à¨£  
- **Tool Integration System**: à¨¬à¨¾à¨¹à¨°à©€ à¨¸à©‡à¨µà¨¾à¨µà¨¾à¨‚ à¨…à¨¤à©‡ APIs à¨¨à©‚à©° à¨•à¨¨à©ˆà¨•à¨Ÿ à¨•à¨°à¨¨ à¨²à¨ˆ à¨µà¨§à¨¾à¨‰à¨£à¨¯à©‹à¨— à¨ªà¨²à©±à¨—à¨‡à¨¨ à¨†à¨°à¨•à©€à¨Ÿà©ˆà¨•à¨šà¨°  
- **State Management**: à¨¸à©ˆà¨¸à¨¼à¨¨à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨¸à¨¥à¨¿à¨° à¨à¨œà©°à¨Ÿ à¨®à©ˆà¨®à©‹à¨°à©€ à¨…à¨¤à©‡ à¨¸à©°à¨¦à¨°à¨­ à¨¸à©°à¨­à¨¾à¨²  
- **Security Layer**: à¨‡à©°à¨Ÿà¨°à¨ªà©à¨°à¨¾à¨ˆà¨œà¨¼ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨²à¨ˆ à¨¬à¨¿à¨²à¨Ÿ-à¨‡à¨¨ à¨¸à©à¨°à©±à¨–à¨¿à¨† à¨¨à¨¿à¨¯à©°à¨¤à¨°à¨£  
- **Orchestration Engine**: à¨®à¨²à¨Ÿà©€-à¨à¨œà©°à¨Ÿ à¨¸à¨®à¨¨à©à¨¯à©‹ à¨…à¨¤à©‡ à¨µà¨°à¨•à¨«à¨²à©‹ à¨ªà©à¨°à¨¬à©°à¨§à¨¨  

### à¨à¨œ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨²à¨ˆ à¨®à©à©±à¨– à¨«à©€à¨šà¨°à¨¸

**à¨†à¨«à¨²à¨¾à¨ˆà¨¨-à¨ªà¨¹à¨¿à¨²à¨¾ à¨†à¨°à¨•à©€à¨Ÿà©ˆà¨•à¨šà¨°**:  
à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨†à¨«à¨²à¨¾à¨ˆà¨¨-à¨ªà¨¹à¨¿à¨²à©‡ à¨¸à¨¿à¨§à¨¾à¨‚à¨¤à¨¾à¨‚ à¨¨à¨¾à¨² à¨¡à¨¿à¨œà¨¼à¨¾à¨ˆà¨¨ à¨•à©€à¨¤à¨¾ à¨—à¨¿à¨† à¨¹à©ˆ, à¨œà¨¿à¨¸ à¨¨à¨¾à¨² à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¨à©‚à©° à¨²à¨—à¨¾à¨¤à¨¾à¨° à¨‡à©°à¨Ÿà¨°à¨¨à©ˆà¨Ÿ à¨•à¨¨à©ˆà¨•à¨Ÿà¨¿à¨µà¨¿à¨Ÿà©€ à¨¤à©‹à¨‚ à¨¬à¨¿à¨¨à¨¾à¨‚ à¨ªà©à¨°à¨­à¨¾à¨µà¨¸à¨¼à¨¾à¨²à©€ à¨¤à¨°à©€à¨•à©‡ à¨¨à¨¾à¨² à¨•à©°à¨® à¨•à¨°à¨¨ à¨¦à©€ à¨¯à©‹à¨—à¨¤à¨¾ à¨®à¨¿à¨²à¨¦à©€ à¨¹à©ˆà¥¤ à¨‡à¨¸ à¨µà¨¿à©±à¨š à¨¸à¨¥à¨¾à¨¨à¨• à¨®à¨¾à¨¡à¨² à¨‡à©°à¨«à¨°à©ˆà¨‚à¨¸, à¨•à©ˆà¨¸à¨¼ à¨•à©€à¨¤à©‡ à¨—à¨¿à¨†à¨¨ à¨…à¨§à¨¾à¨°, à¨†à¨«à¨²à¨¾à¨ˆà¨¨ à¨Ÿà©‚à¨² à¨à¨—à¨œà¨¼à¨¿à¨•à¨¿à¨Šà¨¸à¨¼à¨¨, à¨…à¨¤à©‡ à¨•à¨²à¨¾à¨‰à¨¡ à¨¸
**à¨à¨œà©°à¨Ÿ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨²à¨ˆ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨šà©‹à¨£**: à¨Ÿà¨¾à¨°à¨—à¨Ÿ à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨…à¨¤à©‡ à¨à¨œà©°à¨Ÿ à¨¦à©€à¨†à¨‚ à¨œà¨¼à¨°à©‚à¨°à¨¤à¨¾à¨‚ à¨¦à©‡ à¨…à¨§à¨¾à¨° 'à¨¤à©‡ à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨šà©à¨£à©‹à¥¤ CPU-à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à¨¡ à¨à¨œà©°à¨Ÿ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨²à¨ˆ Llama.cpp à¨µà¨°à¨¤à©‹, Apple Silicon à¨à¨œà©°à¨Ÿ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨ à¨²à¨ˆ Apple MLX à¨…à¨¤à©‡ à¨•à©à¨°à¨¾à¨¸-à¨ªà¨²à©‡à¨Ÿà¨«à¨¾à¨°à¨® à¨à¨œà©°à¨Ÿ à¨…à¨¨à©à¨•à©‚à¨²à¨¤à¨¾ à¨²à¨ˆ ONNX à¨µà¨°à¨¤à©‹à¥¤

## à¨ªà©à¨°à©ˆà¨•à¨Ÿà¨¿à¨•à¨² SLM à¨à¨œà©°à¨Ÿ à¨•à¨¨à¨µà¨°à¨œà¨¼à¨¨ à¨…à¨¤à©‡ à¨µà¨°à¨¤à©‹à¨‚ à¨¦à©‡ à¨•à©‡à¨¸

### à¨…à¨¸à¨² à¨¦à©à¨¨à©€à¨† à¨¦à©‡ à¨à¨œà©°à¨Ÿ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨¸à¨¥à¨¿à¨¤à©€à¨†à¨‚

**à¨®à©‹à¨¬à¨¾à¨ˆà¨² à¨à¨œà©°à¨Ÿ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨**: Q4_K à¨«à¨¾à¨°à¨®à©ˆà¨Ÿà¨¸ à¨¸à¨®à¨¾à¨°à¨Ÿà¨«à©‹à¨¨ à¨à¨œà©°à¨Ÿ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨ à¨µà¨¿à©±à¨š à¨˜à©±à¨Ÿ à¨¯à¨¾à¨¦à¨¸à¨¼à¨•à¨¤à©€ à¨¦à©€ à¨²à©‹à©œ à¨¨à¨¾à¨² à¨¬à¨¹à©à¨¤ à¨µà¨§à©€à¨† à¨•à©°à¨® à¨•à¨°à¨¦à©‡ à¨¹à¨¨, à¨œà¨¦à¨•à¨¿ Q8_0 à¨Ÿà©ˆà¨¬à¨²à©ˆà¨Ÿ-à¨…à¨§à¨¾à¨°à¨¿à¨¤ à¨à¨œà©°à¨Ÿ à¨¸à¨¿à¨¸à¨Ÿà¨®à¨¾à¨‚ à¨²à¨ˆ à¨¸à©°à¨¤à©à¨²à¨¿à¨¤ à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨ªà©à¨°à¨¦à¨¾à¨¨ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆà¥¤ Q5_K à¨«à¨¾à¨°à¨®à©ˆà¨Ÿà¨¸ à¨®à©‹à¨¬à¨¾à¨ˆà¨² à¨‰à¨¤à¨ªà¨¾à¨¦à¨•à¨¤à¨¾ à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨‰à©±à¨¤à¨® à¨—à©à¨£à¨µà©±à¨¤à¨¾ à¨ªà©à¨°à¨¦à¨¾à¨¨ à¨•à¨°à¨¦à©‡ à¨¹à¨¨à¥¤

**à¨¡à©ˆà¨¸à¨•à¨Ÿà¨¾à¨ª à¨…à¨¤à©‡ à¨à¨œ à¨à¨œà©°à¨Ÿ à¨•à©°à¨ªà¨¿à¨Šà¨Ÿà¨¿à©°à¨—**: Q5_K à¨¡à©ˆà¨¸à¨•à¨Ÿà¨¾à¨ª à¨à¨œà©°à¨Ÿ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨ à¨²à¨ˆ à¨µà¨§à©€à¨† à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨¦à¨¿à©°à¨¦à¨¾ à¨¹à©ˆ, Q8_0 à¨µà¨°à¨•à¨¸à¨Ÿà©‡à¨¸à¨¼à¨¨ à¨à¨œà©°à¨Ÿ à¨µà¨¾à¨¤à¨¾à¨µà¨°à¨£ à¨²à¨ˆ à¨‰à©±à¨š-à¨—à©à¨£à¨µà©±à¨¤à¨¾ à¨‡à©°à¨«à¨°à©ˆà¨‚à¨¸ à¨ªà©à¨°à¨¦à¨¾à¨¨ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ, à¨…à¨¤à©‡ Q4_K à¨à¨œ à¨à¨œà©°à¨Ÿ à¨¡à¨¿à¨µà¨¾à¨ˆà¨¸à¨¾à¨‚ 'à¨¤à©‡ à¨•à©à¨¸à¨¼à¨² à¨ªà©à¨°à©‹à¨¸à©ˆà¨¸à¨¿à©°à¨— à¨²à¨ˆ à¨¯à©‹à¨— à¨¹à©ˆà¥¤

**à¨—à¨µà©ˆà¨¸à¨¼à¨£à¨¾ à¨…à¨¤à©‡ à¨ªà©à¨°à¨¯à©‹à¨—à¨¾à¨¤à¨®à¨• à¨à¨œà©°à¨Ÿ**: à¨…à¨—à¨°à¨¸à¨° à¨•à¨µà¨¾à¨‚à¨Ÿà©€à¨œà¨¼à©‡à¨¸à¨¼à¨¨ à¨«à¨¾à¨°à¨®à©ˆà¨Ÿà¨¸ à¨…à¨•à¨¾à¨¦à¨®à¨¿à¨• à¨—à¨µà©ˆà¨¸à¨¼à¨£à¨¾ à¨…à¨¤à©‡ à¨¸à¨¬à©‚à¨¤-à¨…à¨«à¨¼à¨•à¨¾à¨°à©€ à¨à¨œà©°à¨Ÿ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨²à¨ˆ à¨…à¨¤à¨¿ à¨˜à©±à¨Ÿ à¨¸à©°à¨¸à¨¾à¨§à¨¨ à¨¦à©€ à¨²à©‹à©œ à¨µà¨¾à¨²à©‡ à¨…à¨²à¨Ÿà¨°à¨¾-à¨²à©‹ à¨ªà©à¨°à¨¿à¨¸à©€à¨¸à¨¼à¨¨ à¨à¨œà©°à¨Ÿ à¨‡à©°à¨«à¨°à©ˆà¨‚à¨¸ à¨¦à©€ à¨ªà©œà¨šà©‹à¨² à¨•à¨°à¨¨ à¨¦à©€ à¨¯à©‹à¨—à¨¤à¨¾ à¨¦à¨¿à©°à¨¦à©‡ à¨¹à¨¨à¥¤

### SLM à¨à¨œà©°à¨Ÿ à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨¬à©ˆà¨‚à¨šà¨®à¨¾à¨°à¨•

**à¨à¨œà©°à¨Ÿ à¨‡à©°à¨«à¨°à©ˆà¨‚à¨¸ à¨—à¨¤à©€**: Q4_K à¨®à©‹à¨¬à¨¾à¨ˆà¨² CPUs 'à¨¤à©‡ à¨¸à¨­ à¨¤à©‹à¨‚ à¨¤à©‡à¨œà¨¼ à¨à¨œà©°à¨Ÿ à¨œà¨µà¨¾à¨¬ à¨¸à¨®à¨¾à¨‚ à¨ªà©à¨°à¨¾à¨ªà¨¤ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ, Q5_K à¨†à¨® à¨à¨œà©°à¨Ÿ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨ à¨²à¨ˆ à¨—à¨¤à©€-à¨—à©à¨£à¨µà©±à¨¤à¨¾ à¨¸à©°à¨¤à©à¨²à¨¨ à¨ªà©à¨°à¨¦à¨¾à¨¨ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ, Q8_0 à¨œà¨Ÿà¨¿à¨² à¨à¨œà©°à¨Ÿ à¨•à©°à¨®à¨¾à¨‚ à¨²à¨ˆ à¨‰à©±à¨¤à¨® à¨—à©à¨£à¨µà©±à¨¤à¨¾ à¨¦à¨¿à©°à¨¦à¨¾ à¨¹à©ˆ, à¨…à¨¤à©‡ à¨ªà©à¨°à¨¯à©‹à¨—à¨¾à¨¤à¨®à¨• à¨«à¨¾à¨°à¨®à©ˆà¨Ÿà¨¸ à¨µà¨¿à¨¸à¨¼à©‡à¨¸à¨¼ à¨à¨œà©°à¨Ÿ à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨²à¨ˆ à¨µà©±à¨§ à¨¤à©‹à¨‚ à¨µà©±à¨§ à¨”à¨Ÿà¨ªà©à©±à¨Ÿ à¨ªà©à¨°à¨¦à¨¾à¨¨ à¨•à¨°à¨¦à©‡ à¨¹à¨¨à¥¤

**à¨à¨œà©°à¨Ÿ à¨¯à¨¾à¨¦à¨¸à¨¼à¨•à¨¤à©€ à¨¦à©€ à¨²à©‹à©œ**: à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨•à¨µà¨¾à¨‚à¨Ÿà©€à¨œà¨¼à©‡à¨¸à¨¼à¨¨ à¨ªà©±à¨§à¨° Q2_K (à¨›à©‹à¨Ÿà©‡ à¨à¨œà©°à¨Ÿ à¨®à¨¾à¨¡à¨²à¨¾à¨‚ à¨²à¨ˆ 500MB à¨¤à©‹à¨‚ à¨˜à©±à¨Ÿ) à¨¤à©‹à¨‚ Q8_0 (à¨®à©‚à¨² à¨†à¨•à¨¾à¨° à¨¦à¨¾ à¨²à¨—à¨­à¨— 50%) à¨¤à©±à¨• à¨¹à©à©°à¨¦à©‡ à¨¹à¨¨, à¨œà¨¦à¨•à¨¿ à¨ªà©à¨°à¨¯à©‹à¨—à¨¾à¨¤à¨®à¨• à¨¸à©°à¨°à¨šà¨¨à¨¾à¨µà¨¾à¨‚ à¨¸à©°à¨¸à¨¾à¨§à¨¨-à¨¸à©€à¨®à¨¿à¨¤ à¨à¨œà©°à¨Ÿ à¨µà¨¾à¨¤à¨¾à¨µà¨°à¨£à¨¾à¨‚ à¨²à¨ˆ à¨µà©±à¨§ à¨¤à©‹à¨‚ à¨µà©±à¨§ à¨•à©°à¨ªà©à¨°à©ˆà¨¸à¨¼à¨¨ à¨ªà©à¨°à¨¾à¨ªà¨¤ à¨•à¨°à¨¦à©‡ à¨¹à¨¨à¥¤

## SLM à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨šà©à¨£à©Œà¨¤à©€à¨†à¨‚ à¨…à¨¤à©‡ à¨µà¨¿à¨šà¨¾à¨°

### à¨à¨œà©°à¨Ÿ à¨¸à¨¿à¨¸à¨Ÿà¨®à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨¦à©‡ à¨¤à¨¿à¨†à¨—

SLM à¨à¨œà©°à¨Ÿ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨®à¨¾à¨¡à¨² à¨†à¨•à¨¾à¨°, à¨à¨œà©°à¨Ÿ à¨œà¨µà¨¾à¨¬ à¨¦à©€ à¨—à¨¤à©€, à¨…à¨¤à©‡ à¨†à¨‰à¨Ÿà¨ªà©à©±à¨Ÿ à¨—à©à¨£à¨µà©±à¨¤à¨¾ à¨¦à©‡ à¨µà¨¿à¨šà¨•à¨¾à¨° à¨¤à¨¿à¨†à¨—à¨¾à¨‚ à¨¦à©€ à¨¸à¨¾à¨µà¨§à¨¾à¨¨à©€ à¨¨à¨¾à¨² à¨µà¨¿à¨šà¨¾à¨° à¨•à¨°à¨¨ à¨¦à©€ à¨²à©‹à©œ à¨¦à¨¿à©°à¨¦à¨¾ à¨¹à©ˆà¥¤ à¨œà¨¦à¨•à¨¿ Q4_K à¨®à©‹à¨¬à¨¾à¨ˆà¨² à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨…à¨¸à¨§à¨¾à¨°à¨¨ à¨—à¨¤à©€ à¨…à¨¤à©‡ à¨•à©à¨¸à¨¼à¨²à¨¤à¨¾ à¨ªà©à¨°à¨¦à¨¾à¨¨ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ, Q8_0 à¨œà¨Ÿà¨¿à¨² à¨à¨œà©°à¨Ÿ à¨•à©°à¨®à¨¾à¨‚ à¨²à¨ˆ à¨‰à©±à¨¤à¨® à¨—à©à¨£à¨µà©±à¨¤à¨¾ à¨¦à¨¿à©°à¨¦à¨¾ à¨¹à©ˆà¥¤ Q5_K à¨œà¨¼à¨¿à¨†à¨¦à¨¾à¨¤à¨° à¨†à¨® à¨à¨œà©°à¨Ÿ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨ à¨²à¨ˆ à¨‡à©±à¨• à¨®à©±à¨§à¨® à¨°à¨¾à¨¹ à¨ªà©à¨°à¨¦à¨¾à¨¨ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆà¥¤

### SLM à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨…à¨¨à©à¨•à©‚à¨²à¨¤à¨¾

à¨µà©±à¨–-à¨µà©±à¨– à¨à¨œ à¨¡à¨¿à¨µà¨¾à¨ˆà¨¸à¨¾à¨‚ à¨µà¨¿à©±à¨š SLM à¨à¨œà©°à¨Ÿ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨²à¨ˆ à¨µà©±à¨–-à¨µà©±à¨– à¨¯à©‹à¨—à¨¤à¨¾à¨µà¨¾à¨‚ à¨¹à©à©°à¨¦à©€à¨†à¨‚ à¨¹à¨¨à¥¤ Q4_K à¨¸à¨§à¨¾à¨°à¨¨ à¨ªà©à¨°à©‹à¨¸à©ˆà¨¸à¨°à¨¾à¨‚ 'à¨¤à©‡ à¨¸à¨§à¨¾à¨°à¨¨ à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨•à©à¨¸à¨¼à¨²à¨¤à¨¾à¨ªà©‚à¨°à¨µà¨• à¨šà©±à¨²à¨¦à¨¾ à¨¹à©ˆ, Q5_K à¨¸à©°à¨¤à©à¨²à¨¿à¨¤ à¨à¨œà©°à¨Ÿ à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨²à¨ˆ à¨®à¨§à¨® à¨—à¨£à¨¨à¨¾ à¨¸à©°à¨¸à¨¾à¨§à¨¨ à¨¦à©€ à¨²à©‹à©œ à¨¦à¨¿à©°à¨¦à¨¾ à¨¹à©ˆ, à¨…à¨¤à©‡ Q8_0 à¨‰à©±à¨š-à¨…à©°à¨¤ à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨¤à©‹à¨‚ à¨²à¨¾à¨­ à¨‰à¨ à¨¾à¨‰à¨‚à¨¦à¨¾ à¨¹à©ˆ à¨œà¨Ÿà¨¿à¨² à¨à¨œà©°à¨Ÿ à¨¯à©‹à¨—à¨¤à¨¾à¨µà¨¾à¨‚ à¨²à¨ˆà¥¤

### SLM à¨à¨œà©°à¨Ÿ à¨¸à¨¿à¨¸à¨Ÿà¨®à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨¸à©à¨°à©±à¨–à¨¿à¨† à¨…à¨¤à©‡ à¨—à©‹à¨ªà¨¨à©€à¨¯à¨¤à¨¾

à¨œà¨¦à¨•à¨¿ SLM à¨à¨œà©°à¨Ÿ à¨¸à¨¥à¨¾à¨¨à¨• à¨ªà©à¨°à©‹à¨¸à©ˆà¨¸à¨¿à©°à¨— à¨²à¨ˆ à¨µà¨§à©‡à¨°à©‡ à¨—à©‹à¨ªà¨¨à©€à¨¯à¨¤à¨¾ à¨¯à©‹à¨— à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à©‡ à¨¹à¨¨, à¨¸à¨¹à©€ à¨¸à©à¨°à©±à¨–à¨¿à¨† à¨‰à¨ªà¨¾à¨… à¨²à¨¾à¨—à©‚ à¨•à¨°à¨¨ à¨¦à©€ à¨²à©‹à©œ à¨¹à©à©°à¨¦à©€ à¨¹à©ˆ à¨¤à¨¾à¨‚ à¨œà©‹ à¨à¨œ à¨µà¨¾à¨¤à¨¾à¨µà¨°à¨£à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨à¨œà©°à¨Ÿ à¨®à¨¾à¨¡à¨²à¨¾à¨‚ à¨…à¨¤à©‡ à¨¡à¨¾à¨Ÿà¨¾ à¨¦à©€ à¨°à©±à¨–à¨¿à¨† à¨•à©€à¨¤à©€ à¨œà¨¾ à¨¸à¨•à©‡à¥¤ à¨‡à¨¹ à¨–à¨¾à¨¸ à¨¤à©Œà¨° 'à¨¤à©‡ à¨®à¨¹à©±à¨¤à¨µà¨ªà©‚à¨°à¨¨ à¨¹à©ˆ à¨œà¨¦à©‹à¨‚ à¨‰à©±à¨š-à¨ªà©à¨°à¨¿à¨¸à©€à¨¸à¨¼à¨¨ à¨à¨œà©°à¨Ÿ à¨«à¨¾à¨°à¨®à©ˆà¨Ÿà¨¸ à¨¨à©‚à©° à¨à¨¨à¨Ÿà¨°à¨ªà©à¨°à¨¾à¨ˆà¨œà¨¼ à¨µà¨¾à¨¤à¨¾à¨µà¨°à¨£à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨œà¨¾à¨‚ à¨¸à©°à¨•à©à¨šà¨¿à¨¤ à¨à¨œà©°à¨Ÿ à¨«à¨¾à¨°à¨®à©ˆà¨Ÿà¨¸ à¨¨à©‚à©° à¨¸à©°à¨µà©‡à¨¦à¨¨à¨¸à¨¼à©€à¨² à¨¡à¨¾à¨Ÿà¨¾ à¨¸à©°à¨­à¨¾à¨²à¨£ à¨µà¨¾à¨²à©€à¨†à¨‚ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨¤à©ˆà¨¨à¨¾à¨¤ à¨•à©€à¨¤à¨¾ à¨œà¨¾à¨‚à¨¦à¨¾ à¨¹à©ˆà¥¤

## SLM à¨à¨œà©°à¨Ÿ à¨µà¨¿à¨•à¨¾à¨¸ à¨µà¨¿à©±à¨š à¨­à¨µà¨¿à©±à¨– à¨¦à©‡ à¨°à©à¨à¨¾à¨¨

SLM à¨à¨œà©°à¨Ÿ à¨²à©ˆà¨‚à¨¡à¨¸à¨•à©‡à¨ª à¨¸à©°à¨•à©à¨šà¨¨ à¨¤à¨•à¨¨à©€à¨•à¨¾à¨‚, à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨ à¨µà¨¿à¨§à©€à¨†à¨‚, à¨…à¨¤à©‡ à¨à¨œ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨°à¨£à¨¨à©€à¨¤à©€à¨†à¨‚ à¨µà¨¿à©±à¨š à¨¤à¨°à©±à¨•à©€ à¨¨à¨¾à¨² à¨µà¨¿à¨•à¨¸à¨¤ à¨¹à©‹ à¨°à¨¿à¨¹à¨¾ à¨¹à©ˆà¥¤ à¨­à¨µà¨¿à©±à¨– à¨¦à©‡ à¨µà¨¿à¨•à¨¾à¨¸ à¨µà¨¿à©±à¨š à¨à¨œà©°à¨Ÿ à¨®à¨¾à¨¡à¨²à¨¾à¨‚ à¨²à¨ˆ à¨¹à©‹à¨° à¨•à©à¨¸à¨¼à¨² à¨•à¨µà¨¾à¨‚à¨Ÿà©€à¨œà¨¼à©‡à¨¸à¨¼à¨¨ à¨à¨²à¨—à©‹à¨°à¨¿à¨¥à¨®, à¨à¨œà©°à¨Ÿ à¨µà¨°à¨•à¨«à¨²à©‹à¨œà¨¼ à¨²à¨ˆ à¨¸à©à¨§à¨¾à¨°à¨¿à¨† à¨¸à©°à¨•à©à¨šà¨¨ à¨µà¨¿à¨§à©€à¨†à¨‚, à¨…à¨¤à©‡ à¨à¨œ à¨¹à¨¾à¨°à¨¡à¨µà©‡à¨…à¨° à¨à¨•à¨¸à©ˆà¨²à©‡à¨°à©‡à¨Ÿà¨°à¨¾à¨‚ à¨¨à¨¾à¨² à¨¬à¨¿à¨¹à¨¤à¨° à¨‡à©°à¨Ÿà©€à¨—à©à¨°à©‡à¨¸à¨¼à¨¨ à¨¸à¨¼à¨¾à¨®à¨² à¨¹à¨¨à¥¤

**SLM à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨®à¨¾à¨°à¨•à©€à¨Ÿ à¨…à¨¨à©à¨®à¨¾à¨¨**: à¨¹à¨¾à¨²à©€à¨† à¨—à¨µà©ˆà¨¸à¨¼à¨£à¨¾ à¨¦à©‡ à¨…à¨¨à©à¨¸à¨¾à¨°, 2027 à¨¤à©±à¨• à¨à¨¨à¨Ÿà¨°à¨ªà©à¨°à¨¾à¨ˆà¨œà¨¼ à¨µà¨°à¨•à¨«à¨²à©‹à¨œà¨¼ à¨µà¨¿à©±à¨š 40â€“60% à¨¦à©à¨¹à¨°à¨¾à¨ à¨œà¨¾à¨£ à¨µà¨¾à¨²à©‡ à¨œà¨¼à¨¹à¨¿à¨¨à©€ à¨•à©°à¨®à¨¾à¨‚ à¨¨à©‚à©° à¨à¨œà©°à¨Ÿ-à¨šà¨¾à¨²à¨¤ à¨†à¨Ÿà©‹à¨®à©‡à¨¸à¨¼à¨¨ à¨¦à©à¨†à¨°à¨¾ à¨–à¨¤à¨® à¨•à©€à¨¤à¨¾ à¨œà¨¾ à¨¸à¨•à¨¦à¨¾ à¨¹à©ˆ, à¨œà¨¿à¨¸ à¨µà¨¿à©±à¨š SLMs à¨‡à¨¸ à¨¬à¨¦à¨²à¨¾à¨… à¨¦à¨¾ à¨…à¨—à¨µà¨¾à¨ˆ à¨•à¨°à¨¦à©‡ à¨¹à¨¨ à¨•à¨¿à¨‰à¨‚à¨•à¨¿ à¨‡à¨¹à¨¨à¨¾à¨‚ à¨¦à©€ à¨²à¨¾à¨—à¨¤ à¨•à©à¨¸à¨¼à¨²à¨¤à¨¾ à¨…à¨¤à©‡ à¨¤à©ˆà¨¨à¨¾à¨¤ à¨•à¨°à¨¨ à¨¦à©€ à¨²à¨šà©€à¨²à¨¤à¨¾ à¨¹à©ˆà¥¤

**SLM à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨µà¨¿à©±à¨š à¨¤à¨•à¨¨à¨¾à¨²à©‹à¨œà©€ à¨°à©à¨à¨¾à¨¨**:
- **à¨µà¨¿à¨¸à¨¼à©‡à¨¸à¨¼ SLM à¨à¨œà©°à¨Ÿ**: à¨–à¨¾à¨¸ à¨à¨œà©°à¨Ÿ à¨•à©°à¨®à¨¾à¨‚ à¨…à¨¤à©‡ à¨‰à¨¦à¨¯à©‹à¨—à¨¾à¨‚ à¨²à¨ˆ à¨¤à¨¿à¨†à¨° à¨•à©€à¨¤à©‡ à¨¡à©‹à¨®à©‡à¨¨-à¨µà¨¿à¨¸à¨¼à©‡à¨¸à¨¼ à¨®à¨¾à¨¡à¨²
- **à¨à¨œ à¨à¨œà©°à¨Ÿ à¨•à©°à¨ªà¨¿à¨Šà¨Ÿà¨¿à©°à¨—**: à¨¸à©à¨§à¨¾à¨°à¨¿à¨† à¨¸à¨¥à¨¾à¨¨à¨• à¨à¨œà©°à¨Ÿ à¨¯à©‹à¨—à¨¤à¨¾à¨µà¨¾à¨‚ à¨¨à¨¾à¨² à¨µà¨§à©‡à¨°à©‡ à¨—à©‹à¨ªà¨¨à©€à¨¯à¨¤à¨¾ à¨…à¨¤à©‡ à¨˜à©±à¨Ÿ à¨²à©ˆà¨Ÿà©ˆà¨‚à¨¸à©€
- **à¨à¨œà©°à¨Ÿ à¨†à¨°à¨•à©ˆà¨¸à¨Ÿà©à¨°à©‡à¨¸à¨¼à¨¨**: à¨•à¨ˆ SLM à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¦à©‡ à¨µà¨¿à¨šà¨•à¨¾à¨° à¨¬à¨¿à¨¹à¨¤à¨° à¨¸à¨¹à¨¿-à¨¸à©°à¨¯à©‹à¨œà¨¨ à¨¨à¨¾à¨² à¨—à¨¤à©€à¨¸à¨¼à©€à¨² à¨°à©‚à¨Ÿà¨¿à©°à¨— à¨…à¨¤à©‡ à¨²à©‹à¨¡ à¨¬à©ˆà¨²à©ˆà¨‚à¨¸à¨¿à©°à¨—
- **à¨¡à©ˆà¨®à©‹à¨•à©à¨°à©ˆà¨Ÿà¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨**: SLM à¨¦à©€ à¨²à¨šà©€à¨²à¨¤à¨¾ à¨¸à©°à¨—à¨ à¨¨à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨à¨œà©°à¨Ÿ à¨µà¨¿à¨•à¨¾à¨¸ à¨µà¨¿à©±à¨š à¨µà¨¿à¨†à¨ªà¨• à¨­à¨¾à¨—à©€à¨¦à¨¾à¨°à©€ à¨¯à©‹à¨— à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à©€ à¨¹à©ˆ

## SLM à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¨à¨¾à¨² à¨¸à¨¼à©à¨°à©‚à¨†à¨¤

### à¨ªà¨¹à¨²à¨¾ à¨•à¨¦à¨®: à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨µà¨¾à¨¤à¨¾à¨µà¨°à¨£ à¨¸à©ˆà¨Ÿà¨…à¨ª à¨•à¨°à©‹

**à¨¡à¨¿à¨ªà©ˆà¨‚à¨¡à©ˆà¨‚à¨¸à©€à¨œà¨¼ à¨‡à©°à¨¸à¨Ÿà¨¾à¨² à¨•à¨°à©‹**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**à¨«à¨¾à¨‰à¨‚à¨¡à¨°à©€ à¨²à©‹à¨•à¨² à¨¸à¨¼à©à¨°à©‚ à¨•à¨°à©‹**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### à¨¦à©‚à¨œà¨¾ à¨•à¨¦à¨®: à¨†à¨ªà¨£à©‡ à¨à¨œà©°à¨Ÿ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨ à¨²à¨ˆ SLM à¨šà©à¨£à©‹
à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨²à¨ˆ à¨ªà©à¨°à¨¸à¨¿à©±à¨§ à¨µà¨¿à¨•à¨²à¨ª:
- **Microsoft Phi-4 Mini (3.8B)**: à¨¸à©°à¨¤à©à¨²à¨¿à¨¤ à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨¨à¨¾à¨² à¨†à¨® à¨à¨œà©°à¨Ÿ à¨•à©°à¨®à¨¾à¨‚ à¨²à¨ˆ à¨¸à¨¼à¨¾à¨¨à¨¦à¨¾à¨°
- **Qwen2.5-0.5B (0.5B)**: à¨¸à¨§à¨¾à¨°à¨¨ à¨°à©‚à¨Ÿà¨¿à©°à¨— à¨…à¨¤à©‡ à¨µà¨°à¨—à©€à¨•à¨°à¨¨ à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨…à¨¤à¨¿-à¨•à©à¨¸à¨¼à¨²
- **Qwen2.5-Coder-0.5B (0.5B)**: à¨•à©‹à¨¡-à¨¸à©°à¨¬à©°à¨§à©€ à¨à¨œà©°à¨Ÿ à¨•à©°à¨®à¨¾à¨‚ à¨²à¨ˆ à¨–à¨¾à¨¸
- **Phi-4 (7B)**: à¨œà¨Ÿà¨¿à¨² à¨à¨œ à¨¸à¨¥à¨¿à¨¤à©€à¨†à¨‚ à¨²à¨ˆ à¨…à¨—à¨°à¨¸à¨° à¨¤à¨°à¨• à¨œà¨¦à©‹à¨‚ à¨¸à©°à¨¸à¨¾à¨§à¨¨ à¨¯à©‹à¨— à¨¹à©‹à¨£

### à¨¤à©€à¨œà¨¾ à¨•à¨¦à¨®: à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨¨à¨¾à¨² à¨†à¨ªà¨£à¨¾ à¨ªà¨¹à¨¿à¨²à¨¾ à¨à¨œà©°à¨Ÿ à¨¬à¨£à¨¾à¨“

**à¨®à©à©±à¨¢à¨²à©€ à¨à¨œà©°à¨Ÿ à¨¸à©ˆà¨Ÿà¨…à¨ª**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### à¨šà©Œà¨¥à¨¾ à¨•à¨¦à¨®: à¨à¨œà©°à¨Ÿ à¨¦à¨¾ à¨¦à¨¾à¨‡à¨°à¨¾ à¨…à¨¤à©‡ à¨œà¨¼à¨°à©‚à¨°à¨¤à¨¾à¨‚ à¨ªà¨°à¨¿à¨­à¨¾à¨¸à¨¼à¨¿à¨¤ à¨•à¨°à©‹
à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨•à¨°à¨•à©‡ à¨•à©‡à¨‚à¨¦à¨°à¨¿à¨¤, à¨šà©°à¨—à©€ à¨¤à¨°à©à¨¹à¨¾à¨‚ à¨ªà¨°à¨¿à¨­à¨¾à¨¸à¨¼à¨¿à¨¤ à¨à¨œà©°à¨Ÿ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨ à¨¨à¨¾à¨² à¨¸à¨¼à©à¨°à©‚à¨†à¨¤ à¨•à¨°à©‹:
- **à¨‡à©±à¨• à¨¡à©‹à¨®à©‡à¨¨ à¨µà¨¾à¨²à©‡ à¨à¨œà©°à¨Ÿ**: à¨—à¨¾à¨¹à¨• à¨¸à©‡à¨µà¨¾ à¨œà¨¾à¨‚ à¨¸à¨¼à¨¡à¨¿à¨Šà¨²à¨¿à©°à¨— à¨œà¨¾à¨‚ à¨—à¨µà©ˆà¨¸à¨¼à¨£à¨¾
- **à¨¸à¨ªà¨¸à¨¼à¨Ÿ à¨à¨œà©°à¨Ÿ à¨‰à¨¦à©‡à¨¸à¨¼**: à¨à¨œà©°à¨Ÿ à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨²à¨ˆ à¨–à¨¾à¨¸, à¨®à¨¾à¨ªà¨£à¨¯à©‹à¨— à¨²à¨•à¨¸à¨¼
- **à¨¸à¨¿à¨®à¨¤ à¨Ÿà©‚à¨² à¨‡à©°à¨Ÿà©€à¨—à©à¨°à©‡à¨¸à¨¼à¨¨**: à¨¸à¨¼à©à¨°à©‚à¨†à¨¤à©€ à¨à¨œà©°à¨Ÿ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨²à¨ˆ 3-5 à¨Ÿà©‚à¨² à¨µà©±à¨§ à¨¤à©‹à¨‚ à¨µà©±à¨§
- **à¨ªà¨°à¨¿à¨­à¨¾à¨¸à¨¼à¨¿à¨¤ à¨à¨œà©°à¨Ÿ à¨¸à©€à¨®à¨¾à¨µà¨¾à¨‚**: à¨œà¨Ÿà¨¿à¨² à¨¸à¨¥à¨¿à¨¤à©€à¨†à¨‚ à¨²à¨ˆ à¨¸à¨ªà¨¸à¨¼à¨Ÿ à¨à¨¸à¨•à¨²à©‡à¨¸à¨¼à¨¨ à¨ªà¨¾à¨¥
- **à¨à¨œ-à¨ªà¨¹à¨¿à¨²à¨¾ à¨¡à¨¿à¨œà¨¼à¨¾à¨ˆà¨¨**: à¨†à¨«à¨²à¨¾à¨ˆà¨¨ à¨«à©°à¨•à¨¸à¨¼à¨¨à¨²à¨Ÿà©€ à¨…à¨¤à©‡ à¨¸à¨¥à¨¾à¨¨à¨• à¨ªà©à¨°à©‹à¨¸à©ˆà¨¸à¨¿à©°à¨— à¨¨à©‚à©° à¨¤à¨°à¨œà©€à¨¹ à¨¦à¨¿à¨“

### à¨ªà©°à¨œà¨µà¨¾à¨‚ à¨•à¨¦à¨®: à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨¨à¨¾à¨² à¨à¨œ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨²à¨¾à¨—à©‚ à¨•à¨°à©‹

**à¨¸à©°à¨¸à¨¾à¨§à¨¨ à¨¸à©°à¨°à¨šà¨¨à¨¾**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**à¨à¨œ à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨¸à©à¨°à©±à¨–à¨¿à¨† à¨‰à¨ªà¨¾à¨… à¨¤à©ˆà¨¨à¨¾à¨¤ à¨•à¨°à©‹**:
- **à¨¸à¨¥à¨¾à¨¨à¨• à¨‡à¨¨à¨ªà©à¨Ÿ à¨µà©ˆà¨°à©€à¨«à¨¿à¨•à©‡à¨¸à¨¼à¨¨**: à¨¬à¨¿à¨¨à¨¾à¨‚ à¨•à¨²à¨¾à¨‰à¨¡ à¨¨à¨¿à¨°à¨­à¨°à¨¤à¨¾ à¨¦à©‡ à¨¬à©‡à¨¨à¨¤à©€à¨†à¨‚ à¨¦à©€ à¨œà¨¾à¨‚à¨š à¨•à¨°à©‹
- **à¨†à¨«à¨²à¨¾à¨ˆà¨¨ à¨†à¨‰à¨Ÿà¨ªà©à©±à¨Ÿ à¨«à¨¿à¨²à¨Ÿà¨°à©‡à¨¸à¨¼à¨¨**: à¨¯à¨•à©€à¨¨à©€ à¨¬à¨£à¨¾à¨“ à¨•à¨¿ à¨œà¨µà¨¾à¨¬ à¨¸à¨¥à¨¾à¨¨à¨• à¨¤à©Œà¨° 'à¨¤à©‡ à¨—à©à¨£à¨µà©±à¨¤à¨¾ à¨®à¨¿à¨†à¨°à¨¾à¨‚ à¨¨à©‚à©° à¨ªà©‚à¨°à¨¾ à¨•à¨°à¨¦à©‡ à¨¹à¨¨
- **à¨à¨œ à¨¸à©à¨°à©±à¨–à¨¿à¨† à¨¨à¨¿à¨¯à©°à¨¤à¨°à¨£**: à¨‡à©°à¨Ÿà¨°à¨¨à©ˆà¨Ÿ à¨•à¨¨à©ˆà¨•à¨Ÿà©€à¨µà¨¿à¨Ÿà©€ à¨¦à©€ à¨²à©‹à©œ à¨¬à¨¿à¨¨à¨¾à¨‚ à¨¸à©à¨°à©±à¨–à¨¿à¨† à¨²à¨¾à¨—à©‚ à¨•à¨°à©‹
- **à¨¸à¨¥à¨¾à¨¨à¨• à¨¨à¨¿à¨—à¨°à¨¾à¨¨à©€**: à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨¨à©‚à©° à¨Ÿà©à¨°à©ˆà¨• à¨•à¨°à©‹ à¨…à¨¤à©‡ à¨à¨œ à¨Ÿà©ˆà¨²à©€à¨®à©ˆà¨Ÿà¨°à©€ à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨•à¨°à¨•à©‡ à¨¸à¨®à©±à¨¸à¨¿à¨†à¨µà¨¾à¨‚ à¨¨à©‚à©° à¨«à¨²à©ˆà¨— à¨•à¨°à©‹

### à¨›à©‡à¨µà¨¾à¨‚ à¨•à¨¦à¨®: à¨à¨œ à¨à¨œà©°à¨Ÿ à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨¨à©‚à©° à¨®à¨¾à¨ªà©‹ à¨…à¨¤à©‡ à¨¸à©à¨§à¨¾à¨°à©‹
- **à¨à¨œà©°à¨Ÿ à¨Ÿà¨¾à¨¸à¨• à¨ªà©‚à¨°à¨¨ à¨¦à¨°**: à¨†à¨«à¨²à¨¾à¨ˆà¨¨ à¨¸à¨¥à¨¿à¨¤à©€à¨†à¨‚ à¨µà¨¿à©±à¨š à¨¸à¨«à¨²à¨¤à¨¾ à¨¦à¨°à¨¾à¨‚ à¨¦à©€ à¨¨à¨¿à¨—à¨°à¨¾à¨¨à©€ à¨•à¨°à©‹
- **à¨à¨œà©°à¨Ÿ à¨œà¨µà¨¾à¨¬ à¨¸à¨®à¨¾à¨‚**: à¨à¨œ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨²à¨ˆ à¨¸à¨¬-à¨¸à©ˆà¨•à©°à¨¡ à¨œà¨µà¨¾à¨¬ à¨¸à¨®à¨¾à¨‚ à¨¯à¨•à©€à¨¨à©€ à¨¬à¨£à¨¾à¨“
- **à¨¸à©°à¨¸à¨¾à¨§à¨¨ à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚**: à¨à¨œ à¨¡à¨¿à¨µà¨¾à¨ˆà¨¸à¨¾à¨‚ 'à¨¤à©‡ à¨¯à¨¾à¨¦à¨¸à¨¼à¨•à¨¤à©€, CPU, à¨…à¨¤à©‡ à¨¬à©ˆà¨Ÿà¨°à©€ à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨¨à©‚à©° à¨Ÿà©à¨°à©ˆà¨• à¨•à¨°à©‹
- **à¨²à¨¾à¨—à¨¤ à¨•à©à¨¸à¨¼à¨²à¨¤à¨¾**: à¨•à¨²à¨¾à¨‰à¨¡-à¨…à¨§à¨¾à¨°à¨¿à¨¤ à¨µà¨¿à¨•à¨²à¨ªà¨¾à¨‚ à¨¨à¨¾à¨² à¨à¨œ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨²à¨¾à¨—à¨¤à¨¾à¨‚ à¨¦à©€ à¨¤à©à¨²à¨¨à¨¾ à¨•à¨°à©‹
- **à¨†à¨«à¨²à¨¾à¨ˆà¨¨ à¨­à¨°à©‹à¨¸à©‡à¨¯à©‹à¨—à¨¤à¨¾**: à¨¨à©ˆà¨Ÿà¨µà¨°à¨• à¨¬à©°à¨¦ à¨¹à©‹à¨£ à¨¦à©‡ à¨¦à©Œà¨°à¨¾à¨¨ à¨à¨œà©°à¨Ÿ à¨ªà©à¨°à¨¦à¨°à¨¸à¨¼à¨¨ à¨¨à©‚à©° à¨®à¨¾à¨ªà©‹

## SLM à¨à¨œà©°à¨Ÿ à¨²à¨¾à¨—à©‚ à¨•à¨°à¨¨ à¨²à¨ˆ à¨®à©à©±à¨– à¨¸à¨¿à©±à¨–à¨£à©€à¨†à¨‚

1. **SLMs à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨¯à©‹à¨— à¨¹à¨¨**: à¨œà¨¼à¨¿à¨†à¨¦à¨¾à¨¤à¨° à¨à¨œà©°à¨Ÿ à¨•à©°à¨®à¨¾à¨‚ à¨²à¨ˆ, à¨›à©‹à¨Ÿà©‡ à¨®à¨¾à¨¡à¨² à¨µà©±à¨¡à©‡ à¨®à¨¾à¨¡à¨²à¨¾à¨‚ à¨¦à©‡ à¨¬à¨°à¨¾à¨¬à¨° à¨•à©°à¨® à¨•à¨°à¨¦à©‡ à¨¹à¨¨ à¨œà¨¦à¨•à¨¿ à¨®à¨¹à©±à¨¤à¨µà¨ªà©‚à¨°à¨¨ à¨«à¨¾à¨‡à¨¦à©‡ à¨ªà©à¨°à¨¦à¨¾à¨¨ à¨•à¨°à¨¦à©‡ à¨¹à¨¨
2. **à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨µà¨¿à©±à¨š à¨²à¨¾à¨—à¨¤ à¨•à©à¨¸à¨¼à¨²à¨¤à¨¾**: SLM à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¨à©‚à©° à¨šà¨²à¨¾à¨‰à¨£ à¨²à¨ˆ 10-30x à¨¸à¨¸à¨¤à©‡, à¨‡à¨¹à¨¨à¨¾à¨‚ à¨¨à©‚à©° à¨µà¨¿à¨†à¨ªà¨• à¨¤à©ˆà¨¨à¨¾à¨¤ à¨•à¨°à¨¨ à¨²à¨ˆ à¨†à¨°à¨¥à¨¿à¨• à¨¤à©Œà¨° 'à¨¤à©‡ à¨¯à©‹à¨— à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à©‡ à¨¹à¨¨
3. **à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨²à¨ˆ à¨µà¨¿à¨¸à¨¼à©‡à¨¸à¨¼à¨¤à¨¾**: à¨–à¨¾à¨¸-à¨¤à©Œà¨° 'à¨¤à©‡ à¨¸à©à¨§à¨¾à¨°à¨¿à¨† SLMs à¨…à¨•à¨¸à¨° à¨–à¨¾à¨¸ à¨à¨œà©°à¨Ÿ à¨à¨ªà¨²à©€à¨•à©‡à¨¸à¨¼à¨¨à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨†à¨®-à¨‰à¨¦à©‡à¨¸à¨¼ LLMs à¨¤à©‹à¨‚ à¨µà¨§à©€à¨† à¨•à©°à¨® à¨•à¨°à¨¦à©‡ à¨¹à¨¨
4. **à¨¹à¨¾à¨ˆà¨¬à©à¨°à¨¿à¨¡ à¨à¨œà©°à¨Ÿ à¨†à¨°à¨•à©€à¨Ÿà©ˆà¨•à¨šà¨°**: à¨°à©‹à¨œà¨¼à¨¾à¨¨à¨¾ à¨à¨œà©°à¨Ÿ à¨•à©°à¨®à¨¾à¨‚ à¨²à¨ˆ SLMs à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨•à¨°à©‹, à¨œà¨¦à©‹à¨‚ à¨²à©‹à©œ à¨¹à©‹à¨µà©‡ à¨¤à¨¾à¨‚ à¨œà¨Ÿà¨¿à¨² à¨¤à¨°à¨• à¨²à¨ˆ LLMs à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨•à¨°à©‹
5. **à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨‰à¨¤à¨ªà¨¾à¨¦à¨¨ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨¯à©‹à¨— à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à¨¾ à¨¹à©ˆ**: à¨à¨¨à¨Ÿà¨°à¨ªà©à¨°à¨¾à¨ˆà¨œà¨¼-à¨—à¨°à©‡à¨¡ à¨Ÿà©‚à¨² à¨ªà©à¨°à¨¦à¨¾à¨¨ à¨•à¨°à¨¦à¨¾ à¨¹à©ˆ à¨œà©‹ à¨à¨œ à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¨à©‚à©° à¨¬à¨£à¨¾à¨‰à¨£, à¨¤à©ˆà¨¨à¨¾à¨¤ à¨•à¨°à¨¨, à¨…à¨¤à©‡ à¨ªà©à¨°à¨¬à©°à¨§à¨¿à¨¤ à¨•à¨°à¨¨ à¨²à¨ˆ
6. **à¨à¨œ-à¨ªà¨¹à¨¿à¨²à¨¾ à¨¡à¨¿à¨œà¨¼à¨¾à¨ˆà¨¨ à¨¸à¨¿à¨§à¨¾à¨‚à¨¤**: à¨—à©‹à¨ªà¨¨à©€à¨¯à¨¤à¨¾ à¨…à¨¤à©‡ à¨­à¨°à©‹à¨¸à©‡à¨¯à©‹à¨—à¨¤à¨¾ à¨¯à¨•à©€à¨¨à©€ à¨¬à¨£à¨¾à¨‰à¨£ à¨²à¨ˆ à¨†à¨«à¨²à¨¾à¨ˆà¨¨-à¨¯à©‹à¨— à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¨à¨¾à¨² à¨¸à¨¥à¨¾à¨¨à¨• à¨ªà©à¨°à©‹à¨¸à©ˆà¨¸à¨¿à©°à¨—
7. **à¨«à¨¾à¨‰à¨‚à¨¡à¨°à©€ à¨²à©‹à¨•à¨² à¨‡à©°à¨Ÿà©€à¨—à©à¨°à©‡à¨¸à¨¼à¨¨**: à¨®à¨¾à¨ˆà¨•à¨°à©‹à¨¸à¨¾à¨«à¨Ÿ à¨à¨œà©°à¨Ÿ à¨«à¨°à©‡à¨®à¨µà¨°à¨• à¨…à¨¤à©‡ à¨¸à¨¥à¨¾à¨¨à¨• à¨®à¨¾à¨¡à¨² à¨‡à©°à¨«à¨°à©ˆà¨‚à¨¸ à¨¦à©‡ à¨µà¨¿à¨šà¨•à¨¾à¨° à¨¸à¨¹à©€ à¨•à¨¨à©ˆà¨•à¨¸à¨¼à¨¨
8. **à¨­à¨µà¨¿à©±à¨– SLM à¨à¨œà©°à¨Ÿà¨¾à¨‚ à¨¦à¨¾ à¨¹à©ˆ**: à¨‰à¨¤à¨ªà¨¾à¨¦à¨¨ à¨«à¨°à©‡à¨®à¨µà¨°à¨•à¨¾à¨‚ à¨¨à¨¾à¨² à¨›à©‹à¨Ÿà©‡ à¨­à¨¾à¨¸à¨¼à¨¾ à¨®à¨¾à¨¡à¨² à¨à¨œà©°à¨Ÿà¨¿à¨• AI à¨¦à¨¾ à¨­à¨µà¨¿à©±à¨– à¨¹à¨¨, à¨œà©‹ à¨¡à©ˆà¨®à©‹à¨•à©à¨°à©ˆà¨Ÿà¨¾à¨ˆà¨œà¨¼à¨¡ à¨…à¨¤à©‡ à¨•à©à¨¸à¨¼à¨² à¨à¨œà©°à¨Ÿ à¨¡à¨¿à¨ªà¨²à©Œà¨‡à¨®à©ˆà¨‚à¨Ÿ à¨¯à©‹à¨— à¨¬à¨£à¨¾à¨‰à¨‚à¨¦à©‡ à¨¹à¨¨

## à¨¹à¨µà¨¾à¨²à©‡ à¨…à¨¤à©‡ à¨¹à©‹à¨° à¨ªà©œà©à¨¹à¨¾à¨ˆ

### à¨®à©à©±à¨– à¨—à¨µà©ˆà¨¸à¨¼à¨£à¨¾ à¨ªà©‡à¨ªà¨° à¨…à¨¤à©‡ à¨ªà©à¨°à¨•à¨¾à¨¸à¨¼à¨¨

#### AI à¨à¨œà©°à¨Ÿ à¨…à¨¤à©‡ à¨à¨œà©°à¨Ÿà¨¿à¨• à¨¸à¨¿à¨¸à¨Ÿà¨®
- **"Language Agents as Optimizable Graphs"** (2024) - à¨à¨œà©°à¨Ÿ à¨†à¨°à¨•à©€à¨Ÿà©ˆà¨•à¨šà¨° à¨…à¨¤à©‡ à¨…à¨ªà¨Ÿà¨¿à¨®à¨¾à¨ˆà¨œà¨¼à©‡à¨¸à¨¼à¨¨ à¨°à¨£à¨¨à©€à¨¤à©€à¨†à¨‚ 'à¨¤à©‡ à¨®à©‚à¨² à¨—à¨µà©ˆà¨¸à¨¼à¨£à¨¾
  - à¨²à©‡à¨–à¨•: Wenyue Hua, Lishan Yang, à¨†à¨¦à¨¿
  - à¨²

---

**à¨…à¨¸à¨µà©€à¨•à¨°à¨¤à¨¾**:  
à¨‡à¨¹ à¨¦à¨¸à¨¤à¨¾à¨µà©‡à¨œà¨¼ AI à¨…à¨¨à©à¨µà¨¾à¨¦ à¨¸à©‡à¨µà¨¾ [Co-op Translator](https://github.com/Azure/co-op-translator) à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨•à¨°à¨•à©‡ à¨…à¨¨à©à¨µà¨¾à¨¦ à¨•à©€à¨¤à¨¾ à¨—à¨¿à¨† à¨¹à©ˆà¥¤ à¨œà¨¦à©‹à¨‚ à¨•à¨¿ à¨…à¨¸à©€à¨‚ à¨¸à¨¹à©€ à¨¹à©‹à¨£ à¨¦à©€ à¨•à©‹à¨¸à¨¼à¨¿à¨¸à¨¼ à¨•à¨°à¨¦à©‡ à¨¹à¨¾à¨‚, à¨•à¨¿à¨°à¨ªà¨¾ à¨•à¨°à¨•à©‡ à¨§à¨¿à¨†à¨¨ à¨¦à¨¿à¨“ à¨•à¨¿ à¨¸à¨µà©ˆà¨šà¨¾à¨²à¨¿à¨¤ à¨…à¨¨à©à¨µà¨¾à¨¦à¨¾à¨‚ à¨µà¨¿à©±à¨š à¨—à¨²à¨¤à©€à¨†à¨‚ à¨œà¨¾à¨‚ à¨…à¨¸à©à¨šà©€à¨¤à¨¤à¨¾à¨µà¨¾à¨‚ à¨¹à©‹ à¨¸à¨•à¨¦à©€à¨†à¨‚ à¨¹à¨¨à¥¤ à¨‡à¨¸ à¨¦à©€ à¨®à©‚à¨² à¨­à¨¾à¨¸à¨¼à¨¾ à¨µà¨¿à©±à¨š à¨®à©‚à¨² à¨¦à¨¸à¨¤à¨¾à¨µà©‡à¨œà¨¼ à¨¨à©‚à©° à¨…à¨§à¨¿à¨•à¨¾à¨°à¨¤ à¨¸à¨°à©‹à¨¤ à¨®à©°à¨¨à¨¿à¨† à¨œà¨¾à¨£à¨¾ à¨šà¨¾à¨¹à©€à¨¦à¨¾ à¨¹à©ˆà¥¤ à¨®à¨¹à©±à¨¤à¨µà¨ªà©‚à¨°à¨¨ à¨œà¨¾à¨£à¨•à¨¾à¨°à©€ à¨²à¨ˆ, à¨ªà©‡à¨¸à¨¼à©‡à¨µà¨° à¨®à¨¨à©à©±à¨–à©€ à¨…à¨¨à©à¨µà¨¾à¨¦ à¨¦à©€ à¨¸à¨¿à¨«à¨¾à¨°à¨¸à¨¼ à¨•à©€à¨¤à©€ à¨œà¨¾à¨‚à¨¦à©€ à¨¹à©ˆà¥¤ à¨‡à¨¸ à¨…à¨¨à©à¨µà¨¾à¨¦ à¨¦à©€ à¨µà¨°à¨¤à©‹à¨‚ à¨¤à©‹à¨‚ à¨ªà©ˆà¨¦à¨¾ à¨¹à©‹à¨£ à¨µà¨¾à¨²à©‡ à¨•à¨¿à¨¸à©‡ à¨µà©€ à¨—à¨²à¨¤à¨«à¨¹à¨¿à¨®à©€ à¨œà¨¾à¨‚ à¨—à¨²à¨¤ à¨µà¨¿à¨†à¨–à¨¿à¨† à¨²à¨ˆ à¨…à¨¸à©€à¨‚ à¨œà¨¼à¨¿à©°à¨®à©‡à¨µà¨¾à¨° à¨¨à¨¹à©€à¨‚ à¨¹à¨¾à¨‚à¥¤