<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8c30436578b1bd604c48233ecdd39701",
  "translation_date": "2025-11-11T21:49:44+00:00",
  "source_file": "Workshop/Session01-GettingStartedFoundryLocal.md",
  "language_code": "zh"
}
-->
# ç¬¬1èŠ‚ï¼šå¼€å§‹ä½¿ç”¨ Foundry Local

## æ‘˜è¦

å­¦ä¹ å¦‚ä½•å®‰è£…ã€é…ç½®å¹¶è¿è¡Œæ‚¨çš„ç¬¬ä¸€ä¸ª AI æ¨¡å‹ï¼Œä½¿ç”¨ Microsoft Foundry Localã€‚æœ¬æ¬¡åŠ¨æ‰‹å®è·µè¯¾ç¨‹å°†ä»å®‰è£…åˆ°ä½¿ç”¨ Phi-4ã€Qwen å’Œ DeepSeek ç­‰æ¨¡å‹æ„å»ºæ‚¨çš„ç¬¬ä¸€ä¸ªèŠå¤©åº”ç”¨ç¨‹åºï¼Œæä¾›é€æ­¥çš„æœ¬åœ°æ¨ç†å…¥é—¨æŒ‡å¯¼ã€‚

## å­¦ä¹ ç›®æ ‡

å®Œæˆæœ¬èŠ‚è¯¾ç¨‹åï¼Œæ‚¨å°†èƒ½å¤Ÿï¼š

- **å®‰è£…å’Œé…ç½®**ï¼šæ­£ç¡®å®‰è£…å¹¶éªŒè¯ Foundry Local
- **æŒæ¡ CLI æ“ä½œ**ï¼šä½¿ç”¨ Foundry Local CLI è¿›è¡Œæ¨¡å‹ç®¡ç†å’Œéƒ¨ç½²
- **è¿è¡Œæ‚¨çš„ç¬¬ä¸€ä¸ªæ¨¡å‹**ï¼šæˆåŠŸéƒ¨ç½²å¹¶ä¸æœ¬åœ° AI æ¨¡å‹äº¤äº’
- **æ„å»ºèŠå¤©åº”ç”¨ç¨‹åº**ï¼šä½¿ç”¨ Foundry Local Python SDK åˆ›å»ºä¸€ä¸ªåŸºç¡€èŠå¤©åº”ç”¨ç¨‹åº
- **ç†è§£æœ¬åœ° AI**ï¼šæŒæ¡æœ¬åœ°æ¨ç†å’Œæ¨¡å‹ç®¡ç†çš„åŸºç¡€çŸ¥è¯†

## å‰ç½®æ¡ä»¶

### ç³»ç»Ÿè¦æ±‚

- **Windows**ï¼šWindows 11ï¼ˆ22H2æˆ–æ›´é«˜ç‰ˆæœ¬ï¼‰æˆ– **macOS**ï¼šmacOS 11+ï¼ˆæœ‰é™æ”¯æŒï¼‰
- **å†…å­˜**ï¼šæœ€ä½8GBï¼Œæ¨è16GBä»¥ä¸Š
- **å­˜å‚¨ç©ºé—´**ï¼šæ¨¡å‹éœ€è¦è‡³å°‘10GBçš„å¯ç”¨ç©ºé—´
- **Python**ï¼šå®‰è£…3.10æˆ–æ›´é«˜ç‰ˆæœ¬
- **ç®¡ç†å‘˜æƒé™**ï¼šå®‰è£…éœ€è¦ç®¡ç†å‘˜æƒé™

### å¼€å‘ç¯å¢ƒ

- æ¨èä½¿ç”¨å¸¦æœ‰ Python æ‰©å±•çš„ Visual Studio Code
- å‘½ä»¤è¡Œè®¿é—®ï¼ˆWindowsä¸Šçš„PowerShellï¼ŒmacOSä¸Šçš„ç»ˆç«¯ï¼‰
- Gitï¼ˆç”¨äºå…‹éš†ä»£ç åº“ï¼Œå¯é€‰ï¼‰

## è¯¾ç¨‹æµç¨‹ï¼ˆ30åˆ†é’Ÿï¼‰

### ç¬¬1æ­¥ï¼šå®‰è£… Foundry Localï¼ˆ5åˆ†é’Ÿï¼‰

#### Windows å®‰è£…

ä½¿ç”¨ Windows åŒ…ç®¡ç†å™¨å®‰è£… Foundry Localï¼š

```powershell
# Install via winget (recommended)
winget install Microsoft.FoundryLocal
```

æ›¿ä»£æ–¹æ³•ï¼šç›´æ¥ä» [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install) ä¸‹è½½

#### macOS å®‰è£…ï¼ˆæœ‰é™æ”¯æŒï¼‰

> [!NOTE]  
> macOS æ”¯æŒç›®å‰å¤„äºé¢„è§ˆé˜¶æ®µã€‚è¯·æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£ä»¥è·å–æœ€æ–°ä¿¡æ¯ã€‚

å¦‚æœæ”¯æŒï¼Œä½¿ç”¨ Homebrew å®‰è£…ï¼š

```bash
# If Homebrew formula is available
brew update
brew install foundry-local

# Or manual download (check official docs for latest)
curl -L -o foundry-local.tar.gz "https://download.microsoft.com/foundry-local/latest/macos/foundry-local.tar.gz"
tar -xzf foundry-local.tar.gz
sudo ./install.sh
```

**macOS ç”¨æˆ·çš„æ›¿ä»£æ–¹æ³•ï¼š**
- ä½¿ç”¨ Windows 11 è™šæ‹Ÿæœºï¼ˆParallels/UTMï¼‰ï¼Œå¹¶æŒ‰ç…§ Windows æ­¥éª¤æ“ä½œ
- å¦‚æœå¯ç”¨ï¼Œé€šè¿‡å®¹å™¨è¿è¡Œå¹¶é…ç½® `FOUNDRY_LOCAL_ENDPOINT`

### ç¬¬2æ­¥ï¼šéªŒè¯å®‰è£…ï¼ˆ3åˆ†é’Ÿï¼‰

å®‰è£…å®Œæˆåï¼Œé‡å¯ç»ˆç«¯å¹¶éªŒè¯ Foundry Local æ˜¯å¦æ­£å¸¸å·¥ä½œï¼š

```powershell
# Check if Foundry Local is installed correctly
foundry --version

# View available commands
foundry --help
```

é¢„æœŸè¾“å‡ºåº”æ˜¾ç¤ºç‰ˆæœ¬ä¿¡æ¯å’Œå¯ç”¨å‘½ä»¤ã€‚

### ç¬¬3æ­¥ï¼šè®¾ç½® Python ç¯å¢ƒï¼ˆ5åˆ†é’Ÿï¼‰

ä¸ºæœ¬æ¬¡è¯¾ç¨‹åˆ›å»ºä¸€ä¸ªä¸“ç”¨çš„ Python ç¯å¢ƒï¼š

**Windowsï¼š**
```powershell
# Create virtual environment
py -m venv .venv

# Activate environment
.\.venv\Scripts\Activate.ps1

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

**macOS/Linuxï¼š**
```bash
# Create virtual environment
python3 -m venv .venv

# Activate environment
source .venv/bin/activate

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```


### ç¬¬4æ­¥ï¼šè¿è¡Œæ‚¨çš„ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼ˆ7åˆ†é’Ÿï¼‰

ç°åœ¨è®©æˆ‘ä»¬åœ¨æœ¬åœ°è¿è¡Œç¬¬ä¸€ä¸ª AI æ¨¡å‹ï¼

#### ä» Phi-4 Mini å¼€å§‹ï¼ˆæ¨èçš„ç¬¬ä¸€ä¸ªæ¨¡å‹ï¼‰

```powershell
# Download and start phi-4-mini (lightweight, fast)
foundry model run phi-4-mini

# Test the model with a simple prompt
foundry model run phi-4-mini --prompt "Hello, introduce yourself in one sentence"
```

> [!TIP]  
> æ­¤å‘½ä»¤ä¼šä¸‹è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œæ—¶ï¼‰å¹¶è‡ªåŠ¨å¯åŠ¨ Foundry Local æœåŠ¡ã€‚

#### æ£€æŸ¥è¿è¡ŒçŠ¶æ€

```powershell
# List available models (shows downloaded models)
foundry model list

# Check service status
foundry service status

# See what models are cached locally
foundry cache list
```


#### å°è¯•å…¶ä»–æ¨¡å‹

åœ¨ phi-4-mini æ­£å¸¸è¿è¡Œåï¼Œå°è¯•å…¶ä»–æ¨¡å‹ï¼š

```powershell
# Larger model with better capabilities
foundry model run gpt-oss-20b --prompt "Explain edge AI in simple terms"

# Fast, efficient model
foundry model run qwen2.5-0.5b --prompt "What are the benefits of local AI inference?"
```


### ç¬¬5æ­¥ï¼šæ„å»ºæ‚¨çš„ç¬¬ä¸€ä¸ªèŠå¤©åº”ç”¨ç¨‹åºï¼ˆ10åˆ†é’Ÿï¼‰

ç°åœ¨è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªä½¿ç”¨åˆšåˆšå¯åŠ¨çš„æ¨¡å‹çš„ Python åº”ç”¨ç¨‹åºã€‚

#### åˆ›å»ºèŠå¤©è„šæœ¬

åˆ›å»ºä¸€ä¸ªåä¸º `my_first_chat.py` çš„æ–°æ–‡ä»¶ï¼ˆæˆ–ä½¿ç”¨æä¾›çš„ç¤ºä¾‹ï¼‰ï¼š

```python
#!/usr/bin/env python3
"""
My First Foundry Local Chat Application
Using FoundryLocalManager for automatic service management
"""

import os
from foundry_local import FoundryLocalManager
from openai import OpenAI

def main():
    # Get model alias from environment or use default
    alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-4-mini")
    
    try:
        # Initialize Foundry Local Manager (auto-starts service, downloads model)
        manager = FoundryLocalManager(alias)
        
        # Create OpenAI client pointing to local endpoint
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-needed"
        )
        
        # Get the actual model ID for this alias
        model_id = manager.get_model_info(alias).id
        
        print("ğŸ¤– Welcome to your first local AI chat!")
        print(f"ï¿½ Using model: {alias} -> {model_id}")
        print(f"ğŸŒ Endpoint: {manager.endpoint}")
        print("ï¿½ğŸ’¡ Type 'quit' to exit\n")
        
    except Exception as e:
        print(f"âŒ Failed to initialize Foundry Local: {e}")
        print("ğŸ’¡ Make sure Foundry Local is installed: foundry --version")
        return
    
    while True:
        # Get user input
        user_message = input("You: ").strip()
        
        if user_message.lower() in ['quit', 'exit', 'bye']:
            print("ğŸ‘‹ Goodbye!")
            break
            
        if not user_message:
            continue
            
        try:
            # Send message to local AI model
            response = client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": "You are a helpful AI assistant running locally."},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=200,
                temperature=0.7
            )
            
            # Display the response
            ai_response = response.choices[0].message.content
            print(f"ğŸ¤– AI: {ai_response}\n")
            
        except Exception as e:
            print(f"âŒ Error: {e}")
            print("ğŸ’¡ Check service status: foundry service status\n")

if __name__ == "__main__":
    main()
```

> [!TIP]  
> **ç›¸å…³ç¤ºä¾‹**ï¼šæœ‰å…³æ›´é«˜çº§çš„ç”¨æ³•ï¼Œè¯·å‚é˜…ï¼š
>
> - **Python ç¤ºä¾‹**ï¼š`Workshop/samples/session01/chat_bootstrap.py` - åŒ…æ‹¬æµå¼å“åº”å’Œé”™è¯¯å¤„ç†
> - **Jupyter Notebook**ï¼š`Workshop/notebooks/session01_chat_bootstrap.ipynb` - å¸¦è¯¦ç»†è¯´æ˜çš„äº¤äº’ç‰ˆæœ¬

#### æµ‹è¯•æ‚¨çš„èŠå¤©åº”ç”¨ç¨‹åº

```powershell
# No need to manually start models - FoundryLocalManager handles this!
# Just run your chat application
python my_first_chat.py
```

æ›¿ä»£æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨æä¾›çš„ç¤ºä¾‹

```powershell
# Try the complete sample with streaming support
cd Workshop/samples
python -m session01.chat_bootstrap "Your question here"
```

æˆ–è€…æ¢ç´¢äº¤äº’å¼ç¬”è®°æœ¬  
åœ¨ VS Code ä¸­æ‰“å¼€ Workshop/notebooks/session01_chat_bootstrap.ipynb

å°è¯•ä»¥ä¸‹ç¤ºä¾‹å¯¹è¯ï¼š

- â€œä»€ä¹ˆæ˜¯ Microsoft Foundry Localï¼Ÿâ€
- â€œåˆ—å‡ºè¿è¡Œæœ¬åœ° AI æ¨¡å‹çš„ä¸‰ä¸ªå¥½å¤„â€
- â€œå¸®æˆ‘ç†è§£è¾¹ç¼˜ AIâ€

## æ‚¨çš„æˆæœ

æ­å–œï¼æ‚¨å·²ç»æˆåŠŸå®Œæˆï¼š

1. âœ… **å®‰è£… Foundry Local** å¹¶éªŒè¯å…¶æ­£å¸¸å·¥ä½œ  
2. âœ… **æœ¬åœ°å¯åŠ¨æ‚¨çš„ç¬¬ä¸€ä¸ª AI æ¨¡å‹**ï¼ˆphi-4-miniï¼‰  
3. âœ… **é€šè¿‡å‘½ä»¤è¡Œæµ‹è¯•ä¸åŒæ¨¡å‹**  
4. âœ… **æ„å»ºäº†ä¸€ä¸ªè¿æ¥åˆ°æœ¬åœ° AI çš„èŠå¤©åº”ç”¨ç¨‹åº**  
5. âœ… **ä½“éªŒäº†æ— éœ€äº‘ä¾èµ–çš„æœ¬åœ° AI æ¨ç†**  

## ç†è§£å‘ç”Ÿäº†ä»€ä¹ˆ

### æœ¬åœ° AI æ¨ç†

- æ‚¨çš„ AI æ¨¡å‹å®Œå…¨åœ¨æ‚¨çš„è®¡ç®—æœºä¸Šè¿è¡Œ
- æ²¡æœ‰æ•°æ®å‘é€åˆ°äº‘ç«¯
- å“åº”ç”±æ‚¨çš„ CPU/GPU æœ¬åœ°ç”Ÿæˆ
- éšç§å’Œå®‰å…¨æ€§å¾—åˆ°ä¿éšœ

### æ¨¡å‹ç®¡ç†

- `foundry model run` ä¸‹è½½å¹¶å¯åŠ¨æ¨¡å‹
- **FoundryLocalManager SDK** è‡ªåŠ¨å¤„ç†æœåŠ¡å¯åŠ¨å’Œæ¨¡å‹åŠ è½½
- æ¨¡å‹ä¼šç¼“å­˜åˆ°æœ¬åœ°ä»¥ä¾›å°†æ¥ä½¿ç”¨
- å¯ä»¥ä¸‹è½½å¤šä¸ªæ¨¡å‹ï¼Œä½†é€šå¸¸ä¸€æ¬¡è¿è¡Œä¸€ä¸ª
- æœåŠ¡ä¼šè‡ªåŠ¨ç®¡ç†æ¨¡å‹ç”Ÿå‘½å‘¨æœŸ

### SDK ä¸ CLI æ–¹æ³•

- **CLI æ–¹æ³•**ï¼šä½¿ç”¨ `foundry model run <model>` æ‰‹åŠ¨ç®¡ç†æ¨¡å‹
- **SDK æ–¹æ³•**ï¼šä½¿ç”¨ `FoundryLocalManager(alias)` è‡ªåŠ¨æœåŠ¡å’Œæ¨¡å‹ç®¡ç†
- **æ¨è**ï¼šåº”ç”¨ç¨‹åºä½¿ç”¨ SDKï¼Œæµ‹è¯•å’Œæ¢ç´¢ä½¿ç”¨ CLI

## å¸¸ç”¨å‘½ä»¤å‚è€ƒ

### åŸºæœ¬ CLI å‘½ä»¤

```powershell
# Installation & Setup
foundry --version              # Check installation
foundry --help                 # View all commands

# Model Management
foundry model list             # List available models
foundry model run <model>      # Download and start a model
foundry model run <model> --prompt "text"  # One-shot prompt
foundry cache list             # Show downloaded models

# Service Management
foundry service status         # Check if service is running
foundry service start          # Start the service manually
foundry service stop           # Stop the service
```


### æ¨¡å‹æ¨è

- **phi-4-mini**ï¼šæœ€ä½³å…¥é—¨æ¨¡å‹ - å¿«é€Ÿã€è½»é‡ã€è´¨é‡è‰¯å¥½
- **qwen2.5-0.5b**ï¼šæ¨ç†é€Ÿåº¦æœ€å¿«ï¼Œå†…å­˜å ç”¨æœ€å°‘
- **gpt-oss-20b**ï¼šå“åº”è´¨é‡æ›´é«˜ï¼Œä½†éœ€è¦æ›´å¤šèµ„æº
- **deepseek-coder-1.3b**ï¼šé’ˆå¯¹ç¼–ç¨‹å’Œä»£ç ä»»åŠ¡ä¼˜åŒ–

## æ•…éšœæ’é™¤

### â€œæ‰¾ä¸åˆ° Foundry å‘½ä»¤â€

**è§£å†³æ–¹æ¡ˆï¼š**

```powershell
# Restart your terminal after installation
# Or manually add to PATH (Windows)
$env:PATH += ";C:\Program Files\Microsoft\FoundryLocal"
```


### â€œæ¨¡å‹åŠ è½½å¤±è´¥â€

**è§£å†³æ–¹æ¡ˆï¼š**

```powershell
# Check available system memory
foundry service status

# Try a smaller model first
foundry model run phi-4-mini

# Check disk space for model downloads
# Models are stored in: %USERPROFILE%\.foundry\models (Windows)
```


### â€œæœ¬åœ°ä¸»æœºè¿æ¥è¢«æ‹’ç»â€

**è§£å†³æ–¹æ¡ˆï¼š**

```powershell
# Check if service is running
foundry service status

# Start service if needed
foundry service start

# Verify the port (default is 5273)
# Check for port conflicts with: netstat -an | findstr 5273
```


## ä¸‹ä¸€æ­¥

### ç«‹å³è¡ŒåŠ¨

1. **å°è¯•**ä¸åŒçš„æ¨¡å‹å’Œæç¤º
2. **ä¿®æ”¹**æ‚¨çš„èŠå¤©åº”ç”¨ç¨‹åºä»¥å°è¯•ä¸åŒçš„æ¨¡å‹
3. **åˆ›å»º**æ‚¨è‡ªå·±çš„æç¤ºå¹¶æµ‹è¯•å“åº”
4. **æ¢ç´¢**ç¬¬2èŠ‚ï¼šæ„å»º RAG åº”ç”¨ç¨‹åº

### é«˜çº§å­¦ä¹ è·¯å¾„

1. **ç¬¬2èŠ‚**ï¼šä½¿ç”¨ RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰æ„å»º AI è§£å†³æ–¹æ¡ˆ
2. **ç¬¬3èŠ‚**ï¼šæ¯”è¾ƒä¸åŒçš„å¼€æºæ¨¡å‹
3. **ç¬¬4èŠ‚**ï¼šä½¿ç”¨æœ€å‰æ²¿çš„æ¨¡å‹
4. **ç¬¬5èŠ‚**ï¼šæ„å»ºå¤šä»£ç† AI ç³»ç»Ÿ

## ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰

å¯¹äºæ›´é«˜çº§çš„ç”¨æ³•ï¼Œæ‚¨å¯ä»¥è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ï¼š

| å˜é‡ | ç”¨é€” | ç¤ºä¾‹ |
|------|------|------|
| `FOUNDRY_LOCAL_ALIAS` | é»˜è®¤ä½¿ç”¨çš„æ¨¡å‹ | `phi-4-mini` |
| `FOUNDRY_LOCAL_ENDPOINT` | è¦†ç›–ç«¯ç‚¹ URL | `http://localhost:5273/v1` |

åœ¨é¡¹ç›®ç›®å½•ä¸­åˆ›å»ºä¸€ä¸ª `.env` æ–‡ä»¶ï¼š
```
FOUNDRY_LOCAL_ALIAS=phi-4-mini
FOUNDRY_LOCAL_ENDPOINT=auto
```


## å…¶ä»–èµ„æº

### æ–‡æ¡£

- [Foundry Local Python SDK å‚è€ƒ](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk?pivots=programming-language-python)
- [Foundry Local å®‰è£…æŒ‡å—](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)
- [æ¨¡å‹ç›®å½•](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/models)

### ç¤ºä¾‹ä»£ç 

- **ç¬¬1èŠ‚ Python ç¤ºä¾‹**ï¼š`Workshop/samples/session01/chat_bootstrap.py` - åŒ…å«æµå¼èŠå¤©åº”ç”¨ç¨‹åº
- **ç¬¬1èŠ‚ç¬”è®°æœ¬**ï¼š`Workshop/notebooks/session01_chat_bootstrap.ipynb` - äº¤äº’å¼æ•™ç¨‹  
- [æ¨¡å—08 ç¤ºä¾‹01](../Module08/samples/01/README.md) - REST èŠå¤©å¿«é€Ÿå…¥é—¨
- [æ¨¡å—08 ç¤ºä¾‹02](../Module08/samples/02/README.md) - OpenAI SDK é›†æˆ
- [æ¨¡å—08 ç¤ºä¾‹03](../Module08/samples/03/README.md) - æ¨¡å‹å‘ç°ä¸åŸºå‡†æµ‹è¯•

### ç¤¾åŒº

- [Foundry Local GitHub è®¨è®ºåŒº](https://github.com/microsoft/Foundry-Local/discussions)
- [Azure AI ç¤¾åŒº](https://techcommunity.microsoft.com/category/artificialintelligence)

---

**è¯¾ç¨‹æ—¶é•¿**ï¼š30åˆ†é’ŸåŠ¨æ‰‹å®è·µ + 15åˆ†é’Ÿé—®ç­”  
**éš¾åº¦çº§åˆ«**ï¼šåˆå­¦è€…  
**å‰ç½®æ¡ä»¶**ï¼šWindows 11/macOS 11+ï¼ŒPython 3.10+ï¼Œç®¡ç†å‘˜æƒé™

## è¯¾ç¨‹ç¤ºä¾‹åœºæ™¯

### çœŸå®åœºæ™¯

**åœºæ™¯**ï¼šæŸä¼ä¸š IT å›¢é˜Ÿéœ€è¦è¯„ä¼°è®¾å¤‡ä¸Šçš„ AI æ¨ç†ï¼Œä»¥å¤„ç†æ•æ„Ÿçš„å‘˜å·¥åé¦ˆæ•°æ®ï¼Œè€Œæ— éœ€å°†æ•°æ®å‘é€åˆ°å¤–éƒ¨æœåŠ¡ã€‚

**æ‚¨çš„ç›®æ ‡**ï¼šå±•ç¤ºæœ¬åœ° AI æ¨¡å‹èƒ½å¤Ÿåœ¨ä¿æŒæ•°æ®å®Œå…¨éšç§çš„åŒæ—¶ï¼Œä»¥äºšç§’çº§çš„å»¶è¿Ÿæä¾›é«˜è´¨é‡çš„å“åº”ã€‚

### æµ‹è¯•æç¤º

ä½¿ç”¨ä»¥ä¸‹æç¤ºéªŒè¯æ‚¨çš„è®¾ç½®ï¼š

```json
[
    "List two benefits of local inference.",
    "Summarize why keeping data on device improves privacy.",
    "Give one trade-off when choosing a small model over a large model."
]
```


### æˆåŠŸæ ‡å‡†

- âœ… æ‰€æœ‰æç¤ºåœ¨2ç§’å†…è·å¾—å“åº”  
- âœ… æ²¡æœ‰æ•°æ®ç¦»å¼€æ‚¨çš„æœ¬åœ°æœºå™¨  
- âœ… å“åº”ç›¸å…³ä¸”æœ‰å¸®åŠ©  
- âœ… æ‚¨çš„èŠå¤©åº”ç”¨ç¨‹åºè¿è¡Œé¡ºç•…  

æ­¤éªŒè¯ç¡®ä¿æ‚¨çš„ Foundry Local è®¾ç½®å·²å‡†å¤‡å¥½å‚åŠ ç¬¬2è‡³ç¬¬6èŠ‚çš„é«˜çº§è¯¾ç¨‹ã€‚

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**å…è´£å£°æ˜**ï¼š  
æœ¬æ–‡æ¡£ä½¿ç”¨AIç¿»è¯‘æœåŠ¡[Co-op Translator](https://github.com/Azure/co-op-translator)è¿›è¡Œç¿»è¯‘ã€‚å°½ç®¡æˆ‘ä»¬åŠªåŠ›ç¡®ä¿ç¿»è¯‘çš„å‡†ç¡®æ€§ï¼Œä½†è¯·æ³¨æ„ï¼Œè‡ªåŠ¨ç¿»è¯‘å¯èƒ½åŒ…å«é”™è¯¯æˆ–ä¸å‡†ç¡®ä¹‹å¤„ã€‚åŸå§‹è¯­è¨€çš„æ–‡æ¡£åº”è¢«è§†ä¸ºæƒå¨æ¥æºã€‚å¯¹äºé‡è¦ä¿¡æ¯ï¼Œå»ºè®®ä½¿ç”¨ä¸“ä¸šäººå·¥ç¿»è¯‘ã€‚æˆ‘ä»¬ä¸å¯¹å› ä½¿ç”¨æ­¤ç¿»è¯‘è€Œäº§ç”Ÿçš„ä»»ä½•è¯¯è§£æˆ–è¯¯è¯»æ‰¿æ‹…è´£ä»»ã€‚
<!-- CO-OP TRANSLATOR DISCLAIMER END -->