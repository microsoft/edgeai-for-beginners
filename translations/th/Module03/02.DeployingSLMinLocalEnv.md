# ส่วนที่ 2: การปรับใช้ในสภาพแวดล้อมท้องถิ่น - โซลูชันที่เน้นความเป็นส่วนตัว

การปรับใช้ Small Language Models (SLMs) ในสภาพแวดล้อมท้องถิ่นเป็นการเปลี่ยนแปลงที่สำคัญไปสู่โซลูชัน AI ที่รักษาความเป็นส่วนตัวและคุ้มค่า คู่มือฉบับสมบูรณ์นี้จะสำรวจสองกรอบการทำงานที่ทรงพลัง—Ollama และ Microsoft Foundry Local—ที่ช่วยให้นักพัฒนาสามารถใช้ประโยชน์จาก SLMs ได้อย่างเต็มที่ พร้อมทั้งควบคุมสภาพแวดล้อมการปรับใช้ได้อย่างสมบูรณ์

## บทนำ

ในบทเรียนนี้ เราจะสำรวจกลยุทธ์การปรับใช้ขั้นสูงสำหรับ Small Language Models ในสภาพแวดล้อมท้องถิ่น เราจะครอบคลุมแนวคิดพื้นฐานของการปรับใช้ AI ในพื้นที่ ตรวจสอบสองแพลตฟอร์มชั้นนำ (Ollama และ Microsoft Foundry Local) และให้คำแนะนำการใช้งานจริงสำหรับโซลูชันที่พร้อมใช้งานในระดับการผลิต

## วัตถุประสงค์การเรียนรู้

เมื่อจบบทเรียนนี้ คุณจะสามารถ:

- เข้าใจสถาปัตยกรรมและประโยชน์ของกรอบการทำงานการปรับใช้ SLM ในพื้นที่
- ปรับใช้โซลูชันที่พร้อมใช้งานในระดับการผลิตโดยใช้ Ollama และ Microsoft Foundry Local
- เปรียบเทียบและเลือกแพลตฟอร์มที่เหมาะสมตามข้อกำหนดและข้อจำกัดเฉพาะ
- ปรับปรุงการปรับใช้ในพื้นที่ให้มีประสิทธิภาพ ความปลอดภัย และความสามารถในการขยายตัว

## การทำความเข้าใจสถาปัตยกรรมการปรับใช้ SLM ในพื้นที่

การปรับใช้ SLM ในพื้นที่เป็นการเปลี่ยนแปลงพื้นฐานจากบริการ AI ที่พึ่งพา Cloud ไปสู่โซลูชันที่รักษาความเป็นส่วนตัวในองค์กร วิธีการนี้ช่วยให้องค์กรสามารถควบคุมโครงสร้างพื้นฐาน AI ได้อย่างสมบูรณ์ พร้อมทั้งรักษาความเป็นอิสระในการดำเนินงานและการปกป้องข้อมูล

### การจำแนกกรอบการทำงานการปรับใช้

การทำความเข้าใจวิธีการปรับใช้ที่แตกต่างกันช่วยในการเลือกกลยุทธ์ที่เหมาะสมสำหรับการใช้งานเฉพาะ:

- **เน้นการพัฒนา**: การตั้งค่าที่ง่ายสำหรับการทดลองและการสร้างต้นแบบ
- **ระดับองค์กร**: โซลูชันที่พร้อมใช้งานในระดับการผลิตพร้อมความสามารถในการผสานรวมกับองค์กร
- **ข้ามแพลตฟอร์ม**: ความเข้ากันได้สากลกับระบบปฏิบัติการและฮาร์ดแวร์ที่หลากหลาย

### ข้อดีหลักของการปรับใช้ SLM ในพื้นที่

การปรับใช้ SLM ในพื้นที่มีข้อดีพื้นฐานหลายประการที่ทำให้เหมาะสำหรับการใช้งานในองค์กรและแอปพลิเคชันที่ต้องการความเป็นส่วนตัว:

**ความเป็นส่วนตัวและความปลอดภัย**: การประมวลผลในพื้นที่ช่วยให้ข้อมูลที่สำคัญไม่ออกจากโครงสร้างพื้นฐานขององค์กร ทำให้สามารถปฏิบัติตาม GDPR, HIPAA และข้อกำหนดด้านกฎระเบียบอื่น ๆ การปรับใช้แบบแยกเครือข่ายเป็นไปได้สำหรับสภาพแวดล้อมที่มีการจัดประเภท ในขณะที่การตรวจสอบอย่างสมบูรณ์ช่วยรักษาการกำกับดูแลด้านความปลอดภัย

**ความคุ้มค่า**: การกำจัดโมเดลการกำหนดราคาตามโทเค็นช่วยลดต้นทุนการดำเนินงานได้อย่างมาก ความต้องการแบนด์วิดท์ที่ต่ำลงและการลดการพึ่งพา Cloud ช่วยให้โครงสร้างต้นทุนที่คาดการณ์ได้สำหรับการวางแผนงบประมาณขององค์กร

**ประสิทธิภาพและความน่าเชื่อถือ**: เวลาการประมวลผลที่เร็วขึ้นโดยไม่มีความล่าช้าของเครือข่ายช่วยให้แอปพลิเคชันแบบเรียลไทม์ทำงานได้ ฟังก์ชันการทำงานแบบออฟไลน์ช่วยให้การดำเนินงานต่อเนื่องไม่ว่าจะมีการเชื่อมต่ออินเทอร์เน็ตหรือไม่ก็ตาม ในขณะที่การปรับทรัพยากรในพื้นที่ช่วยให้ประสิทธิภาพคงที่

## Ollama: แพลตฟอร์มการปรับใช้ในพื้นที่แบบสากล

### สถาปัตยกรรมและปรัชญาหลัก

Ollama ถูกออกแบบมาเป็นแพลตฟอร์มที่เป็นมิตรกับนักพัฒนา ซึ่งช่วยให้การปรับใช้ LLM ในพื้นที่เป็นไปได้ในฮาร์ดแวร์และระบบปฏิบัติการที่หลากหลาย

**พื้นฐานทางเทคนิค**: สร้างขึ้นบนกรอบ llama.cpp ที่แข็งแกร่ง Ollama ใช้รูปแบบโมเดล GGUF ที่มีประสิทธิภาพเพื่อประสิทธิภาพที่ดีที่สุด ความเข้ากันได้ข้ามแพลตฟอร์มช่วยให้พฤติกรรมที่สม่ำเสมอใน Windows, macOS และ Linux ในขณะที่การจัดการทรัพยากรอัจฉริยะช่วยเพิ่มประสิทธิภาพการใช้ CPU, GPU และหน่วยความจำ

**ปรัชญาการออกแบบ**: Ollama ให้ความสำคัญกับความเรียบง่ายโดยไม่ลดทอนฟังก์ชันการทำงาน โดยมีการปรับใช้แบบไม่มีการตั้งค่าที่ซับซ้อนเพื่อเพิ่มประสิทธิภาพทันที แพลตฟอร์มนี้รักษาความเข้ากันได้กับโมเดลที่หลากหลาย พร้อมทั้งให้ API ที่สม่ำเสมอสำหรับสถาปัตยกรรมโมเดลที่แตกต่างกัน

### คุณสมบัติและความสามารถขั้นสูง

**การจัดการโมเดลที่ยอดเยี่ยม**: Ollama มีการจัดการวงจรชีวิตของโมเดลที่ครอบคลุม พร้อมการดึงข้อมูลอัตโนมัติ การแคช และการจัดการเวอร์ชัน แพลตฟอร์มรองรับระบบนิเวศของโมเดลที่กว้างขวาง เช่น Llama 3.2, Google Gemma 2, Microsoft Phi-4, Qwen 2.5, DeepSeek, Mistral และโมเดลฝังตัวเฉพาะทาง

**การปรับแต่งผ่าน Modelfiles**: ผู้ใช้ขั้นสูงสามารถสร้างการกำหนดค่าโมเดลที่กำหนดเองด้วยพารามิเตอร์เฉพาะ คำสั่งระบบ และการปรับเปลี่ยนพฤติกรรม สิ่งนี้ช่วยให้การปรับแต่งเฉพาะด้านและความต้องการของแอปพลิเคชันเฉพาะทาง

**การปรับปรุงประสิทธิภาพ**: Ollama ตรวจจับและใช้การเร่งฮาร์ดแวร์ที่มีอยู่โดยอัตโนมัติ รวมถึง NVIDIA CUDA, Apple Metal และ OpenCL การจัดการหน่วยความจำอัจฉริยะช่วยให้การใช้ทรัพยากรเหมาะสมที่สุดในฮาร์ดแวร์ที่แตกต่างกัน

### กลยุทธ์การปรับใช้ในระดับการผลิต

**การติดตั้งและการตั้งค่า**: Ollama มีการติดตั้งที่ง่ายในทุกแพลตฟอร์มผ่านตัวติดตั้งแบบเนทีฟ ตัวจัดการแพ็คเกจ (WinGet, Homebrew, APT) และคอนเทนเนอร์ Docker สำหรับการปรับใช้แบบคอนเทนเนอร์

```bash
# Cross-platform installation examples
# Windows (WinGet)
winget install Ollama.Ollama

# macOS (Homebrew)  
brew install ollama

# Linux (curl)
curl -fsSL https://ollama.com/install.sh | sh

# Docker deployment
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

**คำสั่งและการดำเนินการที่จำเป็น**:

```bash
# Model management
ollama pull qwen2.5:3b          # Download specific model
ollama pull phi4:mini           # Download Phi-4 mini variant
ollama list                     # List installed models
ollama rm <model>               # Remove model

# Model execution
ollama run qwen2.5:3b           # Interactive mode
ollama run phi4:mini "Explain quantum computing"  # Single query

# Custom model creation
ollama create enterprise-assistant -f ./Modelfile
```

**การตั้งค่าขั้นสูง**: Modelfiles ช่วยให้การปรับแต่งที่ซับซ้อนสำหรับความต้องการขององค์กร:

```dockerfile
FROM qwen2.5:3b

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER context_length 4096
PARAMETER num_gpu 1
PARAMETER num_thread 8

SYSTEM """
You are an enterprise assistant for Contoso Corporation.
Always maintain a professional tone and prioritize security best practices.
Never share confidential information without proper authentication.
"""

# Custom model knowledge (optional)
FILE ./contoso_guidelines.txt
FILE ./security_protocols.pdf
```

### ตัวอย่างการผสานรวมสำหรับนักพัฒนา

**การผสานรวม API Python**:

```python
import requests
import json

# API endpoint configuration
OLLAMA_ENDPOINT = "http://localhost:11434/api/generate"

# Model parameters
params = {
    "model": "phi4:mini",
    "prompt": "Write a function to calculate the Fibonacci sequence in Python",
    "system": "You are a helpful Python programming assistant. Provide clean, efficient code with comments.",
    "stream": False,
    "options": {
        "temperature": 0.2,
        "top_p": 0.95,
        "num_predict": 1024
    }
}

# Make API request
response = requests.post(OLLAMA_ENDPOINT, json=params)
result = response.json()

# Process and display response
print(result["response"])

# Streaming example (for real-time responses)
def stream_response():
    params["stream"] = True
    response = requests.post(OLLAMA_ENDPOINT, json=params, stream=True)
    
    for line in response.iter_lines():
        if line:
            chunk = json.loads(line)
            if "response" in chunk:
                print(chunk["response"], end="", flush=True)
            if chunk.get("done", False):
                print()
                break

# stream_response()  # Uncomment to run streaming example
```

**การผสานรวม JavaScript/TypeScript (Node.js)**:

```javascript
const axios = require('axios');

// API configuration
const OLLAMA_API = 'http://localhost:11434/api';

// Function to generate text with Ollama
async function generateText(model, prompt, systemPrompt = '') {
  try {
    const response = await axios.post(`${OLLAMA_API}/generate`, {
      model: model,
      prompt: prompt,
      system: systemPrompt,
      stream: false,
      options: {
        temperature: 0.7,
        top_k: 40,
        top_p: 0.9,
        num_predict: 1024
      }
    });
    
    return response.data.response;
  } catch (error) {
    console.error('Error generating text:', error.message);
    throw error;
  }
}

// Example usage in an Express API
const express = require('express');
const app = express();
app.use(express.json());

app.post('/api/chat', async (req, res) => {
  const { message } = req.body;
  
  try {
    const response = await generateText(
      'phi4:mini',
      message,
      'You are a helpful AI assistant.'
    );
    
    res.json({ response });
  } catch (error) {
    res.status(500).json({ error: 'Failed to generate response' });
  }
});

app.listen(3000, () => {
  console.log('API server running on port 3000');
});
```

**การใช้งาน RESTful API ด้วย cURL**:

```bash
# Basic text generation
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "phi4:mini",
    "prompt": "Write a recursive function to calculate factorial",
    "stream": false
  }'

# Chat completion (conversational)
curl -X POST http://localhost:11434/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen2.5:3b",
    "messages": [
      {"role": "system", "content": "You are a helpful assistant."},
      {"role": "user", "content": "What is edge computing?"}
    ],
    "stream": false
  }'

# Embedding generation (for vector databases)
curl -X POST http://localhost:11434/api/embeddings \
  -H "Content-Type: application/json" \
  -d '{
    "model": "nomic-embed-text",
    "prompt": "Edge AI represents a paradigm shift in artificial intelligence deployment"
  }'
```

### การปรับแต่งและการเพิ่มประสิทธิภาพ

**การตั้งค่าหน่วยความจำและเธรด**:

```bash
# Adjust memory and thread allocation for large models
OLLAMA_HOST=0.0.0.0 OLLAMA_NUM_GPU=1 OLLAMA_NUM_THREAD=8 ollama serve

# GPU layer configuration for optimal performance
OLLAMA_GPU_LAYERS=35 ollama run qwen2.5:3b

# Run with specific CUDA device (multi-GPU systems)
CUDA_VISIBLE_DEVICES=0 ollama run phi4:mini
```

**การเลือก Quantization สำหรับฮาร์ดแวร์ที่แตกต่างกัน**:

```bash
# Pull specific quantization variants for performance/quality tradeoffs
# F16 format (highest quality, highest memory usage)
ollama pull phi4:mini-f16

# Q8_0 format (high quality, moderate memory usage)
ollama pull phi4:mini-q8_0

# Q4_K_M format (good quality, lowest memory usage)
ollama pull phi4:mini-q4_k_m
```

```SYSTEM """
You are a specialized enterprise assistant focused on technical documentation and code analysis.
Provide concise, accurate responses with practical examples.
"""

TEMPLATE """{{ .System }}
User: {{ .Prompt }}
Assistant: """
```

## Microsoft Foundry Local: แพลตฟอร์ม AI ระดับองค์กรที่ Edge

### สถาปัตยกรรมระดับองค์กร

Microsoft Foundry Local เป็นโซลูชันระดับองค์กรที่ครอบคลุม ซึ่งออกแบบมาโดยเฉพาะสำหรับการปรับใช้ AI ที่ Edge ในระดับการผลิต พร้อมการผสานรวมลึกเข้าไปในระบบนิเวศของ Microsoft

**พื้นฐาน ONNX**: สร้างขึ้นบน ONNX Runtime มาตรฐานอุตสาหกรรม Foundry Local ให้ประสิทธิภาพที่เหมาะสมในสถาปัตยกรรมฮาร์ดแวร์ที่หลากหลาย แพลตฟอร์มนี้ใช้การผสานรวม Windows ML เพื่อการปรับแต่ง Windows โดยธรรมชาติ ในขณะที่ยังคงความเข้ากันได้ข้ามแพลตฟอร์ม

**ความเป็นเลิศด้านการเร่งฮาร์ดแวร์**: Foundry Local มีการตรวจจับฮาร์ดแวร์และการปรับแต่งอัจฉริยะใน CPU, GPU และ NPU ความร่วมมืออย่างลึกซึ้งกับผู้จำหน่ายฮาร์ดแวร์ (AMD, Intel, NVIDIA, Qualcomm) ช่วยให้ประสิทธิภาพที่เหมาะสมที่สุดในการกำหนดค่าฮาร์ดแวร์ระดับองค์กร

### ประสบการณ์นักพัฒนาขั้นสูง

**การเข้าถึงหลายอินเทอร์เฟซ**: Foundry Local มีอินเทอร์เฟซการพัฒนาที่ครอบคลุม รวมถึง CLI ที่ทรงพลังสำหรับการจัดการและการปรับใช้โมเดล SDK หลายภาษา (Python, NodeJS) สำหรับการผสานรวมแบบเนทีฟ และ RESTful APIs ที่เข้ากันได้กับ OpenAI เพื่อการโยกย้ายที่ราบรื่น

**การผสานรวม Visual Studio**: แพลตฟอร์มนี้ผสานรวมอย่างราบรื่นกับ AI Toolkit สำหรับ VS Code โดยมีเครื่องมือการแปลงโมเดล การปรับแต่ง และการเพิ่มประสิทธิภาพภายในสภาพแวดล้อมการพัฒนา การผสานรวมนี้ช่วยเร่งกระบวนการพัฒนาและลดความซับซ้อนในการปรับใช้

**กระบวนการเพิ่มประสิทธิภาพโมเดล**: การผสานรวม Microsoft Olive ช่วยให้สามารถทำงานเพิ่มประสิทธิภาพโมเดลที่ซับซ้อน รวมถึงการปรับแต่งแบบไดนามิก การเพิ่มประสิทธิภาพกราฟ และการปรับแต่งเฉพาะฮาร์ดแวร์ ความสามารถในการแปลงผ่าน Cloud ผ่าน Azure ML ช่วยให้การเพิ่มประสิทธิภาพสำหรับโมเดลขนาดใหญ่สามารถปรับขยายได้

### กลยุทธ์การปรับใช้ในระดับการผลิต

**การติดตั้งและการตั้งค่า**:

```bash
# Windows installation via WinGet
winget install Microsoft.FoundryLocal

# Verify installation
foundry-local --version

# Initialize local environment
foundry-local init
```

**การดำเนินการจัดการโมเดล**:

```bash
# Browse available models
foundry-local models list

# Filter by specific criteria
foundry-local models list --size small --type instruct

# Download and deploy models
foundry-local models pull microsoft/phi-4-mini
foundry-local models pull deepseek/r1-distill-qwen-1.5b

# Test model performance
foundry-local models test microsoft/phi-4-mini --benchmark
```

**การตั้งค่าการปรับใช้ขั้นสูง**:

```json
{
  "deployment": {
    "model": "microsoft/phi-4-mini",
    "hardware": {
      "preferred": "npu",
      "fallback": ["gpu", "cpu"]
    },
    "optimization": {
      "quantization": "dynamic",
      "batch_size": 4,
      "max_context": 4096
    },
    "api": {
      "port": 8080,
      "openai_compatible": true
    }
  }
}
```

### การผสานรวมระบบนิเวศระดับองค์กร

**ความปลอดภัยและการปฏิบัติตามข้อกำหนด**: Foundry Local มีคุณสมบัติด้านความปลอดภัยระดับองค์กร รวมถึงการควบคุมการเข้าถึงตามบทบาท การบันทึกการตรวจสอบ การรายงานการปฏิบัติตามข้อกำหนด และการจัดเก็บโมเดลที่เข้ารหัส การผสานรวมกับโครงสร้างพื้นฐานด้านความปลอดภัยของ Microsoft ช่วยให้ปฏิบัติตามนโยบายความปลอดภัยขององค์กร

**บริการ AI ในตัว**: แพลตฟอร์มนี้มีความสามารถ AI ที่พร้อมใช้งาน รวมถึง Phi Silica สำหรับการประมวลผลภาษาท้องถิ่น AI Imaging สำหรับการปรับปรุงและวิเคราะห์ภาพ และ APIs เฉพาะทางสำหรับงาน AI ทั่วไปในองค์กร

## การวิเคราะห์เปรียบเทียบ: Ollama vs Foundry Local

### การเปรียบเทียบสถาปัตยกรรมทางเทคนิค

| **ด้าน** | **Ollama** | **Foundry Local** |
|------------|------------|-------------------|
| **รูปแบบโมเดล** | GGUF (ผ่าน llama.cpp) | ONNX (ผ่าน ONNX Runtime) |
| **โฟกัสแพลตฟอร์ม** | ข้ามแพลตฟอร์มแบบสากล | การปรับแต่ง Windows/องค์กร |
| **การผสานฮาร์ดแวร์** | รองรับ GPU/CPU ทั่วไป | การสนับสนุน NPU, Windows ML |
| **การเพิ่มประสิทธิภาพ** | การปรับแต่ง llama.cpp | Microsoft Olive + ONNX Runtime |
| **คุณสมบัติองค์กร** | ขับเคลื่อนโดยชุมชน | ระดับองค์กรพร้อม SLAs |

### ลักษณะประสิทธิภาพ

**จุดแข็งด้านประสิทธิภาพของ Ollama**:
- ประสิทธิภาพ CPU ที่ยอดเยี่ยมผ่านการปรับแต่ง llama.cpp
- พฤติกรรมที่สม่ำเสมอในแพลตฟอร์มและฮาร์ดแวร์ที่แตกต่างกัน
- การใช้หน่วยความจำที่มีประสิทธิภาพพร้อมการโหลดโมเดลอัจฉริยะ
- เวลาเริ่มต้นที่รวดเร็วสำหรับสถานการณ์การพัฒนาและการทดสอบ

**ข้อได้เปรียบด้านประสิทธิภาพของ Foundry Local**:
- การใช้ NPU ที่เหนือกว่าในฮาร์ดแวร์ Windows รุ่นใหม่
- การเร่ง GPU ที่ปรับแต่งผ่านความร่วมมือกับผู้จำหน่าย
- การตรวจสอบและการเพิ่มประสิทธิภาพระดับองค์กร
- ความสามารถในการปรับใช้ที่ปรับขยายได้สำหรับสภาพแวดล้อมการผลิต

### การวิเคราะห์ประสบการณ์นักพัฒนา

**ประสบการณ์นักพัฒนาของ Ollama**:
- ข้อกำหนดการตั้งค่าที่น้อยพร้อมประสิทธิภาพทันที
- อินเทอร์เฟซบรรทัดคำสั่งที่ใช้งานง่ายสำหรับทุกการดำเนินการ
- การสนับสนุนจากชุมชนและเอกสารประกอบที่กว้างขวาง
- การปรับแต่งที่ยืดหยุ่นผ่าน Modelfiles

**ประสบการณ์นักพัฒนาของ Foundry Local**:
- การผสานรวม IDE ที่ครอบคลุมกับระบบนิเวศ Visual Studio
- เวิร์กโฟลว์การพัฒนาระดับองค์กรพร้อมคุณสมบัติการทำงานร่วมกันในทีม
- ช่องทางการสนับสนุนระดับมืออาชีพพร้อมการสนับสนุนจาก Microsoft
- เครื่องมือการดีบักและการเพิ่มประสิทธิภาพขั้นสูง

### การปรับใช้กรณีการใช้งาน

**เลือก Ollama เมื่อ**:
- พัฒนาแอปพลิเคชันข้ามแพลตฟอร์มที่ต้องการพฤติกรรมที่สม่ำเสมอ
- ให้ความสำคัญกับความโปร่งใสของโอเพ่นซอร์สและการสนับสนุนจากชุมชน
- ทำงานด้วยทรัพยากรหรือข้อจำกัดด้านงบประมาณ
- สร้างแอปพลิเคชันที่เน้นการทดลองหรือการวิจัย
- ต้องการความเข้ากันได้กับโมเดลที่หลากหลายในสถาปัตยกรรมที่แตกต่างกัน

**เลือก Foundry Local เมื่อ**:
- ปรับใช้แอปพลิเคชันระดับองค์กรที่มีข้อกำหนดด้านประสิทธิภาพที่เข้มงวด
- ใช้ประโยชน์จากการปรับแต่งฮาร์ดแวร์เฉพาะ Windows (NPU, Windows ML)
- ต้องการการสนับสนุนระดับองค์กร SLAs และคุณสมบัติการปฏิบัติตามข้อกำหนด
- สร้างแอปพลิเคชันการผลิตที่ผสานรวมกับระบบนิเวศของ Microsoft
- ต้องการเครื่องมือการเพิ่มประสิทธิภาพขั้นสูงและเวิร์กโฟลว์การพัฒนาระดับมืออาชีพ

## กลยุทธ์การปรับใช้ขั้นสูง

### รูปแบบการปรับใช้แบบคอนเทนเนอร์

**การปรับใช้แบบคอนเทนเนอร์ของ Ollama**:

```dockerfile
FROM ollama/ollama:latest

# Pre-load models for faster startup
RUN ollama pull qwen2.5:3b
RUN ollama pull phi4:mini

# Custom configuration
COPY modelfile ./
RUN ollama create enterprise-model -f modelfile

# Expose API port
EXPOSE 11434

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:11434/api/health || exit 1
```

**การปรับใช้ระดับองค์กรของ Foundry Local**:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foundry-local-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: foundry-local
  template:
    metadata:
      labels:
        app: foundry-local
    spec:
      containers:
      - name: foundry-local
        image: microsoft/foundry-local:latest
        ports:
        - containerPort: 8080
        resources:
          requests:
            memory: "4Gi"
            cpu: "2000m"
          limits:
            memory: "8Gi"
            cpu: "4000m"
        env:
        - name: FOUNDRY_MODEL
          value: "microsoft/phi-4-mini"
        - name: FOUNDRY_HARDWARE
          value: "npu,gpu,cpu"
```

### เทคนิคการเพิ่มประสิทธิภาพ

**กลยุทธ์การเพิ่มประสิทธิภาพของ Ollama**:

```bash
# GPU acceleration configuration
export OLLAMA_NUM_PARALLEL=4
export OLLAMA_MAX_LOADED_MODELS=2
export OLLAMA_FLASH_ATTENTION=1

# Memory optimization
export OLLAMA_MAX_VRAM=8G
export OLLAMA_KEEP_ALIVE=10m

# Start optimized server
ollama serve
```

**การเพิ่มประสิทธิภาพของ Foundry Local**:

```json
{
  "performance": {
    "batch_processing": true,
    "parallel_requests": 8,
    "memory_optimization": {
      "enable_kv_cache": true,
      "max_cache_size": "4GB"
    },
    "hardware_scheduling": {
      "enable_dynamic_batching": true,
      "max_batch_size": 16
    }
  }
}
```

## การพิจารณาด้านความปลอดภัยและการปฏิบัติตามข้อกำหนด

### การดำเนินการด้านความปลอดภัยระดับองค์กร

**แนวปฏิบัติที่ดีที่สุดด้านความปลอดภัยของ Ollama**:
- การแยกเครือข่ายด้วยกฎไฟร์วอลล์และการเข้าถึง VPN
- การรับรองความถูกต้องผ่านการผสานรวม Reverse Proxy
- การตรวจสอบความสมบูรณ์ของโมเดลและการแจกจ่ายโมเดลอย่างปลอดภัย
- การบันทึกการตรวจสอบสำหรับการเข้าถึง API และการดำเนินการโมเดล

**ความปลอดภัยระดับองค์กรของ Foundry Local**:
- การควบคุมการเข้าถึงตามบทบาทในตัวพร้อมการผสาน

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้