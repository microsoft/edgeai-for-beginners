# ส่วนที่ 3 : Microsoft Olive Optimization Suite

## สารบัญ
1. [แนะนำ](../../../Module04)
2. [Microsoft Olive คืออะไร?](../../../Module04)
3. [การติดตั้ง](../../../Module04)
4. [คู่มือเริ่มต้นใช้งาน](../../../Module04)
5. [ตัวอย่าง: การแปลง Qwen3 เป็น ONNX INT4](../../../Module04)
6. [การใช้งานขั้นสูง](../../../Module04)
7. [คลังสูตร Olive](../../../Module04)
8. [แนวทางปฏิบัติที่ดีที่สุด](../../../Module04)
9. [การแก้ไขปัญหา](../../../Module04)
10. [แหล่งข้อมูลเพิ่มเติม](../../../Module04)

## แนะนำ

Microsoft Olive เป็นเครื่องมือที่ทรงพลังและใช้งานง่ายสำหรับการปรับแต่งโมเดลที่คำนึงถึงฮาร์ดแวร์ ซึ่งช่วยให้การปรับแต่งโมเดลการเรียนรู้ของเครื่องสำหรับการใช้งานบนแพลตฟอร์มฮาร์ดแวร์ต่างๆ เป็นเรื่องง่าย ไม่ว่าคุณจะกำหนดเป้าหมายไปที่ CPU, GPU หรือ AI accelerators เฉพาะ Olive จะช่วยให้คุณได้ประสิทธิภาพที่ดีที่สุดโดยยังคงรักษาความแม่นยำของโมเดลไว้

## Microsoft Olive คืออะไร?

Olive เป็นเครื่องมือปรับแต่งโมเดลที่คำนึงถึงฮาร์ดแวร์และใช้งานง่าย โดยรวบรวมเทคนิคชั้นนำในอุตสาหกรรมด้านการบีบอัดโมเดล การปรับแต่ง และการคอมไพล์ มันทำงานร่วมกับ ONNX Runtime เป็นโซลูชันการปรับแต่งการอนุมานแบบ E2E

### คุณสมบัติเด่น

- **การปรับแต่งที่คำนึงถึงฮาร์ดแวร์**: เลือกเทคนิคการปรับแต่งที่ดีที่สุดสำหรับฮาร์ดแวร์เป้าหมายของคุณโดยอัตโนมัติ
- **ส่วนประกอบการปรับแต่งในตัวกว่า 40+**: ครอบคลุมการบีบอัดโมเดล การปรับแต่งกราฟ และอื่นๆ
- **อินเทอร์เฟซ CLI ที่ใช้งานง่าย**: คำสั่งง่ายๆ สำหรับงานปรับแต่งทั่วไป
- **รองรับหลายเฟรมเวิร์ก**: ใช้งานร่วมกับ PyTorch, Hugging Face models และ ONNX
- **รองรับโมเดลยอดนิยม**: Olive สามารถปรับแต่งสถาปัตยกรรมโมเดลยอดนิยม เช่น Llama, Phi, Qwen, Gemma เป็นต้น ได้โดยอัตโนมัติ

### ประโยชน์

- **ลดเวลาในการพัฒนา**: ไม่จำเป็นต้องทดลองเทคนิคการปรับแต่งต่างๆ ด้วยตัวเอง
- **เพิ่มประสิทธิภาพ**: ปรับปรุงความเร็วอย่างมีนัยสำคัญ (สูงสุดถึง 6 เท่าในบางกรณี)
- **การใช้งานข้ามแพลตฟอร์ม**: โมเดลที่ปรับแต่งแล้วสามารถทำงานบนฮาร์ดแวร์และระบบปฏิบัติการต่างๆ
- **รักษาความแม่นยำ**: การปรับแต่งยังคงรักษาคุณภาพของโมเดลไว้ในขณะที่ปรับปรุงประสิทธิภาพ

## การติดตั้ง

### ข้อกำหนดเบื้องต้น

- Python 3.8 หรือสูงกว่า
- ตัวจัดการแพ็กเกจ pip
- สภาพแวดล้อมเสมือน (แนะนำ)

### การติดตั้งพื้นฐาน

สร้างและเปิดใช้งานสภาพแวดล้อมเสมือน:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

ติดตั้ง Olive พร้อมฟีเจอร์การปรับแต่งอัตโนมัติ:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### การติดตั้งส่วนเสริม (ไม่บังคับ)

Olive มีส่วนเสริมต่างๆ สำหรับฟีเจอร์เพิ่มเติม:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### ตรวจสอบการติดตั้ง

```bash
olive --help
```

หากสำเร็จ คุณควรเห็นข้อความช่วยเหลือของ Olive CLI

## คู่มือเริ่มต้นใช้งาน

### การปรับแต่งครั้งแรกของคุณ

มาลองปรับแต่งโมเดลภาษาขนาดเล็กโดยใช้ฟีเจอร์การปรับแต่งอัตโนมัติของ Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### สิ่งที่คำสั่งนี้ทำ

กระบวนการปรับแต่งประกอบด้วย: การดึงโมเดลจากแคชในเครื่อง การจับ ONNX Graph และจัดเก็บน้ำหนักในไฟล์ข้อมูล ONNX การปรับแต่ง ONNX Graph และการปรับโมเดลให้เป็น int4 โดยใช้วิธี RTN

### อธิบายพารามิเตอร์คำสั่ง

- `--model_name_or_path`: ตัวระบุโมเดล Hugging Face หรือเส้นทางในเครื่อง
- `--output_path`: ไดเรกทอรีที่โมเดลที่ปรับแต่งแล้วจะถูกบันทึก
- `--device`: อุปกรณ์เป้าหมาย (cpu, gpu)
- `--provider`: ผู้ให้บริการการดำเนินการ (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ใช้ ONNX Runtime Generate AI สำหรับการอนุมาน
- `--precision`: ความแม่นยำของการปรับแต่ง (int4, int8, fp16)
- `--log_level`: ระดับความละเอียดของการบันทึก (0=น้อยที่สุด, 1=ละเอียด)

## ตัวอย่าง: การแปลง Qwen3 เป็น ONNX INT4

จากตัวอย่างที่ให้ไว้ใน Hugging Face ที่ [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) นี่คือวิธีการปรับแต่งโมเดล Qwen3:

### ขั้นตอนที่ 1: ดาวน์โหลดโมเดล (ไม่บังคับ)

เพื่อให้เวลาการดาวน์โหลดลดลง ให้แคชเฉพาะไฟล์ที่จำเป็น:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### ขั้นตอนที่ 2: ปรับแต่งโมเดล Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ขั้นตอนที่ 3: ทดสอบโมเดลที่ปรับแต่งแล้ว

สร้างสคริปต์ Python ง่ายๆ เพื่อทดสอบโมเดลที่ปรับแต่งแล้วของคุณ:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### โครงสร้างผลลัพธ์

หลังการปรับแต่ง ไดเรกทอรีผลลัพธ์ของคุณจะมี:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## การใช้งานขั้นสูง

### ไฟล์การตั้งค่า

สำหรับเวิร์กโฟลว์การปรับแต่งที่ซับซ้อนยิ่งขึ้น คุณสามารถใช้ไฟล์การตั้งค่า JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

เรียกใช้ด้วยการตั้งค่า:

```bash
olive run --config config.json
```

### การปรับแต่ง GPU

สำหรับการปรับแต่ง CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

สำหรับ DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### การปรับแต่งโมเดลด้วย Olive

Olive ยังรองรับการปรับแต่งโมเดล:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## แนวทางปฏิบัติที่ดีที่สุด

### 1. การเลือกโมเดล
- เริ่มต้นด้วยโมเดลขนาดเล็กสำหรับการทดสอบ (เช่น 0.5B-7B พารามิเตอร์)
- ตรวจสอบให้แน่ใจว่าสถาปัตยกรรมโมเดลเป้าหมายของคุณรองรับโดย Olive

### 2. การพิจารณาฮาร์ดแวร์
- จับคู่เป้าหมายการปรับแต่งของคุณกับฮาร์ดแวร์ที่คุณจะใช้งาน
- ใช้การปรับแต่ง GPU หากคุณมีฮาร์ดแวร์ที่รองรับ CUDA
- พิจารณา DirectML สำหรับเครื่อง Windows ที่มีกราฟิกในตัว

### 3. การเลือกความแม่นยำ
- **INT4**: การบีบอัดสูงสุด สูญเสียความแม่นยำเล็กน้อย
- **INT8**: สมดุลที่ดีระหว่างขนาดและความแม่นยำ
- **FP16**: สูญเสียความแม่นยำน้อยที่สุด ลดขนาดปานกลาง

### 4. การทดสอบและการตรวจสอบ
- ทดสอบโมเดลที่ปรับแต่งแล้วกับกรณีการใช้งานเฉพาะของคุณเสมอ
- เปรียบเทียบเมตริกประสิทธิภาพ (ความหน่วง, ความเร็ว, ความแม่นยำ)
- ใช้ข้อมูลอินพุตที่เป็นตัวแทนสำหรับการประเมินผล

### 5. การปรับแต่งแบบวนซ้ำ
- เริ่มต้นด้วยการปรับแต่งอัตโนมัติสำหรับผลลัพธ์ที่รวดเร็ว
- ใช้ไฟล์การตั้งค่าสำหรับการควบคุมที่ละเอียด
- ทดลองกับการปรับแต่งแบบต่างๆ

## การแก้ไขปัญหา

### ปัญหาทั่วไป

#### 1. ปัญหาการติดตั้ง
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. ปัญหา CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. ปัญหาหน่วยความจำ
- ใช้ขนาดแบตช์ที่เล็กลงระหว่างการปรับแต่ง
- ลองปรับแต่งด้วยความแม่นยำที่สูงขึ้นก่อน (int8 แทน int4)
- ตรวจสอบให้แน่ใจว่ามีพื้นที่ดิสก์เพียงพอสำหรับการแคชโมเดล

#### 4. ข้อผิดพลาดในการโหลดโมเดล
- ตรวจสอบเส้นทางโมเดลและสิทธิ์การเข้าถึง
- ตรวจสอบว่าโมเดลต้องการ `trust_remote_code=True`
- ตรวจสอบให้แน่ใจว่าไฟล์โมเดลที่จำเป็นทั้งหมดถูกดาวน์โหลด

### การขอความช่วยเหลือ

- **เอกสาร**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **ปัญหาใน GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **ตัวอย่าง**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## คลังสูตร Olive

### แนะนำคลังสูตร Olive

คลัง [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) เป็นส่วนเสริมของเครื่องมือ Olive หลัก โดยมีการรวบรวมสูตรการปรับแต่งที่พร้อมใช้งานสำหรับโมเดล AI ยอดนิยม คลังนี้เป็นแหล่งอ้างอิงที่ใช้งานได้จริงสำหรับทั้งการปรับแต่งโมเดลที่มีอยู่ทั่วไปและการสร้างเวิร์กโฟลว์การปรับแต่งสำหรับโมเดลเฉพาะ

### คุณสมบัติเด่น

- **สูตรที่สร้างไว้ล่วงหน้ากว่า 100+**: การตั้งค่าการปรับแต่งที่พร้อมใช้งานสำหรับโมเดลยอดนิยม
- **รองรับหลายสถาปัตยกรรม**: ครอบคลุมโมเดลทรานส์ฟอร์เมอร์ โมเดลภาพ และสถาปัตยกรรมมัลติโหมด
- **การปรับแต่งเฉพาะฮาร์ดแวร์**: สูตรที่ปรับแต่งสำหรับ CPU, GPU และ accelerators เฉพาะ
- **ครอบคลุมโมเดลยอดนิยม**: รวมถึง Phi, Llama, Qwen, Gemma, Mistral และอีกมากมาย

### โมเดลที่รองรับ

คลังนี้มีสูตรการปรับแต่งสำหรับ:

#### โมเดลภาษา
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 series (0.5B ถึง 14B)
- **Google Gemma**: การตั้งค่าโมเดล Gemma ต่างๆ
- **Mistral AI**: ซีรีส์ Mistral-7B
- **DeepSeek**: โมเดลซีรีส์ R1-Distill

#### โมเดลภาพและมัลติโหมด
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP Models**: การตั้งค่า CLIP-ViT ต่างๆ
- **ResNet**: การปรับแต่ง ResNet-50
- **Vision Transformers**: ViT-base-patch16-224

#### โมเดลเฉพาะทาง
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: รุ่น Base และ Multilingual
- **Sentence Transformers**: all-MiniLM-L6-v2

### การใช้สูตร Olive

#### วิธีที่ 1: โคลนสูตรเฉพาะ

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### วิธีที่ 2: ใช้สูตรเป็นแม่แบบ

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### โครงสร้างสูตร

แต่ละไดเรกทอรีสูตรมักจะมี:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### ตัวอย่าง: การใช้สูตร Phi-4-mini

มาลองใช้สูตร Phi-4-mini เป็นตัวอย่าง:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

ไฟล์การตั้งค่ามักจะมี:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### การปรับแต่งสูตร

#### การเปลี่ยนฮาร์ดแวร์เป้าหมาย

เพื่อเปลี่ยนฮาร์ดแวร์เป้าหมาย ให้แก้ไขส่วน `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### การปรับพารามิเตอร์การปรับแต่ง

แก้ไขส่วน `passes` สำหรับระดับการปรับแต่งที่แตกต่างกัน:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### การสร้างสูตรของคุณเอง

1. **เริ่มต้นด้วยโมเดลที่คล้ายกัน**: ค้นหาสูตรสำหรับโมเดลที่มีสถาปัตยกรรมคล้ายกัน
2. **อัปเดตการตั้งค่าโมเดล**: เปลี่ยนชื่อ/เส้นทางโมเดลในไฟล์การตั้งค่า
3. **ปรับพารามิเตอร์**: แก้ไขพารามิเตอร์การปรับแต่งตามต้องการ
4. **ทดสอบและตรวจสอบ**: เรียกใช้การปรับแต่งและตรวจสอบผลลัพธ์
5. **มีส่วนร่วมกลับ**: พิจารณาส่งสูตรของคุณกลับไปยังคลัง

### ประโยชน์ของการใช้สูตร

#### 1. **การตั้งค่าที่พิสูจน์แล้ว**
- การตั้งค่าการปรับแต่งที่ผ่านการทดสอบสำหรับโมเดลเฉพาะ
- หลีกเลี่ยงการลองผิดลองถูกในการค้นหาพารามิเตอร์ที่เหมาะสม

#### 2. **การปรับแต่งเฉพาะฮาร์ดแวร์**
- ปรับแต่งล่วงหน้าสำหรับผู้ให้บริการการดำเนินการต่างๆ
- การตั้งค่าที่พร้อมใช้งานสำหรับเป้าหมาย CPU, GPU และ NPU

#### 3. **ครอบคลุมอย่างครอบคลุม**
- รองรับโมเดลโอเพ่นซอร์สยอดนิยมที่สุด
- อัปเดตเป็นประจำพร้อมการเปิดตัวโมเดลใหม่

#### 4. **การมีส่วนร่วมของชุมชน**
- การพัฒนาร่วมกับชุมชน AI
- การแบ่งปันความรู้และแนวทางปฏิบัติที่ดีที่สุด

### การมีส่วนร่วมในคลังสูตร Olive

หากคุณได้ปรับแต่งโมเดลที่ไม่ได้ครอบคลุมในคลัง:

1. **Fork คลัง**: สร้าง fork ของ olive-recipes ของคุณเอง
2. **สร้างไดเรกทอรีสูตร**: เพิ่มไดเรกทอรีใหม่สำหรับโมเดลของคุณ
3. **เพิ่มการตั้งค่า**: เพิ่ม olive_config.json และไฟล์สนับสนุน
4. **เอกสารการใช้งาน**: ให้คำแนะนำที่ชัดเจนใน README
5. **ส่ง Pull Request**: มีส่วนร่วมกลับไปยังชุมชน

### การวัดประสิทธิภาพ

สูตรหลายสูตรมีการวัดประสิทธิภาพที่แสดง:
- **การปรับปรุงความหน่วง**: ความเร็วเพิ่มขึ้น 2-6 เท่าจากพื้นฐาน
- **การลดการใช้หน่วยความจำ**: ลดการใช้หน่วยความจำ 50-75% ด้วยการปรับแต่ง
- **การรักษาความแม่นยำ**: รักษาความแม่นยำไว้ 95-99%

### การผสานรวมกับเครื่องมือ AI

สูตรทำงานร่วมกันได้อย่างราบรื่นกับ:
- **VS Code AI Toolkit**: การผสานรวมโดยตรงสำหรับการปรับแต่งโมเดล
- **Azure Machine Learning**: เวิร์กโฟลว์การปรับแต่งบนคลาวด์
- **ONNX Runtime**: การใช้งานการอนุมานที่ปรับแต่งแล้ว

## แหล่งข้อมูลเพิ่มเติม

### ลิงก์ทางการ
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipes Repository**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime Documentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Example**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU

---

**ข้อจำกัดความรับผิดชอบ**:  
เอกสารนี้ได้รับการแปลโดยใช้บริการแปลภาษา AI [Co-op Translator](https://github.com/Azure/co-op-translator) แม้ว่าเราจะพยายามให้การแปลมีความถูกต้อง แต่โปรดทราบว่าการแปลโดยอัตโนมัติอาจมีข้อผิดพลาดหรือความไม่ถูกต้อง เอกสารต้นฉบับในภาษาดั้งเดิมควรถือเป็นแหล่งข้อมูลที่เชื่อถือได้ สำหรับข้อมูลที่สำคัญ ขอแนะนำให้ใช้บริการแปลภาษามืออาชีพ เราไม่รับผิดชอบต่อความเข้าใจผิดหรือการตีความผิดที่เกิดจากการใช้การแปลนี้