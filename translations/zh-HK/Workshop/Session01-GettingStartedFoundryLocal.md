# ç¬¬1ç¯€ï¼šé–‹å§‹ä½¿ç”¨ Foundry Local

## æ‘˜è¦

å­¸ç¿’å¦‚ä½•å®‰è£ã€é…ç½®åŠé‹è¡Œæ‚¨çš„ç¬¬ä¸€å€‹ AI æ¨¡å‹ï¼Œä½¿ç”¨ Microsoft Foundry Localã€‚æœ¬æ¬¡å¯¦ä½œèª²ç¨‹å°‡é€æ­¥ä»‹ç´¹æœ¬åœ°æ¨ç†çš„éç¨‹ï¼Œå¾å®‰è£åˆ°ä½¿ç”¨ Phi-4ã€Qwen å’Œ DeepSeek ç­‰æ¨¡å‹å»ºç«‹æ‚¨çš„ç¬¬ä¸€å€‹èŠå¤©æ‡‰ç”¨ç¨‹å¼ã€‚

## å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ç¯€å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š

- **å®‰è£èˆ‡é…ç½®**ï¼šæ­£ç¢ºè¨­ç½® Foundry Local ä¸¦é©—è­‰å®‰è£
- **æŒæ¡ CLI æ“ä½œ**ï¼šä½¿ç”¨ Foundry Local CLI é€²è¡Œæ¨¡å‹ç®¡ç†èˆ‡éƒ¨ç½²
- **é‹è¡Œæ‚¨çš„ç¬¬ä¸€å€‹æ¨¡å‹**ï¼šæˆåŠŸéƒ¨ç½²ä¸¦èˆ‡æœ¬åœ° AI æ¨¡å‹äº’å‹•
- **å»ºç«‹èŠå¤©æ‡‰ç”¨ç¨‹å¼**ï¼šä½¿ç”¨ Foundry Local Python SDK å‰µå»ºåŸºæœ¬èŠå¤©æ‡‰ç”¨ç¨‹å¼
- **ç†è§£æœ¬åœ° AI**ï¼šæŒæ¡æœ¬åœ°æ¨ç†åŠæ¨¡å‹ç®¡ç†çš„åŸºæœ¬æ¦‚å¿µ

## å…ˆæ±ºæ¢ä»¶

### ç³»çµ±éœ€æ±‚

- **Windows**ï¼šWindows 11 (22H2 æˆ–æ›´æ–°ç‰ˆæœ¬) æˆ– **macOS**ï¼šmacOS 11+ï¼ˆæœ‰é™æ”¯æ´ï¼‰
- **RAM**ï¼šæœ€ä½ 8GBï¼Œå»ºè­° 16GB æˆ–ä»¥ä¸Š
- **å„²å­˜ç©ºé–“**ï¼šæ¨¡å‹éœ€è¦è‡³å°‘ 10GB çš„å¯ç”¨ç©ºé–“
- **Python**ï¼šå·²å®‰è£ 3.10 æˆ–æ›´æ–°ç‰ˆæœ¬
- **ç®¡ç†å“¡æ¬Šé™**ï¼šå®‰è£éœ€è¦ç®¡ç†å“¡æ¬Šé™

### é–‹ç™¼ç’°å¢ƒ

- å»ºè­°ä½¿ç”¨ Visual Studio Code ä¸¦å®‰è£ Python æ“´å±•
- å‘½ä»¤è¡Œè¨ªå•ï¼ˆWindows ä½¿ç”¨ PowerShellï¼ŒmacOS ä½¿ç”¨ Terminalï¼‰
- Git ç”¨æ–¼å…‹éš†å­˜å„²åº«ï¼ˆå¯é¸ï¼‰

## å·¥ä½œåŠæµç¨‹ï¼ˆ30 åˆ†é˜ï¼‰

### ç¬¬1æ­¥ï¼šå®‰è£ Foundry Localï¼ˆ5 åˆ†é˜ï¼‰

#### Windows å®‰è£

ä½¿ç”¨ Windows å¥—ä»¶ç®¡ç†å™¨å®‰è£ Foundry Localï¼š

```powershell
# Install via winget (recommended)
winget install Microsoft.FoundryLocal
```

æ›¿ä»£æ–¹æ³•ï¼šç›´æ¥å¾ [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install) ä¸‹è¼‰

#### macOS å®‰è£ï¼ˆæœ‰é™æ”¯æ´ï¼‰

> [!NOTE]  
> macOS æ”¯æ´ç›®å‰è™•æ–¼é è¦½éšæ®µã€‚è«‹æŸ¥çœ‹å®˜æ–¹æ–‡ä»¶ä»¥ç²å–æœ€æ–°è³‡è¨Šã€‚

å¦‚æœå¯ç”¨ï¼Œä½¿ç”¨ Homebrew å®‰è£ï¼š

```bash
# If Homebrew formula is available
brew update
brew install foundry-local

# Or manual download (check official docs for latest)
curl -L -o foundry-local.tar.gz "https://download.microsoft.com/foundry-local/latest/macos/foundry-local.tar.gz"
tar -xzf foundry-local.tar.gz
sudo ./install.sh
```

**macOS ä½¿ç”¨è€…çš„æ›¿ä»£æ–¹æ³•ï¼š**
- ä½¿ç”¨ Windows 11 è™›æ“¬æ©Ÿï¼ˆParallels/UTMï¼‰ï¼Œä¸¦æŒ‰ç…§ Windows æ­¥é©Ÿæ“ä½œ
- å¦‚æœå¯ç”¨ï¼Œé€šéå®¹å™¨é‹è¡Œä¸¦é…ç½® `FOUNDRY_LOCAL_ENDPOINT`

### ç¬¬2æ­¥ï¼šé©—è­‰å®‰è£ï¼ˆ3 åˆ†é˜ï¼‰

å®‰è£å®Œæˆå¾Œï¼Œé‡å•Ÿçµ‚ç«¯ä¸¦é©—è­‰ Foundry Local æ˜¯å¦æ­£å¸¸é‹è¡Œï¼š

```powershell
# Check if Foundry Local is installed correctly
foundry --version

# View available commands
foundry --help
```

é æœŸè¼¸å‡ºæ‡‰é¡¯ç¤ºç‰ˆæœ¬è³‡è¨ŠåŠå¯ç”¨å‘½ä»¤ã€‚

### ç¬¬3æ­¥ï¼šè¨­ç½® Python ç’°å¢ƒï¼ˆ5 åˆ†é˜ï¼‰

ç‚ºæœ¬æ¬¡å·¥ä½œåŠå‰µå»ºå°ˆç”¨çš„ Python ç’°å¢ƒï¼š

**Windowsï¼š**
```powershell
# Create virtual environment
py -m venv .venv

# Activate environment
.\.venv\Scripts\Activate.ps1

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

**macOS/Linuxï¼š**
```bash
# Create virtual environment
python3 -m venv .venv

# Activate environment
source .venv/bin/activate

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```


### ç¬¬4æ­¥ï¼šé‹è¡Œæ‚¨çš„ç¬¬ä¸€å€‹æ¨¡å‹ï¼ˆ7 åˆ†é˜ï¼‰

ç¾åœ¨è®“æˆ‘å€‘åœ¨æœ¬åœ°é‹è¡Œç¬¬ä¸€å€‹ AI æ¨¡å‹ï¼

#### å¾ Phi-4 Mini é–‹å§‹ï¼ˆæ¨è–¦çš„ç¬¬ä¸€å€‹æ¨¡å‹ï¼‰

```powershell
# Download and start phi-4-mini (lightweight, fast)
foundry model run phi-4-mini

# Test the model with a simple prompt
foundry model run phi-4-mini --prompt "Hello, introduce yourself in one sentence"
```

> [!TIP]  
> æ­¤å‘½ä»¤æœƒä¸‹è¼‰æ¨¡å‹ï¼ˆé¦–æ¬¡é‹è¡Œæ™‚ï¼‰ä¸¦è‡ªå‹•å•Ÿå‹• Foundry Local æœå‹™ã€‚

#### æª¢æŸ¥æ­£åœ¨é‹è¡Œçš„å…§å®¹

```powershell
# List available models (shows downloaded models)
foundry model list

# Check service status
foundry service status

# See what models are cached locally
foundry cache list
```


#### å˜—è©¦ä¸åŒçš„æ¨¡å‹

Phi-4 Mini é‹è¡Œå¾Œï¼Œå˜—è©¦å…¶ä»–æ¨¡å‹ï¼š

```powershell
# Larger model with better capabilities
foundry model run gpt-oss-20b --prompt "Explain edge AI in simple terms"

# Fast, efficient model
foundry model run qwen2.5-0.5b --prompt "What are the benefits of local AI inference?"
```


### ç¬¬5æ­¥ï¼šå»ºç«‹æ‚¨çš„ç¬¬ä¸€å€‹èŠå¤©æ‡‰ç”¨ç¨‹å¼ï¼ˆ10 åˆ†é˜ï¼‰

ç¾åœ¨è®“æˆ‘å€‘å‰µå»ºä¸€å€‹ä½¿ç”¨å‰›å•Ÿå‹•çš„æ¨¡å‹çš„ Python æ‡‰ç”¨ç¨‹å¼ã€‚

#### å‰µå»ºèŠå¤©è…³æœ¬

å‰µå»ºä¸€å€‹åç‚º `my_first_chat.py` çš„æ–°æ–‡ä»¶ï¼ˆæˆ–ä½¿ç”¨æä¾›çš„ç¯„ä¾‹ï¼‰ï¼š

```python
#!/usr/bin/env python3
"""
My First Foundry Local Chat Application
Using FoundryLocalManager for automatic service management
"""

import os
from foundry_local import FoundryLocalManager
from openai import OpenAI

def main():
    # Get model alias from environment or use default
    alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-4-mini")
    
    try:
        # Initialize Foundry Local Manager (auto-starts service, downloads model)
        manager = FoundryLocalManager(alias)
        
        # Create OpenAI client pointing to local endpoint
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-needed"
        )
        
        # Get the actual model ID for this alias
        model_id = manager.get_model_info(alias).id
        
        print("ğŸ¤– Welcome to your first local AI chat!")
        print(f"ï¿½ Using model: {alias} -> {model_id}")
        print(f"ğŸŒ Endpoint: {manager.endpoint}")
        print("ï¿½ğŸ’¡ Type 'quit' to exit\n")
        
    except Exception as e:
        print(f"âŒ Failed to initialize Foundry Local: {e}")
        print("ğŸ’¡ Make sure Foundry Local is installed: foundry --version")
        return
    
    while True:
        # Get user input
        user_message = input("You: ").strip()
        
        if user_message.lower() in ['quit', 'exit', 'bye']:
            print("ğŸ‘‹ Goodbye!")
            break
            
        if not user_message:
            continue
            
        try:
            # Send message to local AI model
            response = client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": "You are a helpful AI assistant running locally."},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=200,
                temperature=0.7
            )
            
            # Display the response
            ai_response = response.choices[0].message.content
            print(f"ğŸ¤– AI: {ai_response}\n")
            
        except Exception as e:
            print(f"âŒ Error: {e}")
            print("ğŸ’¡ Check service status: foundry service status\n")

if __name__ == "__main__":
    main()
```

> [!TIP]  
> **ç›¸é—œç¯„ä¾‹**ï¼šå¦‚éœ€æ›´é«˜ç´šçš„ç”¨æ³•ï¼Œè«‹åƒè€ƒï¼š
>
> - **Python ç¯„ä¾‹**ï¼š`Workshop/samples/session01/chat_bootstrap.py` - åŒ…å«æµå¼å›æ‡‰åŠéŒ¯èª¤è™•ç†
> - **Jupyter Notebook**ï¼š`Workshop/notebooks/session01_chat_bootstrap.ipynb` - è©³ç´°è§£é‡‹çš„äº’å‹•ç‰ˆæœ¬

#### æ¸¬è©¦æ‚¨çš„èŠå¤©æ‡‰ç”¨ç¨‹å¼

```powershell
# No need to manually start models - FoundryLocalManager handles this!
# Just run your chat application
python my_first_chat.py
```

æ›¿ä»£æ–¹æ³•ï¼šç›´æ¥ä½¿ç”¨æä¾›çš„ç¯„ä¾‹

```powershell
# Try the complete sample with streaming support
cd Workshop/samples
python -m session01.chat_bootstrap "Your question here"
```

æˆ–æ¢ç´¢äº’å‹•å¼ç­†è¨˜æœ¬  
åœ¨ VS Code ä¸­æ‰“é–‹ Workshop/notebooks/session01_chat_bootstrap.ipynb

å˜—è©¦ä»¥ä¸‹ç¤ºä¾‹å°è©±ï¼š

- "ä»€éº¼æ˜¯ Microsoft Foundry Localï¼Ÿ"
- "åˆ—å‡ºæœ¬åœ°é‹è¡Œ AI æ¨¡å‹çš„ä¸‰å€‹å¥½è™•"
- "å¹«æˆ‘ç†è§£é‚Šç·£ AI"

## æ‚¨å·²å®Œæˆçš„å…§å®¹

æ­å–œï¼æ‚¨å·²æˆåŠŸï¼š

1. âœ… **å®‰è£ Foundry Local** ä¸¦é©—è­‰å…¶æ­£å¸¸é‹è¡Œ  
2. âœ… **æœ¬åœ°å•Ÿå‹•æ‚¨çš„ç¬¬ä¸€å€‹ AI æ¨¡å‹**ï¼ˆPhi-4 Miniï¼‰  
3. âœ… **é€šéå‘½ä»¤è¡Œæ¸¬è©¦ä¸åŒæ¨¡å‹**  
4. âœ… **å»ºç«‹ä¸€å€‹é€£æ¥æœ¬åœ° AI çš„èŠå¤©æ‡‰ç”¨ç¨‹å¼**  
5. âœ… **é«”é©—æœ¬åœ° AI æ¨ç†**ï¼Œç„¡éœ€ä¾è³´é›²ç«¯  

## ç†è§£ç™¼ç”Ÿäº†ä»€éº¼

### æœ¬åœ° AI æ¨ç†

- æ‚¨çš„ AI æ¨¡å‹å®Œå…¨åœ¨æ‚¨çš„é›»è…¦ä¸Šé‹è¡Œ
- æ²’æœ‰æ•¸æ“šè¢«ç™¼é€åˆ°é›²ç«¯
- å›æ‡‰ç”±æ‚¨çš„ CPU/GPU æœ¬åœ°ç”Ÿæˆ
- ä¿æŒéš±ç§å’Œå®‰å…¨æ€§

### æ¨¡å‹ç®¡ç†

- `foundry model run` ä¸‹è¼‰ä¸¦å•Ÿå‹•æ¨¡å‹
- **FoundryLocalManager SDK** è‡ªå‹•è™•ç†æœå‹™å•Ÿå‹•åŠæ¨¡å‹åŠ è¼‰
- æ¨¡å‹æœƒç·©å­˜æ–¼æœ¬åœ°ä»¥ä¾›æœªä¾†ä½¿ç”¨
- å¯ä¸‹è¼‰å¤šå€‹æ¨¡å‹ï¼Œä½†é€šå¸¸ä¸€æ¬¡åªé‹è¡Œä¸€å€‹
- æœå‹™æœƒè‡ªå‹•ç®¡ç†æ¨¡å‹ç”Ÿå‘½å‘¨æœŸ

### SDK èˆ‡ CLI æ–¹æ³•

- **CLI æ–¹æ³•**ï¼šä½¿ç”¨ `foundry model run <model>` æ‰‹å‹•ç®¡ç†æ¨¡å‹
- **SDK æ–¹æ³•**ï¼šä½¿ç”¨ `FoundryLocalManager(alias)` è‡ªå‹•ç®¡ç†æœå‹™åŠæ¨¡å‹
- **æ¨è–¦**ï¼šæ‡‰ç”¨ç¨‹å¼ä½¿ç”¨ SDKï¼Œæ¸¬è©¦åŠæ¢ç´¢ä½¿ç”¨ CLI

## å¸¸ç”¨å‘½ä»¤åƒè€ƒ

### åŸºæœ¬ CLI å‘½ä»¤

```powershell
# Installation & Setup
foundry --version              # Check installation
foundry --help                 # View all commands

# Model Management
foundry model list             # List available models
foundry model run <model>      # Download and start a model
foundry model run <model> --prompt "text"  # One-shot prompt
foundry cache list             # Show downloaded models

# Service Management
foundry service status         # Check if service is running
foundry service start          # Start the service manually
foundry service stop           # Stop the service
```


### æ¨¡å‹æ¨è–¦

- **phi-4-mini**ï¼šæœ€ä½³å…¥é–€æ¨¡å‹ - å¿«é€Ÿã€è¼•é‡ã€è³ªé‡è‰¯å¥½
- **qwen2.5-0.5b**ï¼šæ¨ç†é€Ÿåº¦æœ€å¿«ï¼Œå…§å­˜ä½¿ç”¨æœ€å°‘
- **gpt-oss-20b**ï¼šå›æ‡‰è³ªé‡æ›´é«˜ï¼Œä½†éœ€è¦æ›´å¤šè³‡æº
- **deepseek-coder-1.3b**ï¼šé‡å°ç·¨ç¨‹åŠä»£ç¢¼ä»»å‹™é€²è¡Œå„ªåŒ–

## ç–‘é›£æ’è§£

### "æ‰¾ä¸åˆ° Foundry å‘½ä»¤"

**è§£æ±ºæ–¹æ¡ˆï¼š**

```powershell
# Restart your terminal after installation
# Or manually add to PATH (Windows)
$env:PATH += ";C:\Program Files\Microsoft\FoundryLocal"
```


### "æ¨¡å‹åŠ è¼‰å¤±æ•—"

**è§£æ±ºæ–¹æ¡ˆï¼š**

```powershell
# Check available system memory
foundry service status

# Try a smaller model first
foundry model run phi-4-mini

# Check disk space for model downloads
# Models are stored in: %USERPROFILE%\.foundry\models (Windows)
```


### "æœ¬åœ°ä¸»æ©Ÿé€£æ¥è¢«æ‹’çµ•"

**è§£æ±ºæ–¹æ¡ˆï¼š**

```powershell
# Check if service is running
foundry service status

# Start service if needed
foundry service start

# Verify the port (default is 5273)
# Check for port conflicts with: netstat -an | findstr 5273
```


## ä¸‹ä¸€æ­¥

### å³æ™‚è¡Œå‹•

1. **å˜—è©¦**ä¸åŒçš„æ¨¡å‹åŠæç¤º
2. **ä¿®æ”¹**æ‚¨çš„èŠå¤©æ‡‰ç”¨ç¨‹å¼ä»¥å˜—è©¦ä¸åŒæ¨¡å‹
3. **å‰µå»º**è‡ªå·±çš„æç¤ºä¸¦æ¸¬è©¦å›æ‡‰
4. **æ¢ç´¢**ç¬¬2ç¯€ï¼šå»ºç«‹ RAG æ‡‰ç”¨ç¨‹å¼

### é«˜ç´šå­¸ç¿’è·¯å¾‘

1. **ç¬¬2ç¯€**ï¼šä½¿ç”¨ RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰æ§‹å»º AI è§£æ±ºæ–¹æ¡ˆ
2. **ç¬¬3ç¯€**ï¼šæ¯”è¼ƒä¸åŒçš„é–‹æºæ¨¡å‹
3. **ç¬¬4ç¯€**ï¼šä½¿ç”¨å°–ç«¯æ¨¡å‹
4. **ç¬¬5ç¯€**ï¼šæ§‹å»ºå¤šä»£ç† AI ç³»çµ±

## ç’°å¢ƒè®Šæ•¸ï¼ˆå¯é¸ï¼‰

å¦‚éœ€æ›´é«˜ç´šçš„ç”¨æ³•ï¼Œæ‚¨å¯ä»¥è¨­ç½®ä»¥ä¸‹ç’°å¢ƒè®Šæ•¸ï¼š

| è®Šæ•¸ | ç”¨é€” | ç¤ºä¾‹ |
|------|------|------|
| `FOUNDRY_LOCAL_ALIAS` | é è¨­ä½¿ç”¨çš„æ¨¡å‹ | `phi-4-mini` |
| `FOUNDRY_LOCAL_ENDPOINT` | è¦†è“‹ç«¯é» URL | `http://localhost:5273/v1` |

åœ¨æ‚¨çš„é …ç›®ç›®éŒ„ä¸­å‰µå»º `.env` æ–‡ä»¶ï¼š
```
FOUNDRY_LOCAL_ALIAS=phi-4-mini
FOUNDRY_LOCAL_ENDPOINT=auto
```


## é™„åŠ è³‡æº

### æ–‡ä»¶

- [Foundry Local Python SDK åƒè€ƒ](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk?pivots=programming-language-python)
- [Foundry Local å®‰è£æŒ‡å—](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)
- [æ¨¡å‹ç›®éŒ„](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/models)

### ç¯„ä¾‹ä»£ç¢¼

- **Session01 Python ç¯„ä¾‹**ï¼š`Workshop/samples/session01/chat_bootstrap.py` - å®Œæ•´çš„èŠå¤©æ‡‰ç”¨ç¨‹å¼ï¼ŒåŒ…å«æµå¼å›æ‡‰
- **Session01 Notebook**ï¼š`Workshop/notebooks/session01_chat_bootstrap.ipynb` - äº’å‹•å¼æ•™ç¨‹  
- [Module08 ç¯„ä¾‹ 01](../Module08/samples/01/README.md) - REST èŠå¤©å¿«é€Ÿå…¥é–€
- [Module08 ç¯„ä¾‹ 02](../Module08/samples/02/README.md) - OpenAI SDK é›†æˆ
- [Module08 ç¯„ä¾‹ 03](../Module08/samples/03/README.md) - æ¨¡å‹ç™¼ç¾åŠåŸºæº–æ¸¬è©¦

### ç¤¾ç¾¤

- [Foundry Local GitHub è¨è«–](https://github.com/microsoft/Foundry-Local/discussions)
- [Azure AI ç¤¾ç¾¤](https://techcommunity.microsoft.com/category/artificialintelligence)

---

**èª²ç¨‹æ™‚é•·**ï¼š30 åˆ†é˜å¯¦ä½œ + 15 åˆ†é˜å•ç­”  
**é›£åº¦ç­‰ç´š**ï¼šåˆå­¸è€…  
**å…ˆæ±ºæ¢ä»¶**ï¼šWindows 11/macOS 11+ï¼ŒPython 3.10+ï¼Œç®¡ç†å“¡æ¬Šé™

## å·¥ä½œåŠç¤ºä¾‹å ´æ™¯

### çœŸå¯¦ä¸–ç•ŒèƒŒæ™¯

**å ´æ™¯**ï¼šæŸä¼æ¥­ IT åœ˜éšŠéœ€è¦è©•ä¼°è¨­å‚™ä¸Šçš„ AI æ¨ç†ï¼Œä»¥è™•ç†æ•æ„Ÿçš„å“¡å·¥åé¥‹ï¼Œè€Œä¸å°‡æ•¸æ“šç™¼é€åˆ°å¤–éƒ¨æœå‹™ã€‚

**æ‚¨çš„ç›®æ¨™**ï¼šå±•ç¤ºæœ¬åœ° AI æ¨¡å‹èƒ½å¤ åœ¨ä¿æŒæ•¸æ“šå®Œå…¨éš±ç§çš„æƒ…æ³ä¸‹ï¼Œä»¥æ¯«ç§’ç´šå»¶é²æä¾›é«˜è³ªé‡å›æ‡‰ã€‚

### æ¸¬è©¦æç¤º

ä½¿ç”¨ä»¥ä¸‹æç¤ºé©—è­‰æ‚¨çš„è¨­ç½®ï¼š

```json
[
    "List two benefits of local inference.",
    "Summarize why keeping data on device improves privacy.",
    "Give one trade-off when choosing a small model over a large model."
]
```


### æˆåŠŸæ¨™æº–

- âœ… æ‰€æœ‰æç¤ºåœ¨ 2 ç§’å…§ç²å¾—å›æ‡‰  
- âœ… æ²’æœ‰æ•¸æ“šé›¢é–‹æ‚¨çš„æœ¬åœ°æ©Ÿå™¨  
- âœ… å›æ‡‰ç›¸é—œä¸”æœ‰å¹«åŠ©  
- âœ… æ‚¨çš„èŠå¤©æ‡‰ç”¨ç¨‹å¼é‹è¡Œæµæš¢  

æ­¤é©—è­‰ç¢ºä¿æ‚¨çš„ Foundry Local è¨­ç½®å·²æº–å‚™å¥½åƒåŠ ç¬¬2è‡³ç¬¬6ç¯€çš„é«˜ç´šå·¥ä½œåŠã€‚

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**å…è²¬è²æ˜**ï¼š  
æ­¤æ–‡ä»¶å·²ä½¿ç”¨äººå·¥æ™ºèƒ½ç¿»è­¯æœå‹™ [Co-op Translator](https://github.com/Azure/co-op-translator) é€²è¡Œç¿»è­¯ã€‚å„˜ç®¡æˆ‘å€‘è‡´åŠ›æ–¼æä¾›æº–ç¢ºçš„ç¿»è­¯ï¼Œä½†è«‹æ³¨æ„ï¼Œè‡ªå‹•ç¿»è­¯å¯èƒ½åŒ…å«éŒ¯èª¤æˆ–ä¸æº–ç¢ºä¹‹è™•ã€‚åŸå§‹æ–‡ä»¶çš„æ¯èªç‰ˆæœ¬æ‡‰è¢«è¦–ç‚ºæ¬Šå¨ä¾†æºã€‚å°æ–¼é‡è¦ä¿¡æ¯ï¼Œå»ºè­°ä½¿ç”¨å°ˆæ¥­äººå·¥ç¿»è­¯ã€‚æˆ‘å€‘å°å› ä½¿ç”¨æ­¤ç¿»è­¯è€Œå¼•èµ·çš„ä»»ä½•èª¤è§£æˆ–èª¤é‡‹ä¸æ‰¿æ“”è²¬ä»»ã€‚
<!-- CO-OP TRANSLATOR DISCLAIMER END -->