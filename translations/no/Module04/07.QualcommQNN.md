# Seksjon 7: Qualcomm QNN (Qualcomm Neural Network) Optimaliseringssuite

## Innholdsfortegnelse
1. [Introduksjon](../../../Module04)
2. [Hva er Qualcomm QNN?](../../../Module04)
3. [Installasjon](../../../Module04)
4. [Hurtigstartguide](../../../Module04)
5. [Eksempel: Konvertering og optimalisering av modeller med QNN](../../../Module04)
6. [Avansert bruk](../../../Module04)
7. [Beste praksis](../../../Module04)
8. [Feilsøking](../../../Module04)
9. [Ekstra ressurser](../../../Module04)

## Introduksjon

Qualcomm QNN (Qualcomm Neural Network) er et omfattende AI-inferensrammeverk designet for å utnytte det fulle potensialet til Qualcomms AI-maskinvareakseleratorer, inkludert Hexagon NPU, Adreno GPU og Kryo CPU. Enten du retter deg mot mobile enheter, edge computing-plattformer eller bilsystemer, gir QNN optimaliserte inferensmuligheter som drar nytte av Qualcomms spesialiserte AI-prosesseringsenheter for maksimal ytelse og energieffektivitet.

## Hva er Qualcomm QNN?

Qualcomm QNN er et enhetlig AI-inferensrammeverk som gjør det mulig for utviklere å distribuere AI-modeller effektivt på Qualcomms heterogene databehandlingsarkitektur. Det gir et enhetlig programmeringsgrensesnitt for tilgang til Hexagon NPU (Neural Processing Unit), Adreno GPU og Kryo CPU, og velger automatisk den optimale prosesseringsenheten for ulike modellag og operasjoner.

### Nøkkelfunksjoner

- **Heterogen databehandling**: Enhetlig tilgang til NPU, GPU og CPU med automatisk arbeidsfordeling
- **Maskinvarebevisst optimalisering**: Spesialiserte optimaliseringer for Qualcomm Snapdragon-plattformer
- **Kvantiseringstøtte**: Avanserte INT8-, INT16- og blandet presisjon kvantiseringsteknikker
- **Modellkonverteringsverktøy**: Direkte støtte for TensorFlow-, PyTorch-, ONNX- og Caffe-modeller
- **Edge AI-optimalisert**: Spesielt designet for mobile og edge-distribusjonsscenarier med fokus på energieffektivitet

### Fordeler

- **Maksimal ytelse**: Utnytt spesialisert AI-maskinvare for opptil 15x ytelsesforbedringer
- **Energieffektivitet**: Optimalisert for mobile og batteridrevne enheter med intelligent strømstyring
- **Lav latens**: Maskinvareakselerert inferens med minimal overhead for sanntidsapplikasjoner
- **Skalerbar distribusjon**: Fra smarttelefoner til bilplattformer på tvers av Qualcomms økosystem
- **Produksjonsklar**: Velprøvd rammeverk brukt i millioner av distribuerte enheter

## Installasjon

### Forutsetninger

- Qualcomm QNN SDK (krever registrering hos Qualcomm)
- Python 3.7 eller nyere
- Kompatibel Qualcomm-maskinvare eller simulator
- Android NDK (for mobil distribusjon)
- Linux- eller Windows-utviklingsmiljø

### QNN SDK-oppsett

1. **Registrer og last ned**: Besøk Qualcomm Developer Network for å registrere deg og laste ned QNN SDK
2. **Pakk ut SDK**: Pakk ut QNN SDK til din utviklingsmappe
3. **Sett miljøvariabler**: Konfigurer stier for QNN-verktøy og biblioteker

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Python-miljøoppsett

Opprett og aktiver et virtuelt miljø:

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Installer nødvendige Python-pakker:

```bash
pip install numpy tensorflow torch onnx
```

### Verifiser installasjon

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Hvis vellykket, bør du se hjelpinformasjon for hvert QNN-verktøy.

## Hurtigstartguide

### Din første modellkonvertering

La oss konvertere en enkel PyTorch-modell for å kjøre på Qualcomm-maskinvare:

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Konverter ONNX til QNN-format

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Generer QNN-modellbibliotek

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Hva denne prosessen gjør

Optimaliseringsarbeidsflyten innebærer: konvertering av den originale modellen til ONNX-format, oversettelse av ONNX til QNNs mellomliggende representasjon, anvendelse av maskinvare-spesifikke optimaliseringer, og generering av et kompilert modellbibliotek for distribusjon.

### Forklaring av nøkkelparametere

- `--input_network`: Kilde ONNX-modellfil
- `--output_path`: Generert C++-kildekodefil
- `--input_dim`: Inndatadimensjoner for optimalisering
- `--quantization_overrides`: Tilpasset kvantiseringskonfigurasjon
- `-t x86_64-linux-clang`: Målarkitektur og kompilator

## Eksempel: Konvertering og optimalisering av modeller med QNN

### Trinn 1: Avansert modellkonvertering med kvantisering

Slik bruker du tilpasset kvantisering under konvertering:

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Konverter med tilpasset kvantisering:

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Trinn 2: Multi-backend optimalisering

Konfigurer for heterogen utførelse på tvers av NPU, GPU og CPU:

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Trinn 3: Opprett kontekstbinær for distribusjon

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Trinn 4: Inferens med QNN Runtime

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Utgangsstruktur

Etter optimalisering vil distribusjonsmappen din inneholde:

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Avansert bruk

### Tilpasset backend-konfigurasjon

Konfigurer spesifikke backend-optimaliseringer:

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Dynamisk kvantisering

Bruk kvantisering under kjøring for bedre nøyaktighet:

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Ytelsesprofilering

Overvåk ytelse på tvers av ulike backends:

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Automatisk backend-valg

Implementer intelligent backend-valg basert på modellkarakteristikker:

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Beste praksis

### 1. Modellarkitekturoptimalisering
- **Lagfletting**: Kombiner operasjoner som Conv+BatchNorm+ReLU for bedre NPU-utnyttelse
- **Depth-wise separable convolutions**: Foretrekk disse fremfor standard convolutions for mobil distribusjon
- **Kvantisering-vennlige design**: Bruk ReLU-aktiveringer og unngå operasjoner som ikke kvantiserer godt

### 2. Kvantiseringsstrategi
- **Post-trening kvantisering**: Start med dette for rask distribusjon
- **Kalibreringsdatasett**: Bruk representativt data som dekker alle inndatavariasjoner
- **Blandet presisjon**: Bruk INT8 for de fleste lag, behold kritiske lag i høyere presisjon

### 3. Retningslinjer for backend-valg
- **NPU (HTP)**: Best for CNN-arbeidsmengder, kvantiserte modeller og strømsensitive applikasjoner
- **GPU**: Optimalt for beregningsintensive operasjoner, større modeller og FP16-presisjon
- **CPU**: Fallback for ikke-støttede operasjoner og feilsøking

### 4. Ytelsesoptimalisering
- **Batch-størrelse**: Bruk batch-størrelse 1 for sanntidsapplikasjoner, større batcher for gjennomstrømning
- **Inndataforbehandling**: Minimer datakopiering og konverteringsoverhead
- **Kontekstgjenbruk**: Forhåndskompliser kontekster for å unngå kompileringsoverhead under kjøring

### 5. Minnehåndtering
- **Tensorallokering**: Bruk statisk allokering når mulig for å unngå overhead under kjøring
- **Minnepools**: Implementer tilpassede minnepools for ofte allokerte tensorer
- **Buffergjenbruk**: Gjenbruk inndata-/utdatabuffere på tvers av inferenskall

### 6. Strømsparing
- **Ytelsesmoduser**: Bruk passende ytelsesmoduser basert på termiske begrensninger
- **Dynamisk frekvensskalering**: La systemet skalere frekvens basert på arbeidsmengde
- **Håndtering av hviletilstand**: Frigi ressurser riktig når de ikke er i bruk

## Feilsøking

### Vanlige problemer

#### 1. SDK-installasjonsproblemer
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Modellkonverteringsfeil
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Kvantiseringsproblemer
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Ytelsesproblemer
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Minneproblemer
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Backend-kompatibilitet
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Ytelsesfeilsøking

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Få hjelp

- **Qualcomm Developer Network**: [developer.qualcomm.com](https://developer.qualcomm.com/)
- **QNN-dokumentasjon**: Tilgjengelig i SDK-pakken
- **Community-forum**: [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Teknisk støtte**: Gjennom Qualcomm utviklerportal

## Ekstra ressurser

### Offisielle lenker
- **Qualcomm AI Hub**: [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Snapdragon-plattformer**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Utviklerportal**: [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **AI Engine**: [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Læringsressurser
- **Kom i gang-guide**: Tilgjengelig i QNN SDK-dokumentasjonen
- **Modellzoo**: [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Optimaliseringsguide**: SDK-dokumentasjonen inkluderer omfattende optimaliseringsretningslinjer
- **Videoveiledninger**: [Qualcomm Developer YouTube Channel](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Integrasjonsverktøy
- **SNPE (Legacy)**: [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub**: Forhåndsoptimaliserte modeller for Qualcomm-maskinvare
- **Android Neural Networks API**: Integrasjon med Android NNAPI
- **TensorFlow Lite Delegate**: Qualcomm-delegat for TFLite

### Ytelsesbenchmarker
- **MLPerf Mobile**: [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark**: [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Qualcomm AI Research**: [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Eksempler fra fellesskapet
- **Eksempelapplikasjoner**: Tilgjengelig i QNN SDK-eksempelmappen
- **GitHub-repositorier**: Eksempler og verktøy bidratt av fellesskapet
- **Tekniske blogger**: [Qualcomm Developer Blog](https://developer.qualcomm.com/blog)

### Relaterte verktøy
- **Qualcomm AI Model Efficiency Toolkit (AIMET)**: [github.com/quic/aimet](https://github.com/quic/aimet) - Avanserte kvantiserings- og komprimeringsteknikker
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - For sammenligning og fallback-distribusjon
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - Plattformuavhengig inferensmotor

### Maskinvare-spesifikasjoner
- **Hexagon NPU**: [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **Adreno GPU**: [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Snapdragon-plattformer**: [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Hva skjer videre

Fortsett din Edge AI-reise ved å utforske [Modul 5: SLMOps og produksjonsdistribusjon](../Module05/README.md) for å lære om operasjonelle aspekter ved livssyklushåndtering av små språkmodeller.

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter nøyaktighet, vær oppmerksom på at automatiske oversettelser kan inneholde feil eller unøyaktigheter. Det originale dokumentet på dets opprinnelige språk bør anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforståelser eller feiltolkninger som oppstår ved bruk av denne oversettelsen.