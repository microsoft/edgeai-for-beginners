# Secțiunea 3: Microsoft Olive Optimization Suite

## Cuprins
1. [Introducere](../../../Module04)
2. [Ce este Microsoft Olive?](../../../Module04)
3. [Instalare](../../../Module04)
4. [Ghid de început rapid](../../../Module04)
5. [Exemplu: Conversia Qwen3 la ONNX INT4](../../../Module04)
6. [Utilizare avansată](../../../Module04)
7. [Repository-ul Olive Recipes](../../../Module04)
8. [Cele mai bune practici](../../../Module04)
9. [Depanare](../../../Module04)
10. [Resurse suplimentare](../../../Module04)

## Introducere

Microsoft Olive este un instrument puternic și ușor de utilizat pentru optimizarea modelelor, care ține cont de hardware-ul vizat. Acesta simplifică procesul de optimizare a modelelor de învățare automată pentru implementare pe diverse platforme hardware. Indiferent dacă vizați CPU-uri, GPU-uri sau acceleratoare AI specializate, Olive vă ajută să obțineți performanțe optime, menținând în același timp acuratețea modelului.

## Ce este Microsoft Olive?

Olive este un instrument de optimizare a modelelor, ușor de utilizat, care ține cont de hardware-ul vizat și integrează tehnici de top din industrie pentru compresia, optimizarea și compilarea modelelor. Acesta funcționează cu ONNX Runtime ca soluție completă de optimizare pentru inferență.

### Caracteristici principale

- **Optimizare bazată pe hardware**: Selectează automat cele mai bune tehnici de optimizare pentru hardware-ul vizat
- **Peste 40 de componente de optimizare integrate**: Include compresia modelelor, cuantificarea, optimizarea graficului și altele
- **Interfață CLI ușor de utilizat**: Comenzi simple pentru sarcini comune de optimizare
- **Suport multi-cadru**: Funcționează cu PyTorch, modele Hugging Face și ONNX
- **Suport pentru modele populare**: Olive poate optimiza automat arhitecturi de modele populare precum Llama, Phi, Qwen, Gemma etc., direct din cutie

### Beneficii

- **Reducerea timpului de dezvoltare**: Nu mai este nevoie să experimentați manual cu diferite tehnici de optimizare
- **Creșteri de performanță**: Îmbunătățiri semnificative ale vitezei (până la 6x în unele cazuri)
- **Implementare pe mai multe platforme**: Modelele optimizate funcționează pe diverse hardware-uri și sisteme de operare
- **Menținerea acurateței**: Optimizările păstrează calitatea modelului, îmbunătățind în același timp performanța

## Instalare

### Cerințe preliminare

- Python 3.8 sau mai recent
- Manager de pachete pip
- Mediu virtual (recomandat)

### Instalare de bază

Creați și activați un mediu virtual:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Instalați Olive cu funcții de auto-optimizare:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Dependențe opționale

Olive oferă diverse dependențe opționale pentru funcții suplimentare:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verificarea instalării

```bash
olive --help
```

Dacă instalarea este reușită, ar trebui să vedeți mesajul de ajutor al Olive CLI.

## Ghid de început rapid

### Prima optimizare

Să optimizăm un model de limbaj mic folosind funcția de auto-optimizare a Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Ce face această comandă

Procesul de optimizare implică: obținerea modelului din cache-ul local, capturarea graficului ONNX și stocarea greutăților într-un fișier de date ONNX, optimizarea graficului ONNX și cuantificarea modelului la int4 folosind metoda RTN.

### Explicația parametrilor comenzii

- `--model_name_or_path`: Identificatorul modelului Hugging Face sau calea locală
- `--output_path`: Directorul unde va fi salvat modelul optimizat
- `--device`: Dispozitivul țintă (cpu, gpu)
- `--provider`: Furnizorul de execuție (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Utilizați ONNX Runtime Generate AI pentru inferență
- `--precision`: Precizia cuantificării (int4, int8, fp16)
- `--log_level`: Nivelul de detaliere al jurnalului (0=minimal, 1=detaliat)

## Exemplu: Conversia Qwen3 la ONNX INT4

Pe baza exemplului oferit de Hugging Face la [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), iată cum să optimizați un model Qwen3:

### Pasul 1: Descărcarea modelului (opțional)

Pentru a minimiza timpul de descărcare, cache-uiți doar fișierele esențiale:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Pasul 2: Optimizarea modelului Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Pasul 3: Testarea modelului optimizat

Creați un script Python simplu pentru a testa modelul optimizat:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Structura ieșirii

După optimizare, directorul de ieșire va conține:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Utilizare avansată

### Fișiere de configurare

Pentru fluxuri de lucru mai complexe de optimizare, puteți utiliza fișiere de configurare JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Rulați cu configurația:

```bash
olive run --config config.json
```

### Optimizare GPU

Pentru optimizarea GPU CUDA:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

Pentru DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fine-tuning cu Olive

Olive suportă și ajustarea fină a modelelor:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Cele mai bune practici

### 1. Selectarea modelului
- Începeți cu modele mai mici pentru testare (de exemplu, 0.5B-7B parametri)
- Asigurați-vă că arhitectura modelului țintă este suportată de Olive

### 2. Considerații hardware
- Potriviți ținta de optimizare cu hardware-ul de implementare
- Utilizați optimizarea GPU dacă aveți hardware compatibil CUDA
- Luați în considerare DirectML pentru mașinile Windows cu grafică integrată

### 3. Selectarea preciziei
- **INT4**: Compresie maximă, pierdere ușoară de acuratețe
- **INT8**: Echilibru bun între dimensiune și acuratețe
- **FP16**: Pierdere minimă de acuratețe, reducere moderată a dimensiunii

### 4. Testare și validare
- Testați întotdeauna modelele optimizate cu cazurile dvs. specifice de utilizare
- Comparați metricile de performanță (latență, throughput, acuratețe)
- Utilizați date de intrare reprezentative pentru evaluare

### 5. Optimizare iterativă
- Începeți cu auto-optimizarea pentru rezultate rapide
- Utilizați fișiere de configurare pentru control detaliat
- Experimentați cu diferite treceri de optimizare

## Depanare

### Probleme comune

#### 1. Probleme de instalare
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. Probleme CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Probleme de memorie
- Utilizați dimensiuni mai mici ale batch-ului în timpul optimizării
- Încercați cuantificarea cu o precizie mai mare mai întâi (int8 în loc de int4)
- Asigurați spațiu suficient pe disc pentru cache-ul modelului

#### 4. Erori la încărcarea modelului
- Verificați calea modelului și permisiunile de acces
- Verificați dacă modelul necesită `trust_remote_code=True`
- Asigurați-vă că toate fișierele necesare ale modelului sunt descărcate

### Obținerea ajutorului

- **Documentație**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **Probleme GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Exemple**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Repository-ul Olive Recipes

### Introducere în Olive Recipes

Repository-ul [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) completează instrumentul principal Olive, oferind o colecție cuprinzătoare de rețete de optimizare gata de utilizat pentru modele AI populare. Acest repository servește ca referință practică atât pentru optimizarea modelelor disponibile public, cât și pentru crearea fluxurilor de lucru de optimizare pentru modele proprietare.

### Caracteristici principale

- **Peste 100 de rețete predefinite**: Configurații de optimizare gata de utilizat pentru modele populare
- **Suport multi-arhitectură**: Include modele de transformare, modele de viziune și arhitecturi multimodale
- **Optimizări specifice hardware**: Rețete adaptate pentru CPU, GPU și acceleratoare specializate
- **Familii de modele populare**: Include Phi, Llama, Qwen, Gemma, Mistral și multe altele

### Familii de modele suportate

Repository-ul include rețete de optimizare pentru:

#### Modele de limbaj
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, seria Qwen2.5 (0.5B până la 14B)
- **Google Gemma**: Diverse configurații Gemma
- **Mistral AI**: Seria Mistral-7B
- **DeepSeek**: Modele din seria R1-Distill

#### Modele de viziune și multimodale
- **Stable Diffusion**: v1.4, XL-base-1.0
- **Modele CLIP**: Diverse configurații CLIP-ViT
- **ResNet**: Optimizări pentru ResNet-50
- **Transformatoare de viziune**: ViT-base-patch16-224

#### Modele specializate
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Variante de bază și multilingve
- **Transformatoare de propoziții**: all-MiniLM-L6-v2

### Utilizarea Olive Recipes

#### Metoda 1: Clonarea unei rețete specifice

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Metoda 2: Utilizarea unei rețete ca șablon

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Structura rețetei

Fiecare director de rețetă conține de obicei:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Exemplu: Utilizarea rețetei Phi-4-mini

Să utilizăm rețeta Phi-4-mini ca exemplu:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Fișierul de configurare include de obicei:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Personalizarea rețetelor

#### Modificarea hardware-ului țintă

Pentru a schimba hardware-ul țintă, actualizați secțiunea `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Ajustarea parametrilor de optimizare

Modificați secțiunea `passes` pentru niveluri diferite de optimizare:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Crearea propriei rețete

1. **Începeți cu un model similar**: Găsiți o rețetă pentru un model cu arhitectură similară
2. **Actualizați configurația modelului**: Schimbați numele/calea modelului în configurație
3. **Ajustați parametrii**: Modificați parametrii de optimizare după necesități
4. **Testați și validați**: Rulați optimizarea și validați rezultatele
5. **Contribuiți înapoi**: Luați în considerare contribuirea rețetei dvs. la repository

### Beneficiile utilizării rețetelor

#### 1. **Configurații dovedite**
- Setări de optimizare testate pentru modele specifice
- Evită încercările și erorile în găsirea parametrilor optimi

#### 2. **Ajustare specifică hardware-ului**
- Pre-optimizat pentru diferiți furnizori de execuție
- Configurații gata de utilizat pentru ținte CPU, GPU și NPU

#### 3. **Acoperire cuprinzătoare**
- Suportă cele mai populare modele open-source
- Actualizări regulate cu noi lansări de modele

#### 4. **Contribuții comunitare**
- Dezvoltare colaborativă cu comunitatea AI
- Cunoștințe și practici împărtășite

### Contribuirea la Olive Recipes

Dacă ați optimizat un model care nu este acoperit în repository:

1. **Fork repository-ul**: Creați propriul fork al olive-recipes
2. **Creați directorul rețetei**: Adăugați un nou director pentru modelul dvs.
3. **Includeți configurația**: Adăugați olive_config.json și fișierele suport
4. **Documentați utilizarea**: Furnizați un README clar cu instrucțiuni
5. **Trimiteți un Pull Request**: Contribuiți înapoi comunității

### Benchmarks de performanță

Multe rețete includ benchmarks de performanță care arată:
- **Îmbunătățiri ale latenței**: De obicei, o creștere de 2-6x față de baza inițială
- **Reducerea memoriei**: Reducere de 50-75% a utilizării memoriei prin cuantificare
- **Păstrarea acurateței**: 95-99% păstrarea acurateței

### Integrare cu instrumentele AI

Rețetele funcționează perfect cu:
- **VS Code AI Toolkit**: Integrare directă pentru optimizarea modelelor
- **Azure Machine Learning**: Fluxuri de lucru de optimizare bazate pe cloud
- **ONNX Runtime**: Implementare optimizată pentru inferență

## Resurse suplimentare

### Linkuri oficiale
- **Repository GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Repository Olive Recipes**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **Documentație ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Exemplu Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Exemple comunitare
- **Jupyter Notebooks**: Disponibile în repository-ul Olive GitHub — https://github.com/microsoft/Olive/tree/main/examples
- **Extensie VS Code**: Prezentare generală AI Toolkit pentru VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Postări pe blog**: Blogul Microsoft Open Source — https://opensource.microsoft.com/blog/

### Instrumente conexe
- **ONNX Runtime**: Motor de inferență de înaltă performanță — https://onnxruntime.ai/
- **Hugging Face Transformers**: Sursa multor modele compatibile — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Fluxuri de lucru de optimizare bazate pe cloud — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Ce urmează

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Declinare de responsabilitate**:  
Acest document a fost tradus folosind serviciul de traducere AI [Co-op Translator](https://github.com/Azure/co-op-translator). Deși ne străduim să asigurăm acuratețea, vă rugăm să fiți conștienți că traducerile automate pot conține erori sau inexactități. Documentul original în limba sa maternă ar trebui considerat sursa autoritară. Pentru informații critice, se recomandă traducerea profesională realizată de oameni. Nu ne asumăm responsabilitatea pentru eventualele neînțelegeri sau interpretări greșite care pot apărea din utilizarea acestei traduceri.