# Abschnitt 03 - Integration des Model Context Protocol (MCP)

## Einführung in MCP (Model Context Protocol)

Das Model Context Protocol (MCP) ist ein Open-Source-Standard, der KI-Anwendungen mit externen Systemen verbindet. Mit MCP können KI-Anwendungen wie Claude oder ChatGPT auf Datenquellen (z. B. lokale Dateien, Datenbanken), Tools (z. B. Suchmaschinen, Taschenrechner) und Workflows (z. B. spezialisierte Prompts) zugreifen, um wichtige Informationen zu erhalten und Aufgaben auszuführen.

Stellen Sie sich MCP wie einen **USB-C-Anschluss für KI-Anwendungen** vor. Genau wie USB-C eine standardisierte Verbindungsmöglichkeit für elektronische Geräte bietet, stellt MCP eine standardisierte Methode bereit, um KI-Anwendungen mit externen Systemen zu verbinden.

### Was ermöglicht MCP?

MCP eröffnet leistungsstarke Möglichkeiten für KI-Anwendungen:

- **Personalisierte KI-Assistenten**: Agenten können auf Ihren Google Kalender und Notion zugreifen und als personalisierter KI-Assistent agieren.
- **Fortschrittliche Codegenerierung**: Claude Code kann eine komplette Web-App basierend auf einem Figma-Design erstellen.
- **Integration von Unternehmensdaten**: Unternehmens-Chatbots können auf mehrere Datenbanken innerhalb einer Organisation zugreifen und Benutzern ermöglichen, Daten per Chat zu analysieren.
- **Kreative Workflows**: KI-Modelle können 3D-Designs in Blender erstellen und diese mit einem 3D-Drucker ausdrucken.
- **Echtzeit-Zugriff auf Informationen**: Verbindung zu externen Datenquellen für aktuelle Informationen.
- **Komplexe mehrstufige Operationen**: Ausführung anspruchsvoller Workflows, die mehrere Tools und Systeme kombinieren.

### Warum ist MCP wichtig?

MCP bietet Vorteile für das gesamte Ökosystem:

**Für Entwickler**: MCP reduziert Entwicklungszeit und Komplexität bei der Erstellung oder Integration einer KI-Anwendung oder eines Agenten.

**Für KI-Anwendungen**: MCP bietet Zugang zu einem Ökosystem aus Datenquellen, Tools und Apps, die die Fähigkeiten erweitern und die Benutzererfahrung verbessern.

**Für Endbenutzer**: MCP führt zu leistungsfähigeren KI-Anwendungen oder Agenten, die auf Ihre Daten zugreifen und bei Bedarf in Ihrem Namen handeln können.

## Kleine Sprachmodelle (SLMs) in MCP

Kleine Sprachmodelle bieten einen effizienten Ansatz für den Einsatz von KI und bringen mehrere Vorteile mit sich:

### Vorteile von SLMs
- **Ressourceneffizienz**: Geringere Anforderungen an die Rechenleistung
- **Schnellere Antwortzeiten**: Reduzierte Latenz für Echtzeitanwendungen  
- **Kostenersparnis**: Minimale Infrastrukturanforderungen
- **Datenschutz**: Kann lokal ausgeführt werden, ohne Datenübertragung
- **Anpassbarkeit**: Einfacher für spezifische Domänen zu optimieren

### Warum SLMs gut mit MCP funktionieren

SLMs in Kombination mit MCP schaffen eine leistungsstarke Verbindung, bei der die Denkfähigkeiten des Modells durch externe Tools ergänzt werden. Dies gleicht die geringere Parameteranzahl durch erweiterte Funktionalität aus.

## Überblick über das Python MCP SDK

Das Python MCP SDK bildet die Grundlage für die Entwicklung von MCP-fähigen Anwendungen. Das SDK umfasst:

- **Client-Bibliotheken**: Für die Verbindung zu MCP-Servern
- **Server-Framework**: Für die Erstellung benutzerdefinierter MCP-Server
- **Protokoll-Handler**: Für die Verwaltung der Kommunikation
- **Tool-Integration**: Für die Ausführung externer Funktionen

## Praktische Implementierung: Phi-4 MCP Client

Lassen Sie uns eine reale Implementierung mit dem Phi-4 Mini-Modell von Microsoft und dessen MCP-Fähigkeiten erkunden.

### Überblick über die MCP-Architektur

MCP folgt einer **Client-Server-Architektur**, bei der ein MCP-Host (eine KI-Anwendung wie Claude Code oder Claude Desktop) Verbindungen zu einem oder mehreren MCP-Servern herstellt. Der MCP-Host erreicht dies, indem er für jeden MCP-Server einen MCP-Client erstellt.

#### Wichtige Teilnehmer

- **MCP Host**: Die KI-Anwendung, die einen oder mehrere MCP-Clients koordiniert und verwaltet
- **MCP Client**: Eine Komponente, die eine Verbindung zu einem MCP-Server aufrechterhält und Kontext vom MCP-Server für den MCP-Host bereitstellt
- **MCP Server**: Ein Programm, das Kontext für MCP-Clients bereitstellt

#### Zwei-Schichten-Architektur

MCP besteht aus zwei klar definierten Schichten:

**Datenebene**: Definiert das JSON-RPC-basierte Protokoll für die Client-Server-Kommunikation, einschließlich:
- Lebenszyklusmanagement (Verbindungsinitialisierung, Fähigkeitsverhandlung)
- Kernprimitive (Tools, Ressourcen, Prompts)
- Client-Funktionen (Sampling, Elicitation, Logging)
- Utility-Funktionen (Benachrichtigungen, Fortschrittsverfolgung)

**Transportebene**: Definiert die Kommunikationsmechanismen und -kanäle:
- **STDIO Transport**: Verwendet Standard-Eingabe-/Ausgabeströme für lokale Prozesse (optimale Leistung, keine Netzwerkbelastung)
- **Streamable HTTP Transport**: Verwendet HTTP POST mit optionalen Server-Sent Events für entfernte Server (unterstützt Standard-HTTP-Authentifizierung)

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### MCP Kernprimitive

MCP definiert Primitive, die die Arten von Kontextinformationen spezifizieren, die mit KI-Anwendungen geteilt werden können, und die Bandbreite der auszuführenden Aktionen.

#### Server-Primitiven

MCP definiert drei Kernprimitive, die Server bereitstellen können:

**Tools**: Ausführbare Funktionen, die von KI-Anwendungen aufgerufen werden können, um Aktionen auszuführen
- Beispiele: Dateioperationen, API-Aufrufe, Datenbankabfragen
- Methoden: `tools/list`, `tools/call`
- Unterstützt dynamische Entdeckung und Ausführung

**Ressourcen**: Datenquellen, die Kontextinformationen für KI-Anwendungen bereitstellen
- Beispiele: Dateiinhalte, Datenbankeinträge, API-Antworten
- Methoden: `resources/list`, `resources/read`
- Ermöglicht den Zugriff auf strukturierte Daten

**Prompts**: Wiederverwendbare Vorlagen, die Interaktionen mit Sprachmodellen strukturieren
- Beispiele: System-Prompts, Few-Shot-Beispiele
- Methoden: `prompts/list`, `prompts/get`
- Standardisiert KI-Interaktionsmuster

#### Client-Primitiven

MCP definiert auch Primitive, die Clients bereitstellen können, um reichhaltigere Interaktionen zu ermöglichen:

**Sampling**: Ermöglicht Servern, Sprachmodell-Vervollständigungen von der KI-Anwendung des Clients anzufordern
- Methode: `sampling/complete`
- Ermöglicht modellunabhängige Serverentwicklung
- Bietet Zugriff auf das Sprachmodell des Hosts

**Elicitation**: Ermöglicht Servern, zusätzliche Informationen von Benutzern anzufordern
- Methode: `elicitation/request`
- Unterstützt Benutzerinteraktion und Bestätigung
- Ermöglicht dynamische Informationssammlung

**Logging**: Ermöglicht Servern, Log-Nachrichten an Clients zu senden
- Wird für Debugging und Überwachungszwecke verwendet
- Bietet Einblick in Serveroperationen

### Lebenszyklus des MCP-Protokolls

#### Initialisierung und Fähigkeitsverhandlung

MCP ist ein zustandsbehaftetes Protokoll, das ein Lebenszyklusmanagement erfordert. Der Initialisierungsprozess erfüllt mehrere wichtige Zwecke:

1. **Protokollversionsverhandlung**: Stellt sicher, dass Client und Server kompatible Protokollversionen verwenden (z. B. "2025-06-18")
2. **Fähigkeitsentdeckung**: Jede Partei gibt unterstützte Funktionen und Primitive an
3. **Identitätsaustausch**: Liefert Identifikations- und Versionsinformationen

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### Tool-Entdeckung und Ausführung

Nach der Initialisierung können Clients Tools entdecken und ausführen:

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### Echtzeit-Benachrichtigungen

MCP unterstützt Echtzeit-Benachrichtigungen für dynamische Updates:

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## Erste Schritte: Schritt-für-Schritt-Anleitung

### Schritt 1: Einrichtung der Umgebung

Installieren Sie die erforderlichen Abhängigkeiten:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Schritt 2: Grundkonfiguration

Richten Sie Ihre Umgebungsvariablen ein:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Schritt 3: Ausführung Ihres ersten MCP-Clients

**Grundlegende Ollama-Einrichtung:**
```bash
python ghmodel_mcp_demo.py
```

**Verwendung des vLLM-Backends:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Server-Sent Events-Verbindung:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Benutzerdefinierter MCP-Server:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Schritt 4: Programmatische Nutzung

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Erweiterte Funktionen

### Unterstützung für mehrere Backends

Die Implementierung unterstützt sowohl Ollama- als auch vLLM-Backends, sodass Sie je nach Ihren Anforderungen wählen können:

- **Ollama**: Besser für lokale Entwicklung und Tests
- **vLLM**: Optimiert für Produktion und Szenarien mit hoher Durchsatzrate

### Flexible Verbindungsprotokolle

Zwei Verbindungsmodi werden unterstützt:

**STDIO-Modus**: Direkte Prozesskommunikation
- Niedrigere Latenz
- Geeignet für lokale Tools
- Einfache Einrichtung

**SSE-Modus**: HTTP-basierte Streaming-Verbindung
- Netzwerkfähig
- Besser für verteilte Systeme
- Echtzeit-Updates

### Tool-Integrationsmöglichkeiten

Das System kann mit verschiedenen Tools integriert werden:
- Web-Automatisierung (Playwright)
- Dateioperationen
- API-Interaktionen
- Systembefehle
- Benutzerdefinierte Funktionen

## Fehlerbehandlung und bewährte Praktiken

### Umfassendes Fehlermanagement

Die Implementierung umfasst robuste Fehlerbehandlung für:

**Verbindungsfehler:**
- MCP-Serverausfälle
- Netzwerk-Zeitüberschreitungen
- Verbindungsprobleme

**Tool-Ausführungsfehler:**
- Fehlende Tools
- Parameterüberprüfung
- Ausführungsfehler

**Antwortverarbeitungsfehler:**
- JSON-Parsing-Probleme
- Formatinkonsistenzen
- Anomalien in LLM-Antworten

### Bewährte Praktiken

1. **Ressourcenmanagement**: Verwenden Sie asynchrone Kontextmanager
2. **Fehlerbehandlung**: Implementieren Sie umfassende Try-Catch-Blöcke
3. **Protokollierung**: Aktivieren Sie geeignete Protokollierungsstufen
4. **Sicherheit**: Validieren Sie Eingaben und bereinigen Sie Ausgaben
5. **Leistung**: Nutzen Sie Verbindungs-Pooling und Caching

## Anwendungen in der Praxis

### Web-Automatisierung
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Datenverarbeitung
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API-Integration
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## Leistungsoptimierung

### Speicherverwaltung
- Effiziente Handhabung der Nachrichtenhistorie
- Ordnungsgemäße Bereinigung von Ressourcen
- Verbindungs-Pooling

### Netzwerkoptimierung
- Asynchrone HTTP-Operationen
- Konfigurierbare Zeitüberschreitungen
- Fehlerbehebung mit Wiederherstellung

### Gleichzeitige Verarbeitung
- Nicht-blockierende I/O
- Parallele Tool-Ausführung
- Effiziente asynchrone Muster

## Sicherheitsüberlegungen

### Datenschutz
- Sichere Verwaltung von API-Schlüsseln
- Eingabevalidierung
- Ausgabe-Sanitierung

### Netzwerksicherheit
- Unterstützung von HTTPS
- Standardmäßig lokale Endpunkte
- Sichere Token-Verwaltung

### Ausführungssicherheit
- Tool-Filterung
- Sandbox-Umgebungen
- Audit-Protokollierung

## MCP-Ökosystem und Entwicklung

### Umfang des MCP-Projekts

Das Model Context Protocol-Ökosystem umfasst mehrere Schlüsselkomponenten:

- **[MCP-Spezifikation](https://modelcontextprotocol.io/specification/latest)**: Offizielle Spezifikation mit Implementierungsanforderungen für Clients und Server
- **[MCP SDKs](https://modelcontextprotocol.io/docs/sdk)**: SDKs für verschiedene Programmiersprachen, die MCP implementieren
- **MCP-Entwicklungstools**: Tools zur Entwicklung von MCP-Servern und -Clients, einschließlich des [MCP Inspector](https://github.com/modelcontextprotocol/inspector)
- **[MCP-Referenzserver-Implementierungen](https://github.com/modelcontextprotocol/servers)**: Referenzimplementierungen von MCP-Servern

### Einstieg in die MCP-Entwicklung

Um mit MCP zu beginnen:

**Server erstellen**: [Erstellen Sie MCP-Server](https://modelcontextprotocol.io/docs/develop/build-server), um Ihre Daten und Tools bereitzustellen.

**Clients erstellen**: [Entwickeln Sie Anwendungen](https://modelcontextprotocol.io/docs/develop/build-client), die sich mit MCP-Servern verbinden.

**Konzepte lernen**: [Verstehen Sie die Kernkonzepte](https://modelcontextprotocol.io/docs/learn/architecture) und die Architektur von MCP.

## Fazit

SLMs, die mit MCP integriert sind, stellen einen Paradigmenwechsel in der Entwicklung von KI-Anwendungen dar. Durch die Kombination der Effizienz kleiner Modelle mit der Leistungsfähigkeit externer Tools können Entwickler intelligente Systeme schaffen, die sowohl ressourceneffizient als auch äußerst leistungsfähig sind.

Das Model Context Protocol bietet eine standardisierte Methode, um KI-Anwendungen mit externen Systemen zu verbinden, ähnlich wie USB-C einen universellen Verbindungsstandard für elektronische Geräte bietet. Diese Standardisierung ermöglicht:

- **Nahtlose Integration**: Verbindung von KI-Modellen mit verschiedenen Datenquellen und Tools
- **Wachstum des Ökosystems**: Einmal entwickeln, mehrfach nutzen
- **Erweiterte Fähigkeiten**: Ergänzung von SLMs durch externe Funktionalitäten
- **Echtzeit-Updates**: Unterstützung dynamischer, reaktionsfähiger KI-Anwendungen

Wichtige Erkenntnisse:
- MCP ist ein offener Standard, der KI-Anwendungen und externe Systeme verbindet.
- Das Protokoll unterstützt Tools, Ressourcen und Prompts als Kernprimitive.
- Echtzeit-Benachrichtigungen ermöglichen dynamische, reaktionsfähige Anwendungen.
- Ordnungsgemäßes Lebenszyklusmanagement und Fehlerbehandlung sind für den Produktionseinsatz unerlässlich.
- Das Ökosystem bietet umfassende SDKs und Entwicklungstools.

## Referenzen und weiterführende Literatur

### Offizielle MCP-Dokumentation

- **[Model Context Protocol Offizielle Seite](https://modelcontextprotocol.io/)** - Vollständige Dokumentation und Spezifikationen
- **[MCP Erste Schritte Leitfaden](https://modelcontextprotocol.io/docs/getting-started/intro)** - Einführung und Kernkonzepte
- **[MCP Architekturübersicht](https://modelcontextprotocol.io/docs/learn/architecture)** - Detaillierte technische Architektur
- **[MCP-Spezifikation](https://modelcontextprotocol.io/specification/latest)** - Offizielle Protokollspezifikation
- **[MCP SDKs Dokumentation](https://modelcontextprotocol.io/docs/sdk)** - Sprachspezifische SDK-Leitfäden

### Entwicklungsressourcen

- **[MCP für Anfänger](https://aka.ms/mcp-for-beginners)** - Umfassender Anfängerleitfaden zum Model Context Protocol
- **[MCP GitHub Organisation](https://github.com/modelcontextprotocol)** - Offizielle Repositories und Beispiele
- **[MCP Server Repository](https://github.com/modelcontextprotocol/servers)** - Referenzserver-Implementierungen
- **[MCP Inspector](https://github.com/modelcontextprotocol/inspector)** - Entwicklungs- und Debugging-Tool
- **[MCP Server erstellen Leitfaden](https://modelcontextprotocol.io/docs/develop/build-server)** - Tutorial zur Serverentwicklung
- **[MCP Clients erstellen Leitfaden](https://modelcontextprotocol.io/docs/develop/build-client)** - Tutorial zur Cliententwicklung

### Kleine Sprachmodelle und Edge-KI

- **[Microsoft Phi Modelle](https://aka.ms/phicookbook)** - Phi-Modellfamilie 
- **[Foundry Local Dokumentation](https://github.com/microsoft/Foundry-Local)** - Microsofts Edge-KI-Laufzeit
- **[Ollama Dokumentation](https://ollama.ai/docs)** - Plattform für lokale LLM-Bereitstellung  
- **[vLLM Dokumentation](https://docs.vllm.ai/)** - Hochleistungs-LLM-Servicelösung  

### Technische Standards und Protokolle  

- **[JSON-RPC 2.0 Spezifikation](https://www.jsonrpc.org/)** - Zugrunde liegendes RPC-Protokoll, das von MCP verwendet wird  
- **[JSON Schema](https://json-schema.org/)** - Standard für Schema-Definitionen in MCP-Tools  
- **[OpenAPI Spezifikation](https://swagger.io/specification/)** - Standard für API-Dokumentation  
- **[Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - Webstandard für Echtzeit-Updates  

### Entwicklung von KI-Agenten  

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - Produktionsreifes Framework für die Entwicklung von Agenten  
- **[LangChain Dokumentation](https://docs.langchain.com/)** - Framework für die Integration von Agenten und Tools  
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - Microsofts SDK für KI-Orchestrierung  

### Branchenberichte und Forschung  

- **[Anthropics Ankündigung des Model Context Protocol](https://www.anthropic.com/news/model-context-protocol)** - Ursprüngliche Einführung des MCP  
- **[Umfrage zu kleinen Sprachmodellen](https://arxiv.org/abs/2410.20011)** - Akademische Untersuchung zu SLM-Forschung  
- **[Analyse des Edge-AI-Marktes](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - Branchentrends und Prognosen  
- **[Best Practices für die Entwicklung von KI-Agenten](https://arxiv.org/abs/2309.02427)** - Forschung zu Agentenarchitekturen  

Dieser Abschnitt bietet die Grundlage für die Entwicklung eigener MCP-Anwendungen, die von SLM unterstützt werden, und eröffnet Möglichkeiten für Automatisierung, Datenverarbeitung und intelligente Systemintegration.  

## ➡️ Was kommt als Nächstes  

- [Modul 7. Edge-AI-Beispiele](../Module07/README.md)  

---

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-Übersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) übersetzt. Obwohl wir uns um Genauigkeit bemühen, beachten Sie bitte, dass automatisierte Übersetzungen Fehler oder Ungenauigkeiten enthalten können. Das Originaldokument in seiner ursprünglichen Sprache sollte als maßgebliche Quelle betrachtet werden. Für kritische Informationen wird eine professionelle menschliche Übersetzung empfohlen. Wir übernehmen keine Haftung für Missverständnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser Übersetzung ergeben.