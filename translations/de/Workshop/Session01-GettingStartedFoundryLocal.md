# Sitzung 1: Einf√ºhrung in Foundry Local

## Zusammenfassung

Lernen Sie, wie Sie Ihre ersten KI-Modelle mit Microsoft Foundry Local installieren, konfigurieren und ausf√ºhren. Diese praktische Sitzung bietet eine schrittweise Einf√ºhrung in die lokale Inferenz, von der Installation bis hin zur Erstellung Ihrer ersten Chat-Anwendung mit Modellen wie Phi-4, Qwen und DeepSeek.

## Lernziele

Am Ende dieser Sitzung werden Sie:

- **Installieren und Konfigurieren**: Foundry Local korrekt einrichten und die Installation √ºberpr√ºfen
- **CLI-Operationen meistern**: Foundry Local CLI f√ºr Modellverwaltung und -bereitstellung nutzen
- **Ihr erstes Modell ausf√ºhren**: Ein lokales KI-Modell erfolgreich bereitstellen und damit interagieren
- **Eine Chat-App erstellen**: Eine einfache Chat-Anwendung mit dem Foundry Local Python SDK entwickeln
- **Lokale KI verstehen**: Die Grundlagen der lokalen Inferenz und Modellverwaltung begreifen

## Voraussetzungen

### Systemanforderungen

- **Windows**: Windows 11 (22H2 oder sp√§ter) ODER **macOS**: macOS 11+ (eingeschr√§nkte Unterst√ºtzung)
- **RAM**: Mindestens 8GB, empfohlen 16GB+
- **Speicherplatz**: Mindestens 10GB freier Speicherplatz f√ºr Modelle
- **Python**: Version 3.10 oder sp√§ter installiert
- **Administratorzugriff**: Administratorrechte f√ºr die Installation

### Entwicklungsumgebung

- Visual Studio Code mit Python-Erweiterung (empfohlen)
- Zugriff auf die Kommandozeile (PowerShell unter Windows, Terminal unter macOS)
- Git zum Klonen von Repositories (optional)

## Ablauf des Workshops (30 Minuten)

### Schritt 1: Foundry Local installieren (5 Minuten)

#### Installation unter Windows

Installieren Sie Foundry Local mit dem Windows-Paketmanager:

```powershell
# Install via winget (recommended)
winget install Microsoft.FoundryLocal
```

Alternative: Direkt herunterladen von [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)

#### Installation unter macOS (eingeschr√§nkte Unterst√ºtzung)

> [!NOTE] 
> Die Unterst√ºtzung f√ºr macOS befindet sich derzeit in der Vorschau. √úberpr√ºfen Sie die offizielle Dokumentation f√ºr die neuesten Informationen.

Falls verf√ºgbar, installieren Sie Foundry Local mit Homebrew:

```bash
# If Homebrew formula is available
brew update
brew install foundry-local

# Or manual download (check official docs for latest)
curl -L -o foundry-local.tar.gz "https://download.microsoft.com/foundry-local/latest/macos/foundry-local.tar.gz"
tar -xzf foundry-local.tar.gz
sudo ./install.sh
```

**Alternative f√ºr macOS-Nutzer:**
- Verwenden Sie eine Windows 11 VM (Parallels/UTM) und folgen Sie den Windows-Schritten
- F√ºhren Sie Foundry Local √ºber einen Container aus, falls verf√ºgbar, und konfigurieren Sie `FOUNDRY_LOCAL_ENDPOINT`

### Schritt 2: Installation √ºberpr√ºfen (3 Minuten)

Starten Sie nach der Installation Ihr Terminal neu und √ºberpr√ºfen Sie, ob Foundry Local funktioniert:

```powershell
# Check if Foundry Local is installed correctly
foundry --version

# View available commands
foundry --help
```

Die erwartete Ausgabe sollte Versionsinformationen und verf√ºgbare Befehle anzeigen.

### Schritt 3: Python-Umgebung einrichten (5 Minuten)

Erstellen Sie eine dedizierte Python-Umgebung f√ºr diesen Workshop:

**Windows:**
```powershell
# Create virtual environment
py -m venv .venv

# Activate environment
.\.venv\Scripts\Activate.ps1

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

**macOS/Linux:**
```bash
# Create virtual environment
python3 -m venv .venv

# Activate environment
source .venv/bin/activate

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

### Schritt 4: Ihr erstes Modell ausf√ºhren (7 Minuten)

Jetzt f√ºhren wir unser erstes KI-Modell lokal aus!

#### Starten Sie mit Phi-4 Mini (empfohlenes erstes Modell)

```powershell
# Download and start phi-4-mini (lightweight, fast)
foundry model run phi-4-mini

# Test the model with a simple prompt
foundry model run phi-4-mini --prompt "Hello, introduce yourself in one sentence"
```

> [!TIP]
> Dieser Befehl l√§dt das Modell (beim ersten Mal) herunter und startet den Foundry Local-Dienst automatisch.

#### √úberpr√ºfen, was l√§uft

```powershell
# List available models (shows downloaded models)
foundry model list

# Check service status
foundry service status

# See what models are cached locally
foundry cache list
```

#### Verschiedene Modelle ausprobieren

Sobald phi-4-mini funktioniert, experimentieren Sie mit anderen Modellen:

```powershell
# Larger model with better capabilities
foundry model run gpt-oss-20b --prompt "Explain edge AI in simple terms"

# Fast, efficient model
foundry model run qwen2.5-0.5b --prompt "What are the benefits of local AI inference?"
```

### Schritt 5: Ihre erste Chat-Anwendung erstellen (10 Minuten)

Jetzt erstellen wir eine Python-Anwendung, die die Modelle nutzt, die wir gerade gestartet haben.

#### Das Chat-Skript erstellen

Erstellen Sie eine neue Datei namens `my_first_chat.py` (oder verwenden Sie das bereitgestellte Beispiel):

```python
#!/usr/bin/env python3
"""
My First Foundry Local Chat Application
Using FoundryLocalManager for automatic service management
"""

import os
from foundry_local import FoundryLocalManager
from openai import OpenAI

def main():
    # Get model alias from environment or use default
    alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-4-mini")
    
    try:
        # Initialize Foundry Local Manager (auto-starts service, downloads model)
        manager = FoundryLocalManager(alias)
        
        # Create OpenAI client pointing to local endpoint
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-needed"
        )
        
        # Get the actual model ID for this alias
        model_id = manager.get_model_info(alias).id
        
        print("ü§ñ Welcome to your first local AI chat!")
        print(f"ÔøΩ Using model: {alias} -> {model_id}")
        print(f"üåê Endpoint: {manager.endpoint}")
        print("ÔøΩüí° Type 'quit' to exit\n")
        
    except Exception as e:
        print(f"‚ùå Failed to initialize Foundry Local: {e}")
        print("üí° Make sure Foundry Local is installed: foundry --version")
        return
    
    while True:
        # Get user input
        user_message = input("You: ").strip()
        
        if user_message.lower() in ['quit', 'exit', 'bye']:
            print("üëã Goodbye!")
            break
            
        if not user_message:
            continue
            
        try:
            # Send message to local AI model
            response = client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": "You are a helpful AI assistant running locally."},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=200,
                temperature=0.7
            )
            
            # Display the response
            ai_response = response.choices[0].message.content
            print(f"ü§ñ AI: {ai_response}\n")
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            print("üí° Check service status: foundry service status\n")

if __name__ == "__main__":
    main()
```

> [!TIP]
> **Verwandte Beispiele**: F√ºr fortgeschrittene Nutzung siehe:
>
> - **Python-Beispiel**: `Workshop/samples/session01/chat_bootstrap.py` - Enth√§lt Streaming-Antworten und Fehlerbehandlung
> - **Jupyter Notebook**: `Workshop/notebooks/session01_chat_bootstrap.ipynb` - Interaktive Version mit detaillierten Erkl√§rungen

#### Ihre Chat-Anwendung testen

```powershell
# No need to manually start models - FoundryLocalManager handles this!
# Just run your chat application
python my_first_chat.py
```

Alternative: Verwenden Sie direkt die bereitgestellten Beispiele

```powershell
# Try the complete sample with streaming support
cd Workshop/samples
python -m session01.chat_bootstrap "Your question here"
```

Oder erkunden Sie das interaktive Notebook  
√ñffnen Sie Workshop/notebooks/session01_chat_bootstrap.ipynb in VS Code

Probieren Sie diese Beispielgespr√§che aus:

- "Was ist Microsoft Foundry Local?"
- "Nennen Sie 3 Vorteile der Ausf√ºhrung von KI-Modellen lokal"
- "Helfen Sie mir, Edge-KI zu verstehen"

## Was Sie erreicht haben

Herzlichen Gl√ºckwunsch! Sie haben erfolgreich:

1. ‚úÖ **Foundry Local installiert** und √ºberpr√ºft, dass es funktioniert
2. ‚úÖ **Ihr erstes KI-Modell gestartet** (phi-4-mini) lokal
3. ‚úÖ **Verschiedene Modelle getestet** √ºber die Kommandozeile
4. ‚úÖ **Eine Chat-Anwendung erstellt**, die mit Ihrer lokalen KI verbunden ist
5. ‚úÖ **Lokale KI-Inferenz erlebt** ohne Cloud-Abh√§ngigkeiten

## Verst√§ndnis der Vorg√§nge

### Lokale KI-Inferenz

- Ihre KI-Modelle laufen vollst√§ndig auf Ihrem Computer
- Es werden keine Daten in die Cloud gesendet
- Antworten werden lokal mit Ihrer CPU/GPU generiert
- Datenschutz und Sicherheit bleiben erhalten

### Modellverwaltung

- `foundry model run` l√§dt und startet Modelle
- **FoundryLocalManager SDK** startet automatisch Dienste und l√§dt Modelle
- Modelle werden lokal zwischengespeichert f√ºr zuk√ºnftige Nutzung
- Mehrere Modelle k√∂nnen heruntergeladen werden, aber typischerweise l√§uft nur eines gleichzeitig
- Der Dienst verwaltet den Lebenszyklus der Modelle automatisch

### SDK- vs CLI-Ans√§tze

- **CLI-Ansatz**: Manuelle Modellverwaltung mit `foundry model run <model>`
- **SDK-Ansatz**: Automatische Dienst- und Modellverwaltung mit `FoundryLocalManager(alias)`
- **Empfehlung**: SDK f√ºr Anwendungen nutzen, CLI f√ºr Tests und Erkundung

## Referenz f√ºr h√§ufige Befehle

### Wichtige CLI-Befehle

```powershell
# Installation & Setup
foundry --version              # Check installation
foundry --help                 # View all commands

# Model Management
foundry model list             # List available models
foundry model run <model>      # Download and start a model
foundry model run <model> --prompt "text"  # One-shot prompt
foundry cache list             # Show downloaded models

# Service Management
foundry service status         # Check if service is running
foundry service start          # Start the service manually
foundry service stop           # Stop the service
```

### Modell-Empfehlungen

- **phi-4-mini**: Bestes Einstiegsmodell - schnell, leichtgewichtig, gute Qualit√§t
- **qwen2.5-0.5b**: Schnellste Inferenz, minimaler Speicherverbrauch
- **gpt-oss-20b**: H√∂here Qualit√§t der Antworten, ben√∂tigt mehr Ressourcen
- **deepseek-coder-1.3b**: Optimiert f√ºr Programmier- und Codeaufgaben

## Fehlerbehebung

### "Foundry-Befehl nicht gefunden"

**L√∂sung:**

```powershell
# Restart your terminal after installation
# Or manually add to PATH (Windows)
$env:PATH += ";C:\Program Files\Microsoft\FoundryLocal"
```

### "Modell konnte nicht geladen werden"

**L√∂sung:**

```powershell
# Check available system memory
foundry service status

# Try a smaller model first
foundry model run phi-4-mini

# Check disk space for model downloads
# Models are stored in: %USERPROFILE%\.foundry\models (Windows)
```

### "Verbindung zu localhost verweigert"

**L√∂sung:**

```powershell
# Check if service is running
foundry service status

# Start service if needed
foundry service start

# Verify the port (default is 5273)
# Check for port conflicts with: netstat -an | findstr 5273
```

## N√§chste Schritte

### Sofortige n√§chste Aktionen

1. **Experimentieren** Sie mit verschiedenen Modellen und Eingaben
2. **Modifizieren** Sie Ihre Chat-Anwendung, um verschiedene Modelle auszuprobieren
3. **Erstellen** Sie eigene Eingaben und testen Sie die Antworten
4. **Erkunden** Sie Sitzung 2: Aufbau von RAG-Anwendungen

### Fortgeschrittener Lernpfad

1. **Sitzung 2**: KI-L√∂sungen mit RAG (Retrieval-Augmented Generation) entwickeln
2. **Sitzung 3**: Vergleich verschiedener Open-Source-Modelle
3. **Sitzung 4**: Arbeiten mit modernsten Modellen
4. **Sitzung 5**: Aufbau von Multi-Agenten-KI-Systemen

## Umgebungsvariablen (optional)

F√ºr fortgeschrittene Nutzung k√∂nnen Sie diese Umgebungsvariablen setzen:

| Variable | Zweck | Beispiel |
|----------|---------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Standardmodell | `phi-4-mini` |
| `FOUNDRY_LOCAL_ENDPOINT` | Endpunkt-URL √ºberschreiben | `http://localhost:5273/v1` |

Erstellen Sie eine `.env`-Datei in Ihrem Projektverzeichnis:
```
FOUNDRY_LOCAL_ALIAS=phi-4-mini
FOUNDRY_LOCAL_ENDPOINT=auto
```

## Zus√§tzliche Ressourcen

### Dokumentation

- [Foundry Local Python SDK Referenz](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk?pivots=programming-language-python)
- [Foundry Local Installationsanleitung](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)
- [Modellkatalog](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/models)

### Beispielcode

- **Session01 Python-Beispiel**: `Workshop/samples/session01/chat_bootstrap.py` - Vollst√§ndige Chat-App mit Streaming
- **Session01 Notebook**: `Workshop/notebooks/session01_chat_bootstrap.ipynb` - Interaktives Tutorial  
- [Modul08 Beispiel 01](../Module08/samples/01/README.md) - REST Chat Schnellstart
- [Modul08 Beispiel 02](../Module08/samples/02/README.md) - OpenAI SDK Integration
- [Modul08 Beispiel 03](../Module08/samples/03/README.md) - Modellentdeckung & Benchmarking

### Community

- [Foundry Local GitHub Diskussionen](https://github.com/microsoft/Foundry-Local/discussions)
- [Azure AI Community](https://techcommunity.microsoft.com/category/artificialintelligence)

---

**Dauer der Sitzung**: 30 Minuten Praxis + 15 Minuten Q&A  
**Schwierigkeitsgrad**: Anf√§nger  
**Voraussetzungen**: Windows 11/macOS 11+, Python 3.10+, Administratorzugriff

## Beispiel-Szenario des Workshops

### Kontext aus der Praxis

**Szenario**: Ein IT-Team eines Unternehmens muss die lokale KI-Inferenz evaluieren, um sensible Mitarbeiterfeedbacks zu verarbeiten, ohne Daten an externe Dienste zu senden.

**Ihr Ziel**: Zeigen Sie, dass lokale KI-Modelle qualitativ hochwertige Antworten mit einer Latenzzeit von unter einer Sekunde liefern k√∂nnen, w√§hrend die Daten vollst√§ndig privat bleiben.

### Testeingaben

Verwenden Sie diese Eingaben, um Ihre Einrichtung zu validieren:

```json
[
    "List two benefits of local inference.",
    "Summarize why keeping data on device improves privacy.",
    "Give one trade-off when choosing a small model over a large model."
]
```

### Erfolgskriterien

- ‚úÖ Alle Eingaben liefern Antworten in unter 2 Sekunden
- ‚úÖ Es werden keine Daten von Ihrem lokalen Rechner gesendet
- ‚úÖ Antworten sind relevant und hilfreich
- ‚úÖ Ihre Chat-Anwendung funktioniert reibungslos

Diese Validierung stellt sicher, dass Ihre Foundry Local-Einrichtung bereit ist f√ºr die fortgeschrittenen Workshops in den Sitzungen 2-6.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->