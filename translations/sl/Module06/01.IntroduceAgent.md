# AI agenti in majhni jezikovni modeli: Celovit vodnik

## Uvod

V tem priroÄniku bomo raziskali AI agente in majhne jezikovne modele (SLM) ter njihove napredne strategije implementacije za okolja robnega raÄunalniÅ¡tva. Pokrili bomo temeljne koncepte agentne umetne inteligence, tehnike optimizacije SLM, praktiÄne strategije za implementacijo na napravah z omejenimi viri ter Microsoft Agent Framework za gradnjo produkcijsko pripravljenih agentnih sistemov.

Pokrajina umetne inteligence doÅ¾ivlja paradigmatiÄni premik v letu 2025. Medtem ko je bilo leto 2023 leto chatbotov, leto 2024 pa razcvet kopilotov, leto 2025 pripada AI agentom â€” inteligentnim sistemom, ki razmiÅ¡ljajo, naÄrtujejo, uporabljajo orodja in izvajajo naloge z minimalnim ÄloveÅ¡kim vnosom, vse bolj podprti z uÄinkovitimi majhnimi jezikovnimi modeli. Microsoft Agent Framework se pojavlja kot vodilna reÅ¡itev za gradnjo teh inteligentnih sistemov z zmogljivostmi za delovanje brez povezave na robu.

## Cilji uÄenja

Do konca tega priroÄnika boste sposobni:

- ğŸ¤– Razumeti temeljne koncepte AI agentov in agentnih sistemov
- ğŸ”¬ Prepoznati prednosti majhnih jezikovnih modelov v primerjavi z velikimi jezikovnimi modeli pri agentnih aplikacijah
- ğŸš€ NauÄiti se naprednih strategij implementacije SLM za okolja robnega raÄunalniÅ¡tva
- ğŸ“± Implementirati praktiÄne agente, podprte z SLM, za resniÄne aplikacije
- ğŸ—ï¸ Zgraditi produkcijsko pripravljene agente z uporabo Microsoft Agent Framework
- ğŸŒ Implementirati agente za delovanje brez povezave z lokalno integracijo LLM in SLM
- ğŸ”§ Integrirati Microsoft Agent Framework z Foundry Local za robno implementacijo

## Razumevanje AI agentov: Osnove in klasifikacije

### Definicija in temeljni koncepti

Umetni inteligentni (AI) agent se nanaÅ¡a na sistem ali program, ki je sposoben samostojno izvajati naloge v imenu uporabnika ali drugega sistema z oblikovanjem svojega delovnega toka in uporabo razpoloÅ¾ljivih orodij. Za razliko od tradicionalne AI, ki zgolj odgovarja na vaÅ¡a vpraÅ¡anja, agent lahko deluje neodvisno za dosego ciljev.

### Okvir klasifikacije agentov

Razumevanje meja agentov pomaga pri izbiri ustreznih vrst agentov za razliÄne scenarije raÄunalniÅ¡tva:

- **ğŸ”¬ Preprosti refleksni agenti**: Sistemi, ki temeljijo na pravilih in se odzivajo na neposredne zaznave (termostati, osnovna avtomatizacija)
- **ğŸ“± Modelno osnovani agenti**: Sistemi, ki vzdrÅ¾ujejo notranje stanje in spomin (robotski sesalniki, navigacijski sistemi)
- **âš–ï¸ Ciljno usmerjeni agenti**: Sistemi, ki naÄrtujejo in izvajajo zaporedja za dosego ciljev (naÄrtovalci poti, razporejevalniki nalog)
- **ğŸ§  UÄeÄi se agenti**: Prilagodljivi sistemi, ki izboljÅ¡ujejo zmogljivost skozi Äas (sistemi priporoÄil, personalizirani asistenti)

### KljuÄne prednosti AI agentov

AI agenti ponujajo veÄ temeljnih prednosti, zaradi katerih so idealni za aplikacije robnega raÄunalniÅ¡tva:

**Operativna avtonomija**: Agenti omogoÄajo neodvisno izvajanje nalog brez stalnega ÄloveÅ¡kega nadzora, kar jih naredi idealne za aplikacije v realnem Äasu. Zahtevajo minimalen nadzor, hkrati pa ohranjajo prilagodljivo vedenje, kar omogoÄa implementacijo na napravah z omejenimi viri in zmanjÅ¡ano operativno obremenitev.

**Prilagodljivost implementacije**: Ti sistemi omogoÄajo zmogljivosti AI na napravi brez zahtev po internetni povezavi, izboljÅ¡ujejo zasebnost in varnost z lokalno obdelavo, jih je mogoÄe prilagoditi za aplikacije specifiÄne za doloÄeno podroÄje in so primerni za razliÄna okolja robnega raÄunalniÅ¡tva.

**UÄinkovitost stroÅ¡kov**: Agentni sistemi ponujajo stroÅ¡kovno uÄinkovito implementacijo v primerjavi z reÅ¡itvami, ki temeljijo na oblaku, z zmanjÅ¡animi operativnimi stroÅ¡ki in niÅ¾jimi zahtevami po pasovni Å¡irini za aplikacije na robu.

## Napredne strategije za majhne jezikovne modele

### Osnove SLM (Small Language Model)

Majhen jezikovni model (SLM) je jezikovni model, ki se lahko prilega na obiÄajno potroÅ¡niÅ¡ko elektronsko napravo in izvaja sklepanje z dovolj nizko zakasnitvijo, da je praktiÄen za obravnavo agentnih zahtev enega uporabnika. V praktiÄnem smislu so SLM obiÄajno modeli z manj kot 10 milijardami parametrov.

**ZnaÄilnosti odkrivanja formatov**: SLM ponujajo napredno podporo za razliÄne ravni kvantizacije, zdruÅ¾ljivost med platformami, optimizacijo zmogljivosti v realnem Äasu in zmogljivosti za implementacijo na robu. Uporabniki lahko dostopajo do izboljÅ¡ane zasebnosti prek lokalne obdelave in podpore za WebGPU za implementacijo v brskalniku.

**Zbirke ravni kvantizacije**: Priljubljeni formati SLM vkljuÄujejo Q4_K_M za uravnoteÅ¾eno stiskanje v mobilnih aplikacijah, serijo Q5_K_S za implementacijo na robu, osredotoÄeno na kakovost, Q8_0 za skoraj originalno natanÄnost na zmogljivih napravah na robu ter eksperimentalne formate, kot je Q2_K za scenarije z ultra nizkimi viri.

### GGUF (General GGML Universal Format) za implementacijo SLM

GGUF sluÅ¾i kot primarni format za implementacijo kvantiziranih SLM na CPU in napravah na robu, posebej optimiziran za agentne aplikacije:

**Funkcije optimizirane za agente**: Format zagotavlja celovite vire za pretvorbo in implementacijo SLM z izboljÅ¡ano podporo za klicanje orodij, generiranje strukturiranih izhodov in pogovore z veÄ obrati. ZdruÅ¾ljivost med platformami zagotavlja dosledno vedenje agentov na razliÄnih napravah na robu.

**Optimizacija zmogljivosti**: GGUF omogoÄa uÄinkovito uporabo pomnilnika za delovne tokove agentov, podpira dinamiÄno nalaganje modelov za sisteme z veÄ agenti in zagotavlja optimizirano sklepanje za interakcije agentov v realnem Äasu.

### Okviri SLM, optimizirani za rob

#### Optimizacija Llama.cpp za agente

Llama.cpp ponuja najsodobnejÅ¡e tehnike kvantizacije, posebej optimizirane za implementacijo agentnih SLM:

**Kvantizacija, specifiÄna za agente**: Okvir podpira Q4_0 (optimalno za mobilno implementacijo agentov s 75% zmanjÅ¡anjem velikosti), Q5_1 (uravnoteÅ¾ena kakovost-stiskanje za agente na robu) in Q8_0 (skoraj originalna kakovost za produkcijske agentne sisteme). Napredni formati omogoÄajo ultra stisnjene agente za ekstremne scenarije na robu.

**Prednosti implementacije**: Sklepanje, optimizirano za CPU, s pospeÅ¡evanjem SIMD zagotavlja uÄinkovito izvajanje agentov v pomnilniku. ZdruÅ¾ljivost med platformami na arhitekturah x86, ARM in Apple Silicon omogoÄa univerzalne zmogljivosti implementacije agentov.

#### Apple MLX Framework za SLM agente

Apple MLX zagotavlja nativno optimizacijo, posebej zasnovano za agente, podprte z SLM, na napravah Apple Silicon:

**Optimizacija agentov na Apple Silicon**: Okvir uporablja arhitekturo enotnega pomnilnika z integracijo Metal Performance Shaders, samodejno meÅ¡ano natanÄnost za sklepanje agentov in optimizirano pasovno Å¡irino pomnilnika za sisteme z veÄ agenti. Agenti SLM kaÅ¾ejo izjemno zmogljivost na Äipih serije M.

**Razvojne funkcije**: Podpora za Python in Swift API z optimizacijami, specifiÄnimi za agente, samodejna diferenciacija za uÄenje agentov in brezhibna integracija z razvojnimi orodji Apple zagotavljajo celovita razvojna okolja za agente.

#### ONNX Runtime za agente SLM na veÄ platformah

ONNX Runtime zagotavlja univerzalni sklepni stroj, ki omogoÄa agentom SLM dosledno delovanje na razliÄnih strojnih platformah in operacijskih sistemih:

**Univerzalna implementacija**: ONNX Runtime zagotavlja dosledno vedenje agentov SLM na platformah Windows, Linux, macOS, iOS in Android. Ta zdruÅ¾ljivost med platformami omogoÄa razvijalcem, da napiÅ¡ejo enkrat in implementirajo povsod, kar bistveno zmanjÅ¡a stroÅ¡ke razvoja in vzdrÅ¾evanja za aplikacije na veÄ platformah.

**MoÅ¾nosti strojnega pospeÅ¡evanja**: Okvir zagotavlja optimizirane izvajalne ponudnike za razliÄne strojne konfiguracije, vkljuÄno s CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) in specializiranimi pospeÅ¡evalniki (Intel VPU, Qualcomm NPU). Agenti SLM lahko samodejno izkoristijo najboljÅ¡o razpoloÅ¾ljivo strojno opremo brez sprememb kode.

**Funkcije, pripravljene za produkcijo**: ONNX Runtime ponuja funkcije na ravni podjetja, ki so bistvene za implementacijo agentov v produkciji, vkljuÄno z optimizacijo grafov za hitrejÅ¡e sklepanje, upravljanje pomnilnika za okolja z omejenimi viri in celovita orodja za profiliranje za analizo zmogljivosti. Okvir podpira tako Python kot C++ API za prilagodljivo integracijo.

## SLM proti LLM v agentnih sistemih: Napredna primerjava

### Prednosti SLM v aplikacijah za agente

**Operativna uÄinkovitost**: SLM zagotavljajo 10-30Ã— zmanjÅ¡anje stroÅ¡kov v primerjavi z LLM za naloge agentov, kar omogoÄa odzive agentov v realnem Äasu na velikem obsegu. Ponujajo hitrejÅ¡e Äase sklepanja zaradi zmanjÅ¡ane raÄunske kompleksnosti, kar jih naredi idealne za interaktivne aplikacije agentov.

**Zmogljivosti implementacije na robu**: SLM omogoÄajo izvajanje agentov na napravi brez odvisnosti od interneta, izboljÅ¡ano zasebnost prek lokalne obdelave in prilagoditev za aplikacije, specifiÄne za doloÄeno podroÄje, primerne za razliÄna okolja robnega raÄunalniÅ¡tva.

**Optimizacija, specifiÄna za agente**: SLM so odliÄni pri klicanju orodij, generiranju strukturiranih izhodov in rutinskih delovnih tokov odloÄanja, ki predstavljajo 70-80% tipiÄnih nalog agentov.

### Kdaj uporabiti SLM proti LLM v agentnih sistemih

**Idealno za SLM**:
- **PonavljajoÄe se naloge agentov**: Vnos podatkov, izpolnjevanje obrazcev, rutinski klici API
- **Integracija orodij**: Poizvedbe v podatkovnih bazah, operacije z datotekami, interakcije s sistemom
- **Strukturirani delovni tokovi**: Sledenje vnaprej doloÄenim procesom agentov
- **Agenti, specifiÄni za podroÄje**: PomoÄ strankam, razporejanje, osnovna analiza
- **Lokalna obdelava**: Operacije agentov, obÄutljive na zasebnost

**Bolje za LLM**:
- **Kompleksno razmiÅ¡ljanje**: ReÅ¡evanje novih problemov, strateÅ¡ko naÄrtovanje
- **Odprti pogovori**: SploÅ¡ni klepet, ustvarjalne razprave
- **Naloge z obseÅ¾nim znanjem**: Raziskave, ki zahtevajo obseÅ¾no sploÅ¡no znanje
- **Nove situacije**: Obvladovanje popolnoma novih scenarijev agentov

### Hibridna arhitektura agentov

Optimalen pristop zdruÅ¾uje SLM in LLM v heterogenih agentnih sistemih:

**Pametna orkestracija agentov**:
1. **SLM kot primarni**: Obdelava 70-80% rutinskih nalog agentov lokalno
2. **LLM po potrebi**: Usmerjanje kompleksnih poizvedb v veÄje modele v oblaku
3. **Specializirani SLM**: RazliÄni majhni modeli za razliÄna podroÄja agentov
4. **Optimizacija stroÅ¡kov**: ZmanjÅ¡anje dragih klicev LLM z inteligentnim usmerjanjem

## Strategije za produkcijsko implementacijo agentov SLM

### Foundry Local: Robno AI okolje na ravni podjetja

Foundry Local (https://github.com/microsoft/foundry-local) sluÅ¾i kot vodilna reÅ¡itev Microsofta za implementacijo majhnih jezikovnih modelov v produkcijskih robnih okoljih. Ponuja popolno okolje za izvajanje, posebej zasnovano za agente, podprte z SLM, z znaÄilnostmi na ravni podjetja in brezhibnimi integracijskimi zmogljivostmi.

**Osnovna arhitektura in funkcije**:
- **ZdruÅ¾ljiv API z OpenAI**: Popolna zdruÅ¾ljivost z OpenAI SDK in integracijami Agent Framework
- **Samodejna optimizacija strojne opreme**: Inteligentna izbira razliÄic modelov glede na razpoloÅ¾ljivo strojno opremo (CUDA GPU, Qualcomm NPU, CPU)
- **Upravljanje modelov**: Samodejno nalaganje, predpomnjenje in upravljanje Å¾ivljenjskega cikla modelov SLM
- **Odkritje storitev**: Zaznavanje storitev brez konfiguracije za okvire agentov
- **Optimizacija virov**: Inteligentno upravljanje pomnilnika in energetska uÄinkovitost za implementacijo na robu

#### Namestitev in nastavitev

**Namestitev na veÄ platformah**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Hiter zaÄetek za razvoj agentov**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Integracija z Agent Framework

**Integracija Foundry Local SDK**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Samodejna izbira modela in optimizacija strojne opreme**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Vzorci produkcijske implementacije

**Produkcijska nastavitev enega agenta**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Orkestracija veÄ agentov v produkciji**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### Funkcije na ravni podjetja in spremljanje

**Spremljanje zdravja in opazljivost**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Upravljanje virov in samodejno skaliranje**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### Napredna konfiguracija in optimizacija

**Prilagoditev konfiguracije modela**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Kontrolni seznam za produkcijsko implementacijo**:

âœ… **Konfiguracija storitev**:
- Konfigurirajte ustrezne vzdevke modelov za primere uporabe
- Nastavite omejitve virov in prage spremljanja
- OmogoÄite preverjanje zdravja in zbiranje metrik
- Konfigurirajte samodejni ponovni zagon in preklop

âœ… **Varnostna nastavitev**:
- OmogoÄite lokalni dostop do API (brez zunanje izpostavljenosti)
- Konfigurirajte ustrezno upravljanje kljuÄev API
- Nastavite dnevnik revizij za interakcije agentov
- Implementirajte omejevanje hitrosti za produkcijsko uporabo

âœ… **Optimizacija zmogljivosti**:
- Preizkusite zmogljivost modela pod priÄakovano obremenitvijo
- Konfigurirajte ustrezne ravni kvantizacije
- Nastavite strategije predpomnjenja in ogrevanja modela
- Spremljajte vzorce uporabe pomnilnika in CPU

âœ… **Testiranje integracije**:
- Preizkusite integracijo okvira agentov
- Preverite zmogljivosti delovanja brez povezave
- Preizkusite scenarije preklopa in obnovitve
- Validirajte delovne tokove agentov od zaÄetka do konca

### Ollama: Poenostavljena implementacija agentov SLM

### Ollama: Implementacija agentov SLM, osredotoÄena na skupnost

Ollama ponuja pristop, ki ga vodi skupnost, k implementaciji agentov SLM s poudarkom na enostavnosti, obseÅ¾nem ekosistemu modelov in razvijalcem prijaznih delovnih tokov. Medtem ko se Foundry Local osredotoÄa na funkcije na ravni podjetja, Ollama izstopa pri hitrem prototipiranju, dostopu do modelov skupnosti in poenostavljenih scenarijih implementacije.

**Osnovna arhitektura in funkcije**:
- **ZdruÅ¾ljiv API z OpenAI**: Popolna zdruÅ¾ljivost REST API za brezhibno integracijo okvira agentov
- **ObseÅ¾na knjiÅ¾nica modelov**: Dostop do stotin modelov, ki jih prispeva skupnost, in uradnih modelov
- **En
- Preizkusite integracijo Microsoft Agent Framework
- Preverite zmogljivosti delovanja brez povezave
- Preizkusite scenarije preklopa in obravnavo napak
- Validirajte delovne tokove agentov od zaÄetka do konca

**Primerjava s Foundry Local**:

| Funkcija | Foundry Local | Ollama |
|----------|---------------|--------|
| **Ciljni primer uporabe** | Proizvodnja v podjetjih | Razvoj in skupnost |
| **Ekosistem modelov** | Microsoft-kuriran | ObseÅ¾na skupnost |
| **Optimizacija strojne opreme** | Samodejna (CUDA/NPU/CPU) | RoÄna konfiguracija |
| **Funkcije za podjetja** | Vgrajeno spremljanje, varnost | Orodja skupnosti |
| **Kompleksnost uvajanja** | Enostavno (namestitev z winget) | Enostavno (namestitev s curl) |
| **ZdruÅ¾ljivost API** | OpenAI + razÅ¡iritve | Standard OpenAI |
| **Podpora** | Uradna Microsoftova | Skupnostno vodena |
| **NajboljÅ¡e za** | Proizvodni agenti | Prototipiranje, raziskave |

**Kdaj izbrati Ollama**:
- **Razvoj in prototipiranje**: Hitro eksperimentiranje z razliÄnimi modeli
- **Skupnostni modeli**: Dostop do najnovejÅ¡ih modelov, ki jih prispeva skupnost
- **IzobraÅ¾evalna uporaba**: UÄenje in pouÄevanje razvoja AI agentov
- **Raziskovalni projekti**: Akademske raziskave, ki zahtevajo raznolik dostop do modelov
- **Prilagojeni modeli**: Gradnja in testiranje prilagojenih modelov s fino nastavitvijo

### VLLM: Visoko zmogljivo sklepanje SLM agentov

VLLM (sklepanje zelo velikih jezikovnih modelov) zagotavlja visoko zmogljiv, pomnilniÅ¡ko uÄinkovit sklepni motor, posebej optimiziran za proizvodne SLM implementacije v velikem obsegu. Medtem ko se Foundry Local osredotoÄa na enostavnost uporabe in Ollama poudarja skupnostne modele, VLLM izstopa v scenarijih visoke zmogljivosti, ki zahtevajo najveÄji pretok in uÄinkovito uporabo virov.

**Osnovna arhitektura in funkcije**:
- **PagedAttention**: Revolucionarno upravljanje pomnilnika za uÄinkovito raÄunalniÅ¡ko obdelavo pozornosti
- **DinamiÄno zdruÅ¾evanje**: Inteligentno zdruÅ¾evanje zahtev za optimalen pretok
- **Optimizacija GPU**: Napredna podpora za CUDA jedra in paralelizem tenzorjev
- **ZdruÅ¾ljivost z OpenAI**: Popolna zdruÅ¾ljivost API za brezhibno integracijo
- **Spekulativno dekodiranje**: Napredne tehnike pospeÅ¡evanja sklepanja
- **Podpora za kvantizacijo**: Kvantizacija INT4, INT8 in FP16 za pomnilniÅ¡ko uÄinkovitost

#### Namestitev in nastavitev

**MoÅ¾nosti namestitve**:
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```

**Hiter zaÄetek za razvoj agentov**:
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```

#### Integracija z Agent Framework

**VLLM z Microsoft Agent Framework**:
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```

**Visoko zmogljiva nastavitev za veÄ agentov**:
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```

#### Vzorci proizvodne implementacije

**Proizvodna storitev VLLM za podjetja**:
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```

#### Funkcije za podjetja in spremljanje

**Napredno spremljanje zmogljivosti VLLM**:
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```

#### Napredna konfiguracija in optimizacija

**Predloge konfiguracije VLLM za proizvodnjo**:
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```

**Kontrolni seznam za proizvodno implementacijo VLLM**:

âœ… **Optimizacija strojne opreme**:
- Konfigurirajte paralelizem tenzorjev za veÄ-GPU nastavitve
- OmogoÄite kvantizacijo (AWQ/GPTQ) za pomnilniÅ¡ko uÄinkovitost
- Nastavite optimalno uporabo GPU pomnilnika (85-95%)
- Konfigurirajte ustrezne velikosti paketov za pretok

âœ… **IzboljÅ¡anje zmogljivosti**:
- OmogoÄite predpomnjenje predpon za ponavljajoÄe se poizvedbe
- Konfigurirajte razdeljeno predpolnitev za dolge sekvence
- Nastavite spekulativno dekodiranje za hitrejÅ¡e sklepanje
- Optimizirajte max_num_seqs glede na strojno opremo

âœ… **Funkcije za proizvodnjo**:
- Nastavite spremljanje zdravja in zbiranje metrik
- Konfigurirajte samodejni ponovni zagon in preklop
- Implementirajte Äakalne vrste zahtev in uravnoteÅ¾enje obremenitve
- Nastavite celovito beleÅ¾enje in opozarjanje

âœ… **Varnost in zanesljivost**:
- Konfigurirajte pravila poÅ¾arnega zidu in nadzor dostopa
- Nastavite omejevanje hitrosti API in avtentikacijo
- Implementirajte postopno zaustavitev in ÄiÅ¡Äenje
- Konfigurirajte varnostno kopiranje in obnovitev ob nesreÄah

âœ… **Testiranje integracije**:
- Preizkusite integracijo Microsoft Agent Framework
- Validirajte scenarije visoke zmogljivosti
- Preizkusite postopke preklopa in obnovitve
- Primerjajte zmogljivost pod obremenitvijo

**Primerjava z drugimi reÅ¡itvami**:

| Funkcija | VLLM | Foundry Local | Ollama |
|----------|------|---------------|--------|
| **Ciljni primer uporabe** | Visoko zmogljiva proizvodnja | Enostavnost uporabe v podjetjih | Razvoj in skupnost |
| **Zmogljivost** | NajveÄji pretok | UravnoteÅ¾eno | Dobro |
| **PomnilniÅ¡ka uÄinkovitost** | Optimizacija PagedAttention | Samodejna optimizacija | Standard |
| **Kompleksnost nastavitve** | Visoka (veliko parametrov) | Nizka (samodejna) | Nizka (enostavna) |
| **RazÅ¡irljivost** | OdliÄna (paralelizem tenzorjev/cevovodov) | Dobra | Omejena |
| **Kvantizacija** | Napredna (AWQ, GPTQ, FP8) | Samodejna | Standard GGUF |
| **Funkcije za podjetja** | Potrebna prilagojena implementacija | Vgrajene | Orodja skupnosti |
| **NajboljÅ¡e za** | Agenti za proizvodnjo v velikem obsegu | Proizvodnja v podjetjih | Razvoj |

**Kdaj izbrati VLLM**:
- **Zahteve po visoki zmogljivosti**: Obdelava stotin zahtev na sekundo
- **Implementacije v velikem obsegu**: VeÄ-GPU, veÄ-vozliÅ¡Äne implementacije
- **KritiÄna zmogljivost**: Odzivni Äasi pod sekundo v velikem obsegu
- **Napredna optimizacija**: Potreba po prilagojeni kvantizaciji in zdruÅ¾evanju
- **UÄinkovitost virov**: Maksimalna uporaba drage strojne opreme GPU

## ResniÄne aplikacije SLM agentov

### SLM agenti za podporo strankam
- **SLM zmogljivosti**: Iskanje raÄunov, ponastavitve gesel, preverjanje stanja naroÄil
- **StroÅ¡kovne koristi**: 10-kratno zmanjÅ¡anje stroÅ¡kov sklepanja v primerjavi z LLM agenti
- **Zmogljivost**: HitrejÅ¡i odzivni Äasi z dosledno kakovostjo za rutinske poizvedbe

### SLM agenti za poslovne procese
- **Agenti za obdelavo raÄunov**: IzvleÄek podatkov, validacija informacij, usmerjanje za odobritev
- **Agenti za upravljanje e-poÅ¡te**: Samodejno kategoriziranje, doloÄanje prioritet, priprava odgovorov
- **Agenti za naÄrtovanje**: Koordinacija sestankov, upravljanje koledarjev, poÅ¡iljanje opomnikov

### Osebni digitalni asistenti SLM
- **Agenti za upravljanje nalog**: Ustvarjanje, posodabljanje, organizacija seznamov opravil
- **Agenti za zbiranje informacij**: Raziskovanje tem, povzemanje ugotovitev lokalno
- **Agenti za komunikacijo**: Priprava e-poÅ¡te, sporoÄil, objav na druÅ¾benih omreÅ¾jih zasebno

### SLM agenti za trgovanje in finance
- **Agenti za spremljanje trga**: Sledenje cenam, prepoznavanje trendov v realnem Äasu
- **Agenti za generiranje poroÄil**: Samodejno ustvarjanje dnevnih/tedenskih povzetkov
- **Agenti za oceno tveganja**: Ocena pozicij portfelja z uporabo lokalnih podatkov

### SLM agenti za podporo v zdravstvu
- **Agenti za naÄrtovanje pacientov**: Koordinacija terminov, poÅ¡iljanje samodejnih opomnikov
- **Agenti za dokumentacijo**: Lokalno generiranje medicinskih povzetkov, poroÄil
- **Agenti za upravljanje receptov**: Sledenje ponovnim polnjenjem, preverjanje interakcij zasebno

## Microsoft Agent Framework: Razvoj agentov pripravljenih za proizvodnjo

### Pregled in arhitektura

Microsoft Agent Framework zagotavlja celovito, podjetniÅ¡ko platformo za gradnjo, uvajanje in upravljanje AI agentov, ki lahko delujejo tako v oblaku kot v lokalnih okoljih. Okvir je posebej zasnovan za brezhibno delovanje z majhnimi jezikovnimi modeli in scenariji robnega raÄunalniÅ¡tva, kar ga naredi idealnega za zasebnostno obÄutljive in z viri omejene implementacije.

**Osnovne komponente okvira**:
- **Agent Runtime**: Lahkotno okolje za izvajanje, optimizirano za robne naprave
- **Sistem za integracijo orodij**: RazÅ¡irljiva arhitektura vtiÄnikov za povezovanje zunanjih storitev in API-jev
- **Upravljanje stanja**: Trajen pomnilnik agentov in upravljanje konteksta med sejami
- **Varnostna plast**: Vgrajeni varnostni nadzori za podjetniÅ¡ko implementacijo
- **Orkestracijski motor**: Koordinacija veÄ agentov in upravljanje delovnih tokov

### KljuÄne funkcije za robne implementacije

**Arhitektura "najprej brez povezave"**: Microsoft Agent Framework je zasnovan z naÄeli "najprej brez povezave", kar omogoÄa agentom uÄinkovito delovanje brez stalne internetne povezave. To vkljuÄuje lokalno sklepanje modelov, predpomnjene baze znanja, izvajanje orodij brez povezave in postopno zmanjÅ¡evanje zmogljivosti, ko storitve v oblaku niso na voljo.

**Optimizacija virov**: Okvir zagotavlja inteligentno upravljanje virov s samodejno optimizacijo pomnilnika za SLM-je, uravnoteÅ¾enjem obremenitve CPU/GPU za robne naprave, prilagodljivo izbiro modelov glede na razpoloÅ¾ljive vire in energetsko uÄinkovite vzorce sklepanja za mobilne implementacije.

**Varnost in zasebnost**: Funkcije varnosti na ravni podjetja vkljuÄujejo lokalno obdelavo podatkov za ohranjanje zasebnosti, Å¡ifrirane komunikacijske kanale agentov, nadzor dostopa na podlagi vlog za zmogljivosti agentov in beleÅ¾enje revizij za zahteve skladnosti.

### Integracija s Foundry Local

Microsoft Agent Framework se brezhibno integrira s Foundry Local za zagotavljanje celovite reÅ¡itve AI na robu:

**Samodejno odkrivanje modelov**: Okvir samodejno zazna in se poveÅ¾e z instancami Foundry Local, odkrije razpoloÅ¾ljive SLM modele in izbere optimalne modele glede na zahteve agentov in zmogljivosti strojne opreme.

**DinamiÄno nalaganje modelov**: Agenti lahko dinamiÄno nalagajo razliÄne SLM-je za specifiÄne naloge, kar omogoÄa sisteme agentov z veÄ modeli, kjer razliÄni modeli obravnavajo razliÄne vrste zahtev, in samodejno preklapljanje med modeli glede na razpoloÅ¾ljivost in zmogljivost.

**Optimizacija zmogljivosti**: Integrirani mehanizmi predpomnjenja zmanjÅ¡ujejo Äase nalaganja modelov, zdruÅ¾evanje povezav optimizira klice API-jev na Foundry Local, inteligentno zdruÅ¾evanje pa izboljÅ¡a pretok za veÄ zahtev agentov.

### Gradnja agentov z Microsoft Agent Framework

#### Definicija in konfiguracija agentov

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```

#### Integracija orodij za robne scenarije

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```

#### Orkestracija veÄ agentov

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```

### Napredni vzorci implementacije na robu

#### HierarhiÄna arhitektura agentov

**Lokalni grozdi agentov**: Implementirajte veÄ specializiranih SLM agentov na robnih napravah, vsak optimiziran za specifiÄne naloge. Uporabite lahke modele, kot je Qwen2.5-0.5B za enostavno usmerjanje in naÄrtovanje, srednje modele, kot je Phi-4-Mini za podporo strankam in dokumentacijo, ter veÄje modele za kompleksno sklepanje, ko viri to omogoÄajo.

**Koordinacija rob-oblaka**: Implementirajte inteligentne vzorce eskalacije, kjer lokalni agenti obravnavajo rutinske naloge, oblaÄni agenti zagotavljajo kompleksno sklepanje, ko je povezljivost omogoÄena, in brezhiben prenos med obdelavo na robu in v oblaku ohranja kontinuiteto.

#### Konfiguracije implementacije

**Implementacija na eni napravi**:
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```

**Porazdeljena implementacija na robu**:
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```

### Optimizacija zmogljivosti za robne agente

#### Strategije izbire modelov

**Dodelitev modelov na podlagi nalog**: Microsoft Agent Framework omogoÄa inteligentno izbiro modelov glede na kompleksnost naloge in zahteve:

- **Enostavne naloge** (Q&A, usmerjanje): Qwen2.5-0.5B (500MB, <100ms odziv)
- **Srednje zahtevne naloge** (podpora strankam, naÄrtovanje): Phi-4-Mini (2.4GB, 200-500ms odziv)
- **Kompleksne naloge** (tehniÄne analize, naÄrtovanje): Phi-4 (7GB, 1-3s odziv, ko viri to omogoÄajo)

**DinamiÄno preklapljanje modelov**: Agenti lahko preklapljajo med modeli glede na trenutno obremenitev sistema, oceno kompleksnosti naloge, prioritete uporabnika in razpoloÅ¾ljive strojne vire.

#### Upravljanje pomnilnika in virov

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

### Vzorci integracije v podjetju

#### Varnost in skladnost

**Lokalna obdelava podatkov**: Vsa obdelava agentov poteka lokalno, kar zagotavlja, da obÄutljivi podatki nikoli ne zapustijo robne naprave. To vkljuÄuje zaÅ¡Äito informacij o strankah, skladnost s HIPAA za zdravstvene agente, varnost finanÄnih podatkov za banÄne agente in skladnost z GDPR za evropske implementacije.

**Nadzor dostopa**: Dovoljenja na podlagi vlog nadzorujejo, katera orodja lahko agenti uporabljajo, avtentikacija uporabnikov za interakcije z agenti in revizijske sledi za vse akcije in odloÄitve agentov.

#### Spremljanje in opazovanje

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```

### ResniÄni primeri implementacije

#### Sistem robnih agentov za maloprodajo

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```

#### Agent za podporo v zdravstvu

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```

### NajboljÅ¡e prakse za Microsoft Agent Framework

#### Smernice za razvoj

1. **ZaÄnite enostavno**: ZaÄnite s scenariji enega agenta, preden zgradite kompleksne sisteme z veÄ agenti
2. **Pravilna velikost modela**: Izberite najmanjÅ¡i model, ki ustreza vaÅ¡im zahtevam po natanÄnosti
3. **Oblikovanje orodij**: Ustvarite osredotoÄena, enonamenska orodja namesto kompleksnih veÄnamenskih orodij
4. **Obravnava napak**: Implementirajte postopno zmanjÅ¡evanje zmogljivosti za scenarije brez povezave in okvare modelov
5. **Testiranje**: ObseÅ¾no testirajte agente v pogojih brez povezave in z omejenimi viri

#### NajboljÅ¡e prakse za implementacijo

1. **Postopno uvajanje**: Najprej uvedite majhnim skupinam uporabnikov, pozorno spremljajte zmogljivostne metrike
2. **Spremljanje virov**: Nastavite opozorila za pomnilnik, CPU in odzivne Äase
3. **Strategije za izpad**: Vedno
**Izbira okvira za namestitev agentov**: Izberite optimizacijske okvire glede na ciljno strojno opremo in zahteve agenta. Uporabite Llama.cpp za namestitev agentov optimiziranih za CPU, Apple MLX za aplikacije agentov na Apple Silicon in ONNX za zdruÅ¾ljivost agentov na razliÄnih platformah.

## PraktiÄna pretvorba SLM agentov in primeri uporabe

### Scenariji namestitve agentov v resniÄnem svetu

**Mobilne aplikacije agentov**: Formati Q4_K so odliÄni za aplikacije agentov na pametnih telefonih z minimalno porabo pomnilnika, medtem ko Q8_0 zagotavlja uravnoteÅ¾eno zmogljivost za sisteme agentov na tabliÄnih raÄunalnikih. Formati Q5_K ponujajo vrhunsko kakovost za mobilne produktivne agente.

**Namizno in robno raÄunalniÅ¡tvo agentov**: Q5_K zagotavlja optimalno zmogljivost za aplikacije agentov na namiznih raÄunalnikih, Q8_0 omogoÄa visokokakovostno sklepanje za delovne postaje, Q4_K pa omogoÄa uÄinkovito obdelavo na napravah z robnimi agenti.

**Raziskovalni in eksperimentalni agenti**: Napredni kvantizacijski formati omogoÄajo raziskovanje ultra nizkonatanÄnega sklepanja agentov za akademske raziskave in aplikacije dokazovanja koncepta, ki zahtevajo izjemno omejene vire.

### Merila zmogljivosti SLM agentov

**Hitrost sklepanja agentov**: Q4_K dosega najhitrejÅ¡e odzivne Äase agentov na mobilnih CPU-jih, Q5_K zagotavlja uravnoteÅ¾eno razmerje med hitrostjo in kakovostjo za sploÅ¡ne aplikacije agentov, Q8_0 ponuja vrhunsko kakovost za kompleksne naloge agentov, eksperimentalni formati pa omogoÄajo najveÄjo prepustnost za specializirano strojno opremo agentov.

**Zahteve glede pomnilnika agentov**: Kvantizacijske ravni za agente segajo od Q2_K (manj kot 500 MB za majhne modele agentov) do Q8_0 (pribliÅ¾no 50 % prvotne velikosti), pri Äemer eksperimentalne konfiguracije doseÅ¾ejo najveÄjo kompresijo za okolja agentov z omejenimi viri.

## Izzivi in premisleki za SLM agente

### Kompromisi zmogljivosti v sistemih agentov

Namestitev SLM agentov zahteva skrbno razmislek o kompromisih med velikostjo modela, hitrostjo odziva agenta in kakovostjo izhoda. Medtem ko Q4_K ponuja izjemno hitrost in uÄinkovitost za mobilne agente, Q8_0 zagotavlja vrhunsko kakovost za kompleksne naloge agentov. Q5_K predstavlja srednjo pot, primerno za veÄino sploÅ¡nih aplikacij agentov.

### ZdruÅ¾ljivost strojne opreme za SLM agente

RazliÄne robne naprave imajo razliÄne zmogljivosti za namestitev SLM agentov. Q4_K deluje uÄinkovito na osnovnih procesorjih za preproste agente, Q5_K zahteva zmerne raÄunalniÅ¡ke vire za uravnoteÅ¾eno zmogljivost agentov, Q8_0 pa koristi zmogljivejÅ¡o strojno opremo za napredne zmogljivosti agentov.

### Varnost in zasebnost v sistemih SLM agentov

Medtem ko SLM agenti omogoÄajo lokalno obdelavo za izboljÅ¡ano zasebnost, je treba uvesti ustrezne varnostne ukrepe za zaÅ¡Äito modelov agentov in podatkov v robnih okoljih. To je Å¡e posebej pomembno pri nameÅ¡Äanju agentov z visoko natanÄnostjo v poslovnih okoljih ali stisnjenih formatov agentov v aplikacijah, ki obravnavajo obÄutljive podatke.

## Prihodnji trendi v razvoju SLM agentov

Pokrajina SLM agentov se Å¡e naprej razvija z napredkom v tehnikah kompresije, metodah optimizacije in strategijah namestitve na robu. Prihodnji razvoj vkljuÄuje uÄinkovitejÅ¡e algoritme kvantizacije za modele agentov, izboljÅ¡ane metode kompresije za delovne tokove agentov in boljÅ¡o integracijo s strojno opremo za pospeÅ¡evanje obdelave agentov na robu.

**Napovedi trga za SLM agente**: Po nedavnih raziskavah bi avtomatizacija, ki jo poganjajo agenti, lahko do leta 2027 odpravila 40â€“60 % ponavljajoÄih se kognitivnih nalog v poslovnih delovnih tokovih, pri Äemer bodo SLM-i vodili to preobrazbo zaradi svoje stroÅ¡kovne uÄinkovitosti in prilagodljivosti pri namestitvi.

**TehnoloÅ¡ki trendi v SLM agentih**:
- **Specializirani SLM agenti**: Modeli, usposobljeni za specifiÄne naloge agentov in industrije
- **Robno raÄunalniÅ¡tvo agentov**: IzboljÅ¡ane zmogljivosti agentov na napravi z izboljÅ¡ano zasebnostjo in zmanjÅ¡ano zakasnitvijo
- **Orkestracija agentov**: BoljÅ¡a koordinacija med veÄ SLM agenti z dinamiÄnim usmerjanjem in uravnavanjem obremenitve
- **Demokratizacija**: Prilagodljivost SLM omogoÄa Å¡irÅ¡o udeleÅ¾bo pri razvoju agentov v organizacijah

## ZaÄetek dela s SLM agenti

### Korak 1: Nastavite okolje Microsoft Agent Framework

**Namestite odvisnosti**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inicializirajte Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Korak 2: Izberite svoj SLM za aplikacije agentov
Priljubljene moÅ¾nosti za Microsoft Agent Framework:
- **Microsoft Phi-4 Mini (3.8B)**: OdliÄen za sploÅ¡ne naloge agentov z uravnoteÅ¾eno zmogljivostjo
- **Qwen2.5-0.5B (0.5B)**: Izjemno uÄinkovit za preproste agente za usmerjanje in razvrÅ¡Äanje
- **Qwen2.5-Coder-0.5B (0.5B)**: Specializiran za naloge agentov, povezane s kodo
- **Phi-4 (7B)**: Napredno sklepanje za kompleksne scenarije na robu, kadar so na voljo viri

### Korak 3: Ustvarite svojega prvega agenta z Microsoft Agent Framework

**Osnovna nastavitev agenta**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Korak 4: DoloÄite obseg in zahteve agenta
ZaÄnite z osredotoÄenimi, dobro opredeljenimi aplikacijami agentov z Microsoft Agent Framework:
- **Agenti za eno podroÄje**: PomoÄ strankam ALI naÄrtovanje ALI raziskave
- **Jasni cilji agenta**: SpecifiÄni, merljivi cilji za zmogljivost agenta
- **Omejena integracija orodij**: NajveÄ 3â€“5 orodij za zaÄetno namestitev agenta
- **DoloÄene meje agenta**: Jasne poti za eskalacijo pri kompleksnih scenarijih
- **Oblikovanje, osredotoÄeno na rob**: Prednostna funkcionalnost brez povezave in lokalna obdelava

### Korak 5: Izvedite namestitev na robu z Microsoft Agent Framework

**Konfiguracija virov**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Namestite varnostne ukrepe za robne agente**:
- **Lokalna validacija vnosa**: Preverite zahteve brez odvisnosti od oblaka
- **Filtriranje izhodov brez povezave**: Zagotovite, da odgovori lokalno ustrezajo standardom kakovosti
- **Varnostni nadzor na robu**: Uvedite varnost brez potrebe po internetni povezavi
- **Lokalno spremljanje**: Spremljajte zmogljivost in oznaÄite teÅ¾ave z uporabo telemetrije na robu

### Korak 6: Merite in optimizirajte zmogljivost robnih agentov
- **Stopnje dokonÄanja nalog agenta**: Spremljajte stopnje uspeÅ¡nosti v scenarijih brez povezave
- **ÄŒasi odziva agenta**: Zagotovite odzivne Äase pod sekundo za namestitev na robu
- **Poraba virov**: Spremljajte porabo pomnilnika, CPU-ja in baterije na robnih napravah
- **StroÅ¡kovna uÄinkovitost**: Primerjajte stroÅ¡ke namestitve na robu z alternativami v oblaku
- **Zanesljivost brez povezave**: Merite zmogljivost agenta med izpadi omreÅ¾ja

## KljuÄne toÄke za implementacijo SLM agentov

1. **SLM-i so zadostni za agente**: Za veÄino nalog agentov majhni modeli delujejo enako dobro kot veliki, hkrati pa ponujajo pomembne prednosti
2. **StroÅ¡kovna uÄinkovitost pri agentih**: 10â€“30x cenejÅ¡i za delovanje SLM agentov, kar jih naredi ekonomsko izvedljive za Å¡iroko uporabo
3. **Specializacija deluje za agente**: Fino uglaÅ¡eni SLM-i pogosto prekaÅ¡ajo sploÅ¡ne LLM-e v specifiÄnih aplikacijah agentov
4. **Hibridna arhitektura agentov**: Uporabite SLM-e za rutinske naloge agentov, LLM-e za kompleksno sklepanje, kadar je potrebno
5. **Microsoft Agent Framework omogoÄa produkcijsko namestitev**: Ponuja orodja na ravni podjetja za gradnjo, namestitev in upravljanje robnih agentov
6. **NaÄela oblikovanja, osredotoÄena na rob**: Agenti, ki delujejo brez povezave z lokalno obdelavo, zagotavljajo zasebnost in zanesljivost
7. **Integracija Foundry Local**: Brezhibna povezava med Microsoft Agent Framework in lokalnim sklepanjem modelov
8. **Prihodnost so SLM agenti**: Majhni jezikovni modeli s produkcijskimi okviri so prihodnost agentne umetne inteligence, ki omogoÄa demokratizirano in uÄinkovito namestitev agentov

## Reference in dodatno branje

### Temeljni raziskovalni Älanki in publikacije

#### AI agenti in agentni sistemi
- **"Language Agents as Optimizable Graphs"** (2024) - Temeljne raziskave o arhitekturi agentov in strategijah optimizacije
  - Avtorji: Wenyue Hua, Lishan Yang, et al.
  - Povezava: https://arxiv.org/abs/2402.16823
  - KljuÄni vpogledi: Oblikovanje agentov na osnovi grafov in strategije optimizacije

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Avtorji: Zhiheng Xi, Wenxiang Chen, et al.
  - Povezava: https://arxiv.org/abs/2309.07864
  - KljuÄni vpogledi: Celovit pregled zmogljivosti in aplikacij agentov na osnovi LLM

- **"Cognitive Architectures for Language Agents"** (2024)
  - Avtorji: Theodore Sumers, Shunyu Yao, et al.
  - Povezava: https://arxiv.org/abs/2309.02427
  - KljuÄni vpogledi: Kognitivni okviri za oblikovanje inteligentnih agentov

#### Majhni jezikovni modeli in optimizacija
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Avtorji: Microsoft Research Team
  - Povezava: https://arxiv.org/abs/2404.14219
  - KljuÄni vpogledi: NaÄela oblikovanja SLM in strategije mobilne namestitve

- **"Qwen2.5 Technical Report"** (2024)
  - Avtorji: Alibaba Cloud Team
  - Povezava: https://arxiv.org/abs/2407.10671
  - KljuÄni vpogledi: Napredne tehnike usposabljanja SLM in optimizacija zmogljivosti

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Avtorji: Peiyuan Zhang, Guangtao Zeng, et al.
  - Povezava: https://arxiv.org/abs/2401.02385
  - KljuÄni vpogledi: Ultra-kompaktno oblikovanje modelov in uÄinkovitost usposabljanja

### Uradna dokumentacija in okviri

#### Microsoft Agent Framework
- **Uradna dokumentacija**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub repozitorij**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Primarni repozitorij**: https://github.com/microsoft/foundry-local
- **Dokumentacija**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Glavni repozitorij**: https://github.com/vllm-project/vllm
- **Dokumentacija**: https://docs.vllm.ai/


#### Ollama
- **Uradna spletna stran**: https://ollama.ai/
- **GitHub repozitorij**: https://github.com/ollama/ollama

### Okviri za optimizacijo modelov

#### Llama.cpp
- **Repozitorij**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentacija**: https://microsoft.github.io/Olive/
- **GitHub repozitorij**: https://github.com/microsoft/Olive

#### OpenVINO
- **Uradna stran**: https://docs.openvino.ai/

#### Apple MLX
- **Repozitorij**: https://github.com/ml-explore/mlx

### Industrijska poroÄila in analiza trga

#### Raziskave trga AI agentov
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Povezava: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - KljuÄni vpogledi: TrÅ¾ni trendi in vzorci sprejemanja v podjetjih

#### TehniÄna merila

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Povezava: https://mlcommons.org/en/inference-edge/
  - KljuÄni vpogledi: Standardizirani kazalniki zmogljivosti za namestitev na robu

### Standardi in specifikacije

#### Formati modelov in standardi
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Format modela za interoperabilnost na razliÄnih platformah
- **GGUF specifikacija**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Kvantiziran format modela za sklepanje na CPU
- **OpenAI API specifikacija**: https://platform.openai.com/docs/api-reference
  - Standardni API format za integracijo jezikovnih modelov

#### Varnost in skladnost
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI Systems**: Okvir za sisteme AI in varnost
- **IEEE standardi za AI**: https://standards.ieee.org/industry-connections/ai/

Premik k agentom, ki jih poganjajo SLM-i, predstavlja temeljno spremembo v pristopu k namestitvi AI. Microsoft Agent Framework, v kombinaciji z lokalnimi platformami in uÄinkovitimi majhnimi jezikovnimi modeli, ponuja celovito reÅ¡itev za gradnjo produkcijsko pripravljenih agentov, ki uÄinkovito delujejo v robnih okoljih. S poudarkom na uÄinkovitosti, specializaciji in praktiÄni uporabnosti ta tehnoloÅ¡ki sklop omogoÄa, da so AI agenti bolj dostopni, cenovno ugodni in uÄinkoviti za resniÄne aplikacije v vseh industrijah in okoljih robnega raÄunalniÅ¡tva.

Ko napredujemo proti letu 2025, bo kombinacija vse bolj zmogljivih majhnih modelov, sofisticiranih okvirov agentov, kot je Microsoft Agent Framework, in robustnih platform za namestitev na robu odprla nove moÅ¾nosti za avtonomne sisteme, ki lahko uÄinkovito delujejo na robnih napravah, hkrati pa ohranjajo zasebnost, zmanjÅ¡ujejo stroÅ¡ke in zagotavljajo izjemne uporabniÅ¡ke izkuÅ¡nje.

**Naslednji koraki za implementacijo**:
1. **Raziskujte funkcijsko klicanje**: NauÄite se, kako SLM-i obravnavajo integracijo orodij in strukturirane izhode
2. **Obvladujte protokol konteksta modela (MCP)**: Razumite napredne vzorce komunikacije agentov
3. **Gradite produkcijske agente**: Uporabite Microsoft Agent Framework za namestitve na ravni podjetja
4. **Optimizirajte za rob**: Uporabite napredne tehnike optimizacije za okolja z omejenimi viri

## â¡ï¸ Kaj sledi

- [02: Funkcijs

---

**Omejitev odgovornosti**:  
Ta dokument je bil preveden z uporabo storitve za prevajanje AI [Co-op Translator](https://github.com/Azure/co-op-translator). ÄŒeprav si prizadevamo za natanÄnost, vas prosimo, da upoÅ¡tevate, da lahko avtomatski prevodi vsebujejo napake ali netoÄnosti. Izvirni dokument v njegovem maternem jeziku je treba obravnavati kot avtoritativni vir. Za kljuÄne informacije priporoÄamo profesionalni ÄloveÅ¡ki prevod. Ne odgovarjamo za morebitne nesporazume ali napaÄne razlage, ki bi nastale zaradi uporabe tega prevoda.