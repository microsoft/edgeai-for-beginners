# Section 7 : Suite d'Optimisation Qualcomm QNN (Qualcomm Neural Network)

## Table des matières
1. [Introduction](../../../Module04)
2. [Qu'est-ce que Qualcomm QNN ?](../../../Module04)
3. [Installation](../../../Module04)
4. [Guide de démarrage rapide](../../../Module04)
5. [Exemple : Conversion et optimisation de modèles avec QNN](../../../Module04)
6. [Utilisation avancée](../../../Module04)
7. [Bonnes pratiques](../../../Module04)
8. [Dépannage](../../../Module04)
9. [Ressources supplémentaires](../../../Module04)

## Introduction

Qualcomm QNN (Qualcomm Neural Network) est un framework complet d'inférence IA conçu pour exploiter tout le potentiel des accélérateurs matériels IA de Qualcomm, notamment le NPU Hexagon, le GPU Adreno et le CPU Kryo. Que vous cibliez des appareils mobiles, des plateformes de calcul en périphérie ou des systèmes automobiles, QNN offre des capacités d'inférence optimisées qui tirent parti des unités de traitement IA spécialisées de Qualcomm pour des performances maximales et une efficacité énergétique accrue.

## Qu'est-ce que Qualcomm QNN ?

Qualcomm QNN est un framework d'inférence IA unifié qui permet aux développeurs de déployer efficacement des modèles IA sur l'architecture informatique hétérogène de Qualcomm. Il fournit une interface de programmation unifiée pour accéder au NPU Hexagon (Neural Processing Unit), au GPU Adreno et au CPU Kryo, en sélectionnant automatiquement l'unité de traitement optimale pour les différentes couches et opérations du modèle.

### Caractéristiques principales

- **Calcul hétérogène** : Accès unifié au NPU, GPU et CPU avec distribution automatique des charges de travail
- **Optimisation adaptée au matériel** : Optimisations spécialisées pour les plateformes Snapdragon de Qualcomm
- **Support de la quantification** : Techniques avancées de quantification INT8, INT16 et en précision mixte
- **Outils de conversion de modèles** : Support direct pour les modèles TensorFlow, PyTorch, ONNX et Caffe
- **Optimisé pour l'IA en périphérie** : Conçu spécifiquement pour les scénarios de déploiement mobile et en périphérie avec un focus sur l'efficacité énergétique

### Avantages

- **Performances maximales** : Exploitez le matériel IA spécialisé pour des améliorations de performances allant jusqu'à 15x
- **Efficacité énergétique** : Optimisé pour les appareils mobiles et alimentés par batterie avec une gestion intelligente de l'énergie
- **Faible latence** : Inférence accélérée par le matériel avec un minimum de surcharge pour les applications en temps réel
- **Déploiement évolutif** : Des smartphones aux plateformes automobiles dans l'écosystème Qualcomm
- **Prêt pour la production** : Framework éprouvé utilisé dans des millions d'appareils déployés

## Installation

### Prérequis

- SDK Qualcomm QNN (nécessite une inscription auprès de Qualcomm)
- Python 3.7 ou supérieur
- Matériel compatible Qualcomm ou simulateur
- Android NDK (pour le déploiement mobile)
- Environnement de développement Linux ou Windows

### Configuration du SDK QNN

1. **Inscription et téléchargement** : Visitez le réseau des développeurs Qualcomm pour vous inscrire et télécharger le SDK QNN
2. **Extraction du SDK** : Décompressez le SDK QNN dans votre répertoire de développement
3. **Configurer les variables d'environnement** : Configurez les chemins pour les outils et bibliothèques QNN

```bash
# Set QNN environment variables
export QNN_SDK_ROOT=/path/to/qnn-sdk
export PATH=$QNN_SDK_ROOT/bin:$PATH
export LD_LIBRARY_PATH=$QNN_SDK_ROOT/lib:$LD_LIBRARY_PATH
```

### Configuration de l'environnement Python

Créez et activez un environnement virtuel :

```bash
# Create virtual environment
python -m venv qnn-env

# Activate virtual environment
# On Windows:
qnn-env\Scripts\activate
# On Linux:
source qnn-env/bin/activate
```

Installez les packages Python requis :

```bash
pip install numpy tensorflow torch onnx
```

### Vérification de l'installation

```bash
# Check QNN tools availability
qnn-model-lib-generator --help
qnn-context-binary-generator --help
qnn-net-run --help
```

Si l'installation est réussie, vous devriez voir les informations d'aide pour chaque outil QNN.

## Guide de démarrage rapide

### Conversion de votre premier modèle

Convertissons un modèle PyTorch simple pour qu'il fonctionne sur le matériel Qualcomm :

```python
import torch
import torch.nn as nn
import numpy as np

# Define a simple model
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(64, 10)
    
    def forward(self, x):
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

# Create and export model
model = SimpleModel()
model.eval()

# Create dummy input for tracing
dummy_input = torch.randn(1, 3, 224, 224)

# Export to ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    export_params=True,
    opset_version=11,
    do_constant_folding=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'},
                  'output': {0: 'batch_size'}}
)
```

### Convertir ONNX au format QNN

```bash
# Convert ONNX model to QNN model library
qnn-onnx-converter \
    --input_network simple_model.onnx \
    --output_path simple_model.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json
```

### Générer une bibliothèque de modèles QNN

```bash
# Compile model library
qnn-model-lib-generator \
    -c simple_model.cpp \
    -b simple_model.bin \
    -t x86_64-linux-clang \
    -l simple_model \
    -o simple_model_qnn.so
```

### Ce que fait ce processus

Le workflow d'optimisation comprend : la conversion du modèle original au format ONNX, la traduction d'ONNX en représentation intermédiaire QNN, l'application d'optimisations spécifiques au matériel et la génération d'une bibliothèque de modèles compilée pour le déploiement.

### Explication des paramètres clés

- `--input_network` : Fichier modèle ONNX source
- `--output_path` : Fichier source C++ généré
- `--input_dim` : Dimensions du tenseur d'entrée pour l'optimisation
- `--quantization_overrides` : Configuration de quantification personnalisée
- `-t x86_64-linux-clang` : Architecture cible et compilateur

## Exemple : Conversion et optimisation de modèles avec QNN

### Étape 1 : Conversion avancée de modèles avec quantification

Voici comment appliquer une quantification personnalisée lors de la conversion :

```json
// quantization_config.json
{
  "activation_encodings": {
    "conv1/Relu:0": {
      "bitwidth": 8,
      "max": 6.0,
      "min": 0.0,
      "scale": 0.023529,
      "offset": 0
    }
  },
  "param_encodings": {
    "conv1.weight": {
      "bitwidth": 8,
      "max": 2.5,
      "min": -2.5,
      "scale": 0.019608,
      "offset": 127
    }
  },
  "activation_bitwidth": 8,
  "param_bitwidth": 8,
  "bias_bitwidth": 32
}
```

Convertir avec une quantification personnalisée :

```bash
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model_quantized.cpp \
    --input_dim input 1,3,224,224 \
    --quantization_overrides quantization_config.json \
    --target_device hexagon \
    --optimization_level high
```

### Étape 2 : Optimisation multi-backend

Configurer l'exécution hétérogène sur NPU, GPU et CPU :

```bash
# Generate model library with multiple backend support
qnn-model-lib-generator \
    -c model_quantized.cpp \
    -b model_quantized.bin \
    -t aarch64-android \
    -l model_optimized \
    -o model_optimized.so \
    --target_backends htp,gpu,cpu
```

### Étape 3 : Créer un binaire de contexte pour le déploiement

```bash
# Generate optimized context binary
qnn-context-binary-generator \
    --model model_optimized.so \
    --backend libQnnHtp.so \
    --output_dir ./context_binaries \
    --input_list input_data.txt \
    --optimization_level high
```

### Étape 4 : Inférence avec le runtime QNN

```python
import ctypes
import numpy as np

# Load QNN library
qnn_lib = ctypes.CDLL('./libQnn.so')

class QNNInference:
    def __init__(self, model_path, backend='htp'):
        self.model_path = model_path
        self.backend = backend
        self.context = None
        self._initialize()
    
    def _initialize(self):
        # Initialize QNN runtime
        # Load model and create inference context
        pass
    
    def preprocess_input(self, data):
        # Quantize input data if needed
        if self.is_quantized:
            # Apply quantization parameters
            scale = self.input_scale
            offset = self.input_offset
            quantized = np.clip(
                np.round(data / scale + offset), 
                0, 255
            ).astype(np.uint8)
            return quantized
        return data.astype(np.float32)
    
    def inference(self, input_data):
        # Preprocess input
        processed_input = self.preprocess_input(input_data)
        
        # Run inference on Qualcomm hardware
        # This would call into QNN C++ API
        output = self._run_inference(processed_input)
        
        # Postprocess output
        return self.postprocess_output(output)
    
    def postprocess_output(self, output):
        # Dequantize output if needed
        if self.is_quantized:
            scale = self.output_scale
            offset = self.output_offset
            dequantized = (output.astype(np.float32) - offset) * scale
            return dequantized
        return output

# Usage
inference_engine = QNNInference("model_optimized.so", backend="htp")
result = inference_engine.inference(input_tensor)
print(f"Inference result: {result}")
```

### Structure de sortie

Après optimisation, votre répertoire de déploiement contiendra :

```
qnn_model/
├── model_optimized.so          # Compiled model library
├── context_binaries/           # Pre-compiled contexts
│   ├── htp_context.bin        # NPU context
│   ├── gpu_context.bin        # GPU context
│   └── cpu_context.bin        # CPU context
├── quantization_config.json   # Quantization parameters
└── input_specs.json          # Input/output specifications
```

## Utilisation avancée

### Configuration personnalisée du backend

Configurer des optimisations spécifiques au backend :

```json
// backend_config.json
{
  "htp_config": {
    "device_id": 0,
    "performance_mode": "high_performance",
    "precision_mode": "int8",
    "vtcm_mb": 8,
    "enable_dma": true
  },
  "gpu_config": {
    "device_id": 0,
    "performance_mode": "sustained_high_performance",
    "precision_mode": "fp16",
    "enable_transform_optimization": true
  },
  "cpu_config": {
    "num_threads": 4,
    "performance_mode": "balanced",
    "enable_fast_math": true
  }
}
```

### Quantification dynamique

Appliquer la quantification à l'exécution pour une meilleure précision :

```python
class DynamicQuantization:
    def __init__(self, model_path):
        self.model_path = model_path
        self.calibration_data = []
    
    def collect_statistics(self, calibration_dataset):
        """Collect activation statistics for quantization"""
        for data in calibration_dataset:
            # Run inference and collect activation ranges
            activations = self.forward_hooks(data)
            self.calibration_data.append(activations)
    
    def compute_quantization_params(self):
        """Compute optimal quantization parameters"""
        params = {}
        for layer_name, activations in self.calibration_data:
            min_val = np.min(activations)
            max_val = np.max(activations)
            
            # Compute scale and offset for INT8 quantization
            scale = (max_val - min_val) / 255.0
            offset = -min_val / scale
            
            params[layer_name] = {
                "scale": scale,
                "offset": int(offset),
                "min": min_val,
                "max": max_val
            }
        
        return params
    
    def apply_quantization(self, quantization_params):
        """Apply computed quantization parameters"""
        config = {
            "activation_encodings": {},
            "param_encodings": {}
        }
        
        for layer, params in quantization_params.items():
            config["activation_encodings"][layer] = {
                "bitwidth": 8,
                "scale": params["scale"],
                "offset": params["offset"],
                "min": params["min"],
                "max": params["max"]
            }
        
        return config
```

### Profilage des performances

Surveiller les performances sur différents backends :

```python
import time
import psutil

class QNNProfiler:
    def __init__(self):
        self.metrics = {}
    
    def profile_inference(self, inference_func, input_data, num_runs=100):
        """Profile inference performance"""
        latencies = []
        cpu_usage = []
        memory_usage = []
        
        for i in range(num_runs):
            # Monitor system resources
            process = psutil.Process()
            cpu_before = process.cpu_percent()
            memory_before = process.memory_info().rss
            
            # Measure inference time
            start_time = time.perf_counter()
            result = inference_func(input_data)
            end_time = time.perf_counter()
            
            latency = (end_time - start_time) * 1000  # Convert to ms
            latencies.append(latency)
            
            # Collect resource usage
            cpu_after = process.cpu_percent()
            memory_after = process.memory_info().rss
            
            cpu_usage.append(cpu_after - cpu_before)
            memory_usage.append(memory_after - memory_before)
        
        return {
            "avg_latency_ms": np.mean(latencies),
            "p95_latency_ms": np.percentile(latencies, 95),
            "p99_latency_ms": np.percentile(latencies, 99),
            "throughput_fps": 1000 / np.mean(latencies),
            "avg_cpu_usage": np.mean(cpu_usage),
            "avg_memory_delta_mb": np.mean(memory_usage) / (1024 * 1024)
        }

# Usage
profiler = QNNProfiler()
htp_metrics = profiler.profile_inference(htp_inference, test_data)
gpu_metrics = profiler.profile_inference(gpu_inference, test_data)
cpu_metrics = profiler.profile_inference(cpu_inference, test_data)

print("HTP Performance:", htp_metrics)
print("GPU Performance:", gpu_metrics)
print("CPU Performance:", cpu_metrics)
```

### Sélection automatique du backend

Implémenter une sélection intelligente du backend basée sur les caractéristiques du modèle :

```python
class BackendSelector:
    def __init__(self):
        self.backend_capabilities = {
            "htp": {
                "supported_ops": ["Conv2d", "Dense", "BatchNorm", "ReLU"],
                "max_tensor_size": 8 * 1024 * 1024,  # 8MB
                "preferred_precision": "int8",
                "power_efficiency": 0.9
            },
            "gpu": {
                "supported_ops": ["Conv2d", "Dense", "ReLU", "Softmax"],
                "max_tensor_size": 64 * 1024 * 1024,  # 64MB
                "preferred_precision": "fp16",
                "power_efficiency": 0.6
            },
            "cpu": {
                "supported_ops": ["*"],  # All operations
                "max_tensor_size": 512 * 1024 * 1024,  # 512MB
                "preferred_precision": "fp32",
                "power_efficiency": 0.4
            }
        }
    
    def select_optimal_backend(self, model_info, constraints):
        """Select optimal backend based on model and constraints"""
        scores = {}
        
        for backend, caps in self.backend_capabilities.items():
            score = 0
            
            # Check operation support
            if all(op in caps["supported_ops"] or "*" in caps["supported_ops"] 
                   for op in model_info["operations"]):
                score += 30
            
            # Check tensor size compatibility
            if model_info["max_tensor_size"] <= caps["max_tensor_size"]:
                score += 25
            
            # Power efficiency consideration
            if constraints.get("power_critical", False):
                score += caps["power_efficiency"] * 25
            
            # Performance preference
            if constraints.get("performance_critical", False):
                if backend == "htp":
                    score += 20
            
            scores[backend] = score
        
        return max(scores, key=scores.get)

# Usage
selector = BackendSelector()
model_info = {
    "operations": ["Conv2d", "ReLU", "Dense"],
    "max_tensor_size": 4 * 1024 * 1024,
    "precision": "int8"
}
constraints = {
    "power_critical": True,
    "performance_critical": True
}

optimal_backend = selector.select_optimal_backend(model_info, constraints)
print(f"Recommended backend: {optimal_backend}")
```

## Bonnes pratiques

### 1. Optimisation de l'architecture du modèle
- **Fusion de couches** : Combiner des opérations comme Conv+BatchNorm+ReLU pour une meilleure utilisation du NPU
- **Convolutions séparables en profondeur** : Privilégiez ces convolutions aux convolutions standard pour le déploiement mobile
- **Conceptions adaptées à la quantification** : Utilisez des activations ReLU et évitez les opérations qui se quantifient mal

### 2. Stratégie de quantification
- **Quantification après entraînement** : Commencez par cela pour un déploiement rapide
- **Dataset de calibration** : Utilisez des données représentatives couvrant toutes les variations d'entrée
- **Précision mixte** : Utilisez INT8 pour la plupart des couches, conservez les couches critiques en précision supérieure

### 3. Directives de sélection du backend
- **NPU (HTP)** : Idéal pour les charges de travail CNN, les modèles quantifiés et les applications sensibles à l'énergie
- **GPU** : Optimal pour les opérations intensives en calcul, les modèles plus grands et la précision FP16
- **CPU** : Solution de repli pour les opérations non prises en charge et le débogage

### 4. Optimisation des performances
- **Taille de lot** : Utilisez une taille de lot de 1 pour les applications en temps réel, des lots plus grands pour le débit
- **Prétraitement des entrées** : Minimisez la copie et la conversion des données
- **Réutilisation du contexte** : Précompilez les contextes pour éviter les surcharges de compilation à l'exécution

### 5. Gestion de la mémoire
- **Allocation de tenseurs** : Utilisez une allocation statique lorsque cela est possible pour éviter les surcharges à l'exécution
- **Pools de mémoire** : Implémentez des pools de mémoire personnalisés pour les tenseurs fréquemment alloués
- **Réutilisation des tampons** : Réutilisez les tampons d'entrée/sortie entre les appels d'inférence

### 6. Optimisation de l'énergie
- **Modes de performance** : Utilisez des modes de performance appropriés en fonction des contraintes thermiques
- **Scaling dynamique de fréquence** : Permettez au système de moduler la fréquence en fonction de la charge de travail
- **Gestion de l'état d'inactivité** : Libérez correctement les ressources lorsqu'elles ne sont pas utilisées

## Dépannage

### Problèmes courants

#### 1. Problèmes d'installation du SDK
```bash
# Verify QNN SDK installation
echo $QNN_SDK_ROOT
ls $QNN_SDK_ROOT/bin/qnn-*

# Check library dependencies
ldd $QNN_SDK_ROOT/lib/libQnn.so
```

#### 2. Erreurs de conversion de modèles
```bash
# Enable verbose logging
qnn-onnx-converter \
    --input_network model.onnx \
    --output_path model.cpp \
    --debug \
    --log_level verbose
```

#### 3. Problèmes de quantification
```python
# Validate quantization parameters
def validate_quantization_range(data, scale, offset, bitwidth=8):
    quantized = np.clip(
        np.round(data / scale + offset), 
        0, (2**bitwidth) - 1
    )
    dequantized = (quantized - offset) * scale
    mse = np.mean((data - dequantized) ** 2)
    print(f"Quantization MSE: {mse}")
    return mse < threshold
```

#### 4. Problèmes de performances
```bash
# Check hardware utilization
adb shell cat /sys/class/devfreq/soc:qcom,cpu*-cpu-ddr-latfloor/cur_freq
adb shell cat /sys/class/kgsl/kgsl-3d0/gpuclk

# Monitor NPU usage
adb shell cat /sys/kernel/debug/msm_vidc/load
```

#### 5. Problèmes de mémoire
```python
# Monitor memory usage
import tracemalloc

tracemalloc.start()
# Run inference
result = inference_engine.inference(input_data)
current, peak = tracemalloc.get_traced_memory()
print(f"Current memory usage: {current / 1024 / 1024:.1f} MB")
print(f"Peak memory usage: {peak / 1024 / 1024:.1f} MB")
tracemalloc.stop()
```

#### 6. Compatibilité du backend
```python
# Check backend availability
def check_backend_support():
    try:
        # Load backend library
        htp_lib = ctypes.CDLL('./libQnnHtp.so')
        print("HTP backend available")
    except OSError:
        print("HTP backend not available")
    
    try:
        gpu_lib = ctypes.CDLL('./libQnnGpu.so')
        print("GPU backend available")
    except OSError:
        print("GPU backend not available")
```

### Débogage des performances

```python
# Create performance analysis tool
class QNNDebugger:
    def __init__(self, model_path):
        self.model_path = model_path
        self.layer_timings = {}
    
    def profile_layers(self, input_data):
        """Profile individual layer performance"""
        # This would require integration with QNN profiling APIs
        for layer_name in self.get_layer_names():
            start = time.perf_counter()
            # Execute layer
            end = time.perf_counter()
            self.layer_timings[layer_name] = (end - start) * 1000
    
    def analyze_bottlenecks(self):
        """Identify performance bottlenecks"""
        sorted_layers = sorted(
            self.layer_timings.items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        print("Top 5 slowest layers:")
        for layer, time_ms in sorted_layers[:5]:
            print(f"  {layer}: {time_ms:.2f} ms")
    
    def suggest_optimizations(self):
        """Suggest optimization strategies"""
        suggestions = []
        
        for layer, time_ms in self.layer_timings.items():
            if time_ms > 10:  # Layer takes more than 10ms
                if "conv" in layer.lower():
                    suggestions.append(f"Consider depthwise separable conv for {layer}")
                elif "dense" in layer.lower():
                    suggestions.append(f"Consider quantization for {layer}")
        
        return suggestions
```

### Obtenir de l'aide

- **Réseau des développeurs Qualcomm** : [developer.qualcomm.com](https://developer.qualcomm.com/)
- **Documentation QNN** : Disponible dans le package SDK
- **Forums communautaires** : [developer.qualcomm.com/forums](https://developer.qualcomm.com/forums)
- **Support technique** : Via le portail des développeurs Qualcomm

## Ressources supplémentaires

### Liens officiels
- **Qualcomm AI Hub** : [aihub.qualcomm.com](https://aihub.qualcomm.com/)
- **Plateformes Snapdragon** : [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)
- **Portail des développeurs** : [developer.qualcomm.com/software/qualcomm-neural-processing-sdk](https://developer.qualcomm.com/software/qualcomm-neural-processing-sdk)
- **Moteur IA** : [qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct](https://www.qualcomm.com/news/onq/2019/06/qualcomm-ai-engine-direct)

### Ressources d'apprentissage
- **Guide de démarrage** : Disponible dans la documentation du SDK QNN
- **Zoo de modèles** : [aihub.qualcomm.com/models](https://aihub.qualcomm.com/models)
- **Guide d'optimisation** : La documentation du SDK inclut des directives complètes d'optimisation
- **Tutoriels vidéo** : [Chaîne YouTube des développeurs Qualcomm](https://www.youtube.com/c/QualcommDeveloperNetwork)

### Outils d'intégration
- **SNPE (Legacy)** : [developer.qualcomm.com/docs/snpe](https://developer.qualcomm.com/docs/snpe/overview.html)
- **AI Hub** : Modèles pré-optimisés pour le matériel Qualcomm
- **API Android Neural Networks** : Intégration avec Android NNAPI
- **Delegate TensorFlow Lite** : Délégué Qualcomm pour TFLite

### Benchmarks de performance
- **MLPerf Mobile** : [mlcommons.org/en/inference-mobile-21](https://mlcommons.org/en/inference-mobile-21/)
- **AI Benchmark** : [ai-benchmark.com/ranking](https://ai-benchmark.com/ranking.html)
- **Recherche IA Qualcomm** : [qualcomm.com/research/artificial-intelligence](https://www.qualcomm.com/research/artificial-intelligence)

### Exemples communautaires
- **Applications d'exemple** : Disponibles dans le répertoire des exemples du SDK QNN
- **Dépôts GitHub** : Exemples et outils contribué par la communauté
- **Blogs techniques** : [Blog des développeurs Qualcomm](https://developer.qualcomm.com/blog)

### Outils associés
- **Qualcomm AI Model Efficiency Toolkit (AIMET)** : [github.com/quic/aimet](https://github.com/quic/aimet) - Techniques avancées de quantification et de compression
- **TensorFlow Lite** : [tensorflow.org/lite](https://www.tensorflow.org/lite) - Pour comparaison et déploiement de secours
- **ONNX Runtime** : [onnxruntime.ai](https://onnxruntime.ai/) - Moteur d'inférence multiplateforme

### Spécifications matérielles
- **NPU Hexagon** : [developer.qualcomm.com/hardware/hexagon-dsp](https://developer.qualcomm.com/hardware/hexagon-dsp)
- **GPU Adreno** : [developer.qualcomm.com/hardware/adreno-gpu](https://developer.qualcomm.com/hardware/adreno-gpu)
- **Plateformes Snapdragon** : [qualcomm.com/products/mobile/snapdragon](https://www.qualcomm.com/products/mobile/snapdragon)

## ➡️ Et après

Poursuivez votre parcours IA en périphérie en explorant [Module 5 : SLMOps et déploiement en production](../Module05/README.md) pour en apprendre davantage sur les aspects opérationnels de la gestion du cycle de vie des Small Language Models.

---

**Avertissement** :  
Ce document a été traduit à l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatisées peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit être considéré comme la source faisant autorité. Pour des informations critiques, il est recommandé de recourir à une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interprétations erronées résultant de l'utilisation de cette traduction.