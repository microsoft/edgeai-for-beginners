<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "8c30436578b1bd604c48233ecdd39701",
  "translation_date": "2025-11-11T21:19:06+00:00",
  "source_file": "Workshop/Session01-GettingStartedFoundryLocal.md",
  "language_code": "fr"
}
-->
# Session 1 : Premiers pas avec Foundry Local

## R√©sum√©

Apprenez √† installer, configurer et ex√©cuter vos premiers mod√®les d'IA avec Microsoft Foundry Local. Cette session pratique offre une introduction pas √† pas √† l'inf√©rence locale, de l'installation √† la cr√©ation de votre premi√®re application de chat utilisant des mod√®les tels que Phi-4, Qwen et DeepSeek.

## Objectifs d'apprentissage

√Ä la fin de cette session, vous serez capable de :

- **Installer et configurer** : Configurer Foundry Local avec une v√©rification correcte de l'installation
- **Ma√Ætriser les op√©rations CLI** : Utiliser le CLI de Foundry Local pour la gestion et le d√©ploiement des mod√®les
- **Ex√©cuter votre premier mod√®le** : D√©ployer et interagir avec un mod√®le d'IA local
- **Cr√©er une application de chat** : Concevoir une application de chat basique en utilisant le SDK Python de Foundry Local
- **Comprendre l'IA locale** : Assimiler les bases de l'inf√©rence locale et de la gestion des mod√®les

## Pr√©requis

### Configuration syst√®me

- **Windows** : Windows 11 (22H2 ou version ult√©rieure) OU **macOS** : macOS 11+ (support limit√©)
- **RAM** : Minimum 8 Go, recommand√© 16 Go ou plus
- **Stockage** : Au moins 10 Go d'espace libre pour les mod√®les
- **Python** : Version 3.10 ou ult√©rieure install√©e
- **Acc√®s administrateur** : Privil√®ges administratifs pour l'installation

### Environnement de d√©veloppement

- Visual Studio Code avec extension Python (recommand√©)
- Acc√®s √† la ligne de commande (PowerShell sur Windows, Terminal sur macOS)
- Git pour cloner des d√©p√¥ts (optionnel)

## D√©roulement de l'atelier (30 minutes)

### √âtape 1 : Installer Foundry Local (5 minutes)

#### Installation sur Windows

Installez Foundry Local en utilisant le gestionnaire de paquets Windows :

```powershell
# Install via winget (recommended)
winget install Microsoft.FoundryLocal
```

Alternative : T√©l√©chargez directement depuis [Microsoft Learn](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)

#### Installation sur macOS (support limit√©)

> [!NOTE] 
> Le support macOS est actuellement en aper√ßu. Consultez la documentation officielle pour les derni√®res informations.

Si disponible, installez via Homebrew :

```bash
# If Homebrew formula is available
brew update
brew install foundry-local

# Or manual download (check official docs for latest)
curl -L -o foundry-local.tar.gz "https://download.microsoft.com/foundry-local/latest/macos/foundry-local.tar.gz"
tar -xzf foundry-local.tar.gz
sudo ./install.sh
```

**Alternative pour les utilisateurs macOS :**
- Utilisez une VM Windows 11 (Parallels/UTM) et suivez les √©tapes pour Windows
- Ex√©cutez via un conteneur si disponible et configurez `FOUNDRY_LOCAL_ENDPOINT`

### √âtape 2 : V√©rifier l'installation (3 minutes)

Apr√®s l'installation, red√©marrez votre terminal et v√©rifiez que Foundry Local fonctionne :

```powershell
# Check if Foundry Local is installed correctly
foundry --version

# View available commands
foundry --help
```

La sortie attendue doit afficher les informations de version et les commandes disponibles.

### √âtape 3 : Configurer l'environnement Python (5 minutes)

Cr√©ez un environnement Python d√©di√© pour cet atelier :

**Windows :**
```powershell
# Create virtual environment
py -m venv .venv

# Activate environment
.\.venv\Scripts\Activate.ps1

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

**macOS/Linux :**
```bash
# Create virtual environment
python3 -m venv .venv

# Activate environment
source .venv/bin/activate

# Upgrade pip and install dependencies
python -m pip install --upgrade pip
pip install foundry-local-sdk openai
```

### √âtape 4 : Ex√©cuter votre premier mod√®le (7 minutes)

Passons maintenant √† l'ex√©cution de notre premier mod√®le d'IA en local !

#### Commencez avec Phi-4 Mini (mod√®le recommand√©)

```powershell
# Download and start phi-4-mini (lightweight, fast)
foundry model run phi-4-mini

# Test the model with a simple prompt
foundry model run phi-4-mini --prompt "Hello, introduce yourself in one sentence"
```

> [!TIP]
> Cette commande t√©l√©charge le mod√®le (la premi√®re fois) et d√©marre automatiquement le service Foundry Local.

#### V√©rifiez ce qui est en cours d'ex√©cution

```powershell
# List available models (shows downloaded models)
foundry model list

# Check service status
foundry service status

# See what models are cached locally
foundry cache list
```

#### Essayez diff√©rents mod√®les

Une fois que phi-4-mini fonctionne, exp√©rimentez avec d'autres mod√®les :

```powershell
# Larger model with better capabilities
foundry model run gpt-oss-20b --prompt "Explain edge AI in simple terms"

# Fast, efficient model
foundry model run qwen2.5-0.5b --prompt "What are the benefits of local AI inference?"
```

### √âtape 5 : Cr√©er votre premi√®re application de chat (10 minutes)

Cr√©ons maintenant une application Python qui utilise les mod√®les que nous venons de d√©marrer.

#### Cr√©er le script de chat

Cr√©ez un nouveau fichier appel√© `my_first_chat.py` (ou utilisez l'exemple fourni) :

```python
#!/usr/bin/env python3
"""
My First Foundry Local Chat Application
Using FoundryLocalManager for automatic service management
"""

import os
from foundry_local import FoundryLocalManager
from openai import OpenAI

def main():
    # Get model alias from environment or use default
    alias = os.getenv("FOUNDRY_LOCAL_ALIAS", "phi-4-mini")
    
    try:
        # Initialize Foundry Local Manager (auto-starts service, downloads model)
        manager = FoundryLocalManager(alias)
        
        # Create OpenAI client pointing to local endpoint
        client = OpenAI(
            base_url=manager.endpoint,
            api_key=manager.api_key or "not-needed"
        )
        
        # Get the actual model ID for this alias
        model_id = manager.get_model_info(alias).id
        
        print("ü§ñ Welcome to your first local AI chat!")
        print(f"ÔøΩ Using model: {alias} -> {model_id}")
        print(f"üåê Endpoint: {manager.endpoint}")
        print("ÔøΩüí° Type 'quit' to exit\n")
        
    except Exception as e:
        print(f"‚ùå Failed to initialize Foundry Local: {e}")
        print("üí° Make sure Foundry Local is installed: foundry --version")
        return
    
    while True:
        # Get user input
        user_message = input("You: ").strip()
        
        if user_message.lower() in ['quit', 'exit', 'bye']:
            print("üëã Goodbye!")
            break
            
        if not user_message:
            continue
            
        try:
            # Send message to local AI model
            response = client.chat.completions.create(
                model=model_id,
                messages=[
                    {"role": "system", "content": "You are a helpful AI assistant running locally."},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=200,
                temperature=0.7
            )
            
            # Display the response
            ai_response = response.choices[0].message.content
            print(f"ü§ñ AI: {ai_response}\n")
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            print("üí° Check service status: foundry service status\n")

if __name__ == "__main__":
    main()
```

> [!TIP]
> **Exemples associ√©s** : Pour une utilisation plus avanc√©e, consultez :
>
> - **Exemple Python** : `Workshop/samples/session01/chat_bootstrap.py` - Inclut des r√©ponses en streaming et la gestion des erreurs
> - **Notebook Jupyter** : `Workshop/notebooks/session01_chat_bootstrap.ipynb` - Version interactive avec des explications d√©taill√©es

#### Testez votre application de chat

```powershell
# No need to manually start models - FoundryLocalManager handles this!
# Just run your chat application
python my_first_chat.py
```

Alternative : Utilisez directement les exemples fournis

```powershell
# Try the complete sample with streaming support
cd Workshop/samples
python -m session01.chat_bootstrap "Your question here"
```

Ou explorez le notebook interactif
Ouvrez Workshop/notebooks/session01_chat_bootstrap.ipynb dans VS Code

Essayez ces exemples de conversations :

- "Qu'est-ce que Microsoft Foundry Local ?"
- "Listez 3 avantages de l'ex√©cution de mod√®les d'IA en local"
- "Aidez-moi √† comprendre l'IA de p√©riph√©rie"

## Ce que vous avez accompli

F√©licitations ! Vous avez r√©ussi √† :

1. ‚úÖ **Installer Foundry Local** et v√©rifier son fonctionnement
2. ‚úÖ **D√©marrer votre premier mod√®le d'IA** (phi-4-mini) en local
3. ‚úÖ **Tester diff√©rents mod√®les** via la ligne de commande
4. ‚úÖ **Cr√©er une application de chat** connect√©e √† votre IA locale
5. ‚úÖ **Exp√©rimenter l'inf√©rence d'IA locale** sans d√©pendances cloud

## Comprendre ce qui s'est pass√©

### Inf√©rence d'IA locale

- Vos mod√®les d'IA fonctionnent enti√®rement sur votre ordinateur
- Aucune donn√©e n'est envoy√©e au cloud
- Les r√©ponses sont g√©n√©r√©es localement en utilisant votre CPU/GPU
- La confidentialit√© et la s√©curit√© sont pr√©serv√©es

### Gestion des mod√®les

- `foundry model run` t√©l√©charge et d√©marre les mod√®les
- **FoundryLocalManager SDK** g√®re automatiquement le d√©marrage du service et le chargement des mod√®les
- Les mod√®les sont mis en cache localement pour une utilisation future
- Plusieurs mod√®les peuvent √™tre t√©l√©charg√©s, mais g√©n√©ralement un seul fonctionne √† la fois
- Le service g√®re automatiquement le cycle de vie des mod√®les

### Approches SDK vs CLI

- **Approche CLI** : Gestion manuelle des mod√®les avec `foundry model run <model>`
- **Approche SDK** : Gestion automatique du service et des mod√®les avec `FoundryLocalManager(alias)`
- **Recommandation** : Utilisez le SDK pour les applications, le CLI pour les tests et l'exploration

## R√©f√©rence des commandes courantes

### Commandes CLI essentielles

```powershell
# Installation & Setup
foundry --version              # Check installation
foundry --help                 # View all commands

# Model Management
foundry model list             # List available models
foundry model run <model>      # Download and start a model
foundry model run <model> --prompt "text"  # One-shot prompt
foundry cache list             # Show downloaded models

# Service Management
foundry service status         # Check if service is running
foundry service start          # Start the service manually
foundry service stop           # Stop the service
```

### Recommandations de mod√®les

- **phi-4-mini** : Meilleur mod√®le de d√©part - rapide, l√©ger, bonne qualit√©
- **qwen2.5-0.5b** : Inf√©rence la plus rapide, utilisation minimale de m√©moire
- **gpt-oss-20b** : R√©ponses de meilleure qualit√©, n√©cessite plus de ressources
- **deepseek-coder-1.3b** : Optimis√© pour les t√¢ches de programmation et de code

## R√©solution des probl√®mes

### "Commande Foundry introuvable"

**Solution :**

```powershell
# Restart your terminal after installation
# Or manually add to PATH (Windows)
$env:PATH += ";C:\Program Files\Microsoft\FoundryLocal"
```

### "√âchec du chargement du mod√®le"

**Solution :**

```powershell
# Check available system memory
foundry service status

# Try a smaller model first
foundry model run phi-4-mini

# Check disk space for model downloads
# Models are stored in: %USERPROFILE%\.foundry\models (Windows)
```

### "Connexion refus√©e sur localhost"

**Solution :**

```powershell
# Check if service is running
foundry service status

# Start service if needed
foundry service start

# Verify the port (default is 5273)
# Check for port conflicts with: netstat -an | findstr 5273
```

## Prochaines √©tapes

### Actions imm√©diates

1. **Exp√©rimentez** avec diff√©rents mod√®les et invites
2. **Modifiez** votre application de chat pour essayer diff√©rents mod√®les
3. **Cr√©ez** vos propres invites et testez les r√©ponses
4. **Explorez** la Session 2 : Cr√©ation d'applications RAG

### Parcours d'apprentissage avanc√©

1. **Session 2** : Construire des solutions d'IA avec RAG (Retrieval-Augmented Generation)
2. **Session 3** : Comparer diff√©rents mod√®les open-source
3. **Session 4** : Travailler avec des mod√®les de pointe
4. **Session 5** : Construire des syst√®mes d'IA multi-agents

## Variables d'environnement (optionnel)

Pour une utilisation plus avanc√©e, vous pouvez d√©finir ces variables d'environnement :

| Variable | Objectif | Exemple |
|----------|----------|---------|
| `FOUNDRY_LOCAL_ALIAS` | Mod√®le par d√©faut √† utiliser | `phi-4-mini` |
| `FOUNDRY_LOCAL_ENDPOINT` | Remplacer l'URL de l'endpoint | `http://localhost:5273/v1` |

Cr√©ez un fichier `.env` dans le r√©pertoire de votre projet :
```
FOUNDRY_LOCAL_ALIAS=phi-4-mini
FOUNDRY_LOCAL_ENDPOINT=auto
```

## Ressources suppl√©mentaires

### Documentation

- [R√©f√©rence SDK Python Foundry Local](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-sdk?pivots=programming-language-python)
- [Guide d'installation Foundry Local](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/install)
- [Catalogue des mod√®les](https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/models)

### Code exemple

- **Exemple Python Session01** : `Workshop/samples/session01/chat_bootstrap.py` - Application de chat compl√®te avec streaming
- **Notebook Session01** : `Workshop/notebooks/session01_chat_bootstrap.ipynb` - Tutoriel interactif  
- [Exemple Module08 01](../Module08/samples/01/README.md) - D√©marrage rapide REST Chat
- [Exemple Module08 02](../Module08/samples/02/README.md) - Int√©gration SDK OpenAI
- [Exemple Module08 03](../Module08/samples/03/README.md) - D√©couverte et benchmarking des mod√®les

### Communaut√©

- [Discussions GitHub Foundry Local](https://github.com/microsoft/Foundry-Local/discussions)
- [Communaut√© Azure AI](https://techcommunity.microsoft.com/category/artificialintelligence)

---

**Dur√©e de la session** : 30 minutes de pratique + 15 minutes de questions/r√©ponses  
**Niveau de difficult√©** : D√©butant  
**Pr√©requis** : Windows 11/macOS 11+, Python 3.10+, acc√®s administrateur

## Exemple de sc√©nario d'atelier

### Contexte r√©el

**Sc√©nario** : Une √©quipe informatique d'entreprise doit √©valuer l'inf√©rence d'IA sur appareil pour traiter des retours d'employ√©s sensibles sans envoyer de donn√©es √† des services externes.

**Votre objectif** : D√©montrer que les mod√®les d'IA locaux peuvent fournir des r√©ponses de qualit√© avec une latence inf√©rieure √† une seconde tout en maintenant une confidentialit√© totale des donn√©es.

### Prompts de test

Utilisez ces prompts pour valider votre configuration :

```json
[
    "List two benefits of local inference.",
    "Summarize why keeping data on device improves privacy.",
    "Give one trade-off when choosing a small model over a large model."
]
```

### Crit√®res de r√©ussite

- ‚úÖ Toutes les invites obtiennent des r√©ponses en moins de 2 secondes
- ‚úÖ Aucune donn√©e ne quitte votre machine locale
- ‚úÖ Les r√©ponses sont pertinentes et utiles
- ‚úÖ Votre application de chat fonctionne sans probl√®me

Cette validation garantit que votre configuration Foundry Local est pr√™te pour les ateliers avanc√©s des Sessions 2 √† 6.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous ne sommes pas responsables des malentendus ou des interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->