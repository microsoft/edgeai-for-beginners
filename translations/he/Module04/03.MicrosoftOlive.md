# סעיף 3: Microsoft Olive Optimization Suite

## תוכן עניינים
1. [מבוא](../../../Module04)
2. [מה זה Microsoft Olive?](../../../Module04)
3. [התקנה](../../../Module04)
4. [מדריך התחלה מהירה](../../../Module04)
5. [דוגמה: המרת Qwen3 ל-ONNX INT4](../../../Module04)
6. [שימוש מתקדם](../../../Module04)
7. [מאגר מתכוני Olive](../../../Module04)
8. [שיטות עבודה מומלצות](../../../Module04)
9. [פתרון בעיות](../../../Module04)
10. [משאבים נוספים](../../../Module04)

## מבוא

Microsoft Olive הוא כלי עוצמתי וקל לשימוש לאופטימיזציה של מודלים, המותאם לחומרה, שמפשט את תהליך האופטימיזציה של מודלים ללמידת מכונה עבור פריסה על פני פלטפורמות חומרה שונות. בין אם אתם מכוונים ל-CPU, GPU או מאיצי AI ייעודיים, Olive עוזר לכם להשיג ביצועים מיטביים תוך שמירה על דיוק המודל.

## מה זה Microsoft Olive?

Olive הוא כלי אופטימיזציה למודלים המותאם לחומרה, המשלב טכניקות מובילות בתעשייה בתחומי דחיסת מודלים, אופטימיזציה וקומפילציה. הוא עובד עם ONNX Runtime כפתרון אופטימיזציה מקצה לקצה עבור ביצוע.

### תכונות עיקריות

- **אופטימיזציה מותאמת לחומרה**: בוחר אוטומטית את טכניקות האופטימיזציה הטובות ביותר עבור החומרה שלכם
- **40+ רכיבי אופטימיזציה מובנים**: כולל דחיסת מודלים, כימות, אופטימיזציה של גרפים ועוד
- **ממשק CLI קל לשימוש**: פקודות פשוטות למשימות אופטימיזציה נפוצות
- **תמיכה בריבוי מסגרות**: עובד עם PyTorch, מודלים של Hugging Face ו-ONNX
- **תמיכה במודלים פופולריים**: Olive יכול לאופטימיזציה אוטומטית של ארכיטקטורות מודלים פופולריות כמו Llama, Phi, Qwen, Gemma ועוד

### יתרונות

- **חיסכון בזמן פיתוח**: אין צורך להתנסות ידנית בטכניקות אופטימיזציה שונות
- **שיפורי ביצועים**: שיפורי מהירות משמעותיים (עד פי 6 במקרים מסוימים)
- **פריסה חוצת פלטפורמות**: מודלים מותאמים עובדים על פני חומרה ומערכות הפעלה שונות
- **שמירה על דיוק**: האופטימיזציות שומרות על איכות המודל תוך שיפור ביצועים

## התקנה

### דרישות מוקדמות

- Python 3.8 ומעלה
- מנהל חבילות pip
- סביבה וירטואלית (מומלץ)

### התקנה בסיסית

צרו והפעילו סביבה וירטואלית:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```
  
התקינו את Olive עם תכונות אופטימיזציה אוטומטיות:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```
  
### תלות אופציונלית

Olive מציע תלות אופציונלית עבור תכונות נוספות:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```
  
### אימות התקנה

```bash
olive --help
```
  
אם ההתקנה הצליחה, אתם אמורים לראות את הודעת העזרה של Olive CLI.

## מדריך התחלה מהירה

### האופטימיזציה הראשונה שלכם

בואו נאופטם מודל שפה קטן באמצעות תכונת האופטימיזציה האוטומטית של Olive:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
### מה הפקודה הזו עושה

תהליך האופטימיזציה כולל: השגת המודל מהמטמון המקומי, לכידת גרף ONNX ואחסון המשקלים בקובץ נתונים של ONNX, אופטימיזציה של גרף ONNX וכימות המודל ל-int4 באמצעות שיטת RTN.

### הסבר על פרמטרי הפקודה

- `--model_name_or_path`: מזהה מודל של Hugging Face או נתיב מקומי
- `--output_path`: תיקייה שבה יישמר המודל המותאם
- `--device`: חומרה יעד (cpu, gpu)
- `--provider`: ספק ביצוע (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: שימוש ב-ONNX Runtime Generate AI לביצוע
- `--precision`: דיוק כימות (int4, int8, fp16)
- `--log_level`: רמת פירוט ביומן (0=מינימלי, 1=מפורט)

## דוגמה: המרת Qwen3 ל-ONNX INT4

בהתבסס על הדוגמה שסופקה ב-Hugging Face בכתובת [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), כך ניתן לאופטם מודל Qwen3:

### שלב 1: הורדת מודל (אופציונלי)

כדי למזער זמן הורדה, שמרו רק קבצים חיוניים:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```
  
### שלב 2: אופטימיזציה של מודל Qwen3

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
### שלב 3: בדיקת המודל המותאם

צרו סקריפט Python פשוט לבדיקת המודל המותאם שלכם:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```
  
### מבנה הפלט

לאחר האופטימיזציה, תיקיית הפלט שלכם תכיל:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```
  

## שימוש מתקדם

### קבצי תצורה

עבור זרימות עבודה מורכבות יותר של אופטימיזציה, ניתן להשתמש בקבצי תצורה JSON:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```
  
הריצו עם תצורה:

```bash
olive run --config config.json
```
  
### אופטימיזציה ל-GPU

עבור אופטימיזציה ל-CUDA GPU:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
עבור DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```
  
### כיוונון עדין עם Olive

Olive תומך גם בכיוונון עדין של מודלים:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```
  

## שיטות עבודה מומלצות

### 1. בחירת מודל
- התחילו עם מודלים קטנים לבדיקות (למשל, 0.5B-7B פרמטרים)
- ודאו שארכיטקטורת המודל שלכם נתמכת על ידי Olive

### 2. שיקולי חומרה
- התאימו את יעד האופטימיזציה לחומרת הפריסה שלכם
- השתמשו באופטימיזציה ל-GPU אם יש לכם חומרה תואמת CUDA
- שקלו DirectML עבור מחשבים עם גרפיקה משולבת ב-Windows

### 3. בחירת דיוק
- **INT4**: דחיסה מקסימלית, איבוד דיוק קל
- **INT8**: איזון טוב בין גודל לדיוק
- **FP16**: איבוד דיוק מינימלי, הפחתת גודל מתונה

### 4. בדיקות ואימות
- תמיד בדקו מודלים מותאמים עם מקרי השימוש הספציפיים שלכם
- השוו מדדי ביצועים (זמן תגובה, תפוקה, דיוק)
- השתמשו בנתוני קלט מייצגים להערכה

### 5. אופטימיזציה איטרטיבית
- התחילו עם אופטימיזציה אוטומטית לתוצאות מהירות
- השתמשו בקבצי תצורה לשליטה מדויקת
- נסו טכניקות אופטימיזציה שונות

## פתרון בעיות

### בעיות נפוצות

#### 1. בעיות התקנה
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```
  
#### 2. בעיות CUDA/GPU
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```
  
#### 3. בעיות זיכרון
- השתמשו בגודל אצווה קטן יותר במהלך האופטימיזציה
- נסו כימות עם דיוק גבוה יותר תחילה (int8 במקום int4)
- ודאו שיש מספיק מקום בדיסק למטמון המודל

#### 4. שגיאות טעינת מודל
- בדקו את נתיב המודל והרשאות הגישה
- בדקו אם המודל דורש `trust_remote_code=True`
- ודאו שכל קבצי המודל הנדרשים הורדו

### קבלת עזרה

- **תיעוד**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **בעיות GitHub**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **דוגמאות**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## מאגר מתכוני Olive

### מבוא למתכוני Olive

מאגר [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) משלים את כלי Olive הראשי על ידי מתן אוסף מקיף של מתכוני אופטימיזציה מוכנים לשימוש עבור מודלים AI פופולריים. מאגר זה משמש כהפניה מעשית הן לאופטימיזציה של מודלים זמינים לציבור והן ליצירת זרימות עבודה של אופטימיזציה עבור מודלים קנייניים.

### תכונות עיקריות

- **100+ מתכונים מוכנים מראש**: תצורות אופטימיזציה מוכנות לשימוש עבור מודלים פופולריים
- **תמיכה בריבוי ארכיטקטורות**: כולל מודלים טרנספורמרים, מודלים חזותיים וארכיטקטורות מולטימודל
- **אופטימיזציות מותאמות לחומרה**: מתכונים מותאמים ל-CPU, GPU ומאיצים ייעודיים
- **משפחות מודלים פופולריות**: כולל Phi, Llama, Qwen, Gemma, Mistral ועוד

### משפחות מודלים נתמכות

המאגר כולל מתכוני אופטימיזציה עבור:

#### מודלים לשוניים
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 series (0.5B עד 14B)
- **Google Gemma**: תצורות מודלים שונות של Gemma
- **Mistral AI**: סדרת Mistral-7B
- **DeepSeek**: מודלים מסדרת R1-Distill

#### מודלים חזותיים ומולטימודל
- **Stable Diffusion**: v1.4, XL-base-1.0
- **מודלי CLIP**: תצורות שונות של CLIP-ViT
- **ResNet**: אופטימיזציות ל-ResNet-50
- **טרנספורמרים חזותיים**: ViT-base-patch16-224

#### מודלים מיוחדים
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: גרסאות בסיס ורב-לשוניות
- **טרנספורמרים למשפטים**: all-MiniLM-L6-v2

### שימוש במתכוני Olive

#### שיטה 1: שיבוט מתכון ספציפי

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```
  
#### שיטה 2: שימוש במתכון כתבנית

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```
  
### מבנה המתכון

כל תיקיית מתכון כוללת בדרך כלל:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```
  
### דוגמה: שימוש במתכון Phi-4-mini

בואו נשתמש במתכון Phi-4-mini כדוגמה:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```
  
קובץ התצורה כולל בדרך כלל:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```
  
### התאמת מתכונים

#### שינוי חומרה יעד

כדי לשנות את חומרת היעד, עדכנו את סעיף `systems`:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```
  
#### התאמת פרמטרי אופטימיזציה

שנו את סעיף `passes` עבור רמות אופטימיזציה שונות:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```
  
### יצירת מתכון משלכם

1. **התחילו עם מודל דומה**: מצאו מתכון למודל עם ארכיטקטורה דומה
2. **עדכנו את תצורת המודל**: שנו את שם/נתיב המודל בתצורה
3. **התאימו פרמטרים**: שנו פרמטרי אופטימיזציה לפי הצורך
4. **בדקו ואמתו**: הריצו את האופטימיזציה ואמתו תוצאות
5. **תרמו חזרה**: שקלו לתרום את המתכון שלכם למאגר

### יתרונות השימוש במתכונים

#### 1. **תצורות מוכחות**
- הגדרות אופטימיזציה שנבדקו עבור מודלים ספציפיים
- נמנע ניסוי וטעייה במציאת פרמטרים אופטימליים

#### 2. **כיוונון מותאם לחומרה**
- מותאם מראש לספקי ביצוע שונים
- תצורות מוכנות לשימוש עבור יעדי CPU, GPU ו-NPU

#### 3. **כיסוי מקיף**
- תומך במודלים הפתוחים הפופולריים ביותר
- עדכונים שוטפים עם שחרור מודלים חדשים

#### 4. **תרומות קהילתיות**
- פיתוח שיתופי עם קהילת ה-AI
- שיתוף ידע ושיטות עבודה מומלצות

### תרומה למתכוני Olive

אם אופטימיזתם מודל שאינו מכוסה במאגר:

1. **שיבטו את המאגר**: צרו שיבוט משלכם של olive-recipes
2. **צרו תיקיית מתכון**: הוסיפו תיקייה חדשה עבור המודל שלכם
3. **הוסיפו תצורה**: הוסיפו olive_config.json וקבצים תומכים
4. **תעדו שימוש**: ספקו README ברור עם הוראות
5. **שלחו בקשת משיכה**: תרמו חזרה לקהילה

### מדדי ביצועים

מתכונים רבים כוללים מדדי ביצועים המראים:
- **שיפורי זמן תגובה**: שיפור טיפוסי פי 2-6 לעומת הבסיס
- **הפחתת זיכרון**: הפחתת שימוש בזיכרון ב-50-75% עם כימות
- **שמירת דיוק**: שמירה על דיוק של 95-99%

### אינטגרציה עם כלי AI

המתכונים עובדים בצורה חלקה עם:
- **VS Code AI Toolkit**: אינטגרציה ישירה לאופטימיזציה של מודלים
- **Azure Machine Learning**: זרימות עבודה של אופטימיזציה מבוססות ענן
- **ONNX Runtime**: פריסת ביצוע מותאמת

## משאבים נוספים

### קישורים רשמיים
- **מאגר GitHub**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **מאגר מתכוני Olive**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **תיעוד ONNX Runtime**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **דוגמת Hugging Face**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### דוגמאות קהילתיות
- **מחברות Jupyter**: זמינות במאגר GitHub של Olive — https://github.com/microsoft/Olive/tree/main/examples
- **תוסף VS Code**: סקירה של AI Toolkit ל-VS Code — https://learn.microsoft.com/azure/ai-toolkit/overview
- **פוסטים בבלוג**: בלוג הקוד הפתוח של Microsoft — https://opensource.microsoft.com/blog/

### כלים קשורים
- **ONNX Runtime**: מנוע ביצוע בעל ביצועים גבוהים — https://onnxruntime.ai/
- **Hugging Face Transformers**: מקור למודלים רבים תואמים — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: זרימות עבודה של אופטימיזציה מבוססות ענן — https://learn.microsoft.com/azure/machine-learning/

## ➡️ מה הלאה

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**הצהרת אחריות**:  
מסמך זה תורגם באמצעות שירות תרגום AI [Co-op Translator](https://github.com/Azure/co-op-translator). למרות שאנו שואפים לדיוק, יש לקחת בחשבון שתרגומים אוטומטיים עשויים להכיל שגיאות או אי דיוקים. המסמך המקורי בשפתו המקורית צריך להיחשב כמקור סמכותי. עבור מידע קריטי, מומלץ להשתמש בתרגום מקצועי אנושי. אנו לא נושאים באחריות לכל אי הבנות או פרשנויות שגויות הנובעות משימוש בתרגום זה.