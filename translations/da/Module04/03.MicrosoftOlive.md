# Afsnit 3: Microsoft Olive Optimization Suite

## Indholdsfortegnelse
1. [Introduktion](../../../Module04)
2. [Hvad er Microsoft Olive?](../../../Module04)
3. [Installation](../../../Module04)
4. [Hurtig Start Guide](../../../Module04)
5. [Eksempel: Konvertering af Qwen3 til ONNX INT4](../../../Module04)
6. [Avanceret Brug](../../../Module04)
7. [Olive Opskriftsrepository](../../../Module04)
8. [Bedste Fremgangsmåder](../../../Module04)
9. [Fejlfinding](../../../Module04)
10. [Yderligere Ressourcer](../../../Module04)

## Introduktion

Microsoft Olive er et kraftfuldt og brugervenligt hardware-bevidst værktøj til modeloptimering, der gør det nemt at optimere maskinlæringsmodeller til implementering på forskellige hardwareplatforme. Uanset om du arbejder med CPU'er, GPU'er eller specialiserede AI-acceleratorer, hjælper Olive dig med at opnå optimal ydeevne, samtidig med at modelnøjagtigheden bevares.

## Hvad er Microsoft Olive?

Olive er et brugervenligt værktøj til hardware-bevidst modeloptimering, der samler førende teknikker inden for modelkomprimering, optimering og kompilering. Det fungerer med ONNX Runtime som en E2E-løsning til optimering af inferens.

### Nøglefunktioner

- **Hardware-bevidst optimering**: Vælger automatisk de bedste optimeringsteknikker til din målhardware
- **40+ indbyggede optimeringskomponenter**: Dækker modelkomprimering, kvantisering, grafoptimering og mere
- **Nem CLI-grænseflade**: Enkle kommandoer til almindelige optimeringsopgaver
- **Understøttelse af flere frameworks**: Fungerer med PyTorch, Hugging Face-modeller og ONNX
- **Understøttelse af populære modeller**: Olive kan automatisk optimere populære modelarkitekturer som Llama, Phi, Qwen, Gemma osv. direkte

### Fordele

- **Reduceret udviklingstid**: Ingen behov for manuelt at eksperimentere med forskellige optimeringsteknikker
- **Ydeevneforbedringer**: Betydelige hastighedsforbedringer (op til 6x i nogle tilfælde)
- **Platformuafhængig implementering**: Optimerede modeller fungerer på tværs af forskellige hardware og operativsystemer
- **Bevaret nøjagtighed**: Optimeringer bevarer modelkvaliteten, mens ydeevnen forbedres

## Installation

### Forudsætninger

- Python 3.8 eller nyere
- pip-pakkehåndtering
- Virtuelt miljø (anbefales)

### Grundlæggende Installation

Opret og aktiver et virtuelt miljø:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Installer Olive med auto-optimeringsfunktioner:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Valgfrie Afhængigheder

Olive tilbyder forskellige valgfrie afhængigheder for ekstra funktioner:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Bekræft Installation

```bash
olive --help
```

Hvis installationen lykkes, bør du se Olive CLI-hjælpebeskeden.

## Hurtig Start Guide

### Din Første Optimering

Lad os optimere en lille sprogmodel ved hjælp af Olives auto-optimeringsfunktion:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Hvad Denne Kommando Gør

Optimeringsprocessen indebærer: at hente modellen fra den lokale cache, fange ONNX-grafen og gemme vægtene i en ONNX-datafil, optimere ONNX-grafen og kvantisere modellen til int4 ved hjælp af RTN-metoden.

### Forklaring af Kommandoens Parametre

- `--model_name_or_path`: Hugging Face-modelidentifikator eller lokal sti
- `--output_path`: Mappe, hvor den optimerede model gemmes
- `--device`: Målhardware (cpu, gpu)
- `--provider`: Eksekveringsudbyder (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Brug ONNX Runtime Generate AI til inferens
- `--precision`: Kvantiseringspræcision (int4, int8, fp16)
- `--log_level`: Logningsdetaljeringsgrad (0=minimal, 1=detaljeret)

## Eksempel: Konvertering af Qwen3 til ONNX INT4

Baseret på det angivne Hugging Face-eksempel på [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), her er hvordan man optimerer en Qwen3-model:

### Trin 1: Download Model (Valgfrit)

For at minimere downloadtid, cache kun nødvendige filer:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Trin 2: Optimer Qwen3 Model

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Trin 3: Test Den Optimerede Model

Opret et simpelt Python-script for at teste din optimerede model:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Outputstruktur

Efter optimering vil din outputmappe indeholde:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Avanceret Brug

### Konfigurationsfiler

For mere komplekse optimeringsarbejdsgange kan du bruge JSON-konfigurationsfiler:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Kør med konfiguration:

```bash
olive run --config config.json
```

### GPU Optimering

For CUDA GPU-optimering:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

For DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Finjustering med Olive

Olive understøtter også finjustering af modeller:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Bedste Fremgangsmåder

### 1. Modelvalg
- Start med mindre modeller til test (f.eks. 0.5B-7B parametre)
- Sørg for, at din målmodelarkitektur understøttes af Olive

### 2. Hardwareovervejelser
- Match din optimeringsmål med din implementeringshardware
- Brug GPU-optimering, hvis du har CUDA-kompatibel hardware
- Overvej DirectML til Windows-maskiner med integreret grafik

### 3. Præcisionsvalg
- **INT4**: Maksimal komprimering, let tab af nøjagtighed
- **INT8**: God balance mellem størrelse og nøjagtighed
- **FP16**: Minimal tab af nøjagtighed, moderat størrelsesreduktion

### 4. Test og Validering
- Test altid optimerede modeller med dine specifikke anvendelsesscenarier
- Sammenlign ydeevnemålinger (latens, gennemløb, nøjagtighed)
- Brug repræsentative inputdata til evaluering

### 5. Iterativ Optimering
- Start med auto-optimering for hurtige resultater
- Brug konfigurationsfiler for finjusteret kontrol
- Eksperimenter med forskellige optimeringspasser

## Fejlfinding

### Almindelige Problemer

#### 1. Installationsproblemer
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU Problemer
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Hukommelsesproblemer
- Brug mindre batchstørrelser under optimering
- Prøv kvantisering med højere præcision først (int8 i stedet for int4)
- Sørg for tilstrækkelig diskplads til modelcaching

#### 4. Problemer med Modelloading
- Bekræft modelsti og adgangstilladelser
- Tjek om modellen kræver `trust_remote_code=True`
- Sørg for, at alle nødvendige modelfiler er downloadet

### Få Hjælp

- **Dokumentation**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Eksempler**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Opskriftsrepository

### Introduktion til Olive Opskrifter

Repositoryet [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) supplerer det primære Olive-værktøj ved at tilbyde en omfattende samling af færdige optimeringsopskrifter til populære AI-modeller. Dette repository fungerer som en praktisk reference både til optimering af offentligt tilgængelige modeller og til oprettelse af optimeringsarbejdsgange for proprietære modeller.

### Nøglefunktioner

- **100+ Forudbyggede Opskrifter**: Færdige optimeringskonfigurationer til populære modeller
- **Understøttelse af Flere Arkitekturer**: Dækker transformer-modeller, visionsmodeller og multimodale arkitekturer
- **Hardware-specifikke Optimeringer**: Opskrifter skræddersyet til CPU, GPU og specialiserede acceleratorer
- **Populære Modelfamilier**: Inkluderer Phi, Llama, Qwen, Gemma, Mistral og mange flere

### Understøttede Modelfamilier

Repositoryet inkluderer optimeringsopskrifter for:

#### Sproglige Modeller
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5-serien (0.5B til 14B)
- **Google Gemma**: Forskellige Gemma-modelkonfigurationer
- **Mistral AI**: Mistral-7B-serien
- **DeepSeek**: R1-Distill-seriemodeller

#### Visions- og Multimodale Modeller
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP Modeller**: Forskellige CLIP-ViT-konfigurationer
- **ResNet**: ResNet-50 optimeringer
- **Vision Transformers**: ViT-base-patch16-224

#### Specialiserede Modeller
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Basis- og flersprogede varianter
- **Sentence Transformers**: all-MiniLM-L6-v2

### Brug af Olive Opskrifter

#### Metode 1: Klon Specifik Opskrift

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Metode 2: Brug Opskrift som Skabelon

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Opskriftsstruktur

Hver opskriftsmappe indeholder typisk:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Eksempel: Brug af Phi-4-mini Opskrift

Lad os bruge Phi-4-mini opskriften som eksempel:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

Konfigurationsfilen indeholder typisk:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Tilpasning af Opskrifter

#### Ændring af Målhardware

For at ændre målhardware, opdater `systems` sektionen:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Justering af Optimeringsparametre

Rediger `passes` sektionen for forskellige optimeringsniveauer:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Oprettelse af Din Egen Opskrift

1. **Start med en Lignende Model**: Find en opskrift for en model med lignende arkitektur
2. **Opdater Modelkonfiguration**: Skift modelnavn/sti i konfigurationen
3. **Justér Parametre**: Tilpas optimeringsparametre efter behov
4. **Test og Valider**: Kør optimeringen og valider resultaterne
5. **Bidrag Tilbage**: Overvej at bidrage med din opskrift til repositoryet

### Fordele ved Brug af Opskrifter

#### 1. **Beviste Konfigurationer**
- Testede optimeringsindstillinger for specifikke modeller
- Undgår trial-and-error i at finde optimale parametre

#### 2. **Hardware-specifik Tuning**
- Forudoptimeret til forskellige eksekveringsudbydere
- Færdige konfigurationer til CPU, GPU og NPU-mål

#### 3. **Omfattende Dækning**
- Understøtter de mest populære open source-modeller
- Regelmæssige opdateringer med nye modeludgivelser

#### 4. **Fællesskabsbidrag**
- Samarbejdende udvikling med AI-fællesskabet
- Delte erfaringer og bedste praksis

### Bidrag til Olive Opskrifter

Hvis du har optimeret en model, der ikke er dækket i repositoryet:

1. **Fork Repositoryet**: Opret din egen fork af olive-recipes
2. **Opret Opskriftsmappe**: Tilføj en ny mappe til din model
3. **Inkluder Konfiguration**: Tilføj olive_config.json og understøttende filer
4. **Dokumentér Brug**: Giv en klar README med instruktioner
5. **Indsend Pull Request**: Bidrag tilbage til fællesskabet

### Ydeevne Benchmarks

Mange opskrifter inkluderer ydeevne benchmarks, der viser:
- **Latensforbedringer**: Typisk 2-6x hastighedsforøgelse i forhold til baseline
- **Reduktion af Hukommelse**: 50-75% reduktion i hukommelsesforbrug med kvantisering
- **Bevarelse af Nøjagtighed**: 95-99% bevarelse af nøjagtighed

### Integration med AI Toolkit

Opskrifterne fungerer problemfrit med:
- **VS Code AI Toolkit**: Direkte integration til modeloptimering
- **Azure Machine Learning**: Cloud-baserede optimeringsarbejdsgange
- **ONNX Runtime**: Optimeret inferensimplementering

## Yderligere Ressourcer

### Officielle Links
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Opskriftsrepository**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime Dokumentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Eksempel**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Fællesskabseksempler
- **Jupyter Notebooks**: Tilgængelige i Olive GitHub repository — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: AI Toolkit for VS Code oversigt — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blogindlæg**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Relaterede Værktøjer
- **ONNX Runtime**: Højtydende inferensmotor — https://onnxruntime.ai/
- **Hugging Face Transformers**: Kilde til mange kompatible modeller — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Cloud-baserede optimeringsarbejdsgange — https://learn.microsoft.com/azure/machine-learning/

## ➡️ Hvad er det næste

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hjælp af AI-oversættelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestræber os på nøjagtighed, skal du være opmærksom på, at automatiserede oversættelser kan indeholde fejl eller unøjagtigheder. Det originale dokument på dets oprindelige sprog bør betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig oversættelse. Vi er ikke ansvarlige for eventuelle misforståelser eller fejltolkninger, der opstår som følge af brugen af denne oversættelse.