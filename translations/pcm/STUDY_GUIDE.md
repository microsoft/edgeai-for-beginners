<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "f86e720f67bb196e2fb6625b2338a1fb",
  "translation_date": "2025-11-11T17:19:12+00:00",
  "source_file": "STUDY_GUIDE.md",
  "language_code": "pcm"
}
-->
# EdgeAI for Beginners: Learning Paths and Study Schedule

### Concentrated Learning Path (1 week)

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 0 | Module 0: Introduction to EdgeAI | 1-2 hours |
| Day 1 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 2 | Module 2: SLM Foundations | 3 hours |
| Day 3 | Module 3: SLM Deployment | 2 hours |
| Day 4-5 | Module 4: Model Optimization (6 frameworks) | 4 hours |
| Day 6 | Module 5: SLMOps | 3 hours |
| Day 7 | Module 6-7: AI Agents & Development Tools | 4 hours |
| Day 8 | Module 8: Foundry Local Toolkit (Modern Implementation) | 1 hour |

### Concentrated Learning Path (2 weeks)

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 1-2 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 3-4 | Module 2: SLM Foundations | 3 hours |
| Day 5-6 | Module 3: SLM Deployment | 2 hours |
| Day 7-8 | Module 4: Model Optimization | 4 hours |
| Day 9-10 | Module 5: SLMOps | 3 hours |
| Day 11-12 | Module 6: AI Agents | 2 hours |
| Day 13-14 | Module 7: Development Tools | 3 hours |

### Part-time Study (4 weeks)

| Week | Focus | Estimated Hours |
|------|-------|------------------|
| Week 1 | Module 1-2: Fundamentals & SLM Foundations | 6 hours |
| Week 2 | Module 3-4: Deployment & Optimization | 6 hours |
| Week 3 | Module 5-6: SLMOps & AI Agents | 5 hours |
| Week 4 | Module 7: Development Tools & Integration | 3 hours |

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 0 | Module 0: Introduction to EdgeAI | 1-2 hours |
| Day 1-2 | Module 1: EdgeAI Fundamentals | 3 hours |
| Day 3-4 | Module 2: SLM Foundations | 3 hours |
| Day 5-6 | Module 3: SLM Deployment | 2 hours |
| Day 7-8 | Module 4: Model Optimization | 4 hours |
| Day 9-10 | Module 5: SLMOps | 3 hours |
| Day 11-12 | Module 6: SLM Agentic Systems | 2 hours |
| Day 13-14 | Module 7: EdgeAI Implementation Samples | 2 hours |

| Module | Completion Date | Hours Spent | Key Takeaways |
|--------|----------------|-------------|--------------|
| Module 0: Introduction to EdgeAI | | | |
| Module 1: EdgeAI Fundamentals | | | |
| Module 2: SLM Foundations | | | |
| Module 3: SLM Deployment | | | |
| Module 4: Model Optimization (6 frameworks) | | | |
| Module 5: SLMOps | | | |
| Module 6: SLM Agentic Systems | | | |
| Module 7: EdgeAI Implementation Samples | | | |
| Hands-on Exercises | | | |
| Mini-Project | | | |

### Part-time Study (4 weeks)

| Week | Focus | Estimated Hours |
|------|-------|------------------|
| Week 1 | Module 1-2: Fundamentals & SLM Foundations | 6 hours |
| Week 2 | Module 3-4: Deployment & Optimization | 6 hours |
| Week 3 | Module 5-6: SLMOps & AI Agents | 5 hours |
| Week 4 | Module 7: Development Tools & Integration | 3 hours |

## Introduction

Welcome to the EdgeAI for Beginners study guide! This document go help you sabi how to waka through di course materials well-well and make sure say you learn di tin wey you need. E dey show you structured learning paths, study schedules wey dem suggest, key concept summaries, and extra resources wey go help you sabi Edge AI technology better.

Dis na short 20-hour course wey go teach you di main tin about EdgeAI quick-quick, e good for people wey dey busy or students wey wan sabi di practical skills for dis new area.

## Course Overview

Dis course get eight modules wey dey cover everything:

0. **Introduction to EdgeAI** - Di foundation and wetin e mean for di industry, plus wetin you go learn
1. **EdgeAI Fundamentals and Transformation** - Di main idea and di technology change
2. **Small Language Model Foundations** - Di different SLM families and how dem dey work
3. **Small Language Model Deployment** - How to deploy am for real-life
4. **Model Format Conversion and Quantization** - Advanced optimization with 6 frameworks like OpenVINO
5. **SLMOps - Small Language Model Operations** - How to manage production lifecycle and deployment
6. **SLM Agentic Systems** - AI agents, function calling, and Model Context Protocol
7. **EdgeAI Implementation Samples** - AI Toolkit, Windows development, and platform-specific implementations
8. **Microsoft Foundry Local – Complete Developer Toolkit** - Local-first development with hybrid Azure integration (Module 08)

## How to Use This Study Guide

- **Progressive Learning**: Follow di modules one by one so e go make sense
- **Knowledge Checkpoints**: Answer di self-assessment questions after each section
- **Hands-on Practice**: Do di exercises wey dem suggest to understand di theory well
- **Supplementary Resources**: Check extra materials for di topics wey you like pass

## Study Schedule Recommendations

### Concentrated Learning Path (1 week)

| Day | Focus | Estimated Hours |
|------|-------|------------------|
| Day 0 | Module 0: Introduction to EdgeAI | 1-2 hours |
| Day 1-2 | Module 1: EdgeAI Fundamentals | 6 hours |
| Day 3-4 | Module 2: SLM Foundations | 8 hours |
| Day 5 | Module 3: SLM Deployment | 3 hours |
| Day 6 | Module 8: Foundry Local Toolkit | 3 hours |

### Part-time Study (3 weeks)

| Week | Focus | Estimated Hours |
|------|-------|------------------|
| Week 1 | Module 0: Introduction + Module 1: EdgeAI Fundamentals | 7-9 hours |
| Week 2 | Module 2: SLM Foundations | 7-8 hours |
| Week 3 | Module 3: SLM Deployment (3h) + Module 8: Foundry Local Toolkit (2-3h) | 5-6 hours |

## Module 0: Introduction to EdgeAI

### Key Learning Objectives

- Sabi wetin Edge AI be and why e dey important for technology today
- Know di main industries wey Edge AI don change and di examples
- Understand di benefits of Small Language Models (SLMs) for edge deployment
- Get clear idea of wetin you go learn for di course
- See di career opportunities and skills wey you need for Edge AI

### Study Focus Areas

#### Section 1: Edge AI Paradigm and Definition
- **Priority Concepts**: 
  - Di difference between Edge AI and di normal cloud AI processing
  - How hardware, model optimization, and business dey work together
  - Real-time, privacy-preserving, and cost-efficient AI deployment

#### Section 2: Industry Applications
- **Priority Concepts**: 
  - Manufacturing & Industry 4.0: Predictive maintenance and quality control
  - Healthcare: Diagnostic imaging and patient monitoring
  - Autonomous Systems: Self-driving cars and transportation
  - Smart Cities: Traffic management and public safety
  - Consumer Technology: Smartphones, wearables, and smart homes

#### Section 3: Small Language Models Foundation
- **Priority Concepts**: 
  - SLM characteristics and performance comparisons
  - Parameter efficiency vs. capability trade-offs
  - Edge deployment constraints and optimization strategies

#### Section 4: Learning Framework and Career Path
- **Priority Concepts**: 
  - How di course dey structured and how you go sabi am step by step
  - Di technical skills and practical goals
  - Career opportunities and industry applications

### Self-Assessment Questions

1. Wetin be di three main technology trends wey make Edge AI possible?
2. Compare di benefits and challenges of Edge AI and cloud-based AI.
3. Name three industries wey Edge AI dey help and explain why.
4. How Small Language Models dey make Edge AI work for real-life deployment?
5. Wetin be di main technical skills wey you go learn for dis course?
6. Explain di four-phase learning approach wey dis course dey use.

### Hands-on Exercises

1. **Industry Research**: Pick one industry application and find out how dem dey use Edge AI for real-life (30 minutes)
2. **Model Exploration**: Check di Small Language Models wey dey Hugging Face and compare di parameter counts and capabilities (30 minutes)
3. **Learning Planning**: Look di course structure and plan your study schedule (15 minutes)

### Supplementary Materials

- [Edge AI Market Overview - McKinsey](https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/the-age-of-ai)
- [Small Language Models Overview - Hugging Face](https://huggingface.co/blog/small-language-models)
- [Edge Computing Foundation](https://www.edgecomputing.org/)

## Module 1: EdgeAI Fundamentals and Transformation

### Key Learning Objectives

- Understand di difference between cloud-based and edge-based AI
- Learn di main optimization techniques for environments wey no get plenty resources
- Study real-life examples of EdgeAI technologies
- Set up development environment for EdgeAI projects

### Study Focus Areas

#### Section 1: EdgeAI Fundamentals
- **Priority Concepts**: 
  - Di difference between Edge and Cloud computing
  - Model quantization techniques
  - Hardware acceleration options (NPUs, GPUs, CPUs)
  - Privacy and security benefits

- **Supplementary Materials**:
  - [TensorFlow Lite Documentation](https://www.tensorflow.org/lite)
  - [ONNX Runtime GitHub](https://github.com/microsoft/onnxruntime)
  - [Edge Impulse Documentation](https://docs.edgeimpulse.com)

#### Section 2: Real-World Case Studies
- **Priority Concepts**: 
  - Microsoft Phi & Mu model ecosystem
  - How dem dey use am for different industries
  - Deployment considerations

#### Section 3: Practical Implementation Guide
- **Priority Concepts**: 
  - How to set up development environment
  - Quantization and optimization tools
  - How to check EdgeAI implementations

#### Section 4: Edge Deployment Hardware
- **Priority Concepts**: 
  - Compare hardware platforms
  - Optimization strategies for di hardware
  - Deployment considerations

### Self-Assessment Questions

1. Compare cloud-based AI and edge-based AI implementations.
2. Explain three main techniques for optimizing models for edge deployment.
3. Wetin be di main benefits of running AI models at di edge?
4. Describe how to quantize a model and wetin e go do for performance.
5. Explain how different hardware accelerators (NPUs, GPUs, CPUs) dey affect EdgeAI deployment.

### Hands-on Exercises

1. **Quick Environment Setup**: Set up small development environment with di main packages (30 minutes)
2. **Model Exploration**: Download and check one pre-trained small language model (1 hour)
3. **Basic Quantization**: Try simple quantization on one small model (1 hour)

## Module 2: Small Language Model Foundations

### Key Learning Objectives

- Understand di architecture of different SLM families
- Compare model capabilities for different parameter sizes
- Check models based on efficiency, capability, and deployment needs
- Know di right use cases for different model families

### Study Focus Areas

#### Section 1: Microsoft Phi Model Family
- **Priority Concepts**: 
  - How dem design am
  - Efficiency-first architecture
  - Specialized capabilities

#### Section 2: Qwen Family
- **Priority Concepts**: 
  - Open source contributions
  - Scalable deployment options
  - Advanced reasoning architecture

#### Section 3: Gemma Family
- **Priority Concepts**: 
  - Research-driven innovation
  - Multimodal capabilities
  - Mobile optimization

#### Section 4: BitNET Family
- **Priority Concepts**: 
  - 1-bit quantization technology
  - Inference optimization framework
  - Sustainability considerations

#### Section 5: Microsoft Mu Model
- **Priority Concepts**: 
  - Device-first architecture
  - System integration with Windows
  - Privacy-preserving operation

#### Section 6: Phi-Silica
- **Priority Concepts**: 
  - NPU-optimized architecture
  - Performance metrics
  - Developer integration

### Self-Assessment Questions

1. Compare di architecture of Phi and Qwen model families.
2. Explain how BitNET's quantization technology dey different from di normal quantization.
3. Wetin be di special advantage wey Mu model get for Windows integration?
4. Explain how Phi-Silica dey use NPU hardware to make performance better.
5. For mobile app wey no dey get beta network, which model family go fit well and why?

### Hands-on Exercises

1. **Model Comparison**: Do quick benchmark for two different SLM models (1 hour)
2. **Simple Text Generation**: Try basic text generation with small model (1 hour)
3. **Fast Optimization**: Use one optimization technique to make inference speed fast (1 hour)

## Module 3: Small Language Model Deployment

### Key Learning Objectives

- Choose correct models based on deployment wahala
- Learn optimization techniques for different deployment situations
- Use SLMs for local and cloud environments
- Design production-ready setup for EdgeAI applications

### Study Focus Areas

#### Section 1: SLM Advanced Learning
- **Important Concepts**: 
  - Parameter classification framework
  - Advanced optimization techniques
  - Model acquisition strategies

#### Section 2: Local Environment Deployment
- **Important Concepts**: 
  - Ollama platform deployment
  - Microsoft Foundry local solutions
  - Framework comparative analysis

#### Section 3: Containerized Cloud Deployment
- **Important Concepts**: 
  - vLLM high-performance inference
  - Container orchestration
  - ONNX Runtime implementation

### Self-Assessment Questions

1. Wetin person go think about before choosing local deployment or cloud deployment?
2. Compare Ollama and Microsoft Foundry Local as deployment options.
3. Wetin be di benefit of containerization for SLM deployment?
4. Wetin be di key performance metrics wey person go monitor for edge-deployed SLM?
5. Explain full deployment workflow from model selection to production implementation.

### Hands-on Exercises

1. **Basic Local Deployment**: Deploy simple SLM using Ollama (1 hour)
2. **Performance Check**: Do quick benchmark for di model wey you deploy (30 minutes)
3. **Simple Integration**: Create small app wey go use di model wey you deploy (1 hour)

## Module 4: Model Format Conversion and Quantization

### Key Learning Objectives

- Learn advanced quantization techniques from 1-bit to 8-bit precision
- Understand format conversion strategies (GGUF, ONNX)
- Use optimization across six frameworks (Llama.cpp, Olive, OpenVINO, MLX, workflow synthesis)
- Deploy optimized models for production edge environments across Intel, Apple, and cross-platform hardware

### Study Focus Areas

#### Section 1: Quantization Foundations
- **Important Concepts**: 
  - Precision classification framework
  - Performance vs. accuracy trade-offs
  - Memory footprint optimization

#### Section 2: Llama.cpp Implementation
- **Important Concepts**: 
  - Cross-platform deployment
  - GGUF format optimization
  - Hardware acceleration techniques

#### Section 3: Microsoft Olive Suite
- **Important Concepts**: 
  - Hardware-aware optimization
  - Enterprise-grade deployment
  - Automated optimization workflows

#### Section 4: OpenVINO Toolkit
- **Important Concepts**: 
  - Intel hardware optimization
  - Neural Network Compression Framework (NNCF)
  - Cross-platform inference deployment
  - OpenVINO GenAI for LLM deployment

#### Section 5: Apple MLX Framework
- **Important Concepts**: 
  - Apple Silicon optimization
  - Unified memory architecture
  - LoRA fine-tuning capabilities

#### Section 6: Edge AI Development Workflow Synthesis
- **Important Concepts**: 
  - Unified workflow architecture
  - Framework selection decision trees
  - Production readiness validation
  - Future-proofing strategies

### Self-Assessment Questions

1. Compare quantization strategies for different precision levels (1-bit to 8-bit).
2. Explain di advantage of GGUF format for edge deployment.
3. How hardware-aware optimization for Microsoft Olive dey make deployment better?
4. Wetin be di key benefits of OpenVINO's NNCF for model compression?
5. Explain how Apple MLX dey use unified memory architecture for optimization.
6. How workflow synthesis dey help person choose di best optimization frameworks?

### Hands-on Exercises

1. **Model Quantization**: Try different quantization levels for model and compare results (1 hour)
2. **OpenVINO Optimization**: Use NNCF to compress model for Intel hardware (1 hour)
3. **Framework Comparison**: Test di same model across three different optimization frameworks (1 hour)
4. **Performance Benchmarking**: Measure di impact of optimization on inference speed and memory usage (1 hour)

## Module 5: SLMOps - Small Language Model Operations

### Key Learning Objectives

- Understand SLMOps lifecycle management principles
- Learn distillation and fine-tuning techniques for edge deployment
- Use production deployment strategies with monitoring
- Build enterprise-grade SLM operations and maintenance workflows

### Study Focus Areas

#### Section 1: Introduction to SLMOps
- **Important Concepts**: 
  - SLMOps paradigm shift in AI operations
  - Cost efficiency and privacy-first architecture
  - Strategic business impact and competitive advantages

#### Section 2: Model Distillation
- **Important Concepts**: 
  - Knowledge transfer techniques
  - Two-stage distillation process implementation
  - Azure ML distillation workflows

#### Section 3: Fine-tuning Strategies
- **Important Concepts**: 
  - Parameter-efficient fine-tuning (PEFT)
  - LoRA and QLoRA advanced methods
  - Multi-adapter training and hyperparameter optimization

#### Section 4: Production Deployment
- **Important Concepts**: 
  - Model conversion and quantization for production
  - Foundry Local deployment configuration
  - Performance benchmarking and quality validation

### Self-Assessment Questions

1. How SLMOps take different from traditional MLOps?
2. Explain di benefits of model distillation for edge deployment.
3. Wetin person go think about for fine-tuning SLMs in resource-constrained environments?
4. Explain full production deployment pipeline for edge AI applications.

### Hands-on Exercises

1. **Basic Distillation**: Create smaller model from bigger teacher model (1 hour)
2. **Fine-tuning Experiment**: Fine-tune model for specific domain (1 hour)
3. **Deployment Pipeline**: Set up basic CI/CD pipeline for model deployment (1 hour)

## Module 6: SLM Agentic Systems - AI Agents and Function Calling

### Key Learning Objectives

- Build smart AI agents for edge environments using Small Language Models
- Use function calling capabilities with systematic workflows
- Learn Model Context Protocol (MCP) integration for standardized tool interaction
- Create advanced agentic systems with small human intervention

### Study Focus Areas

#### Section 1: AI Agents and SLM Foundations
- **Important Concepts**: 
  - Agent classification framework (reflex, model-based, goal-based, learning agents)
  - SLM vs LLM trade-offs analysis
  - Edge-specific agent design patterns
  - Resource optimization for agents

#### Section 2: Function Calling in Small Language Models
- **Important Concepts**: 
  - Systematic workflow implementation (intent detection, JSON output, external execution)
  - Platform-specific implementations (Phi-4-mini, selected Qwen models, Microsoft Foundry Local)
  - Advanced examples (multi-agent collaboration, dynamic tool selection)
  - Production considerations (rate limiting, audit logging, security measures)

#### Section 3: Model Context Protocol (MCP) Integration
- **Important Concepts**: 
  - Protocol architecture and layered system design
  - Multi-backend support (Ollama for development, vLLM for production)
  - Connection protocols (STDIO and SSE modes)
  - Real-world applications (web automation, data processing, API integration)

### Self-Assessment Questions

1. Wetin be di key architectural things wey person go think about for edge AI agents?
2. How function calling dey make agent capabilities better?
3. Explain di role of Model Context Protocol for agent communication.

### Hands-on Exercises

1. **Simple Agent**: Build basic AI agent with function calling (1 hour)
2. **MCP Integration**: Use MCP for agent application (30 minutes)

## Workshop: Hands-On Learning Path

### Key Learning Objectives

- Build production-ready AI applications using Foundry Local SDK and best practices
- Use error handling and user feedback patterns well
- Create RAG pipelines with quality check and performance monitoring
- Develop multi-agent systems with coordinator patterns
- Learn smart model routing for task-based model selection
- Deploy local-first AI solutions with privacy-preserving architectures

### Study Focus Areas

#### Session 01: Getting Started with Foundry Local
- **Important Concepts**:
  - FoundryLocalManager SDK integration and automatic service discovery
  - Basic and streaming chat implementations
  - Error handling patterns and user feedback
  - Environment-based configuration

#### Session 02: Building AI Solutions with RAG
- **Important Concepts**:
  - In-memory vector embeddings with sentence-transformers
  - RAG pipeline implementation (retrieve → generate)
  - Quality evaluation with RAGAS metrics
  - Import safety for optional dependencies

#### Session 03: Open Source Models
- **Important Concepts**:
  - Multi-model benchmarking strategies
  - Latency and throughput measurements
  - Graceful degradation and error recovery
  - Performance comparison across model families

#### Session 04: Cutting-Edge Models
- **Important Concepts**:
  - SLM vs LLM comparison methodology
  - Type hints and comprehensive output formatting
  - Per-model error handling
  - Structured results for analysis

#### Session 05: AI-Powered Agents
- **Important Concepts**:
  - Multi-agent orchestration with coordinator pattern
  - Agent memory management and state tracking
  - Pipeline error handling and stage logging
  - Performance monitoring and statistics

#### Session 06: Models as Tools
- **Important Concepts**:
  - Intent detection and pattern matching
  - Keyword-based model routing algorithms
  - Multi-step pipelines (plan → execute → refine)
  - Comprehensive function documentation

### Self-Assessment Questions

1. How FoundryLocalManager dey make service management easy compared to manual REST calls?
2. Explain why import guards dey important for optional dependencies like sentence-transformers.
3. Wetin be di strategies wey go make sure multi-model benchmarking no go fail?
4. How coordinator pattern dey manage multiple specialist agents?
5. Explain di components of smart model router.
6. Wetin be di key things for production-ready error handling?

### Hands-on Exercises

1. **Chat Application**: Create streaming chat with error handling (45 minutes)
2. **RAG Pipeline**: Build small RAG with quality check (1 hour)
3. **Model Benchmarking**: Compare 3+ models for performance (1 hour)
4. **Multi-Agent System**: Create coordinator with 2 specialist agents (1.5 hours)
5. **Smart Router**: Build task-based model selection (1 hour)
6. **Production Deployment**: Add monitoring and full error handling (45 minutes)

### Time Allocation

**Concentrated Learning (1 week)**:
- Day 1: Session 01-02 (Chat + RAG) - 3 hours
- Day 2: Session 03-04 (Benchmarking + Comparison) - 3 hours
- Day 3: Session 05-06 (Agents + Routing) - 3 hours
- Day 4: Hands-on exercises and validation - 2 hours

**Part-time Study (2 weeks)**:
- Week 1: Sessions 01-03 (6 hours total)
- Week 2: Sessions 04-06 + exercises (5 hours total)

## Module 7: EdgeAI Implementation Samples

### Key Learning Objectives

- Learn AI Toolkit for Visual Studio Code for full EdgeAI development workflows
- Get knowledge about Windows AI Foundry platform and NPU optimization strategies
- Use EdgeAI for different hardware platforms and deployment situations
- Build production-ready EdgeAI applications with platform-specific optimizations

### Study Focus Areas

#### Section 1: AI Toolkit for Visual Studio Code
- **Important Concepts**: 
  - Full Edge AI development environment inside VS Code
  - Model catalog and discovery for edge deployment
  - Local testing, optimization, and agent development workflows
  - Performance monitoring and evaluation for edge situations

#### Section 2: Windows EdgeAI Development Guide
- **Important Concepts**: 
  - Windows AI Foundry platform full overview
  - Phi Silica API for better NPU inference
  - Computer Vision APIs for image processing and OCR
  - Foundry Local CLI for local development and testing

#### Section 3: Platform-Specific Implementations
- **Important Concepts**: 
  - NVIDIA Jetson Orin Nano deployment (67 TOPS AI performance)
  - Mobile applications with .NET MAUI and ONNX Runtime GenAI
  - Azure EdgeAI solutions with cloud-edge hybrid architecture
  - Windows ML optimization with universal hardware support
  - Foundry Local applications with privacy-focused RAG implementation

### Self-Assessment Questions

1. How AI Toolkit dey make EdgeAI development workflow easy?
2. Compare deployment strategies for different hardware platforms.
3. Wetin be di benefits of Windows AI Foundry for edge development?
4. Wetin NPU optimization dey do for modern edge AI apps na to make sure say AI dey run fast and smooth for devices wey dey close to users. E go help reduce how much power e dey use, make AI dey work better, and make sure say e fit handle plenty tasks without wahala.

5. Phi Silica API dey use NPU hardware to boost performance by making sure say e dey use the hardware well well. E go help AI apps run faster, use less power, and make sure say e fit handle heavy tasks without slowing down.

6. If you dey talk about privacy-sensitive apps, local deployment dey better because e no go need internet to work, so your data go dey safe. But cloud deployment fit give you more power and resources, especially if you dey handle big tasks. E just depend on wetin you need.

### Hands-on Exercises

1. **AI Toolkit Setup**: Set up AI Toolkit and make model better (1 hour)
2. **Windows AI Foundry**: Create simple Windows AI app with Phi Silica API (1 hour)
3. **Cross-Platform Deployment**: Put same model for two different platforms (1 hour)
4. **NPU Optimization**: Test how NPU dey perform with Windows AI Foundry tools (30 minutes)

## Module 8: Microsoft Foundry Local – Complete Developer Toolkit (Modernized)

### Key Learning Objectives

- Install and set up Foundry Local with modern SDK
- Use advanced multi-agent systems with coordinator patterns
- Create smart model routers wey go select tasks automatically
- Deploy AI solutions wey ready for production with monitoring
- Connect with Azure AI Foundry for hybrid deployment
- Learn modern SDK patterns with FoundryLocalManager and OpenAI client

### Study Focus Areas

#### Section 1: Modern Installation and Configuration
- **Important Things**: 
  - FoundryLocalManager SDK setup
  - Automatic service discovery and health monitoring
  - Configuration based on environment
  - Things to think about for production deployment

#### Section 2: Advanced Multi-Agent Systems
- **Important Things**: 
  - Coordinator pattern with specialist agents
  - Retrieval, reasoning, and execution agent specialization
  - Feedback loop for improvement
  - Monitoring performance and tracking statistics

#### Section 3: Intelligent Model Routing
- **Important Things**: 
  - Model selection based on keywords
  - Support for different models (general, reasoning, code, creative)
  - Flexible configuration with environment variables
  - Service health check and error handling

#### Section 4: Production-Ready Implementation
- **Important Things**: 
  - Handle errors well and fallback mechanisms
  - Monitor requests and track performance
  - Use Jupyter notebook examples with benchmarks
  - Patterns for connecting with existing apps

### Self-Assessment Questions

1. Wetin make FoundryLocalManager approach different from manual REST calls?
2. Explain how coordinator pattern dey work with specialist agents.
3. How intelligent router dey choose correct models based on query content?
4. Wetin be the main parts of production-ready AI agent system?
5. How you go take do health monitoring for Foundry Local services?
6. Compare the benefits of modernized approach with traditional implementation patterns.

### Hands-on Exercises

1. **Modern SDK Setup**: Set up FoundryLocalManager with automatic service discovery (30 minutes)
2. **Multi-Agent System**: Run advanced coordinator with specialist agents (30 minutes)
3. **Intelligent Routing**: Test model router with different query types (30 minutes)
4. **Interactive Exploration**: Use Jupyter notebooks to check advanced features (45 minutes)
5. **Production Deployment**: Set up monitoring and error handling patterns (30 minutes)
6. **Hybrid Integration**: Set up Azure AI Foundry fallback scenarios (30 minutes)

## Time Allocation Guide

To help you use the 30-hour course time well (including Workshop), here na how you fit share your time:

| Activity | Time Allocation | Description |
|----------|----------------|-------------|
| Reading Core Materials | 12 hours | Focus on the main ideas for each module |
| Hands-on Exercises | 10 hours | Practice key techniques (including Workshop) |
| Self-Assessment | 3 hours | Test your understanding with questions and reflection |
| Mini-Project | 5 hours | Use your knowledge for small practical work |

### Key Focus Areas by Time Constraint

**If you only get 10 hours:**
- Finish Module 0 (Introduction) and Modules 1, 2, and 3 (main EdgeAI ideas)
- Do at least one hands-on exercise for each module
- Focus on understanding the main ideas, no need for too much detail

**If you fit spend 20 hours:**
- Finish all eight modules (including Introduction)
- Do main hands-on exercises for each module
- Finish one mini-project from Module 7
- Check 2-3 extra resources

**If you get more than 20 hours:**
- Finish all modules (including Introduction) with detailed exercises
- Do plenty mini-projects
- Check advanced optimization techniques in Module 4
- Do production deployment from Module 5

## Essential Resources

These resources go help you learn fast:

### Must-Read Documentation
- [ONNX Runtime Getting Started](https://onnxruntime.ai/docs/get-started/with-python.html) - Best tool for model optimization
- [Ollama Quick Start](https://github.com/ollama/ollama#get-started) - Fast way to deploy SLMs locally
- [Microsoft Phi Model Card](https://huggingface.co/microsoft/phi-2) - Reference for top edge-optimized model
- [OpenVINO Documentation](https://docs.openvino.ai/2025/index.html) - Intel toolkit for optimization
- [AI Toolkit for VS Code](https://code.visualstudio.com/docs/intelligentapps/overview) - EdgeAI development environment
- [Windows AI Foundry](https://docs.microsoft.com/en-us/windows/ai/) - Windows EdgeAI development platform

### Time-Saving Tools
- [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) - Easy model access and deployment
- [Gradio](https://www.gradio.app/docs/interface) - Quick UI for AI demos
- [Microsoft Olive](https://github.com/microsoft/Olive) - Simple model optimization
- [Llama.cpp](https://github.com/ggml-ai/llama.cpp) - Efficient CPU inference
- [OpenVINO NNCF](https://github.com/openvinotoolkit/nncf) - Neural network compression framework
- [OpenVINO GenAI](https://github.com/openvinotoolkit/openvino.genai) - Toolkit for deploying large language models

## Progress Tracking Template

Use this template to track your learning progress for the 20-hour course:

| Module | Completion Date | Hours Spent | Key Takeaways |
|--------|----------------|-------------|---------------|
| Module 0: Introduction to EdgeAI | | | |
| Module 1: EdgeAI Fundamentals | | | |
| Module 2: SLM Foundations | | | |
| Module 3: SLM Deployment | | | |
| Module 4: Model Optimization | | | |
| Module 5: SLMOps | | | |
| Module 6: AI Agents | | | |
| Module 7: Development Tools | | | |
| Workshop: Hands-On Learning | | | |
| Module 8: Foundry Local Toolkit | | | |
| Hands-on Exercises | | | |
| Mini-Project | | | |

## Mini Project Ideas

Try one of these projects to practice EdgeAI ideas (each one go take 2-4 hours):

### Beginner Projects (2-3 hours each)
1. **Edge Text Assistant**: Make simple offline text completion tool with small language model
2. **Model Comparison Dashboard**: Create basic visualization for performance metrics of different SLMs
3. **Optimization Experiment**: Check how different quantization levels dey affect same model

### Intermediate Projects (3-4 hours each)
4. **AI Toolkit Workflow**: Use VS Code AI Toolkit to optimize and deploy model from start to finish
5. **Windows AI Foundry Application**: Create Windows app with Phi Silica API and NPU optimization
6. **Cross-Platform Deployment**: Deploy same optimized model for Windows (OpenVINO) and mobile (.NET MAUI)
7. **Function Calling Agent**: Build AI agent wey fit call functions for edge scenarios

### Advanced Integration Projects (4-5 hours each)
8. **OpenVINO Optimization Pipeline**: Do full model optimization with NNCF and GenAI toolkit
9. **SLMOps Pipeline**: Handle model lifecycle from training to edge deployment
10. **Multi-Model Edge System**: Deploy many specialized models wey dey work together for edge hardware
11. **MCP Integration System**: Build agentic system with Model Context Protocol for tool interaction

## References

- Microsoft Learn (Foundry Local)
  - Overview: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/
  - Get started: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/get-started
  - CLI reference: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/reference/reference-cli
  - Integrate with inference SDKs: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-integrate-with-inference-sdks
  - Open WebUI how-to: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-chat-application-with-open-web-ui
  - Compile Hugging Face models: https://learn.microsoft.com/en-us/azure/ai-foundry/foundry-local/how-to/how-to-compile-hugging-face-models
- Azure AI Foundry
  - Overview: https://learn.microsoft.com/en-us/azure/ai-foundry/
  - Agents (overview): https://learn.microsoft.com/en-us/azure/ai-services/agents/overview
- Optimization and Inference Tooling
  - Microsoft Olive (docs): https://microsoft.github.io/Olive/
  - Microsoft Olive (GitHub): https://github.com/microsoft/Olive
  - ONNX Runtime (getting started): https://onnxruntime.ai/docs/get-started/with-python.html
  - ONNX Runtime Olive integration: https://onnxruntime.ai/docs/performance/olive.html
  - OpenVINO (docs): https://docs.openvino.ai/2025/index.html
  - Apple MLX (docs): https://ml-explore.github.io/mlx/build/html/index.html
- Deployment Frameworks and Models
  - Llama.cpp: https://github.com/ggml-ai/llama.cpp
  - Hugging Face Transformers: https://huggingface.co/docs/transformers/index
  - vLLM (docs): https://docs.vllm.ai/
  - Ollama (quick start): https://github.com/ollama/ollama#get-started
- Developer Tools (Windows and VS Code)
  - AI Toolkit for VS Code: https://learn.microsoft.com/en-us/azure/ai-toolkit/overview
  - Windows ML (overview): https://learn.microsoft.com/en-us/windows/ai/new-windows-ml/overview

## Learning Community

Join the discussion and connect with other learners:
- GitHub Discussions for [EdgeAI for Beginners repository](https://github.com/microsoft/edgeai-for-beginners/discussions)
- [Microsoft Tech Community](https://techcommunity.microsoft.com/)
- [Stack Overflow](https://stackoverflow.com/questions/tagged/edge-ai)

## Conclusion

EdgeAI na the future of artificial intelligence wey dey bring strong capabilities directly to devices while e dey solve big problems like privacy, latency, and connectivity. This 20-hour course go give you the main knowledge and skills wey you need to start work with EdgeAI technologies sharp sharp.

The course dey short and focus on the most important ideas, so you fit learn fast without spending too much time. Remember say practice, even with small examples, na the best way to understand wetin you don learn.

Enjoy your learning journey!

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Disclaimer**:  
Dis dokyument don translate wit AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator). Even as we dey try make am accurate, abeg sabi say machine translation fit get mistake or no dey correct well. Di original dokyument for im native language na di main source wey you go trust. For important mata, na beta make you use professional human translation. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->