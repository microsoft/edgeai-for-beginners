# Section 03 - Model Context Protocol (MCP) Integration

## Wetin MCP (Model Context Protocol) Be

Model Context Protocol (MCP) na open-source standard wey AI apps fit use connect wit outside systems. Wit MCP, AI apps like Claude or ChatGPT fit connect wit data sources (like local files, databases), tools (like search engines, calculators), and workflows (like special prompts)—to help dem get beta information and do work.

Think of MCP like **USB-C port for AI apps**. As USB-C dey give one standard way to connect electronic devices, na so MCP dey give one standard way to connect AI apps wit outside systems.

### Wetin MCP Fit Do?

MCP dey open plenty beta tins for AI apps:

- **Personalized AI Assistants**: Agents fit connect wit your Google Calendar and Notion, dey act like one beta AI assistant wey sabi you well well.
- **Advanced Code Generation**: Claude Code fit create full web app from Figma design.
- **Enterprise Data Integration**: Enterprise chatbots fit connect wit plenty databases for one company, dey help users analyze data wit chat.
- **Creative Workflows**: AI models fit create 3D designs for Blender and print am wit 3D printer.
- **Real-time Information Access**: Connect wit outside data sources to get correct information wey dey up-to-date.
- **Complex Multi-step Operations**: Do big workflows wey dey combine plenty tools and systems.

### Why MCP Matter?

MCP dey benefit everybody for the ecosystem:

**For Developers**: MCP dey make am easy and fast to build or connect AI apps or agents.

**For AI Apps**: MCP dey give access to plenty data sources, tools, and apps wey go make the apps beta and improve how users go enjoy am.

**For End-users**: MCP dey make AI apps or agents sabi well well, fit access your data, and do tins for you when you need am.

## Small Language Models (SLMs) for MCP

Small Language Models na one smart way to use AI, and e get plenty benefits:

### Benefits of SLMs
- **Resource Efficiency**: E no dey use plenty computer power.
- **Faster Response Times**: E dey quick for real-time apps.
- **Cost Effectiveness**: E no need big infrastructure.
- **Privacy**: E fit run for your system without sending data outside.
- **Customization**: E easy to adjust am for specific work.

### Why SLMs and MCP Work Well Together

SLMs and MCP dey work like team, where the model sabi reason well and MCP dey add tools to make am do more work, even if the model no big.

## Python MCP SDK Overview

Python MCP SDK na the base wey you go use build MCP apps. E get:

- **Client Libraries**: To connect wit MCP servers.
- **Server Framework**: To create your own MCP servers.
- **Protocol Handlers**: To manage how dem go talk.
- **Tool Integration**: To run outside functions.

## Real-life Example: Phi-4 MCP Client

Make we look one example wey use Microsoft's Phi-4 mini model wit MCP.

### MCP Architecture Overview

MCP dey use **client-server architecture** where MCP host (like Claude Code or Claude Desktop) dey connect wit MCP servers. MCP host dey create one MCP client for each MCP server.

#### Key People for MCP

- **MCP Host**: Na the AI app wey dey manage MCP clients.
- **MCP Client**: Na the part wey dey connect wit MCP server and bring info for MCP host to use.
- **MCP Server**: Na the program wey dey give info to MCP clients.

#### Two-Layer Architecture

MCP get two main layers:

**Data Layer**: E dey use JSON-RPC protocol for client-server talk, e include:
- How dem go manage connection (start, agree on features).
- Main tins (tools, resources, prompts).
- Client features (sampling, elicitation, logging).
- Extra features (notifications, progress tracking).

**Transport Layer**: E dey define how dem go talk:
- **STDIO Transport**: E dey use input/output streams for local processes (e fast, no need network).
- **Streamable HTTP Transport**: E dey use HTTP POST wit Server-Sent Events for remote servers (e support normal HTTP authentication).

```
┌─────────────────────────────────────┐
│           MCP Host                  │
│     (AI Application)                │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Client 1                │
│  ┌─────────────────────────────────┐ │
│  │        Data Layer               │ │
│  │  ├── Lifecycle Management       │ │
│  │  ├── Primitives (Tools/Resources)│ │
│  │  └── Notifications              │ │
│  └─────────────────────────────────┘ │
│  ┌─────────────────────────────────┐ │
│  │      Transport Layer           │ │
│  │  ├── STDIO Transport           │ │
│  │  └── HTTP Transport            │ │
│  └─────────────────────────────────┘ │
└─────────────────┬───────────────────┘
                  │
┌─────────────────┴───────────────────┐
│         MCP Server 1                │
│    (Local/Remote Context Provider)  │
└─────────────────────────────────────┘
```

### MCP Core Primitives

MCP dey define the main tins wey servers fit share wit AI apps and wetin dem fit do.

#### Server Primitives

MCP get three main primitives wey servers fit show:

**Tools**: Functions wey AI apps fit use to do work.
- Examples: file operations, API calls, database queries.
- Methods: `tools/list`, `tools/call`.
- Fit discover and use am anytime.

**Resources**: Data wey dey give AI apps info.
- Examples: file contents, database records, API responses.
- Methods: `resources/list`, `resources/read`.
- Fit access structured data.

**Prompts**: Templates wey dey help arrange how AI go interact.
- Examples: system prompts, few-shot examples.
- Methods: `prompts/list`, `prompts/get`.
- E dey make AI interaction easy.

#### Client Primitives

MCP also get primitives wey clients fit show to make interaction beta:

**Sampling**: E dey allow servers request language model completions from the AI app.
- Method: `sampling/complete`.
- E dey make server development no depend on model.
- E dey give access to the host's language model.

**Elicitation**: E dey allow servers request more info from users.
- Method: `elicitation/request`.
- E dey make user interaction and confirmation easy.
- E dey help gather info.

**Logging**: E dey allow servers send log messages to clients.
- E dey help debug and monitor.
- E dey show wetin server dey do.

### MCP Protocol Lifecycle

#### Initialization and Capability Negotiation

MCP na protocol wey dey need management. The initialization process dey do plenty important tins:

1. **Protocol Version Negotiation**: E dey make sure client and server dey use the same protocol version (e.g., "2025-06-18").
2. **Capability Discovery**: Each side go show wetin dem fit do.
3. **Identity Exchange**: E dey show identification and version info.

```python
# Example initialization request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize",
  "params": {
    "protocolVersion": "2025-06-18",
    "capabilities": {
      "elicitation": {},  # Client supports user interaction
      "sampling": {}      # Client can provide LLM completions
    },
    "clientInfo": {
      "name": "edge-ai-client",
      "version": "1.0.0"
    }
  }
}
```

#### Tool Discovery and Execution

After initialization, clients fit find and use tools:

```python
# Discover available tools
tools_response = await session.list_tools()

# Execute a tool
result = await session.call_tool(
    "weather_current",
    {
        "location": "San Francisco",
        "units": "imperial"
    }
)
```

#### Real-time Notifications

MCP dey support real-time notifications for updates:

```python
# Server sends notification when tools change
{
  "jsonrpc": "2.0",
  "method": "notifications/tools/list_changed"
}

# Client responds by refreshing tool list
await session.list_tools()  # Get updated tools
```

## How to Start: Step-by-Step Guide

### Step 1: Environment Setup

Install wetin you need:
```bash
pip install fastmcp mcp-python-client openai requests pyautogui Pillow
```

### Step 2: Basic Configuration

Set your environment variables:
```python
# System Configuration
SYSTEM_PROMPT = "You are an AI assistant with some tools."

# Ollama Configuration (Local)
OLLAMA_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL_ID = "phi4-mini:3.8b-fp16"

# vLLM Configuration (Server)
VLLM_URL = "http://localhost:8000/v1"
VLLM_MODEL_ID = "microsoft/Phi-4-mini-instruct"
```

### Step 3: Run Your First MCP Client

**Basic Ollama Setup:**
```bash
python ghmodel_mcp_demo.py
```

**Using vLLM Backend:**
```bash
python ghmodel_mcp_demo.py --env vllm
```

**Server-Sent Events Connection:**
```bash
python ghmodel_mcp_demo.py --run sse
```

**Custom MCP Server:**
```bash
python ghmodel_mcp_demo.py --server /path/to/server.py
```

### Step 4: Programmatic Usage

```python
import asyncio
from ghmodel_mcp_demo import OllamaClient, Phi4MiniMCPClient

async def automated_interaction():
    # Configure MCP server parameters
    server_params = StdioServerParameters(
        command="npx",
        args=["@playwright/mcp@latest"],
        env=None,
    )
    
    # Create MCP client and process tools
    async with Phi4MiniMCPClient(server_params) as mcp_client:
        tools = await process_mcp_tools(mcp_client)
        llm_client = OllamaClient()
        
        # Generate response with tool capabilities
        response, messages = await llm_client.generate_response(
            "Help me automate a web task",
            tools
        )
        return response

# Execute the automation
result = asyncio.run(automated_interaction())
print(result)
```

## Advanced Features

### Multi-Backend Support

You fit use both Ollama and vLLM backends, depending on wetin you need:

- **Ollama**: E good for local development and testing.
- **vLLM**: E dey work well for production and high-speed work.

### Flexible Connection Protocols

Two connection modes dey:

**STDIO Mode**: Direct process communication.
- E fast.
- E good for local tools.
- E easy to set up.

**SSE Mode**: HTTP-based streaming.
- E fit work for network.
- E good for systems wey dey far.
- E dey give real-time updates.

### Tool Integration Capabilities

The system fit work wit different tools:
- Web automation (Playwright).
- File operations.
- API interactions.
- System commands.
- Custom functions.

## Error Handling and Best Practices

### How to Manage Errors Well

The system get strong error handling for:

**Connection Errors:**
- MCP server problems.
- Network timeouts.
- Connection wahala.

**Tool Execution Errors:**
- Tools wey no dey.
- Wrong parameters.
- Execution problems.

**Response Processing Errors:**
- JSON parsing wahala.
- Format no correct.
- LLM response no make sense.

### Best Practices

1. **Resource Management**: Use async context managers.
2. **Error Handling**: Put try-catch blocks well.
3. **Logging**: Use correct logging levels.
4. **Security**: Check inputs and clean outputs.
5. **Performance**: Use connection pooling and caching.

## Real-life Examples

### Web Automation
```python
# Example: Automated web testing
async def web_automation_example():
    tools = await setup_playwright_tools()
    response = await llm_client.generate_response(
        "Navigate to example.com and take a screenshot",
        tools
    )
```

### Data Processing
```python
# Example: File analysis
async def data_processing_example():
    tools = await setup_file_tools()
    response = await llm_client.generate_response(
        "Analyze the CSV file and generate a summary report",
        tools
    )
```

### API Integration
```python
# Example: API interactions
async def api_integration_example():
    tools = await setup_api_tools()
    response = await llm_client.generate_response(
        "Fetch weather data and create a forecast summary",
        tools
    )
```

## How to Make Performance Beta

### Memory Management
- Manage message history well.
- Clean resources well.
- Use connection pooling.

### Network Optimization
- Use async HTTP operations.
- Set timeouts wey you fit change.
- Recover well from errors.

### Concurrent Processing
- Use non-blocking I/O.
- Run tools at the same time.
- Use smart async patterns.

## Security Tins

### How to Protect Data
- Keep API keys safe.
- Check inputs.
- Clean outputs.

### Network Security
- Use HTTPS.
- Default to local endpoints.
- Handle tokens well.

### Safe Execution
- Filter tools.
- Use sandbox environments.
- Log actions for audit.

## MCP Ecosystem and Development

### Wetin MCP Project Cover

The Model Context Protocol ecosystem get plenty important parts:

- **[MCP Specification](https://modelcontextprotocol.io/specification/latest)**: Official spec wey dey show how to implement clients and servers.
- **[MCP SDKs](https://modelcontextprotocol.io/docs/sdk)**: SDKs for different programming languages wey dey use MCP.
- **MCP Development Tools**: Tools wey dey help build MCP servers and clients, like [MCP Inspector](https://github.com/modelcontextprotocol/inspector).
- **[MCP Reference Server Implementations](https://github.com/modelcontextprotocol/servers)**: Reference implementations of MCP servers.

### How to Start MCP Development

To start wit MCP:

**Build Servers**: [Create MCP servers](https://modelcontextprotocol.io/docs/develop/build-server) to show your data and tools.

**Build Clients**: [Develop apps](https://modelcontextprotocol.io/docs/develop/build-client) wey go connect wit MCP servers.

**Learn Concepts**: [Understand the main ideas](https://modelcontextprotocol.io/docs/learn/architecture) and MCP architecture.

## Conclusion

SLMs wit MCP dey change how people dey build AI apps. By joining small models wit outside tools, developers fit create smart systems wey no dey use plenty resources but sabi well well.

Model Context Protocol dey give one standard way to connect AI apps wit outside systems, just like USB-C dey give one universal connection for electronic devices. This standard dey make:

- **Easy Integration**: Connect AI models wit different data sources and tools.
- **Ecosystem Growth**: Build once, use for plenty AI apps.
- **Beta Capabilities**: Add outside functionality to SLMs.
- **Real-time Updates**: Support dynamic, responsive AI apps.

Key points:
- MCP na open standard wey dey connect AI apps wit outside systems.
- The protocol dey support tools, resources, and prompts as main tins.
- Real-time notifications dey make apps dynamic and responsive.
- Manage lifecycle and errors well for production use.
- The ecosystem get complete SDKs and tools for development.

## References and More Reading

### Official MCP Documentation

- **[Model Context Protocol Official Site](https://modelcontextprotocol.io/)** - Full documentation and specs.
- **[MCP Getting Started Guide](https://modelcontextprotocol.io/docs/getting-started/intro)** - Introduction and main ideas.
- **[MCP Architecture Overview](https://modelcontextprotocol.io/docs/learn/architecture)** - Detailed technical architecture.
- **[MCP Specification](https://modelcontextprotocol.io/specification/latest)** - Official protocol spec.
- **[MCP SDKs Documentation](https://modelcontextprotocol.io/docs/sdk)** - SDK guides for different languages.

### Development Resources

- **[MCP for Beginners](https://aka.ms/mcp-for-beginners)** - Beginner's guide to MCP.
- **[MCP GitHub Organization](https://github.com/modelcontextprotocol)** - Official repos and examples.
- **[MCP Server Repository](https://github.com/modelcontextprotocol/servers)** - Reference server implementations.
- **[MCP Inspector](https://github.com/modelcontextprotocol/inspector)** - Development and debugging tool.
- **[Build MCP Servers Guide](https://modelcontextprotocol.io/docs/develop/build-server)** - Server development tutorial.
- **[Build MCP Clients Guide](https://modelcontextprotocol.io/docs/develop/build-client)** - Client development tutorial.

### Small Language Models and Edge AI

- **[Microsoft Phi Models](https://aka.ms/phicookbook)** - Phi model family.
- **[Foundry Local Documentation](https://github.com/microsoft/Foundry-Local)** - Microsoft's edge AI runtime.
- **[Ollama Documentation](https://ollama.ai/docs)** - Platform wey dey for local LLM deployment
- **[vLLM Documentation](https://docs.vllm.ai/)** - High-performance LLM serving

### Technical Standards and Protocols

- **[JSON-RPC 2.0 Specification](https://www.jsonrpc.org/)** - Na di RPC protocol wey MCP dey use
- **[JSON Schema](https://json-schema.org/)** - Standard wey dey define schema for MCP tools
- **[OpenAPI Specification](https://swagger.io/specification/)** - Standard for API documentation
- **[Server-Sent Events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)** - Web standard wey dey for real-time updates

### AI Agent Development

- **[Microsoft Agent Framework](https://github.com/microsoft/agent-framework)** - Framework wey ready for production agent development
- **[LangChain Documentation](https://docs.langchain.com/)** - Framework for agent and tool integration
- **[Semantic Kernel](https://learn.microsoft.com/en-us/semantic-kernel/)** - Microsoft's SDK for AI orchestration

### Industry Reports and Research

- **[Anthropic's Model Context Protocol Announcement](https://www.anthropic.com/news/model-context-protocol)** - Di first MCP introduction
- **[Small Language Models Survey](https://arxiv.org/abs/2410.20011)** - Academic survey wey dey talk about SLM research
- **[Edge AI Market Analysis](https://www.marketsandmarkets.com/Market-Reports/edge-ai-software-market-74385617.html)** - Industry trends and forecasts
- **[AI Agent Development Best Practices](https://arxiv.org/abs/2309.02427)** - Research wey dey focus on agent architectures

Dis section go help you build your own SLM-powered MCP applications, wey fit open door for automation, data processing, and intelligent system integration.

## ➡️ Wetin dey next

- [Module 7. Edge AI samples](../Module07/README.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Disclaimer**:  
Dis dokyument don use AI translation service [Co-op Translator](https://github.com/Azure/co-op-translator) do di translation. Even as we dey try make am accurate, abeg sabi say automated translations fit get mistake or no dey correct well. Di original dokyument for im native language na di one wey you go take as di correct source. For important information, e better make professional human translation dey use. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->