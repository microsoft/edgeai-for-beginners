# Section 3 : Microsoft Olive Optimization Suite

## Table of Contents
1. [Introduction](../../../Module04)
2. [Wetin be Microsoft Olive?](../../../Module04)
3. [Installation](../../../Module04)
4. [Quick Start Guide](../../../Module04)
5. [Example: How to change Qwen3 to ONNX INT4](../../../Module04)
6. [Advanced Usage](../../../Module04)
7. [Olive Recipes Repository](../../../Module04)
8. [Best Practices](../../../Module04)
9. [Troubleshooting](../../../Module04)
10. [Additional Resources](../../../Module04)

## Introduction

Microsoft Olive na one strong, easy-to-use toolkit wey sabi optimize machine learning models for different hardware platforms. Whether na CPUs, GPUs, or AI accelerators you wan use, Olive go help you get better performance and still keep model accuracy.

## Wetin be Microsoft Olive?

Olive na easy-to-use tool wey sabi optimize machine learning models based on the hardware wey you wan use. E dey combine better techniques for model compression, optimization, and compilation. E dey work with ONNX Runtime as one end-to-end solution for inference optimization.

### Key Features

- **Hardware-Aware Optimization**: E go automatically choose the best optimization techniques for the hardware wey you wan use.
- **40+ Built-in Optimization Components**: E cover model compression, quantization, graph optimization, and plenty more.
- **Easy CLI Interface**: Simple commands wey fit handle common optimization tasks.
- **Multi-Framework Support**: E dey work with PyTorch, Hugging Face models, and ONNX.
- **Popular Model Support**: Olive fit optimize popular model architectures like Llama, Phi, Qwen, Gemma, etc without stress.

### Benefits

- **Reduced Development Time**: You no need dey try different optimization techniques by yourself.
- **Performance Gains**: E fit make your model run faster (up to 6x improvement sometimes).
- **Cross-Platform Deployment**: Optimized models go work for different hardware and operating systems.
- **Maintained Accuracy**: E go still keep the model quality while e dey improve performance.

## Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- Virtual environment (recommended)

### Basic Installation

Create and activate virtual environment:

```bash
# Create virtual environment
python -m venv olive-env

# Activate virtual environment
# On Windows:
olive-env\Scripts\activate
# On macOS/Linux:
source olive-env/bin/activate
```

Install Olive with auto-optimization features:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### Optional Dependencies

Olive get optional dependencies for extra features:

```bash
# For Azure ML integration
pip install olive-ai[azureml]

# For DirectML (Windows GPU acceleration)
pip install olive-ai[directml]

# For CPU optimization
pip install olive-ai[cpu]

# For all features
pip install olive-ai[all]
```

### Verify Installation

```bash
olive --help
```

If e work, you go see Olive CLI help message.

## Quick Start Guide

### Your First Optimization

Make we optimize one small language model using Olive auto-optimization feature:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Wetin Dis Command Dey Do

Dis optimization process go: collect the model from local cache, capture the ONNX Graph and store the weights inside ONNX data file, optimize the ONNX Graph, and quantize the model to int4 using RTN method.

### Command Parameters Explained

- `--model_name_or_path`: Hugging Face model identifier or local path
- `--output_path`: Folder wey optimized model go dey.
- `--device`: Target device (cpu, gpu)
- `--provider`: Execution provider (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: Use ONNX Runtime Generate AI for inference
- `--precision`: Quantization precision (int4, int8, fp16)
- `--log_level`: Logging verbosity (0=minimal, 1=verbose)

## Example: How to change Qwen3 to ONNX INT4

Based on the Hugging Face example wey dey [lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU), na so you go fit optimize Qwen3 model:

### Step 1: Download Model (Optional)

To reduce download time, cache only the files wey you need:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### Step 2: Optimize Qwen3 Model

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Step 3: Test the Optimized Model

Write one simple Python script to test your optimized model:

```python
import onnxruntime_genai as og

# Load the optimized model
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# Create a chat template
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# Generate text
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### Output Structure

After optimization, your output folder go get:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## Advanced Usage

### Configuration Files

For more complex optimization workflows, you fit use JSON configuration files:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

Run with configuration:

```bash
olive run --config config.json
```

### GPU Optimization

For CUDA GPU optimization:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

For DirectML (Windows):

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### Fine-tuning with Olive

Olive fit also fine-tune models:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## Best Practices

### 1. Model Selection
- Start with smaller models for testing (e.g., 0.5B-7B parameters)
- Make sure say Olive support your target model architecture.

### 2. Hardware Considerations
- Match your optimization target to the hardware wey you wan deploy am.
- Use GPU optimization if you get CUDA-compatible hardware.
- Consider DirectML for Windows machines wey get integrated graphics.

### 3. Precision Selection
- **INT4**: Maximum compression, small accuracy loss.
- **INT8**: Good balance between size and accuracy.
- **FP16**: Minimal accuracy loss, moderate size reduction.

### 4. Testing and Validation
- Always test optimized models with your specific use cases.
- Compare performance metrics (latency, throughput, accuracy).
- Use input data wey represent wetin you go use for real life.

### 5. Iterative Optimization
- Start with auto-optimization for quick results.
- Use configuration files for fine-grained control.
- Try different optimization passes.

## Troubleshooting

### Common Issues

#### 1. Installation Problems
```bash
# If you encounter dependency conflicts:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU Issues
```bash
# Verify CUDA installation:
nvidia-smi

# Install correct ONNX Runtime GPU package:
pip install onnxruntime-gpu
```

#### 3. Memory Issues
- Use smaller batch sizes during optimization.
- Try quantization with higher precision first (int8 instead of int4).
- Make sure say you get enough disk space for model caching.

#### 4. Model Loading Errors
- Check model path and access permissions.
- Confirm if model need `trust_remote_code=True`.
- Make sure say you don download all the model files wey you need.

### Getting Help

- **Documentation**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub Issues**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **Examples**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## Olive Recipes Repository

### Introduction to Olive Recipes

The [microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) repository dey complement the main Olive toolkit by providing plenty ready-to-use optimization recipes for popular AI models. E dey serve as practical reference for optimizing public models and creating workflows for private models.

### Key Features

- **100+ Pre-built Recipes**: Ready-to-use optimization configurations for popular models.
- **Multi-Architecture Support**: E cover transformer models, vision models, and multimodal architectures.
- **Hardware-Specific Optimizations**: Recipes wey dey tailored for CPU, GPU, and specialized accelerators.
- **Popular Model Families**: Includes Phi, Llama, Qwen, Gemma, Mistral, and plenty more.

### Supported Model Families

The repository get optimization recipes for:

#### Language Models
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 series (0.5B to 14B)
- **Google Gemma**: Different Gemma model configurations
- **Mistral AI**: Mistral-7B series
- **DeepSeek**: R1-Distill series models

#### Vision and Multimodal Models
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP Models**: Different CLIP-ViT configurations
- **ResNet**: ResNet-50 optimizations
- **Vision Transformers**: ViT-base-patch16-224

#### Specialized Models
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: Base and multilingual variants
- **Sentence Transformers**: all-MiniLM-L6-v2

### Using Olive Recipes

#### Method 1: Clone Specific Recipe

```bash
# Clone the recipes repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# Navigate to a specific model recipe
cd microsoft-Phi-4-mini-instruct

# Run the optimization
olive run --config olive_config.json
```

#### Method 2: Use Recipe as Template

```bash
# Copy a recipe configuration for your model
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# Modify the configuration for your needs
# Update model paths, optimization parameters, etc.

# Run with your custom configuration
olive run --config my_config.json
```

### Recipe Structure

Each recipe folder go get:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### Example: Using Phi-4-mini Recipe

Make we use Phi-4-mini recipe as example:

```bash
# Clone the repository
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# Install dependencies
pip install -r requirements.txt

# Run the optimization
olive run --config olive_config.json
```

The configuration file go get:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### Customizing Recipes

#### Modifying Target Hardware

To change the target hardware, update the `systems` section:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### Adjusting Optimization Parameters

Change the `passes` section for different optimization levels:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### Creating Your Own Recipe

1. **Start with Similar Model**: Find recipe for model wey get similar architecture.
2. **Update Model Configuration**: Change the model name/path for the configuration.
3. **Adjust Parameters**: Change optimization parameters as you need.
4. **Test and Validate**: Run the optimization and check the results.
5. **Contribute Back**: Share your recipe with the community.

### Benefits of Using Recipes

#### 1. **Proven Configurations**
- Tested optimization settings for specific models.
- E go save you time wey you for use dey try different parameters.

#### 2. **Hardware-Specific Tuning**
- Pre-optimized for different execution providers.
- Ready-to-use configurations for CPU, GPU, and NPU targets.

#### 3. **Comprehensive Coverage**
- E support the most popular open-source models.
- Regular updates with new model releases.

#### 4. **Community Contributions**
- Collaborative development with the AI community.
- Shared knowledge and best practices.

### Contributing to Olive Recipes

If you don optimize model wey no dey the repository:

1. **Fork the Repository**: Create your own fork of olive-recipes.
2. **Create Recipe Directory**: Add new folder for your model.
3. **Include Configuration**: Add olive_config.json and other files wey go help.
4. **Document Usage**: Write clear README with instructions.
5. **Submit Pull Request**: Share your recipe with the community.

### Performance Benchmarks

Plenty recipes dey show performance benchmarks like:
- **Latency Improvements**: Typical 2-6x speedup over baseline.
- **Memory Reduction**: 50-75% memory usage reduction with quantization.
- **Accuracy Retention**: 95-99% accuracy preservation.

### Integration with AI Toolkit

The recipes dey work well with:
- **VS Code AI Toolkit**: Direct integration for model optimization.
- **Azure Machine Learning**: Cloud-based optimization workflows.
- **ONNX Runtime**: Optimized inference deployment.

## Additional Resources

### Official Links
- **GitHub Repository**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **Olive Recipes Repository**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX Runtime Documentation**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face Example**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### Community Examples
- **Jupyter Notebooks**: Dey available for Olive GitHub repository — https://github.com/microsoft/Olive/tree/main/examples
- **VS Code Extension**: AI Toolkit for VS Code overview — https://learn.microsoft.com/azure/ai-toolkit/overview
- **Blog Posts**: Microsoft Open Source Blog — https://opensource.microsoft.com/blog/

### Related Tools
- **ONNX Runtime**: High-performance inference engine — https://onnxruntime.ai/
- **Hugging Face Transformers**: Source of many compatible models — https://huggingface.co/docs/transformers/index
- **Azure Machine Learning**: Cloud-based optimization workflows — https://learn.microsoft.com/azure/machine-learning/


## ➡️ Wetin next

- [04: OpenVINO Toolkit Optimization Suite](./04.openvino.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Disclaimer**:  
Dis dokyument don use AI transleto service [Co-op Translator](https://github.com/Azure/co-op-translator) do di translation. Even as we dey try make am accurate, abeg sabi say machine translation fit get mistake or no dey correct well. Di original dokyument for im native language na di main source wey you go trust. For important mata, e better make professional human transleto check am. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translation.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->