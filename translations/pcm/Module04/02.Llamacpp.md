<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "2a3860eff326dd58983100cc3d5dc685",
  "translation_date": "2025-11-11T17:32:59+00:00",
  "source_file": "Module04/02.Llamacpp.md",
  "language_code": "pcm"
}
-->
# Section 2 : Llama.cpp Implementation Guide

## Table of Contents
1. [Introduction](../../../Module04)
2. [Wetin be Llama.cpp?](../../../Module04)
3. [How to Install](../../../Module04)
4. [How to Build from Source](../../../Module04)
5. [Model Quantization](../../../Module04)
6. [Basic Usage](../../../Module04)
7. [Advanced Features](../../../Module04)
8. [Python Integration](../../../Module04)
9. [Troubleshooting](../../../Module04)
10. [Best Practices](../../../Module04)

## Introduction

Dis tutorial go show you everything wey you need sabi about Llama.cpp, from how to install am to how to use am for advanced things. Llama.cpp na strong C++ implementation wey dey make Large Language Models (LLMs) run well with small setup and better performance for different hardware.

## Wetin be Llama.cpp?

Llama.cpp na framework wey dem write with C/C++ wey fit run big language models for your computer with small setup and better performance. E get plenty features like:

### Main Features
- **C/C++ implementation** wey no need extra software
- **E dey work for different systems** (Windows, macOS, Linux)
- **E dey optimize hardware** for different types
- **Quantization support** (1.5-bit to 8-bit integer quantization)
- **E fit use CPU and GPU** for speed
- **E dey save memory** for places wey no get plenty space

### Advantages
- E dey run well for CPU even if you no get special hardware
- E support different GPU systems (CUDA, Metal, OpenCL, Vulkan)
- E light and portable
- Apple silicon dey work well with am - e dey use ARM NEON, Accelerate and Metal frameworks
- E support different quantization levels to save memory

## How to Install

### Method 1: Pre-built Binaries (E easy for beginners)

#### Download from GitHub Releases
1. Go [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases)
2. Download the correct file for your system:
   - `llama-<version>-bin-win-<feature>-<arch>.zip` for Windows
   - `llama-<version>-bin-macos-<feature>-<arch>.zip` for macOS
   - `llama-<version>-bin-linux-<feature>-<arch>.zip` for Linux

3. Extract the file and add the folder to your system PATH

#### Using Package Managers

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (Different distributions):**
```bash
# Ubuntu/Debian
sudo apt install llama.cpp

# Arch Linux
sudo pacman -S llama.cpp
```

### Method 2: Python Package (llama-cpp-python)

#### Basic Installation
```bash
pip install llama-cpp-python
```

#### With Hardware Acceleration
```bash
# For CUDA (NVIDIA GPUs)
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# For Metal (Apple Silicon)
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# For OpenBLAS (CPU optimization)
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## How to Build from Source

### Wetin you need

**System Requirements:**
- C++ compiler (GCC, Clang, or MSVC)
- CMake (version 3.14 or higher)
- Git
- Build tools for your system

**How to Install Wetin you need:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- Install Visual Studio 2022 with C++ development tools
- Install CMake from the official website
- Install Git

### How to Build

1. **Clone the repository:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **Set up the build:**
```bash
cmake -B build
```

3. **Build the project:**
```bash
cmake --build build --config Release
```

To make am fast, use parallel jobs:
```bash
cmake --build build --config Release -j 8
```

### Builds for Specific Hardware

#### CUDA Support (NVIDIA GPUs)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal Support (Apple Silicon)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS Support (CPU Optimization)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan Support
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### Advanced Build Options

#### Debug Build
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### With Extra Features
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## Model Quantization

### Wetin GGUF Format Mean

GGUF (Generalized GGML Unified Format) na file format wey dem design to make big language models run well with Llama.cpp and other frameworks. E dey provide:

- Standard way to store model weights
- Better compatibility for different systems
- Better performance
- E dey handle metadata well

### Quantization Types

Llama.cpp dey support different quantization levels:

| Type | Bits | Description | Use Case |
|------|------|-------------|----------|
| F16 | 16 | Half precision | High quality, big memory |
| Q8_0 | 8 | 8-bit quantization | Good balance |
| Q4_0 | 4 | 4-bit quantization | Moderate quality, small size |
| Q2_K | 2 | 2-bit quantization | Smallest size, lower quality |

### How to Convert Models

#### From PyTorch to GGUF
```bash
# Convert Hugging Face model
python convert_hf_to_gguf.py path/to/model --outdir ./models

# Quantize the model
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Direct Download from Hugging Face
Plenty models dey GGUF format for Hugging Face:
- Search for models wey get "GGUF" for name
- Download the quantization level wey you want
- Use am directly with llama.cpp

## Basic Usage

### Command Line Interface

#### Simple Text Generation
```bash
# Basic text completion
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# Interactive chat mode
./llama-cli -m model.gguf -cnv
```

#### Using Models from Hugging Face
```bash
# Download and run directly
./llama-cli -hf microsoft/DialoGPT-medium
```

#### Server Mode
```bash
# Start server
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# With GPU acceleration
./llama-server -m model.gguf --n-gpu-layers 32
```

### Common Parameters

| Parameter | Description | Example |
|-----------|-------------|---------|
| `-m` | Model file path | `-m model.gguf` |
| `-p` | Prompt text | `-p "Hello world"` |
| `-n` | Number of tokens to generate | `-n 100` |
| `-c` | Context size | `-c 4096` |
| `-t` | Number of threads | `-t 8` |
| `-ngl` | GPU layers | `-ngl 32` |
| `-temp` | Temperature | `-temp 0.7` |

### Interactive Mode

```bash
# Start interactive session
./llama-cli -m model.gguf -cnv

# Example conversation:
# > Hello, how are you?
# Hi there! I'm doing well, thank you for asking...
# > What can you help me with?
# I can assist with various tasks such as...
```

## Advanced Features

### Server API

#### How to Start the Server
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### How to Use the API
```bash
# Chat completion
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# Text completion
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### How to Make Performance Better

#### Memory Management
```bash
# Set context size
./llama-cli -m model.gguf -c 2048

# Enable memory mapping
./llama-cli -m model.gguf --mlock
```

#### Multi-threading
```bash
# Use all CPU cores
./llama-cli -m model.gguf -t $(nproc)

# Specific thread count
./llama-cli -m model.gguf -t 8
```

#### GPU Acceleration
```bash
# Offload layers to GPU
./llama-cli -m model.gguf -ngl 32

# Use specific GPU
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## Python Integration

### Basic Usage with llama-cpp-python

```python
from llama_cpp import Llama

# Initialize model
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# Generate text
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### Chat Interface

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# Chat completion
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### Streaming Responses

```python
# Streaming text generation
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### Integration with LangChain

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Initialize LLM
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# Create prompt template
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# Create chain
chain = LLMChain(llm=llm, prompt=prompt)

# Use the chain
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## Troubleshooting

### Common Problems and How to Solve Dem

#### Build Errors

**Problem: CMake no dey**
```bash
# Solution: Install CMake
# Ubuntu/Debian
sudo apt install cmake

# macOS
brew install cmake
```

**Problem: Compiler no dey**
```bash
# Solution: Install build tools
# Ubuntu/Debian
sudo apt install build-essential

# macOS
xcode-select --install
```

#### Runtime Problems

**Problem: Model no load**
- Check model file path
- Confirm file permissions
- Make sure RAM dey enough
- Try different quantization levels

**Problem: Performance no good**
- Enable hardware acceleration
- Add more threads
- Use correct quantization
- Check GPU memory usage

#### Memory Problems

**Problem: Memory don finish**
```bash
# Solutions:
# 1. Use smaller quantization
./llama-cli -m model-q4_0.gguf

# 2. Reduce context size
./llama-cli -m model.gguf -c 1024

# 3. Offload to GPU
./llama-cli -m model.gguf -ngl 32
```

### Problems for Specific Systems

#### Windows
- Use MinGW or Visual Studio compiler
- Make sure PATH dey correct
- Check antivirus interference

#### macOS
- Enable Metal for Apple Silicon
- Use Rosetta 2 if e need am
- Check Xcode command line tools

#### Linux
- Install development packages
- Check GPU driver versions
- Confirm CUDA toolkit installation

## Best Practices

### How to Choose Model
1. **Pick correct quantization** for your hardware
2. **Think about model size** and quality
3. **Test different models** for your use case

### How to Make Performance Better
1. **Use GPU acceleration** if e dey available
2. **Set thread count well** for your CPU
3. **Pick correct context size** for your use case
4. **Enable memory mapping** for big models

### How to Deploy for Production
1. **Use server mode** for API access
2. **Handle errors well**
3. **Monitor resource usage**
4. **Set up logging and monitoring**

### Development Workflow
1. **Start with small models** for testing
2. **Use version control** for model settings
3. **Write down your settings**
4. **Test for different systems**

### Security Tips
1. **Check input prompts**
2. **Limit how people use am**
3. **Secure API endpoints**
4. **Watch for abuse**

## Conclusion

Llama.cpp na strong and efficient way to run big language models for your computer across different hardware. Whether you dey build AI apps, do research, or just dey play with LLMs, dis framework go give you the flexibility and performance wey you need.

Key points:
- Pick the installation method wey fit you
- Optimize for your hardware
- Start with basic usage before you try advanced features
- Use Python bindings if you want easy integration
- Follow best practices for production deployment

For more info and updates, go [official Llama.cpp repository](https://github.com/ggml-org/llama.cpp) and check the full documentation and community resources.

## ➡️ Wetin next

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**Disclaimer**:  
Dis dokyument don use AI translet service [Co-op Translator](https://github.com/Azure/co-op-translator) do di translet. Even as we dey try make am correct, abeg sabi say machine translet fit get mistake or no dey accurate well. Di original dokyument for im native language na di one wey you go take as di correct source. For important informate, e good make you use professional human translet. We no go fit take blame for any misunderstanding or wrong interpretation wey fit happen because you use dis translet.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->