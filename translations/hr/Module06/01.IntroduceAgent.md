# AI agenti i mali jeziÄni modeli: Sveobuhvatan vodiÄ

## Uvod

U ovom vodiÄu istraÅ¾it Ä‡emo AI agente i male jeziÄne modele (SLM) te njihove napredne strategije implementacije za okruÅ¾enja rubnog raÄunalstva. Pokrit Ä‡emo osnovne koncepte agentne umjetne inteligencije, tehnike optimizacije SLM-a, praktiÄne strategije implementacije za ureÄ‘aje s ograniÄenim resursima te Microsoft Agent Framework za izgradnju sustava agenata spremnih za proizvodnju.

Krajolik umjetne inteligencije doÅ¾ivljava paradigmski pomak u 2025. Dok je 2023. bila godina chatbotova, a 2024. godina kopilota, 2025. pripada AI agentima â€” inteligentnim sustavima koji razmiÅ¡ljaju, planiraju, koriste alate i izvrÅ¡avaju zadatke uz minimalan ljudski unos, sve viÅ¡e pokretani uÄinkovitim malim jeziÄnim modelima. Microsoft Agent Framework se istiÄe kao vodeÄ‡e rjeÅ¡enje za izgradnju ovih inteligentnih sustava s offline moguÄ‡nostima za rubno raÄunalstvo.

## Ciljevi uÄenja

Na kraju ovog vodiÄa moÄ‡i Ä‡ete:

- ğŸ¤– Razumjeti osnovne koncepte AI agenata i agentnih sustava
- ğŸ”¬ Identificirati prednosti malih jeziÄnih modela u odnosu na velike jeziÄne modele u agentnim aplikacijama
- ğŸš€ NauÄiti napredne strategije implementacije SLM-a za okruÅ¾enja rubnog raÄunalstva
- ğŸ“± Implementirati praktiÄne agente pokretane SLM-om za stvarne aplikacije
- ğŸ—ï¸ Izgraditi agente spremne za proizvodnju koristeÄ‡i Microsoft Agent Framework
- ğŸŒ Implementirati offline agente za rubno raÄunalstvo s lokalnom integracijom LLM-a i SLM-a
- ğŸ”§ Integrirati Microsoft Agent Framework s Foundry Local za implementaciju na rubu

## Razumijevanje AI agenata: Osnove i klasifikacije

### Definicija i osnovni koncepti

Umjetni inteligentni (AI) agent odnosi se na sustav ili program koji je sposoban autonomno obavljati zadatke u ime korisnika ili drugog sustava dizajnirajuÄ‡i svoj tijek rada i koristeÄ‡i dostupne alate. Za razliku od tradicionalne AI koja samo odgovara na vaÅ¡a pitanja, agent moÅ¾e djelovati neovisno kako bi postigao ciljeve.

### Okvir klasifikacije agenata

Razumijevanje granica agenata pomaÅ¾e u odabiru odgovarajuÄ‡ih vrsta agenata za razliÄite scenarije raÄunalstva:

- **ğŸ”¬ Jednostavni refleksni agenti**: Sustavi temeljeni na pravilima koji reagiraju na neposredne percepcije (termostati, osnovna automatizacija)
- **ğŸ“± Modelno utemeljeni agenti**: Sustavi koji odrÅ¾avaju unutarnje stanje i memoriju (robotski usisavaÄi, navigacijski sustavi)
- **âš–ï¸ Ciljno usmjereni agenti**: Sustavi koji planiraju i izvrÅ¡avaju sekvence kako bi postigli ciljeve (planeri ruta, planeri zadataka)
- **ğŸ§  UÄeÄ‡i agenti**: Adaptivni sustavi koji poboljÅ¡avaju performanse tijekom vremena (sustavi preporuka, personalizirani asistenti)

### KljuÄne prednosti AI agenata

AI agenti nude nekoliko osnovnih prednosti koje ih Äine idealnim za aplikacije rubnog raÄunalstva:

**Operativna autonomija**: Agenti omoguÄ‡uju neovisno izvrÅ¡avanje zadataka bez stalnog nadzora, Å¡to ih Äini idealnim za aplikacije u stvarnom vremenu. Zahtijevaju minimalan nadzor uz odrÅ¾avanje adaptivnog ponaÅ¡anja, omoguÄ‡ujuÄ‡i implementaciju na ureÄ‘ajima s ograniÄenim resursima uz smanjene operativne troÅ¡kove.

**Fleksibilnost implementacije**: Ovi sustavi omoguÄ‡uju AI sposobnosti na ureÄ‘aju bez zahtjeva za internetskom povezanoÅ¡Ä‡u, poboljÅ¡avaju privatnost i sigurnost kroz lokalnu obradu, mogu se prilagoditi za aplikacije specifiÄne za domenu i prikladni su za razliÄita okruÅ¾enja rubnog raÄunalstva.

**Isplativost**: Sustavi agenata nude isplativu implementaciju u usporedbi s rjeÅ¡enjima temeljenim na oblaku, uz smanjene operativne troÅ¡kove i niÅ¾e zahtjeve za propusnoÅ¡Ä‡u za aplikacije na rubu.

## Napredne strategije za male jeziÄne modele

### Osnove SLM-a (Small Language Model)

Mali jeziÄni model (SLM) je jeziÄni model koji moÅ¾e stati na uobiÄajeni potroÅ¡aÄki elektroniÄki ureÄ‘aj i obavljati inferenciju s latencijom dovoljno niskom da bude praktiÄan za agentne zahtjeve jednog korisnika. U praktiÄnom smislu, SLM-ovi su obiÄno modeli s manje od 10 milijardi parametara.

**ZnaÄajke otkrivanja formata**: SLM-ovi nude naprednu podrÅ¡ku za razliÄite razine kvantizacije, kompatibilnost izmeÄ‘u platformi, optimizaciju performansi u stvarnom vremenu i moguÄ‡nosti implementacije na rubu. Korisnici mogu pristupiti poboljÅ¡anoj privatnosti kroz lokalnu obradu i podrÅ¡ku za WebGPU za implementaciju u preglednicima.

**Zbirke razina kvantizacije**: Popularni SLM formati ukljuÄuju Q4_K_M za uravnoteÅ¾enu kompresiju u mobilnim aplikacijama, Q5_K_S seriju za implementaciju usmjerenu na kvalitetu na rubu, Q8_0 za gotovo originalnu preciznost na moÄ‡nim rubnim ureÄ‘ajima i eksperimentalne formate poput Q2_K za scenarije s ultra-niskim resursima.

### GGUF (General GGML Universal Format) za implementaciju SLM-a

GGUF sluÅ¾i kao primarni format za implementaciju kvantiziranih SLM-ova na CPU i rubnim ureÄ‘ajima, posebno optimiziran za agentne aplikacije:

**ZnaÄajke optimizirane za agente**: Format pruÅ¾a sveobuhvatne resurse za konverziju i implementaciju SLM-a s poboljÅ¡anom podrÅ¡kom za pozivanje alata, generiranje strukturiranih izlaza i viÅ¡ekratne razgovore. Kompatibilnost izmeÄ‘u platformi osigurava dosljedno ponaÅ¡anje agenata na razliÄitim rubnim ureÄ‘ajima.

**Optimizacija performansi**: GGUF omoguÄ‡uje uÄinkovito koriÅ¡tenje memorije za tijekove rada agenata, podrÅ¾ava dinamiÄko uÄitavanje modela za sustave s viÅ¡e agenata i pruÅ¾a optimiziranu inferenciju za interakcije agenata u stvarnom vremenu.

### Okviri optimizirani za SLM na rubu

#### Optimizacija Llama.cpp za agente

Llama.cpp pruÅ¾a najnovije tehnike kvantizacije posebno optimizirane za implementaciju agentnih SLM-ova:

**Kvantizacija specifiÄna za agente**: Okvir podrÅ¾ava Q4_0 (optimalno za mobilnu implementaciju agenata s 75% smanjenjem veliÄine), Q5_1 (uravnoteÅ¾ena kvaliteta-kompresija za inferenciju agenata na rubu) i Q8_0 (kvaliteta bliska originalu za proizvodne sustave agenata). Napredni formati omoguÄ‡uju ultra-komprimirane agente za ekstremne scenarije na rubu.

**Prednosti implementacije**: Inferencija optimizirana za CPU s SIMD ubrzanjem omoguÄ‡uje uÄinkovito izvrÅ¡avanje agenata u memoriji. Kompatibilnost izmeÄ‘u platformi na x86, ARM i Apple Silicon arhitekturama omoguÄ‡uje univerzalne moguÄ‡nosti implementacije agenata.

#### Apple MLX Framework za SLM agente

Apple MLX pruÅ¾a nativnu optimizaciju posebno dizajniranu za agente pokretane SLM-om na ureÄ‘ajima s Apple Silicon Äipovima:

**Optimizacija agenata za Apple Silicon**: Okvir koristi arhitekturu ujedinjene memorije s integracijom Metal Performance Shaders, automatsku mjeÅ¡ovitu preciznost za inferenciju agenata i optimiziranu propusnost memorije za sustave s viÅ¡e agenata. SLM agenti pokazuju iznimne performanse na M-seriji Äipova.

**ZnaÄajke razvoja**: PodrÅ¡ka za Python i Swift API s optimizacijama specifiÄnim za agente, automatska diferencijacija za uÄenje agenata i besprijekorna integracija s Apple alatima za razvoj pruÅ¾aju sveobuhvatna okruÅ¾enja za razvoj agenata.

#### ONNX Runtime za SLM agente na viÅ¡e platformi

ONNX Runtime pruÅ¾a univerzalni motor za inferenciju koji omoguÄ‡uje SLM agentima da dosljedno rade na razliÄitim hardverskim platformama i operativnim sustavima:

**Univerzalna implementacija**: ONNX Runtime osigurava dosljedno ponaÅ¡anje SLM agenata na Windows, Linux, macOS, iOS i Android platformama. Ova kompatibilnost izmeÄ‘u platformi omoguÄ‡uje programerima da piÅ¡u jednom i implementiraju svugdje, znaÄajno smanjujuÄ‡i troÅ¡kove razvoja i odrÅ¾avanja za aplikacije na viÅ¡e platformi.

**Opcije hardverskog ubrzanja**: Okvir pruÅ¾a optimizirane izvrÅ¡ne pruÅ¾atelje usluga za razliÄite hardverske konfiguracije ukljuÄujuÄ‡i CPU (Intel, AMD, ARM), GPU (NVIDIA CUDA, AMD ROCm) i specijalizirane akceleratore (Intel VPU, Qualcomm NPU). SLM agenti mogu automatski iskoristiti najbolje dostupne hardverske resurse bez promjena u kodu.

**ZnaÄajke spremne za proizvodnju**: ONNX Runtime nudi znaÄajke na razini poduzeÄ‡a koje su kljuÄne za implementaciju agenata u proizvodnji, ukljuÄujuÄ‡i optimizaciju grafova za brÅ¾u inferenciju, upravljanje memorijom za okruÅ¾enja s ograniÄenim resursima i sveobuhvatne alate za profiliranje za analizu performansi. Okvir podrÅ¾ava Python i C++ API-je za fleksibilnu integraciju.

## SLM vs LLM u agentnim sustavima: Napredna usporedba

### Prednosti SLM-a u agentnim aplikacijama

**Operativna uÄinkovitost**: SLM-ovi pruÅ¾aju smanjenje troÅ¡kova od 10-30Ã— u usporedbi s LLM-ovima za zadatke agenata, omoguÄ‡ujuÄ‡i odgovore agenata u stvarnom vremenu na velikoj skali. Nude brÅ¾e vrijeme inferencije zbog smanjene raÄunalne sloÅ¾enosti, Å¡to ih Äini idealnim za interaktivne aplikacije agenata.

**MoguÄ‡nosti implementacije na rubu**: SLM-ovi omoguÄ‡uju izvrÅ¡avanje agenata na ureÄ‘aju bez ovisnosti o internetu, poboljÅ¡anu privatnost kroz lokalnu obradu agenata i prilagodbu za aplikacije specifiÄne za domenu prikladne za razliÄita okruÅ¾enja rubnog raÄunalstva.

**Optimizacija specifiÄna za agente**: SLM-ovi se istiÄu u pozivanju alata, generiranju strukturiranih izlaza i rutinskim tijekovima odluÄivanja koji Äine 70-80% tipiÄnih zadataka agenata.

### Kada koristiti SLM-ove u odnosu na LLM-ove u sustavima agenata

**Idealno za SLM-ove**:
- **PonavljajuÄ‡i zadaci agenata**: Unos podataka, popunjavanje obrazaca, rutinski API pozivi
- **Integracija alata**: Upiti u bazu podataka, operacije s datotekama, interakcije sa sustavom
- **Strukturirani tijekovi rada**: SlijeÄ‘enje unaprijed definiranih procesa agenata
- **Agenti specifiÄni za domenu**: KorisniÄka podrÅ¡ka, zakazivanje, osnovna analiza
- **Lokalna obrada**: Operacije agenata osjetljive na privatnost

**Bolje za LLM-ove**:
- **SloÅ¾eno razmiÅ¡ljanje**: RjeÅ¡avanje novih problema, strateÅ¡ko planiranje
- **Razgovori otvorenog tipa**: OpÄ‡i razgovori, kreativne rasprave
- **Zadaci Å¡irokog znanja**: IstraÅ¾ivanje koje zahtijeva opseÅ¾no opÄ‡e znanje
- **Nove situacije**: Rukovanje potpuno novim scenarijima agenata

### Hibridna arhitektura agenata

Optimalan pristup kombinira SLM-ove i LLM-ove u heterogenim agentnim sustavima:

**Pametna orkestracija agenata**:
1. **SLM kao primarni**: RjeÅ¡avanje 70-80% rutinskih zadataka agenata lokalno
2. **LLM po potrebi**: Usmjeravanje sloÅ¾enih upita na veÄ‡e modele u oblaku
3. **Specijalizirani SLM-ovi**: RazliÄiti mali modeli za razliÄite domene agenata
4. **Optimizacija troÅ¡kova**: Minimiziranje skupih poziva LLM-a kroz inteligentno usmjeravanje

## Strategije implementacije agenata pokretanih SLM-om

### Foundry Local: OkruÅ¾enje za rubno AI raÄunalstvo na razini poduzeÄ‡a

Foundry Local (https://github.com/microsoft/foundry-local) sluÅ¾i kao vodeÄ‡e rjeÅ¡enje Microsofta za implementaciju malih jeziÄnih modela u proizvodnim rubnim okruÅ¾enjima. PruÅ¾a kompletno okruÅ¾enje za pokretanje posebno dizajnirano za agente pokretane SLM-om s znaÄajkama na razini poduzeÄ‡a i besprijekornim moguÄ‡nostima integracije.

**Osnovna arhitektura i znaÄajke**:
- **Kompatibilan s OpenAI API-jem**: Potpuna kompatibilnost s OpenAI SDK-om i integracijama Agent Frameworka
- **Automatska optimizacija hardvera**: Inteligentan odabir varijanti modela na temelju dostupnog hardvera (CUDA GPU, Qualcomm NPU, CPU)
- **Upravljanje modelima**: Automatsko preuzimanje, predmemoriranje i upravljanje Å¾ivotnim ciklusom SLM modela
- **Otkrivanje usluga**: Otkrivanje usluga bez konfiguracije za okvire agenata
- **Optimizacija resursa**: Inteligentno upravljanje memorijom i energetska uÄinkovitost za implementaciju na rubu

#### Instalacija i postavljanje

**Instalacija na viÅ¡e platformi**:
```bash
# Windows (recommended)
winget install Microsoft.FoundryLocal

# macOS
brew tap microsoft/foundrylocal
brew install foundrylocal

# Linux (manual installation)
wget https://github.com/microsoft/foundry-local/releases/latest/download/foundry-local-linux.tar.gz
tar -xzf foundry-local-linux.tar.gz
sudo mv foundry-local /usr/local/bin/
```

**Brzi poÄetak za razvoj agenata**:
```bash
# Start service with automatic model loading
foundry model run phi-4-mini

# Verify service status and endpoint
foundry service status

# List available models
foundry model ls

# Test API endpoint
curl http://localhost:<port>/v1/models
```

#### Integracija s Agent Frameworkom

**Integracija Foundry Local SDK-a**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config
import openai

# Initialize Foundry Local with automatic service management
manager = FoundryLocalManager("phi-4-mini")

# Configure OpenAI client for local inference
client = openai.OpenAI(
    base_url=manager.endpoint,
    api_key=manager.api_key  # Auto-generated for local usage
)

# Create agent with Foundry Local backend
agent_config = Config(
    name="production-agent",
    model_provider="foundry-local",
    model_id=manager.get_model_info("phi-4-mini").id,
    endpoint=manager.endpoint,
    api_key=manager.api_key
)

agent = Agent(config=agent_config)
```

**Automatski odabir modela i optimizacija hardvera**:
```python
# Foundry Local automatically selects optimal model variant
models_by_use_case = {
    "lightweight_routing": "qwen2.5-0.5b",      # 500MB, ultra-fast
    "general_conversation": "phi-4-mini",       # 2.4GB, balanced
    "complex_reasoning": "phi-4",               # 7GB, high-capability
    "code_assistance": "qwen2.5-coder-0.5b"    # 500MB, code-optimized
}

# Foundry Local handles hardware detection and quantization
for use_case, model_alias in models_by_use_case.items():
    manager = FoundryLocalManager(model_alias)
    print(f"{use_case}: {manager.get_model_info(model_alias).variant_selected}")
    # Output examples:
    # lightweight_routing: qwen2.5-0.5b-instruct-q4_k_m.gguf (CPU optimized)
    # general_conversation: phi-4-mini-instruct-cuda-q5_k_m.gguf (GPU accelerated)
```

#### Obrasci implementacije u proizvodnji

**Postavljanje jednog agenta u proizvodnji**:
```python
import asyncio
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import Agent, Config, Tool

class ProductionAgentService:
    def __init__(self, model_alias="phi-4-mini"):
        self.foundry = FoundryLocalManager(model_alias)
        self.agent = self._create_agent()
        
    def _create_agent(self):
        config = Config(
            name="production-customer-service",
            model_provider="foundry-local",
            model_id=self.foundry.get_model_info().id,
            endpoint=self.foundry.endpoint,
            api_key=self.foundry.api_key,
            max_tokens=512,
            temperature=0.1,
            timeout=30.0
        )
        
        agent = Agent(config=config)
        
        # Add production tools
        @agent.tool
        def lookup_customer(customer_id: str) -> dict:
            """Look up customer information from local database."""
            return self.local_db.get_customer(customer_id)
            
        @agent.tool
        def create_ticket(issue: str, priority: str = "medium") -> str:
            """Create a support ticket."""
            ticket_id = self.ticketing_system.create(issue, priority)
            return f"Created ticket {ticket_id}"
            
        return agent
    
    async def process_request(self, user_input: str) -> str:
        """Process user request with error handling and monitoring."""
        try:
            response = await self.agent.chat_async(user_input)
            self.log_interaction(user_input, response, "success")
            return response
        except Exception as e:
            self.log_interaction(user_input, str(e), "error")
            return "I'm experiencing technical difficulties. Please try again."
    
    def health_check(self) -> dict:
        """Check service health for monitoring."""
        return {
            "foundry_status": self.foundry.health_check(),
            "model_loaded": self.foundry.is_model_loaded(),
            "endpoint": self.foundry.endpoint,
            "memory_usage": self.foundry.get_memory_usage()
        }

# Production usage
service = ProductionAgentService("phi-4-mini")
response = await service.process_request("I need help with my order #12345")
```

**Orkestracija viÅ¡e agenata u proizvodnji**:
```python
from foundry_local import FoundryLocalManager
from microsoft_agent_framework import AgentOrchestrator, Agent, Config

class MultiAgentProductionSystem:
    def __init__(self):
        self.agents = self._initialize_agents()
        self.orchestrator = AgentOrchestrator(list(self.agents.values()))
        
    def _initialize_agents(self):
        agents = {}
        
        # Lightweight routing agent
        routing_foundry = FoundryLocalManager("qwen2.5-0.5b")
        agents["router"] = Agent(Config(
            name="request-router",
            model_provider="foundry-local",
            endpoint=routing_foundry.endpoint,
            api_key=routing_foundry.api_key,
            role="Route user requests to appropriate specialized agents"
        ))
        
        # Customer service agent
        service_foundry = FoundryLocalManager("phi-4-mini")
        agents["customer_service"] = Agent(Config(
            name="customer-service",
            model_provider="foundry-local",
            endpoint=service_foundry.endpoint,
            api_key=service_foundry.api_key,
            role="Handle customer service inquiries and support requests"
        ))
        
        # Technical support agent
        tech_foundry = FoundryLocalManager("qwen2.5-coder-0.5b")
        agents["technical"] = Agent(Config(
            name="technical-support",
            model_provider="foundry-local",
            endpoint=tech_foundry.endpoint,
            api_key=tech_foundry.api_key,
            role="Provide technical assistance and troubleshooting"
        ))
        
        return agents
    
    async def process_request(self, user_input: str) -> str:
        """Route and process user requests through appropriate agents."""
        # Route request to appropriate agent
        routing_result = await self.agents["router"].chat_async(
            f"Classify this request and route to customer_service or technical: {user_input}"
        )
        
        # Determine target agent based on routing
        target_agent = "customer_service" if "customer" in routing_result.lower() else "technical"
        
        # Process with specialized agent
        response = await self.agents[target_agent].chat_async(user_input)
        
        return response

# Production deployment
system = MultiAgentProductionSystem()
response = await system.process_request("My application keeps crashing")
```

#### ZnaÄajke za poduzeÄ‡a i praÄ‡enje

**PraÄ‡enje zdravlja i vidljivost**:
```python
from foundry_local import FoundryLocalManager
import asyncio
import logging

class FoundryMonitoringService:
    def __init__(self):
        self.managers = {}
        self.metrics = []
        
    def add_model(self, alias: str) -> FoundryLocalManager:
        """Add a model to monitoring."""
        manager = FoundryLocalManager(alias)
        self.managers[alias] = manager
        return manager
    
    async def collect_metrics(self):
        """Collect performance metrics from all Foundry Local instances."""
        metrics = {
            "timestamp": time.time(),
            "models": {}
        }
        
        for alias, manager in self.managers.items():
            try:
                model_metrics = {
                    "status": "healthy" if manager.health_check() else "unhealthy",
                    "memory_usage": manager.get_memory_usage(),
                    "inference_count": manager.get_inference_count(),
                    "average_latency": manager.get_average_latency(),
                    "error_rate": manager.get_error_rate()
                }
                metrics["models"][alias] = model_metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {alias}: {e}")
                metrics["models"][alias] = {"status": "error", "error": str(e)}
        
        self.metrics.append(metrics)
        return metrics
    
    def get_health_status(self) -> dict:
        """Get overall system health status."""
        healthy_models = 0
        total_models = len(self.managers)
        
        for alias, manager in self.managers.items():
            if manager.health_check():
                healthy_models += 1
        
        return {
            "overall_status": "healthy" if healthy_models == total_models else "degraded",
            "healthy_models": healthy_models,
            "total_models": total_models,
            "health_percentage": (healthy_models / total_models) * 100 if total_models > 0 else 0
        }

# Production monitoring setup
monitor = FoundryMonitoringService()
monitor.add_model("phi-4-mini")
monitor.add_model("qwen2.5-0.5b")

# Continuous monitoring
async def monitoring_loop():
    while True:
        metrics = await monitor.collect_metrics()
        health = monitor.get_health_status()
        
        if health["health_percentage"] < 100:
            logging.warning(f"System health degraded: {health}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds
```

**Upravljanje resursima i automatsko skaliranje**:
```python
class FoundryResourceManager:
    def __init__(self):
        self.model_instances = {}
        self.resource_limits = {
            "max_memory_gb": 8,
            "max_concurrent_models": 3,
            "cpu_threshold": 80
        }
    
    def auto_scale_models(self, demand_metrics: dict):
        """Automatically scale models based on demand."""
        current_memory = self.get_total_memory_usage()
        
        # Scale down if memory usage is high
        if current_memory > self.resource_limits["max_memory_gb"] * 0.8:
            self.scale_down_idle_models()
        
        # Scale up if demand is high and resources allow
        for model_alias, demand in demand_metrics.items():
            if demand > 0.8 and len(self.model_instances) < self.resource_limits["max_concurrent_models"]:
                self.load_model_instance(model_alias)
    
    def load_model_instance(self, alias: str) -> FoundryLocalManager:
        """Load a new model instance if resources allow."""
        if alias not in self.model_instances:
            try:
                manager = FoundryLocalManager(alias)
                self.model_instances[alias] = manager
                logging.info(f"Loaded model instance: {alias}")
                return manager
            except Exception as e:
                logging.error(f"Failed to load model {alias}: {e}")
                return None
        return self.model_instances[alias]
    
    def scale_down_idle_models(self):
        """Remove idle model instances to free resources."""
        idle_models = []
        
        for alias, manager in self.model_instances.items():
            if manager.get_idle_time() > 300:  # 5 minutes idle
                idle_models.append(alias)
        
        for alias in idle_models:
            self.model_instances[alias].shutdown()
            del self.model_instances[alias]
            logging.info(f"Scaled down idle model: {alias}")
```

#### Napredna konfiguracija i optimizacija

**PrilagoÄ‘ena konfiguracija modela**:
```python
# Advanced Foundry Local configuration for production
from foundry_local import FoundryLocalManager, ModelConfig

# Custom configuration for specific use cases
config = ModelConfig(
    alias="phi-4-mini",
    quantization="Q5_K_M",  # Specific quantization level
    context_length=4096,    # Extended context for complex agents
    batch_size=1,          # Optimized for single-user agents
    threads=4,             # CPU thread optimization
    gpu_layers=32,         # GPU acceleration layers
    memory_lock=True,      # Lock model in memory for consistent performance
    numa=True              # NUMA optimization for multi-socket systems
)

manager = FoundryLocalManager(config=config)
```

**Kontrolni popis za implementaciju u proizvodnji**:

âœ… **Konfiguracija usluge**:
- Konfigurirajte odgovarajuÄ‡e alias modele za sluÄajeve upotrebe
- Postavite ograniÄenja resursa i pragove za praÄ‡enje
- OmoguÄ‡ite provjere zdravlja i prikupljanje metrika
- Konfigurirajte automatsko ponovno pokretanje i oporavak

âœ… **Postavljanje sigurnosti**:
- OmoguÄ‡ite API pristup samo lokalno (bez vanjske izloÅ¾enosti)
- Konfigurirajte odgovarajuÄ‡e upravljanje API kljuÄevima
- Postavite zapisivanje revizije za interakcije agenata
- Implementirajte ograniÄenje brzine za upotrebu u proizvodnji

âœ… **Optimizacija performansi**:
- Testirajte performanse modela pod oÄekivanim optereÄ‡enjem
- Konfigurirajte odgovarajuÄ‡e razine kvantizacije
- Postavite strategije predmemoriranja i zagrijavanja modela
- Pratite obrasce koriÅ¡tenja memorije i CPU-a

âœ… **Testiranje integracije**:
- Testirajte integraciju s okvirom agenata
- Provjerite moguÄ‡nosti offline rada
- Testirajte scenarije oporavka nakon kvara
- Potvrdite tijekove rada agenata od poÄetka do kraja

### Ollama: Pojednostavljena implementacija SLM agenata

### Ollama: Implementacija SLM agenata usmjerena na zajednicu

Ollama pruÅ¾a pristup implementaciji SLM agenata voÄ‘en zajednicom s naglaskom na jednostavnost, opseÅ¾an ekosustav modela i radne procese prilagoÄ‘ene programerima. Dok se Foundry Local fokusira na znaÄajke na razini poduzeÄ‡a, Ollama se istiÄe u brzom prototipiranju, pristupu modelima zajednice i pojednostavljenim scenarijima implementacije.

**Osnovna arhitektura i znaÄajke**:
- **Kompatibilan s OpenAI API-jem**: Potpuna kompatibilnost s REST API-jem za besprijekornu integraciju s okvirom agenata
- **OpseÅ¾na biblioteka modela**: Pristup stotinama modela koje je doprinijela zajednica i sluÅ¾benih modela
- **Jednostavno upravljanje modelima**: Instalacija i prebacivanje modela jednim naredbom
- **PodrÅ¡ka za viÅ¡e platformi**: Nativna podrÅ¡ka za Windows, macOS i Linux
- **Optimizacija resursa**
- Testirajte integraciju Microsoft Agent Frameworka  
- Provjerite moguÄ‡nosti rada u offline naÄinu  
- Testirajte scenarije prebacivanja i rukovanje greÅ¡kama  
- Validirajte radne tokove agenata od poÄetka do kraja  

**Usporedba s Foundry Local**:

| ZnaÄajka | Foundry Local | Ollama |
|----------|---------------|--------|
| **Ciljana upotreba** | Produkcija u poduzeÄ‡ima | Razvoj i zajednica |
| **Ekosustav modela** | Kurirano od strane Microsofta | OpseÅ¾na zajednica |
| **Optimizacija hardvera** | Automatska (CUDA/NPU/CPU) | RuÄna konfiguracija |
| **ZnaÄajke za poduzeÄ‡a** | UgraÄ‘eno praÄ‡enje, sigurnost | Alati zajednice |
| **SloÅ¾enost implementacije** | Jednostavna (winget instalacija) | Jednostavna (curl instalacija) |
| **Kompatibilnost API-ja** | OpenAI + proÅ¡irenja | Standard OpenAI |
| **PodrÅ¡ka** | SluÅ¾beni Microsoft | VoÄ‘ena zajednicom |
| **Najbolje za** | Produkcijski agenti | Prototipiranje, istraÅ¾ivanje |

**Kada odabrati Ollama**:  
- **Razvoj i prototipiranje**: Brzo eksperimentiranje s razliÄitim modelima  
- **Modeli zajednice**: Pristup najnovijim modelima koje doprinosi zajednica  
- **Obrazovna upotreba**: UÄenje i poduÄavanje razvoja AI agenata  
- **IstraÅ¾ivaÄki projekti**: Akademska istraÅ¾ivanja koja zahtijevaju raznolik pristup modelima  
- **PrilagoÄ‘eni modeli**: Izrada i testiranje prilagoÄ‘enih modela s finim podeÅ¡avanjem  

### VLLM: Inference visokih performansi za SLM agente  

VLLM (Inference za vrlo velike jeziÄne modele) pruÅ¾a motor za inference visokog kapaciteta i uÄinkovite memorije, posebno optimiziran za produkcijske SLM implementacije u velikim razmjerima. Dok se Foundry Local fokusira na jednostavnost koriÅ¡tenja, a Ollama naglaÅ¡ava modele zajednice, VLLM briljira u scenarijima visokih performansi koji zahtijevaju maksimalni kapacitet i uÄinkovito koriÅ¡tenje resursa.  

**Osnovna arhitektura i znaÄajke**:  
- **PagedAttention**: Revolucionarno upravljanje memorijom za uÄinkovito raÄunanje paÅ¾nje  
- **DinamiÄko grupiranje**: Inteligentno grupiranje zahtjeva za optimalni kapacitet  
- **Optimizacija GPU-a**: Napredni CUDA kernel i podrÅ¡ka za paralelizam tensorima  
- **Kompatibilnost s OpenAI**: Potpuna kompatibilnost API-ja za besprijekornu integraciju  
- **Spekulativno dekodiranje**: Napredne tehnike ubrzanja inferencea  
- **PodrÅ¡ka za kvantizaciju**: INT4, INT8 i FP16 kvantizacija za uÄinkovitost memorije  

#### Instalacija i postavljanje  

**Opcije instalacije**:  
```bash
# Standard installation
pip install vllm

# With additional dependencies for agent frameworks
pip install vllm[agent] openai

# Docker deployment for production
docker pull vllm/vllm-openai:latest

# From source for latest features
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .
```
  
**Brzi poÄetak za razvoj agenata**:  
```bash
# Start VLLM server with SLM model
python -m vllm.entrypoints.openai.api_server \
    --model microsoft/Phi-3.5-mini-instruct \
    --trust-remote-code \
    --max-model-len 4096 \
    --gpu-memory-utilization 0.8

# Alternative: Start with Qwen2.5 for lightweight agents
python -m vllm.entrypoints.openai.api_server \
    --model Qwen/Qwen2.5-0.5B-Instruct \
    --trust-remote-code \
    --max-model-len 2048 \
    --tensor-parallel-size 1

# Test API endpoint
curl http://localhost:8000/v1/models

# Test chat completion
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "microsoft/Phi-3.5-mini-instruct",
        "messages": [{"role": "user", "content": "Hello!"}]
    }'
```
  

#### Integracija s Agent Frameworkom  

**VLLM s Microsoft Agent Frameworkom**:  
```python
from microsoft_agent_framework import Agent, Config
import openai
import subprocess
import time
import requests
from typing import Optional, Dict, Any

class VLLMManager:
    def __init__(self, model_name: str, 
                 host: str = "localhost", 
                 port: int = 8000,
                 gpu_memory_utilization: float = 0.8,
                 max_model_len: int = 4096):
        self.model_name = model_name
        self.host = host
        self.port = port
        self.base_url = f"http://{host}:{port}"
        self.gpu_memory_utilization = gpu_memory_utilization
        self.max_model_len = max_model_len
        self.process = None
        self.client = None
        
    def start_server(self) -> bool:
        """Start VLLM server with optimized settings for agents."""
        try:
            cmd = [
                "python", "-m", "vllm.entrypoints.openai.api_server",
                "--model", self.model_name,
                "--host", self.host,
                "--port", str(self.port),
                "--gpu-memory-utilization", str(self.gpu_memory_utilization),
                "--max-model-len", str(self.max_model_len),
                "--trust-remote-code",
                "--disable-log-requests",  # Reduce logging for agents
                "--served-model-name", self.get_served_model_name()
            ]
            
            self.process = subprocess.Popen(cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE)
            
            # Wait for server to start
            max_retries = 30
            for _ in range(max_retries):
                if self.health_check():
                    self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
                    return True
                time.sleep(2)
                
            return False
            
        except Exception as e:
            print(f"Failed to start VLLM server: {e}")
            return False
    
    def get_served_model_name(self) -> str:
        """Get a clean model name for serving."""
        return self.model_name.replace("/", "--")
    
    def health_check(self) -> bool:
        """Check if VLLM server is healthy."""
        try:
            response = requests.get(f"{self.base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def get_openai_client(self) -> openai.OpenAI:
        """Get OpenAI-compatible client for VLLM."""
        if not self.client:
            self.client = openai.OpenAI(base_url=f"{self.base_url}/v1")
        return self.client
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get model information and statistics."""
        try:
            response = requests.get(f"{self.base_url}/v1/models")
            if response.status_code == 200:
                return response.json()
        except:
            pass
        return {}
    
    def shutdown(self):
        """Shutdown VLLM server."""
        if self.process:
            self.process.terminate()
            self.process.wait()

# Initialize VLLM for high-performance agents
vllm_manager = VLLMManager("microsoft/Phi-3.5-mini-instruct")
if vllm_manager.start_server():
    print("VLLM server started successfully")
    
    # Configure agent with VLLM backend
    agent_config = Config(
        name="vllm-performance-agent",
        model_provider="vllm",
        model_id=vllm_manager.get_served_model_name(),
        endpoint=f"{vllm_manager.base_url}/v1",
        api_key="none"  # VLLM doesn't require API key
    )
    
    agent = Agent(config=agent_config)
else:
    print("Failed to start VLLM server")
```
  
**Postavljanje viÅ¡e agenata visokog kapaciteta**:  
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from microsoft_agent_framework import Agent, Config
import openai

class VLLMHighThroughputManager:
    def __init__(self):
        self.model_configs = {
            "lightweight": {
                "model": "Qwen/Qwen2.5-0.5B-Instruct",
                "port": 8000,
                "max_model_len": 2048,
                "gpu_memory_utilization": 0.3
            },
            "balanced": {
                "model": "microsoft/Phi-3.5-mini-instruct",
                "port": 8001,
                "max_model_len": 4096,
                "gpu_memory_utilization": 0.5
            },
            "capable": {
                "model": "meta-llama/Llama-3.2-3B-Instruct",
                "port": 8002,
                "max_model_len": 8192,
                "gpu_memory_utilization": 0.7
            }
        }
        self.managers = {}
        self.agents = {}
        self.client_pool = {}
        
    async def initialize_all_models(self):
        """Initialize all VLLM models in parallel."""
        initialization_tasks = []
        
        for category, config in self.model_configs.items():
            task = self._initialize_model(category, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_inits = 0
        for i, result in enumerate(results):
            category = list(self.model_configs.keys())[i]
            if isinstance(result, Exception):
                print(f"Failed to initialize {category}: {result}")
            else:
                successful_inits += 1
                print(f"Successfully initialized {category} model")
        
        return successful_inits
    
    async def _initialize_model(self, category: str, config: Dict[str, Any]):
        """Initialize a single VLLM model instance."""
        manager = VLLMManager(
            model_name=config["model"],
            port=config["port"],
            max_model_len=config["max_model_len"],
            gpu_memory_utilization=config["gpu_memory_utilization"]
        )
        
        # Start server in thread to avoid blocking
        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            success = await loop.run_in_executor(executor, manager.start_server)
        
        if success:
            self.managers[category] = manager
            
            # Create agent
            agent_config = Config(
                name=f"vllm-{category}-agent",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none"
            )
            
            self.agents[category] = Agent(config=agent_config)
            
            # Create client pool for high throughput
            self.client_pool[category] = [
                openai.OpenAI(base_url=f"{manager.base_url}/v1")
                for _ in range(5)  # 5 clients per model for parallelism
            ]
            
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {category}")
    
    def get_optimal_agent(self, request_complexity: str, current_load: Dict[str, int]) -> str:
        """Select optimal agent based on request complexity and current load."""
        complexity_mapping = {
            "simple": "lightweight",
            "moderate": "balanced", 
            "complex": "capable"
        }
        
        preferred_category = complexity_mapping.get(request_complexity, "balanced")
        
        # Check if preferred agent is available and not overloaded
        if (preferred_category in self.agents and 
            current_load.get(preferred_category, 0) < 10):  # Max 10 concurrent per agent
            return preferred_category
        
        # Fallback to least loaded available agent
        available_agents = [(cat, load) for cat, load in current_load.items() 
                          if cat in self.agents and load < 10]
        
        if available_agents:
            return min(available_agents, key=lambda x: x[1])[0]
        
        return "balanced"  # Default fallback
    
    async def process_batch_requests(self, requests: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Process multiple requests in parallel for maximum throughput."""
        current_load = {cat: 0 for cat in self.agents.keys()}
        tasks = []
        
        for request in requests:
            # Determine optimal agent
            complexity = request.get("complexity", "moderate")
            agent_category = self.get_optimal_agent(complexity, current_load)
            current_load[agent_category] += 1
            
            # Create processing task
            task = self._process_single_request(request, agent_category)
            tasks.append(task)
        
        # Process all requests in parallel
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Format results
        formatted_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                formatted_results.append({
                    "request_id": requests[i].get("id", i),
                    "status": "error",
                    "error": str(result)
                })
            else:
                formatted_results.append(result)
        
        return formatted_results
    
    async def _process_single_request(self, request: Dict[str, Any], agent_category: str) -> Dict[str, Any]:
        """Process a single request with the specified agent."""
        start_time = time.time()
        
        try:
            agent = self.agents[agent_category]
            response = await agent.chat_async(request["message"])
            
            processing_time = time.time() - start_time
            
            return {
                "request_id": request.get("id"),
                "status": "success",
                "response": response,
                "agent_used": agent_category,
                "processing_time": processing_time
            }
            
        except Exception as e:
            return {
                "request_id": request.get("id"),
                "status": "error",
                "error": str(e),
                "agent_used": agent_category,
                "processing_time": time.time() - start_time
            }

# High-throughput usage example
throughput_manager = VLLMHighThroughputManager()

# Initialize all models
initialized_count = await throughput_manager.initialize_all_models()
print(f"Initialized {initialized_count} models")

# Process batch requests
batch_requests = [
    {"id": 1, "message": "Simple question", "complexity": "simple"},
    {"id": 2, "message": "Complex analysis needed", "complexity": "complex"},
    {"id": 3, "message": "Moderate difficulty task", "complexity": "moderate"}
]

results = await throughput_manager.process_batch_requests(batch_requests)
for result in results:
    print(f"Request {result['request_id']}: {result['status']} in {result.get('processing_time', 0):.2f}s")
```
  

#### Obrasci za produkcijsku implementaciju  

**Produkcijska usluga VLLM za poduzeÄ‡a**:  
```python
import asyncio
import logging
import time
from typing import Dict, List, Optional
from dataclasses import dataclass
from microsoft_agent_framework import Agent, Config
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from pydantic import BaseModel

@dataclass
class VLLMServerConfig:
    model_name: str
    port: int
    gpu_memory_utilization: float
    max_model_len: int
    tensor_parallel_size: int = 1
    quantization: Optional[str] = None

class AgentRequest(BaseModel):
    message: str
    agent_type: str = "general"
    priority: str = "normal"
    timeout: int = 30

class VLLMProductionService:
    def __init__(self, server_configs: Dict[str, VLLMServerConfig]):
        self.server_configs = server_configs
        self.managers = {}
        self.agents = {}
        self.metrics = {
            "requests_processed": 0,
            "requests_failed": 0,
            "total_processing_time": 0,
            "agent_usage": {name: 0 for name in server_configs.keys()},
            "throughput_per_minute": 0
        }
        self.request_queue = asyncio.Queue(maxsize=1000)
        self.processing_workers = []
        self.app = FastAPI(title="VLLM Agent Service")
        self._setup_routes()
        
    async def initialize_production_environment(self):
        """Initialize all VLLM servers for production."""
        logging.info("Initializing VLLM production environment...")
        
        initialization_tasks = []
        for name, config in self.server_configs.items():
            task = self._initialize_server(name, config)
            initialization_tasks.append(task)
        
        results = await asyncio.gather(*initialization_tasks, return_exceptions=True)
        
        successful_servers = 0
        for i, result in enumerate(results):
            server_name = list(self.server_configs.keys())[i]
            if isinstance(result, Exception):
                logging.error(f"Failed to initialize {server_name}: {result}")
            else:
                successful_servers += 1
                logging.info(f"Successfully initialized {server_name}")
        
        if successful_servers == 0:
            raise Exception("No VLLM servers could be initialized")
        
        # Start processing workers
        self.processing_workers = [
            asyncio.create_task(self._processing_worker(i))
            for i in range(min(4, successful_servers))  # 4 workers max
        ]
        
        logging.info(f"Production environment ready with {successful_servers} servers")
        return successful_servers
    
    async def _initialize_server(self, name: str, config: VLLMServerConfig):
        """Initialize a single VLLM server."""
        manager = VLLMManager(
            model_name=config.model_name,
            port=config.port,
            gpu_memory_utilization=config.gpu_memory_utilization,
            max_model_len=config.max_model_len
        )
        
        # Add quantization if specified
        if config.quantization:
            # This would be added to the manager's start command
            pass
        
        success = manager.start_server()
        if success:
            self.managers[name] = manager
            
            # Create production agent
            agent_config = Config(
                name=f"vllm-production-{name}",
                model_provider="vllm",
                model_id=manager.get_served_model_name(),
                endpoint=f"{manager.base_url}/v1",
                api_key="none",
                timeout=30.0
            )
            
            agent = Agent(config=agent_config)
            
            # Add production tools
            self._add_production_tools(agent, name)
            
            self.agents[name] = agent
            return True
        else:
            raise Exception(f"Failed to start VLLM server for {name}")
    
    def _add_production_tools(self, agent: Agent, server_type: str):
        """Add production tools based on server type."""
        if server_type == "customer_service":
            @agent.tool
            def escalate_to_human(issue: str, customer_id: str) -> str:
                """Escalate complex issues to human agents."""
                return f"Escalated issue for customer {customer_id}: {issue}"
            
            @agent.tool
            def lookup_order_status(order_id: str) -> dict:
                """Look up order status from production database."""
                # Production database lookup
                return {"order_id": order_id, "status": "shipped", "eta": "2 days"}
        
        elif server_type == "technical_support":
            @agent.tool
            def run_system_diagnostics(system_id: str) -> dict:
                """Run comprehensive system diagnostics."""
                return {"system_id": system_id, "status": "healthy", "issues": []}
            
            @agent.tool
            def create_incident_report(description: str, severity: str) -> str:
                """Create incident report in production system."""
                incident_id = f"INC-{hash(description) % 100000:05d}"
                return f"Created incident {incident_id} with severity {severity}"
    
    def _setup_routes(self):
        """Set up FastAPI routes for production service."""
        @self.app.post("/chat")
        async def chat_endpoint(request: AgentRequest, background_tasks: BackgroundTasks):
            try:
                # Add request to queue
                await self.request_queue.put({
                    "request": request,
                    "timestamp": time.time(),
                    "future": asyncio.Future()
                })
                
                # Wait for processing (with timeout)
                result = await asyncio.wait_for(
                    self._wait_for_result(request),
                    timeout=request.timeout
                )
                
                return result
                
            except asyncio.TimeoutError:
                raise HTTPException(status_code=408, detail="Request timeout")
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/health")
        async def health_endpoint():
            return await self.get_health_status()
        
        @self.app.get("/metrics")
        async def metrics_endpoint():
            return self.get_production_metrics()
    
    async def _processing_worker(self, worker_id: int):
        """Background worker for processing agent requests."""
        logging.info(f"Starting processing worker {worker_id}")
        
        while True:
            try:
                # Get request from queue
                queue_item = await self.request_queue.get()
                request_data = queue_item["request"]
                request_future = queue_item["future"]
                
                # Select appropriate agent
                agent_name = self._select_agent(request_data.agent_type)
                
                if agent_name not in self.agents:
                    request_future.set_exception(Exception(f"Agent {agent_name} not available"))
                    continue
                
                # Process request
                start_time = time.time()
                try:
                    agent = self.agents[agent_name]
                    response = await agent.chat_async(request_data.message)
                    
                    processing_time = time.time() - start_time
                    
                    # Update metrics
                    self.metrics["requests_processed"] += 1
                    self.metrics["total_processing_time"] += processing_time
                    self.metrics["agent_usage"][agent_name] += 1
                    
                    result = {
                        "response": response,
                        "agent_used": agent_name,
                        "processing_time": processing_time,
                        "worker_id": worker_id
                    }
                    
                    request_future.set_result(result)
                    
                except Exception as e:
                    self.metrics["requests_failed"] += 1
                    request_future.set_exception(e)
                
                finally:
                    self.request_queue.task_done()
                    
            except Exception as e:
                logging.error(f"Worker {worker_id} error: {e}")
                await asyncio.sleep(1)
    
    def _select_agent(self, agent_type: str) -> str:
        """Select appropriate agent based on request type."""
        agent_mapping = {
            "customer_service": "customer_service",
            "technical": "technical_support",
            "general": "general_purpose"
        }
        
        return agent_mapping.get(agent_type, "general_purpose")
    
    async def _wait_for_result(self, request: AgentRequest):
        """Wait for request processing to complete."""
        # This is simplified - in production you'd track futures properly
        await asyncio.sleep(0.1)  # Placeholder
        return {"response": "Processed", "status": "success"}
    
    async def get_health_status(self) -> dict:
        """Get comprehensive health status of all services."""
        health_status = {
            "overall_status": "healthy",
            "servers": {},
            "queue_size": self.request_queue.qsize(),
            "active_workers": len([w for w in self.processing_workers if not w.done()]),
            "timestamp": time.time()
        }
        
        unhealthy_servers = 0
        for name, manager in self.managers.items():
            try:
                is_healthy = manager.health_check()
                health_status["servers"][name] = {
                    "status": "healthy" if is_healthy else "unhealthy",
                    "endpoint": manager.base_url,
                    "model": manager.model_name
                }
                if not is_healthy:
                    unhealthy_servers += 1
            except Exception as e:
                health_status["servers"][name] = {
                    "status": "error",
                    "error": str(e)
                }
                unhealthy_servers += 1
        
        if unhealthy_servers > 0:
            health_status["overall_status"] = "degraded" if unhealthy_servers < len(self.managers) else "unhealthy"
        
        return health_status
    
    def get_production_metrics(self) -> dict:
        """Get production performance metrics."""
        total_requests = self.metrics["requests_processed"] + self.metrics["requests_failed"]
        avg_processing_time = (
            self.metrics["total_processing_time"] / self.metrics["requests_processed"]
            if self.metrics["requests_processed"] > 0 else 0
        )
        
        success_rate = (
            self.metrics["requests_processed"] / total_requests * 100
            if total_requests > 0 else 0
        )
        
        return {
            "total_requests": total_requests,
            "successful_requests": self.metrics["requests_processed"],
            "failed_requests": self.metrics["requests_failed"],
            "success_rate_percent": success_rate,
            "average_processing_time_seconds": avg_processing_time,
            "agent_usage_distribution": self.metrics["agent_usage"],
            "queue_size": self.request_queue.qsize()
        }
    
    async def start_production_server(self, host: str = "0.0.0.0", port: int = 8080):
        """Start the production FastAPI server."""
        config = uvicorn.Config(
            self.app,
            host=host,
            port=port,
            log_level="info",
            workers=1  # Single worker for simplicity
        )
        server = uvicorn.Server(config)
        await server.serve()

# Production deployment example
production_configs = {
    "customer_service": VLLMServerConfig(
        model_name="microsoft/Phi-3.5-mini-instruct",
        port=8000,
        gpu_memory_utilization=0.4,
        max_model_len=4096
    ),
    "technical_support": VLLMServerConfig(
        model_name="meta-llama/Llama-3.2-3B-Instruct",
        port=8001,
        gpu_memory_utilization=0.6,
        max_model_len=8192
    ),
    "general_purpose": VLLMServerConfig(
        model_name="Qwen/Qwen2.5-1.5B-Instruct",
        port=8002,
        gpu_memory_utilization=0.3,
        max_model_len=2048
    )
}

production_service = VLLMProductionService(production_configs)

# Initialize and start production service
# await production_service.initialize_production_environment()
# await production_service.start_production_server()
```
  

#### ZnaÄajke za poduzeÄ‡a i praÄ‡enje  

**Napredno praÄ‡enje performansi VLLM-a**:  
```python
import psutil
import nvidia_ml_py3 as nvml
from dataclasses import dataclass
from typing import List, Dict, Optional
import json
import asyncio

@dataclass
class PerformanceMetrics:
    timestamp: float
    requests_per_second: float
    average_latency_ms: float
    gpu_utilization_percent: float
    gpu_memory_used_gb: float
    cpu_utilization_percent: float
    memory_used_gb: float
    queue_length: int
    active_requests: int

class VLLMAdvancedMonitoring:
    def __init__(self, vllm_managers: Dict[str, VLLMManager]):
        self.managers = vllm_managers
        self.metrics_history = []
        self.alert_thresholds = {
            "gpu_utilization_max": 95,
            "gpu_memory_max_gb": 10,
            "latency_max_ms": 3000,
            "queue_length_max": 50,
            "error_rate_max_percent": 10
        }
        
        # Initialize NVIDIA ML for GPU monitoring
        try:
            nvml.nvmlInit()
            self.gpu_monitoring_available = True
            self.gpu_count = nvml.nvmlDeviceGetCount()
        except:
            self.gpu_monitoring_available = False
            self.gpu_count = 0
    
    async def collect_comprehensive_metrics(self) -> Dict[str, PerformanceMetrics]:
        """Collect detailed performance metrics for all VLLM instances."""
        all_metrics = {}
        
        for name, manager in self.managers.items():
            try:
                metrics = await self._collect_single_instance_metrics(name, manager)
                all_metrics[name] = metrics
            except Exception as e:
                logging.error(f"Failed to collect metrics for {name}: {e}")
                # Create error metrics
                all_metrics[name] = PerformanceMetrics(
                    timestamp=time.time(),
                    requests_per_second=0,
                    average_latency_ms=0,
                    gpu_utilization_percent=0,
                    gpu_memory_used_gb=0,
                    cpu_utilization_percent=0,
                    memory_used_gb=0,
                    queue_length=0,
                    active_requests=0
                )
        
        return all_metrics
    
    async def _collect_single_instance_metrics(self, name: str, manager: VLLMManager) -> PerformanceMetrics:
        """Collect metrics for a single VLLM instance."""
        timestamp = time.time()
        
        # Get VLLM-specific metrics via API
        vllm_stats = await self._get_vllm_stats(manager)
        
        # Get system metrics
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.virtual_memory()
        memory_used_gb = memory_info.used / (1024**3)
        
        # Get GPU metrics if available
        gpu_utilization = 0
        gpu_memory_used = 0
        
        if self.gpu_monitoring_available and self.gpu_count > 0:
            try:
                # Assuming first GPU for simplicity
                handle = nvml.nvmlDeviceGetHandleByIndex(0)
                gpu_util = nvml.nvmlDeviceGetUtilizationRates(handle)
                gpu_utilization = gpu_util.gpu
                
                gpu_mem = nvml.nvmlDeviceGetMemoryInfo(handle)
                gpu_memory_used = gpu_mem.used / (1024**3)
                
            except Exception as e:
                logging.warning(f"GPU monitoring failed: {e}")
        
        return PerformanceMetrics(
            timestamp=timestamp,
            requests_per_second=vllm_stats.get("requests_per_second", 0),
            average_latency_ms=vllm_stats.get("average_latency_ms", 0),
            gpu_utilization_percent=gpu_utilization,
            gpu_memory_used_gb=gpu_memory_used,
            cpu_utilization_percent=cpu_percent,
            memory_used_gb=memory_used_gb,
            queue_length=vllm_stats.get("queue_length", 0),
            active_requests=vllm_stats.get("active_requests", 0)
        )
    
    async def _get_vllm_stats(self, manager: VLLMManager) -> dict:
        """Get VLLM-specific statistics via API calls."""
        try:
            # Test inference to measure latency
            start_time = time.time()
            client = manager.get_openai_client()
            
            response = await asyncio.wait_for(
                asyncio.to_thread(
                    client.chat.completions.create,
                    model=manager.get_served_model_name(),
                    messages=[{"role": "user", "content": "ping"}],
                    max_tokens=1
                ),
                timeout=5.0
            )
            
            latency_ms = (time.time() - start_time) * 1000
            
            return {
                "average_latency_ms": latency_ms,
                "requests_per_second": 1000 / latency_ms if latency_ms > 0 else 0,
                "queue_length": 0,  # Would need to be exposed by VLLM
                "active_requests": 1  # Approximation
            }
            
        except Exception as e:
            logging.warning(f"Failed to get VLLM stats: {e}")
            return {
                "average_latency_ms": 0,
                "requests_per_second": 0,
                "queue_length": 0,
                "active_requests": 0
            }
    
    def generate_performance_report(self, time_window_minutes: int = 60) -> dict:
        """Generate comprehensive performance report."""
        cutoff_time = time.time() - (time_window_minutes * 60)
        recent_metrics = [
            metrics for metrics in self.metrics_history
            if any(m.timestamp > cutoff_time for m in metrics.values())
        ]
        
        if not recent_metrics:
            return {"error": "No recent metrics available"}
        
        report = {
            "time_window_minutes": time_window_minutes,
            "total_samples": len(recent_metrics),
            "instances": {}
        }
        
        # Analyze each instance
        for instance_name in self.managers.keys():
            instance_metrics = [
                metrics[instance_name] for metrics in recent_metrics
                if instance_name in metrics
            ]
            
            if instance_metrics:
                report["instances"][instance_name] = {
                    "avg_latency_ms": sum(m.average_latency_ms for m in instance_metrics) / len(instance_metrics),
                    "max_latency_ms": max(m.average_latency_ms for m in instance_metrics),
                    "avg_gpu_utilization": sum(m.gpu_utilization_percent for m in instance_metrics) / len(instance_metrics),
                    "avg_requests_per_second": sum(m.requests_per_second for m in instance_metrics) / len(instance_metrics),
                    "max_queue_length": max(m.queue_length for m in instance_metrics),
                    "availability_percent": (len(instance_metrics) / len(recent_metrics)) * 100
                }
        
        return report
    
    async def auto_scaling_recommendations(self) -> List[dict]:
        """Generate auto-scaling recommendations based on performance metrics."""
        recommendations = []
        
        if not self.metrics_history:
            return recommendations
        
        latest_metrics = self.metrics_history[-1]
        
        for instance_name, metrics in latest_metrics.items():
            # High latency recommendation
            if metrics.average_latency_ms > self.alert_thresholds["latency_max_ms"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_up",
                    "reason": f"High latency: {metrics.average_latency_ms:.0f}ms",
                    "suggestion": "Consider adding tensor parallelism or increasing GPU memory"
                })
            
            # High GPU utilization recommendation
            if metrics.gpu_utilization_percent > self.alert_thresholds["gpu_utilization_max"]:
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_out",
                    "reason": f"High GPU utilization: {metrics.gpu_utilization_percent:.1f}%",
                    "suggestion": "Consider adding additional GPU instances"
                })
            
            # Low utilization recommendation
            if (metrics.gpu_utilization_percent < 20 and 
                metrics.requests_per_second < 1):
                recommendations.append({
                    "instance": instance_name,
                    "type": "scale_down",
                    "reason": f"Low utilization: {metrics.gpu_utilization_percent:.1f}% GPU, {metrics.requests_per_second:.1f} RPS",
                    "suggestion": "Consider consolidating workloads or reducing resources"
                })
        
        return recommendations

# Advanced monitoring setup
monitoring = VLLMAdvancedMonitoring({
    "customer_service": vllm_manager,
    # Add other managers as needed
})

async def advanced_monitoring_loop():
    """Advanced monitoring with auto-scaling recommendations."""
    while True:
        try:
            # Collect metrics
            metrics = await monitoring.collect_comprehensive_metrics()
            monitoring.metrics_history.append(metrics)
            
            # Keep only last 1000 entries
            if len(monitoring.metrics_history) > 1000:
                monitoring.metrics_history = monitoring.metrics_history[-1000:]
            
            # Generate recommendations every 5 minutes
            if len(monitoring.metrics_history) % 10 == 0:  # Every 10th collection (5 minutes if collecting every 30s)
                recommendations = await monitoring.auto_scaling_recommendations()
                
                if recommendations:
                    logging.info(f"Auto-scaling recommendations: {recommendations}")
            
            # Generate performance report every hour
            if len(monitoring.metrics_history) % 120 == 0:  # Every 120th collection (1 hour)
                report = monitoring.generate_performance_report(60)
                logging.info(f"Performance report: {json.dumps(report, indent=2)}")
            
        except Exception as e:
            logging.error(f"Advanced monitoring error: {e}")
        
        await asyncio.sleep(30)  # Collect metrics every 30 seconds

# Start advanced monitoring
# asyncio.create_task(advanced_monitoring_loop())
```
  

#### Napredna konfiguracija i optimizacija  

**PredloÅ¡ci konfiguracije za produkcijski VLLM**:  
```python
from enum import Enum
from typing import Dict, Any

class DeploymentScenario(Enum):
    DEVELOPMENT = "development"
    STAGING = "staging"
    PRODUCTION_LOW = "production_low"
    PRODUCTION_HIGH = "production_high"
    ENTERPRISE = "enterprise"

class VLLMConfigTemplates:
    """Production-ready VLLM configuration templates."""
    
    @staticmethod
    def get_config_template(scenario: DeploymentScenario) -> Dict[str, Any]:
        """Get optimized configuration for deployment scenario."""
        
        templates = {
            DeploymentScenario.DEVELOPMENT: {
                "gpu_memory_utilization": 0.6,
                "max_model_len": 2048,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": None,
                "enable_prefix_caching": False,
                "max_num_seqs": 32,
                "max_num_batched_tokens": 2048
            },
            
            DeploymentScenario.STAGING: {
                "gpu_memory_utilization": 0.8,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 64,
                "max_num_batched_tokens": 4096
            },
            
            DeploymentScenario.PRODUCTION_LOW: {
                "gpu_memory_utilization": 0.85,
                "max_model_len": 4096,
                "tensor_parallel_size": 1,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 128,
                "max_num_batched_tokens": 8192,
                "enable_chunked_prefill": True
            },
            
            DeploymentScenario.PRODUCTION_HIGH: {
                "gpu_memory_utilization": 0.9,
                "max_model_len": 8192,
                "tensor_parallel_size": 2,
                "pipeline_parallel_size": 1,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 256,
                "max_num_batched_tokens": 16384,
                "enable_chunked_prefill": True,
                "speculative_model": "small_draft_model"
            },
            
            DeploymentScenario.ENTERPRISE: {
                "gpu_memory_utilization": 0.95,
                "max_model_len": 16384,
                "tensor_parallel_size": 4,
                "pipeline_parallel_size": 2,
                "quantization": "awq",
                "enable_prefix_caching": True,
                "max_num_seqs": 512,
                "max_num_batched_tokens": 32768,
                "enable_chunked_prefill": True,
                "speculative_model": "optimized_draft_model",
                "guided_decoding_backend": "outlines"
            }
        }
        
        return templates[scenario]
    
    @staticmethod
    def generate_vllm_command(model_name: str, 
                             scenario: DeploymentScenario,
                             port: int = 8000,
                             host: str = "0.0.0.0") -> List[str]:
        """Generate optimized VLLM command for deployment scenario."""
        
        config = VLLMConfigTemplates.get_config_template(scenario)
        
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_name,
            "--host", host,
            "--port", str(port),
            "--gpu-memory-utilization", str(config["gpu_memory_utilization"]),
            "--max-model-len", str(config["max_model_len"]),
            "--tensor-parallel-size", str(config["tensor_parallel_size"]),
            "--max-num-seqs", str(config["max_num_seqs"]),
            "--max-num-batched-tokens", str(config["max_num_batched_tokens"]),
            "--trust-remote-code",
            "--disable-log-requests"
        ]
        
        # Add optional parameters
        if config.get("quantization"):
            cmd.extend(["--quantization", config["quantization"]])
        
        if config.get("enable_prefix_caching"):
            cmd.append("--enable-prefix-caching")
        
        if config.get("enable_chunked_prefill"):
            cmd.append("--enable-chunked-prefill")
        
        if config.get("pipeline_parallel_size", 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(config["pipeline_parallel_size"])])
        
        if config.get("speculative_model"):
            cmd.extend(["--speculative-model", config["speculative_model"]])
        
        return cmd

# Usage examples
dev_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.DEVELOPMENT,
    port=8000
)

prod_cmd = VLLMConfigTemplates.generate_vllm_command(
    "microsoft/Phi-3.5-mini-instruct",
    DeploymentScenario.PRODUCTION_HIGH,
    port=8001
)

print(f"Development command: {' '.join(dev_cmd)}")
print(f"Production command: {' '.join(prod_cmd)}")
```
  
**Kontrolni popis za produkcijsku implementaciju VLLM-a**:  

âœ… **Optimizacija hardvera**:  
- Konfigurirajte paralelizam tensorima za postavke s viÅ¡e GPU-a  
- OmoguÄ‡ite kvantizaciju (AWQ/GPTQ) za uÄinkovitost memorije  
- Postavite optimalno koriÅ¡tenje GPU memorije (85-95%)  
- Konfigurirajte odgovarajuÄ‡e veliÄine grupa za kapacitet  

âœ… **PodeÅ¡avanje performansi**:  
- OmoguÄ‡ite predmemoriranje prefiksa za ponovljene upite  
- Konfigurirajte segmentirano popunjavanje za duge sekvence  
- Postavite spekulativno dekodiranje za brÅ¾i inference  
- Optimizirajte max_num_seqs prema hardveru  

âœ… **ZnaÄajke za produkciju**:  
- Postavite praÄ‡enje zdravlja i prikupljanje metrika  
- Konfigurirajte automatsko ponovno pokretanje i prebacivanje  
- Implementirajte redove zahtjeva i balansiranje optereÄ‡enja  
- Postavite sveobuhvatno biljeÅ¾enje i upozorenja  

âœ… **Sigurnost i pouzdanost**:  
- Konfigurirajte pravila vatrozida i kontrole pristupa  
- Postavite ograniÄenje brzine API-ja i autentifikaciju  
- Implementirajte postupno gaÅ¡enje i ÄiÅ¡Ä‡enje  
- Konfigurirajte sigurnosne kopije i oporavak od katastrofe  

âœ… **Testiranje integracije**:  
- Testirajte integraciju Microsoft Agent Frameworka  
- Validirajte scenarije visokog kapaciteta  
- Testirajte postupke prebacivanja i oporavka  
- Benchmarkirajte performanse pod optereÄ‡enjem  

**Usporedba s drugim rjeÅ¡enjima**:

| ZnaÄajka | VLLM | Foundry Local | Ollama |
|----------|------|---------------|--------|
| **Ciljana upotreba** | Produkcija visokog kapaciteta | Jednostavnost za poduzeÄ‡a | Razvoj i zajednica |
| **Performanse** | Maksimalni kapacitet | UravnoteÅ¾eno | Dobro |
| **UÄinkovitost memorije** | Optimizacija PagedAttention | Automatska optimizacija | Standardno |
| **SloÅ¾enost postavljanja** | Visoka (mnogi parametri) | Niska (automatski) | Niska (jednostavno) |
| **Skalabilnost** | Izvrsna (tensor/pipeline paralelizam) | Dobra | OgraniÄena |
| **Kvantizacija** | Napredna (AWQ, GPTQ, FP8) | Automatska | Standardna GGUF |
| **ZnaÄajke za poduzeÄ‡a** | Potrebna prilagoÄ‘ena implementacija | UgraÄ‘eno | Alati zajednice |
| **Najbolje za** | Agenti za produkciju velikih razmjera | Produkcija u poduzeÄ‡ima | Razvoj |

**Kada odabrati VLLM**:  
- **Zahtjevi visokog kapaciteta**: Obrada stotina zahtjeva u sekundi  
- **Implementacije velikih razmjera**: Postavke s viÅ¡e GPU-a i Ävorova  
- **KritiÄne performanse**: Odgovori u manje od sekunde u velikim razmjerima  
- **Napredna optimizacija**: Potreba za prilagoÄ‘enom kvantizacijom i grupiranjem  
- **UÄinkovitost resursa**: Maksimalno koriÅ¡tenje skupog GPU hardvera  

## Primjene SLM agenata u stvarnom svijetu  

### SLM agenti za korisniÄku podrÅ¡ku  
- **SLM sposobnosti**: Provjera raÄuna, resetiranje lozinki, provjera statusa narudÅ¾bi  
- **TroÅ¡kovne prednosti**: 10x smanjenje troÅ¡kova inferencea u usporedbi s LLM agentima  
- **Performanse**: BrÅ¾i odgovori uz dosljednu kvalitetu za rutinske upite  

### SLM agenti za poslovne procese  
- **Agenti za obradu faktura**: Ekstrakcija podataka, validacija informacija, prosljeÄ‘ivanje na odobrenje  
- **Agenti za upravljanje e-poÅ¡tom**: Kategorizacija, prioritizacija, automatsko sastavljanje odgovora  
- **Agenti za zakazivanje**: Koordinacija sastanaka, upravljanje kalendarima, slanje podsjetnika  

### Osobni digitalni asistenti SLM  
- **Agenti za upravljanje zadacima**: Stvaranje, aÅ¾uriranje, organizacija popisa zadataka  
- **Agenti za prikupljanje informacija**: IstraÅ¾ivanje tema, lokalno saÅ¾imanje nalaza  
- **Agenti za komunikaciju**: Sastavljanje e-poÅ¡te, poruka, privatnih objava na druÅ¡tvenim mreÅ¾ama  

### SLM agenti za trgovanje i financije  
- **Agenti za praÄ‡enje trÅ¾iÅ¡ta**: PraÄ‡enje cijena, identifikacija trendova u stvarnom vremenu  
- **Agenti za generiranje izvjeÅ¡taja**: Automatsko stvaranje dnevnih/tjednih saÅ¾etaka  
- **Agenti za procjenu rizika**: Procjena pozicija portfelja koristeÄ‡i lokalne podatke  

### SLM agenti za podrÅ¡ku u zdravstvu  
- **Agenti za zakazivanje pacijenata**: Koordinacija termina, slanje automatskih podsjetnika  
- **Agenti za dokumentaciju**: Generiranje medicinskih saÅ¾etaka, lokalnih izvjeÅ¡taja  
- **Agenti za upravljanje receptima**: PraÄ‡enje obnavljanja, provjera interakcija privatno  

## Microsoft Agent Framework: Razvoj agenata spremnih za produkciju  

### Pregled i arhitektura  

Microsoft Agent Framework pruÅ¾a sveobuhvatnu, profesionalnu platformu za izgradnju, implementaciju i upravljanje AI agentima koji mogu raditi i u oblaku i u offline okruÅ¾enjima. Okvir je posebno dizajniran za besprijekoran rad s malim jeziÄnim modelima i scenarijima edge raÄunalstva, Å¡to ga Äini idealnim za implementacije osjetljive na privatnost i ograniÄene resurse.  

**Osnovne komponente okvira**:  
- **Agent Runtime**: Lagano okruÅ¾enje za izvrÅ¡avanje optimizirano za edge ureÄ‘aje  
- **Sustav za integraciju alata**: ProÅ¡iriva arhitektura dodataka za povezivanje vanjskih usluga i API-ja  
- **Upravljanje stanjem**: Trajna memorija agenata i rukovanje kontekstom kroz sesije  
- **Sigurnosni sloj**: UgraÄ‘ene sigurnosne kontrole za implementaciju u poduzeÄ‡ima  
- **Orkestracijski motor**: Koordinacija viÅ¡e agenata i upravljanje radnim tokovima  

### KljuÄne znaÄajke za edge implementaciju  

**Arhitektura usmjerena na offline rad**: Microsoft Agent Framework dizajniran je prema principima offline rada, omoguÄ‡ujuÄ‡i agentima uÄinkovito funkcioniranje bez stalne internetske povezanosti. To ukljuÄuje lokalni inference modela, predmemorirane baze znanja, offline izvrÅ¡avanje alata i postupno smanjenje funkcionalnosti kada oblaÄne usluge nisu dostupne.  

**Optimizacija resursa**: Okvir pruÅ¾a inteligentno upravljanje resursima s automatskom optimizacijom memorije za SLM-ove, balansiranjem optereÄ‡enja CPU/GPU za edge ureÄ‘aje, adaptivnim odabirom modela na temelju dostupnih resursa i energetski uÄinkovitim obrascima inferencea za mobilnu implementaciju.  

**Sigurnost i privatnost**: Sigurnosne znaÄajke na razini poduzeÄ‡a ukljuÄuju lokalnu obradu podataka radi oÄuvanja privatnosti, Å¡ifrirane komunikacijske kanale agenata, kontrole pristupa temeljene na ulogama za sposobnosti agenata i zapisivanje radnji za potrebe usklaÄ‘enosti.  

### Integracija s Foundry Local  

Microsoft Agent Framework besprijekorno se integrira s Foundry Local kako bi pruÅ¾io cjelovito edge AI rjeÅ¡enje:  

**Automatsko otkrivanje modela**: Okvir automatski otkriva i povezuje se s instancama Foundry Local, otkriva dostupne SLM modele i odabire optimalne modele na temelju zahtjeva agenata i moguÄ‡nosti hardvera.  

**DinamiÄko uÄitavanje modela**: Agenti mogu dinamiÄki uÄitavati razliÄite SLM-ove za specifiÄne zadatke, omoguÄ‡ujuÄ‡i sustave agenata s viÅ¡e modela gdje razliÄiti modeli obraÄ‘uju razliÄite vrste zahtjeva, te automatsko prebacivanje izmeÄ‘u modela na temelju dostupnosti i performansi.  

**Optimizacija performansi**: Integrirani mehanizmi predmemoriranja smanjuju vrijeme uÄitavanja modela, grupiranje veza optimizira API pozive prema Foundry Localu, a inteligentno grupiranje poboljÅ¡ava kapacitet za viÅ¡e zahtjeva agenata.  

### Izrada agenata s Microsoft Agent Frameworkom  

#### Definicija i konfiguracija agenata  

```python
from microsoft_agent_framework import Agent, Tool, Config
from foundry_local import FoundryLocalManager

# Configure agent with Foundry Local integration
config = Config(
    name="customer-service-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    max_tokens=512,
    temperature=0.1,
    offline_mode=True
)

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent instance
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)
```
  
#### Integracija alata za edge scenarije  

```python
# Define tools for offline operation
@agent.tool
def lookup_customer_info(customer_id: str) -> dict:
    """Look up customer information from local database."""
    # Local database query - works offline
    return local_db.get_customer(customer_id)

@agent.tool
def create_support_ticket(issue: str, priority: str) -> str:
    """Create a support ticket in local system."""
    # Local ticket creation with sync when online
    ticket_id = local_system.create_ticket(issue, priority)
    return f"Ticket {ticket_id} created successfully"

@agent.tool
def schedule_callback(customer_id: str, preferred_time: str) -> str:
    """Schedule a callback for the customer."""
    # Local scheduling with calendar integration
    return local_calendar.schedule(customer_id, preferred_time)
```
  
#### Orkestracija viÅ¡e agenata  

```python
from microsoft_agent_framework import AgentOrchestrator

# Create specialized agents for different domains
scheduling_agent = Agent(
    config=Config(
        name="scheduling-agent",
        model_alias="qwen2.5-0.5b",  # Lightweight for simple tasks
        specialized_for="scheduling"
    )
)

technical_support_agent = Agent(
    config=Config(
        name="technical-agent",
        model_alias="phi-4-mini",  # More capable for complex issues
        specialized_for="technical_support"
    )
)

# Orchestrate multiple agents
orchestrator = AgentOrchestrator([
    scheduling_agent,
    technical_support_agent
])

# Route requests based on intent
result = orchestrator.process_request(
    "I need to schedule a callback for a technical issue",
    routing_strategy="intent-based"
)
```
  

### Napredni obrasci implementacije na edge ureÄ‘ajima  

#### Hijerarhijska arhitektura agenata  

**Lokalni klasteri agenata**: Implementirajte viÅ¡e specijaliziranih SLM agenata na edge ureÄ‘ajima, svaki optimiziran za specifiÄne zadatke. Koristite lagane modele poput Qwen2.5-0.5B za jednostavno usmjeravanje i zakazivanje, srednje modele poput Phi-4-Mini za korisniÄku podrÅ¡ku i dokumentaciju, te veÄ‡e modele za sloÅ¾eno zakljuÄivanje kada resursi to dopuÅ¡taju.  

**Koordinacija edge-to-cloud**: Implementirajte inteligentne obrasce eskalacije gdje lokalni agenti obraÄ‘uju rutinske zadatke, oblaÄni agenti pruÅ¾aju sloÅ¾eno zakljuÄivanje kada je povezanost omoguÄ‡ena, a besprijekorni prijenos izmeÄ‘u edge i oblaÄne obrade odrÅ¾ava kontinuitet.  

#### Konfiguracije implementacije  

**Implementacija na jednom ureÄ‘aju**:  
```yaml
deployment:
  type: single-device
  hardware: edge-device
  models:
    - alias: "phi-4-mini"
      primary: true
      tasks: ["conversation", "reasoning"]
    - alias: "qwen2.5-0.5b"
      secondary: true
      tasks: ["routing", "classification"]
  agents:
    - name: "primary-agent"
      model: "phi-4-mini"
      tools: ["database", "calendar", "email"]
```
  
**Distribuirana implementacija na edge ureÄ‘ajima**:  
```yaml
deployment:
  type: distributed-edge
  nodes:
    - id: "edge-1"
      agents: ["customer-service", "scheduling"]
      models: ["phi-4-mini"]
    - id: "edge-2"
      agents: ["technical-support", "documentation"]
      models: ["qwen2.5-coder-0.5b"]
  coordination:
    load_balancing: true
    failover: automatic
```
  

### Optimizacija performansi za edge agente  

#### Strategije odabira modela  

**Dodjela modela prema zadatku**: Microsoft Agent Framework omoguÄ‡uje inteligentan odabir modela na temelju sloÅ¾enosti zadatka i zahtjeva:  

- **Jednostavni zadaci** (Q&A, usmjeravanje): Qwen2.5-0.5B (500MB, <100ms odgovor)  
- **Umjereni zadaci** (korisniÄka podrÅ¡ka, zakazivanje): Phi-4-Mini (2.4GB, 200-500ms odgovor)  
- **SloÅ¾eni zadaci** (tehniÄka analiza, planiranje): Phi-4 (7GB, 1-3s odgovor kada resursi dopuÅ¡taju)  

**DinamiÄko prebacivanje modela**: Agenti mogu prebacivati izmeÄ‘u modela na temelju trenutnog optereÄ‡enja sustava, procjene sloÅ¾enosti zadatka, razine prioriteta korisnika i dostupnih hardverskih resursa.  

#### Upravljanje memorijom i resursima  

```python
# Configure resource constraints for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="4GB",
    max_concurrent_agents=3,
    model_cache_size="2GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```
  

### Obrasci integracije za poduzeÄ‡a  

#### Sigurnost i usklaÄ‘enost  

**Lokalna obrada podataka**: Sva obrada agenata odvija se lokalno, osiguravajuÄ‡i da osjetljivi podaci nikada ne napuÅ¡taju edge ureÄ‘aj. To ukljuÄuje zaÅ¡titu informacija o korisnicima, usklaÄ‘enost s HIPAA za zdravstvene agente, sigurnost financijskih podataka za bankovne agente i usklaÄ‘enost s GDPR-om za europske implementacije.  

**Kontrola pristupa**: Dozvole temeljene na ulogama kontroliraju koje alate agenti mogu koristiti, autentifikacija korisnika za interakcije s agentima i zapisivanje svih radnji i odluka agenata.  

#### PraÄ‡enje i preglednost  

```python
from microsoft_agent_framework import AgentMonitor

# Set up monitoring for edge agents
monitor = AgentMonitor(
    metrics=["response_time", "success_rate", "resource_usage"],
    alerts=[
        {"metric": "response_time", "threshold": "2s", "action": "scale_down_model"},
        {"metric": "memory_usage", "threshold": "80%", "action": "unload_idle_agents"}
    ],
    local_storage=True  # Store metrics locally for offline operation
)

agent.add_monitor(monitor)
```
  

### Primjeri implementacije u stvarnom svijetu  

#### Sustav agenata za maloprodaju na edge ureÄ‘ajima  

```python
# Retail kiosk agent for in-store customer assistance
retail_agent = Agent(
    config=Config(
        name="retail-assistant",
        model_alias="phi-4-mini",
        context="You are a helpful retail assistant in an electronics store."
    )
)

@retail_agent.tool
def check_inventory(product_sku: str) -> dict:
    """Check local inventory for a product."""
    return local_inventory.lookup(product_sku)

@retail_agent.tool
def find_alternatives(product_category: str) -> list:
    """Find alternative products in the same category."""
    return local_catalog.find_similar(product_category)

@retail_agent.tool
def create_price_quote(items: list) -> dict:
    """Generate a price quote for multiple items."""
    return pricing_engine.calculate_quote(items)
```
  
#### Agent za podrÅ¡ku u zdravstvu  

```python
# HIPAA-compliant patient support agent
healthcare_agent = Agent(
    config=Config(
        name="patient-support",
        model_alias="phi-4-mini",
        privacy_mode=True,  # Enhanced privacy for healthcare
        compliance=["HIPAA"]
    )
)

@healthcare_agent.tool
def check_appointment_availability(provider_id: str, date_range: str) -> list:
    """Check appointment slots with healthcare provider."""
    return local_scheduling.get_availability(provider_id, date_range)

@healthcare_agent.tool
def access_patient_portal(patient_id: str, auth_token: str) -> dict:
    """Secure access to patient information."""
    if security.validate_token(auth_token):
        return patient_portal.get_summary(patient_id)
    return {"error": "Authentication failed"}
```
  

### Najbolje prakse za Microsoft Agent Framework  

#### Smjernice za razvoj  

1. **PoÄnite jednostavno**: ZapoÄnite s scenarijima jednog agenta prije nego Å¡to izgradite sloÅ¾ene sustave s viÅ¡e agenata  
2. **Prilagodba modela**: Odaberite najmanji model koji zadovoljava vaÅ¡e zahtjeve za toÄnost  
3. **Dizajn alata**: Kreirajte fokusirane, jednostrane alate umjesto sloÅ¾enih alata s viÅ¡e funkcija  
4. **Rukovanje greÅ¡kama**: Implementirajte postupno smanjenje funkcionalnosti za offline scenarije i kvarove modela  
5. **Testiranje**: Temeljito testirajte agente u offline uvjetima i okruÅ¾enjima s ograniÄenim resursima  

#### Najbolje prakse za implementaciju  

1. **Postupno uvoÄ‘enje**: Prvo implementirajte za male grupe korisnika, paÅ¾ljivo pratite metrike performansi  
2. **PraÄ‡enje resursa**: Postavite upozorenja za pragove memorije, CPU-a i vremena odgovora  
3. **Strategije povratka**: Uvijek imajte rezervne planove za kvarove modela ili iscrpljenje resursa  
4. **Sigurnost na prvom mjestu**: Implementirajte sigurnosne kontrole od poÄetka, a ne naknadno  
5. **Dokumentacija**: OdrÅ¾
**Odabir okvira za implementaciju agenata**: Odaberite okvire za optimizaciju na temelju ciljanog hardvera i zahtjeva agenata. Koristite Llama.cpp za implementaciju agenata optimiziranih za CPU, Apple MLX za aplikacije agenata na Apple Siliconu i ONNX za kompatibilnost agenata na viÅ¡e platformi.

## PraktiÄna konverzija SLM agenata i primjeri upotrebe

### Scenariji implementacije agenata u stvarnom svijetu

**Mobilne aplikacije agenata**: Q4_K formati izvrsno funkcioniraju u aplikacijama za pametne telefone s minimalnim zahtjevima za memorijom, dok Q8_0 pruÅ¾a uravnoteÅ¾ene performanse za sustave agenata na tabletima. Q5_K formati nude vrhunsku kvalitetu za mobilne produktivne agente.

**RaÄunalstvo agenata na stolnim raÄunalima i rubnim ureÄ‘ajima**: Q5_K pruÅ¾a optimalne performanse za aplikacije agenata na stolnim raÄunalima, Q8_0 osigurava visoku kvalitetu zakljuÄivanja za radne stanice, a Q4_K omoguÄ‡uje uÄinkovitu obradu na rubnim ureÄ‘ajima.

**IstraÅ¾ivaÄki i eksperimentalni agenti**: Napredni formati kvantizacije omoguÄ‡uju istraÅ¾ivanje ultra-niske preciznosti zakljuÄivanja agenata za akademska istraÅ¾ivanja i aplikacije dokazivanja koncepta koje zahtijevaju ekstremna ograniÄenja resursa.

### Benchmarking performansi SLM agenata

**Brzina zakljuÄivanja agenata**: Q4_K postiÅ¾e najbrÅ¾e vrijeme odgovora agenata na mobilnim CPU-ima, Q5_K pruÅ¾a uravnoteÅ¾en omjer brzine i kvalitete za opÄ‡e aplikacije agenata, Q8_0 nudi vrhunsku kvalitetu za sloÅ¾ene zadatke agenata, a eksperimentalni formati omoguÄ‡uju maksimalan protok za specijalizirani hardver agenata.

**Zahtjevi za memorijom agenata**: Razine kvantizacije za agente kreÄ‡u se od Q2_K (manje od 500MB za male modele agenata) do Q8_0 (otprilike 50% izvorne veliÄine), dok eksperimentalne konfiguracije postiÅ¾u maksimalnu kompresiju za okruÅ¾enja s ograniÄenim resursima.

## Izazovi i razmatranja za SLM agente

### Kompromisi performansi u sustavima agenata

Implementacija SLM agenata zahtijeva paÅ¾ljivo razmatranje kompromisa izmeÄ‘u veliÄine modela, brzine odgovora agenata i kvalitete izlaza. Dok Q4_K nudi iznimnu brzinu i uÄinkovitost za mobilne agente, Q8_0 pruÅ¾a vrhunsku kvalitetu za sloÅ¾ene zadatke agenata. Q5_K predstavlja sredinu koja je prikladna za veÄ‡inu opÄ‡ih aplikacija agenata.

### Kompatibilnost hardvera za SLM agente

RazliÄiti rubni ureÄ‘aji imaju razliÄite moguÄ‡nosti za implementaciju SLM agenata. Q4_K uÄinkovito radi na osnovnim procesorima za jednostavne agente, Q5_K zahtijeva umjerene raÄunalne resurse za uravnoteÅ¾ene performanse agenata, a Q8_0 koristi prednosti vrhunskog hardvera za napredne moguÄ‡nosti agenata.

### Sigurnost i privatnost u sustavima SLM agenata

Iako SLM agenti omoguÄ‡uju lokalnu obradu za poboljÅ¡anu privatnost, potrebno je implementirati odgovarajuÄ‡e sigurnosne mjere za zaÅ¡titu modela agenata i podataka u rubnim okruÅ¾enjima. To je posebno vaÅ¾no pri implementaciji formata agenata visoke preciznosti u poslovnim okruÅ¾enjima ili komprimiranih formata agenata u aplikacijama koje obraÄ‘uju osjetljive podatke.

## BuduÄ‡i trendovi u razvoju SLM agenata

Landskap SLM agenata nastavlja se razvijati s napretkom u tehnikama kompresije, metodama optimizacije i strategijama implementacije na rubnim ureÄ‘ajima. BuduÄ‡i razvoj ukljuÄuje uÄinkovitije algoritme kvantizacije za modele agenata, poboljÅ¡ane metode kompresije za radne procese agenata i bolju integraciju s hardverskim akceleratorima za obradu agenata na rubnim ureÄ‘ajima.

**PredviÄ‘anja trÅ¾iÅ¡ta za SLM agente**: Prema nedavnim istraÅ¾ivanjima, automatizacija voÄ‘ena agentima mogla bi eliminirati 40â€“60% ponavljajuÄ‡ih kognitivnih zadataka u poslovnim radnim procesima do 2027. godine, pri Äemu SLM-ovi predvode ovu transformaciju zbog svoje ekonomiÄnosti i fleksibilnosti implementacije.

**TehnoloÅ¡ki trendovi u SLM agentima**:
- **Specijalizirani SLM agenti**: Modeli specifiÄni za domenu, obuÄeni za odreÄ‘ene zadatke agenata i industrije
- **RaÄunalstvo agenata na rubnim ureÄ‘ajima**: PoboljÅ¡ane moguÄ‡nosti agenata na ureÄ‘ajima s poboljÅ¡anom privatnoÅ¡Ä‡u i smanjenom latencijom
- **Orkestracija agenata**: Bolja koordinacija izmeÄ‘u viÅ¡e SLM agenata s dinamiÄkim usmjeravanjem i balansiranjem optereÄ‡enja
- **Demokratizacija**: Fleksibilnost SLM-ova omoguÄ‡uje Å¡iru participaciju u razvoju agenata meÄ‘u organizacijama

## PoÄetak rada sa SLM agentima

### Korak 1: Postavljanje okruÅ¾enja Microsoft Agent Framework

**Instalirajte ovisnosti**:
```bash
# Install Microsoft Agent Framework
pip install microsoft-agent-framework

# Install Foundry Local SDK for edge deployment
pip install foundry-local-sdk

# Install additional dependencies for edge agents
pip install openai asyncio
```

**Inicijalizirajte Foundry Local**:
```bash
# Start Foundry Local service
foundry service start

# Load default model for agent development
foundry model run phi-4-mini
```

### Korak 2: Odaberite svoj SLM za aplikacije agenata
Popularne opcije za Microsoft Agent Framework:
- **Microsoft Phi-4 Mini (3.8B)**: Izvrsno za opÄ‡e zadatke agenata s uravnoteÅ¾enim performansama
- **Qwen2.5-0.5B (0.5B)**: Ultra-uÄinkovit za jednostavne agente za usmjeravanje i klasifikaciju
- **Qwen2.5-Coder-0.5B (0.5B)**: Specijaliziran za zadatke agenata vezane uz kodiranje
- **Phi-4 (7B)**: Napredno zakljuÄivanje za sloÅ¾ene scenarije na rubnim ureÄ‘ajima kada resursi dopuÅ¡taju

### Korak 3: Kreirajte svog prvog agenta s Microsoft Agent Framework

**Osnovno postavljanje agenata**:
```python
from microsoft_agent_framework import Agent, Config
from foundry_local import FoundryLocalManager

# Initialize Foundry Local connection
foundry = FoundryLocalManager("phi-4-mini")

# Create agent configuration
config = Config(
    name="my-first-agent",
    model_provider="foundry-local",
    model_alias="phi-4-mini",
    offline_mode=True
)

# Create and configure agent
agent = Agent(
    config=config,
    model_endpoint=foundry.endpoint,
    api_key=foundry.api_key
)

# Define a simple tool
@agent.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Test the agent
response = agent.chat("What time is it?")
print(response)
```

### Korak 4: Definirajte opseg i zahtjeve agenata
ZapoÄnite s fokusiranim, dobro definiranim aplikacijama agenata koristeÄ‡i Microsoft Agent Framework:
- **Agenti za jednu domenu**: KorisniÄka podrÅ¡ka ILI zakazivanje ILI istraÅ¾ivanje
- **Jasni ciljevi agenata**: SpecifiÄni, mjerljivi ciljevi za performanse agenata
- **OgraniÄena integracija alata**: Maksimalno 3-5 alata za poÄetnu implementaciju agenata
- **Definirane granice agenata**: Jasni putevi eskalacije za sloÅ¾ene scenarije
- **Dizajn usmjeren na rubne ureÄ‘aje**: Prioritet offline funkcionalnosti i lokalne obrade

### Korak 5: Implementirajte implementaciju na rubnim ureÄ‘ajima s Microsoft Agent Framework

**Konfiguracija resursa**:
```python
from microsoft_agent_framework import ResourceConfig

# Configure for edge deployment
resource_config = ResourceConfig(
    max_memory_usage="2GB",
    max_concurrent_agents=2,
    model_cache_size="1GB",
    auto_unload_idle_models=True,
    power_management=True
)

agent = Agent(
    config=config,
    resource_limits=resource_config
)
```

**Implementirajte sigurnosne mjere za rubne agente**:
- **Lokalna validacija unosa**: Provjerite zahtjeve bez ovisnosti o oblaku
- **Offline filtriranje izlaza**: Osigurajte da odgovori zadovoljavaju lokalne standarde kvalitete
- **Sigurnosne kontrole na rubnim ureÄ‘ajima**: Implementirajte sigurnost bez potrebe za internetskom povezivoÅ¡Ä‡u
- **Lokalno praÄ‡enje**: Pratite performanse i oznaÄite probleme koristeÄ‡i telemetriju na rubnim ureÄ‘ajima

### Korak 6: Mjerite i optimizirajte performanse agenata na rubnim ureÄ‘ajima
- **Stope zavrÅ¡etka zadataka agenata**: Pratite stope uspjeha u offline scenarijima
- **Vrijeme odgovora agenata**: Osigurajte vrijeme odgovora ispod jedne sekunde za implementaciju na rubnim ureÄ‘ajima
- **IskoriÅ¡tenost resursa**: Pratite memoriju, CPU i potroÅ¡nju baterije na rubnim ureÄ‘ajima
- **EkonomiÄnost**: Usporedite troÅ¡kove implementacije na rubnim ureÄ‘ajima s alternativama temeljenim na oblaku
- **Pouzdanost offline rada**: Mjerite performanse agenata tijekom prekida mreÅ¾e

## KljuÄni zakljuÄci za implementaciju SLM agenata

1. **SLM-ovi su dovoljni za agente**: Za veÄ‡inu zadataka agenata, mali modeli funkcioniraju jednako dobro kao veliki, uz znaÄajne prednosti
2. **EkonomiÄnost agenata**: 10-30x jeftinije za pokretanje SLM agenata, Å¡to ih Äini ekonomiÄno odrÅ¾ivima za Å¡iroku implementaciju
3. **Specijalizacija funkcionira za agente**: Fino podeÅ¡eni SLM-ovi Äesto nadmaÅ¡uju opÄ‡e LLM-ove u specifiÄnim aplikacijama agenata
4. **Hibridna arhitektura agenata**: Koristite SLM-ove za rutinske zadatke agenata, LLM-ove za sloÅ¾eno zakljuÄivanje kada je potrebno
5. **Microsoft Agent Framework omoguÄ‡uje produkcijsku implementaciju**: PruÅ¾a alate poslovne klase za izgradnju, implementaciju i upravljanje agentima na rubnim ureÄ‘ajima
6. **Principi dizajna usmjereni na rubne ureÄ‘aje**: Agenti sposobni za offline rad s lokalnom obradom osiguravaju privatnost i pouzdanost
7. **Integracija Foundry Local**: Besprijekorna povezanost izmeÄ‘u Microsoft Agent Frameworka i lokalnog zakljuÄivanja modela
8. **BuduÄ‡nost su SLM agenti**: Mali jeziÄni modeli s produkcijskim okvirima predstavljaju buduÄ‡nost agentiÄke AI, omoguÄ‡ujuÄ‡i demokratiziranu i uÄinkovitu implementaciju agenata

## Reference i dodatna literatura

### Osnovni istraÅ¾ivaÄki radovi i publikacije

#### AI agenti i agentiÄki sustavi
- **"Language Agents as Optimizable Graphs"** (2024) - Temeljno istraÅ¾ivanje o arhitekturi agenata i strategijama optimizacije
  - Autori: Wenyue Hua, Lishan Yang, et al.
  - Link: https://arxiv.org/abs/2402.16823
  - KljuÄni uvidi: Dizajn agenata temeljen na grafovima i strategije optimizacije

- **"The Rise and Potential of Large Language Model Based Agents"** (2023)
  - Autori: Zhiheng Xi, Wenxiang Chen, et al.
  - Link: https://arxiv.org/abs/2309.07864
  - KljuÄni uvidi: Sveobuhvatni pregled sposobnosti i aplikacija agenata temeljenih na LLM-ovima

- **"Cognitive Architectures for Language Agents"** (2024)
  - Autori: Theodore Sumers, Shunyu Yao, et al.
  - Link: https://arxiv.org/abs/2309.02427
  - KljuÄni uvidi: Kognitivni okviri za dizajn inteligentnih agenata

#### Mali jeziÄni modeli i optimizacija
- **"Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone"** (2024)
  - Autori: Microsoft Research Team
  - Link: https://arxiv.org/abs/2404.14219
  - KljuÄni uvidi: Principi dizajna SLM-ova i strategije implementacije na mobilnim ureÄ‘ajima

- **"Qwen2.5 Technical Report"** (2024)
  - Autori: Alibaba Cloud Team
  - Link: https://arxiv.org/abs/2407.10671
  - KljuÄni uvidi: Napredne tehnike obuke SLM-ova i optimizacija performansi

- **"TinyLlama: An Open-Source Small Language Model"** (2024)
  - Autori: Peiyuan Zhang, Guangtao Zeng, et al.
  - Link: https://arxiv.org/abs/2401.02385
  - KljuÄni uvidi: Dizajn ultra-kompaktnih modela i uÄinkovitost obuke

### SluÅ¾bena dokumentacija i okviri

#### Microsoft Agent Framework
- **SluÅ¾bena dokumentacija**: https://docs.microsoft.com/en-us/azure/ai-services/agents/
- **GitHub repozitorij**: https://github.com/microsoft/agent-framework

#### Foundry Local
- **Primarni repozitorij**: https://github.com/microsoft/foundry-local
- **Dokumentacija**: https://github.com/microsoft/foundry-local/blob/main/docs/README.md


#### VLLM
- **Glavni repozitorij**: https://github.com/vllm-project/vllm
- **Dokumentacija**: https://docs.vllm.ai/


#### Ollama
- **SluÅ¾bena web stranica**: https://ollama.ai/
- **GitHub repozitorij**: https://github.com/ollama/ollama

### Okviri za optimizaciju modela

#### Llama.cpp
- **Repozitorij**: https://github.com/ggml-org/llama.cpp


#### Microsoft Olive
- **Dokumentacija**: https://microsoft.github.io/Olive/
- **GitHub repozitorij**: https://github.com/microsoft/Olive

#### OpenVINO
- **SluÅ¾bena stranica**: https://docs.openvino.ai/

#### Apple MLX
- **Repozitorij**: https://github.com/ml-explore/mlx

### Industrijski izvjeÅ¡taji i analiza trÅ¾iÅ¡ta

#### IstraÅ¾ivanje trÅ¾iÅ¡ta AI agenata
- **"The State of AI Agents 2025"** - McKinsey Global Institute
  - Link: https://www.mckinsey.com/capabilities/mckinsey-digital/our-insights/ai-agents-2025
  - KljuÄni uvidi: Trendovi trÅ¾iÅ¡ta i obrasci usvajanja u poslovnom okruÅ¾enju

#### TehniÄki benchmarkovi

- **"Edge AI Inference Benchmarks"** - MLPerf
  - Link: https://mlcommons.org/en/inference-edge/
  - KljuÄni uvidi: Standardizirani metriÄki podaci za implementaciju na rubnim ureÄ‘ajima

### Standardi i specifikacije

#### Formati modela i standardi
- **ONNX (Open Neural Network Exchange)**: https://onnx.ai/
  - Format modela za interoperabilnost na viÅ¡e platformi
- **GGUF specifikacija**: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
  - Kvantizirani format modela za zakljuÄivanje na CPU-u
- **OpenAI API specifikacija**: https://platform.openai.com/docs/api-reference
  - Standardni API format za integraciju jeziÄnih modela

#### Sigurnost i usklaÄ‘enost
- **NIST AI Risk Management Framework**: https://www.nist.gov/itl/ai-risk-management-framework
- **ISO/IEC 23053:2022 - AI sustavi**: Okvir za AI sustave i sigurnost
- **IEEE standardi za AI**: https://standards.ieee.org/industry-connections/ai/

Pomak prema agentima voÄ‘enim SLM-ovima predstavlja temeljnu promjenu u pristupu implementaciji AI-a. Microsoft Agent Framework, u kombinaciji s lokalnim platformama i uÄinkovitim malim jeziÄnim modelima, pruÅ¾a cjelovito rjeÅ¡enje za izgradnju agenata spremnih za produkciju koji uÄinkovito djeluju u rubnim okruÅ¾enjima. Fokusiranjem na uÄinkovitost, specijalizaciju i praktiÄnu upotrebu, ovaj tehnoloÅ¡ki paket Äini AI agente dostupnijima, povoljnijima i uÄinkovitijima za stvarne aplikacije u svim industrijama i okruÅ¾enjima rubnog raÄunalstva.

Kako napredujemo kroz 2025. godinu, kombinacija sve sposobnijih malih modela, sofisticiranih okvira agenata poput Microsoft Agent Frameworka i robusnih platformi za implementaciju na rubnim ureÄ‘ajima otkljuÄat Ä‡e nove moguÄ‡nosti za autonomne sustave koji mogu uÄinkovito djelovati na rubnim ureÄ‘ajima uz odrÅ¾avanje privatnosti, smanjenje troÅ¡kova i pruÅ¾anje izvanrednih korisniÄkih iskustava.

**SljedeÄ‡i koraci za implementaciju**:
1. **IstraÅ¾ite pozivanje funkcija**: NauÄite kako SLM-ovi upravljaju integracijom alata i strukturiranim izlazima
2. **Savladajte Model Context Protocol (MCP)**: Razumite napredne obrasce komunikacije agenata
3. **Izgradite produkcijske agente**: Koristite Microsoft Agent Framework za implementacije poslovne klase
4. **Optimizirajte za rubne ureÄ‘aje**: Primijenite napredne tehnike optimizacije za okruÅ¾enja s ograniÄenim resursima


## â¡ï¸ Å to slijedi

- [02: Pozivanje funkcija u malim jeziÄnim modelima (SLM-ovima)](./02.FunctionCalling.md)

---

**Izjava o odricanju odgovornosti**:  
Ovaj dokument je preveden pomoÄ‡u AI usluge za prevoÄ‘enje [Co-op Translator](https://github.com/Azure/co-op-translator). Iako nastojimo osigurati toÄnost, imajte na umu da automatski prijevodi mogu sadrÅ¾avati pogreÅ¡ke ili netoÄnosti. Izvorni dokument na izvornom jeziku treba smatrati autoritativnim izvorom. Za kljuÄne informacije preporuÄuje se profesionalni prijevod od strane ljudskog prevoditelja. Ne preuzimamo odgovornost za nesporazume ili pogreÅ¡na tumaÄenja koja proizlaze iz koriÅ¡tenja ovog prijevoda.