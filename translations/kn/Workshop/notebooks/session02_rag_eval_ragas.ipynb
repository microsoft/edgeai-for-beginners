{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5392a8a8",
   "metadata": {},
   "source": [
    "# ಸೆಷನ್ 2 – ರಾಗಗಳೊಂದಿಗೆ RAG ಮೌಲ್ಯಮಾಪನ\n",
    "\n",
    "ರಾಗಗಳ ಮೆಟ್ರಿಕ್ಸ್ ಬಳಸಿ ಕನಿಷ್ಠ RAG ಪೈಪ್ಲೈನ್ ಅನ್ನು ಮೌಲ್ಯಮಾಪನ ಮಾಡಿ: answer_relevancy, faithfulness, context_precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34473b",
   "metadata": {},
   "source": [
    "# ದೃಶ್ಯ\n",
    "ಈ ದೃಶ್ಯವು ಸ್ಥಳೀಯವಾಗಿ ಕನಿಷ್ಠ Retrieval Augmented Generation (RAG) ಪೈಪ್ಲೈನ್ ಅನ್ನು ಮೌಲ್ಯಮಾಪನ ಮಾಡುತ್ತದೆ. ನಾವು:\n",
    "- ಒಂದು ಸಣ್ಣ ಸಿಂಥೆಟಿಕ್ ಡಾಕ್ಯುಮೆಂಟ್ ಕಾರ್ಪಸ್ ಅನ್ನು ವ್ಯಾಖ್ಯಾನಿಸುತ್ತೇವೆ.\n",
    "- ಡಾಕ್ಸ್ ಅನ್ನು ಎम्बೆಡ್ ಮಾಡಿ ಮತ್ತು ಸರಳ ಸಾದೃಶ್ಯ ರಿಟ್ರೀವರ್ ಅನ್ನು ಜಾರಿಗೆ ತರುತ್ತೇವೆ.\n",
    "- ಸ್ಥಳೀಯ ಮಾದರಿಯನ್ನು (Foundry Local / OpenAI-ಸಮ್ಮತ) ಬಳಸಿ ನೆಲಸಿದ ಉತ್ತರಗಳನ್ನು ರಚಿಸುತ್ತೇವೆ.\n",
    "- ragas ಮೆಟ್ರಿಕ್ಸ್ (`answer_relevancy`, `faithfulness`, `context_precision`) ಅನ್ನು ಲೆಕ್ಕಹಾಕುತ್ತೇವೆ.\n",
    "- ವೇಗದ ಮೋಡ್ (ಪರಿಸರ `RAG_FAST=1`) ಅನ್ನು ಬೆಂಬಲಿಸುತ್ತೇವೆ, ಇದು ತ್ವರಿತ ಪುನರಾವೃತ್ತಿಗಾಗಿ ಕೇವಲ ಉತ್ತರ ಸಂಬಂಧಿತತೆಯನ್ನು ಲೆಕ್ಕಹಾಕುತ್ತದೆ.\n",
    "\n",
    "ನಿಮ್ಮ ಸ್ಥಳೀಯ ಮಾದರಿ + ಎम्बೆಡಿಂಗ್ಸ್ ಸ್ಟ್ಯಾಕ್ ವಿಸ್ತೃತ ಕಾರ್ಪಸ್‌ಗಳಿಗೆ ವಿಸ್ತರಿಸುವ ಮೊದಲು ವಾಸ್ತವಿಕವಾಗಿ ನೆಲಸಿದ ಉತ್ತರಗಳನ್ನು ಉತ್ಪಾದಿಸುತ್ತಿದೆಯೇ ಎಂದು ಪರಿಶೀಲಿಸಲು ಈ ನೋಟ್ಬುಕ್ ಅನ್ನು ಬಳಸಿ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb1aa2",
   "metadata": {},
   "source": [
    "### ವಿವರಣೆ: ಅವಲಂಬನೆ ಸ್ಥಾಪನೆ\n",
    "ಅಗತ್ಯವಾದ ಗ್ರಂಥಾಲಯಗಳನ್ನು ಸ್ಥಾಪಿಸುತ್ತದೆ:\n",
    "- ಸ್ಥಳೀಯ ಮಾದರಿ ನಿರ್ವಹಣೆಗೆ `foundry-local-sdk`.\n",
    "- `openai` ಕ್ಲೈಂಟ್ ಇಂಟರ್ಫೇಸ್.\n",
    "- ದಟ್ಟವಾದ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳಿಗೆ `sentence-transformers`.\n",
    "- ಮೌಲ್ಯಮಾಪನ ಮತ್ತು ಮೆಟ್ರಿಕ್ ಗಣನೆಗೆ `ragas` + `datasets`.\n",
    "- ragas LLM ಇಂಟರ್ಫೇಸ್‌ಗಾಗಿ `langchain-openai` ಅಡಾಪ್ಟರ್.\n",
    "\n",
    "ಪುನಃ ಚಾಲನೆ ಮಾಡಲು ಸುರಕ್ಷಿತ; ಪರಿಸರವು ಈಗಾಗಲೇ ಸಿದ್ಧವಾಗಿದ್ದರೆ ಬಿಟ್ಟುಹೋಗಿ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff641221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries (ragas pulls datasets, evaluate, etc.)\n",
    "!pip install -q foundry-local-sdk openai sentence-transformers ragas datasets numpy langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e82678",
   "metadata": {},
   "source": [
    "### ವಿವರಣೆ: ಕೋರ್ ಇಂಪೋರ್ಟ್‌ಗಳು ಮತ್ತು ಮೆಟ್ರಿಕ್ಸ್  \n",
    "ಕೋರ್ ಲೈಬ್ರರಿಗಳು ಮತ್ತು ರಾಗಾಸ್ ಮೆಟ್ರಿಕ್ಸ್ ಅನ್ನು ಲೋಡ್ ಮಾಡುತ್ತದೆ. ಪ್ರಮುಖ ಅಂಶಗಳು:  \n",
    "- ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳಿಗೆ SentenceTransformer.  \n",
    "- `evaluate` + ಆಯ್ದ ರಾಗಾಸ್ ಮೆಟ್ರಿಕ್ಸ್.  \n",
    "- ಮೌಲ್ಯಮಾಪನ ಕಾರ್ಪಸ್ ರಚನೆಗೆ `Dataset`.  \n",
    "ಈ ಇಂಪೋರ್ಟ್‌ಗಳು ದೂರಸ್ಥ ಕರೆಗಳನ್ನು ಪ್ರೇರೇಪಿಸುವುದಿಲ್ಲ (ಎಂಬೆಡ್ಡಿಂಗ್‌ಗಳಿಗೆ ಮಾದರಿ ಕ್ಯಾಶ್ ಲೋಡ್ ಹೊರತುಪಡಿಸಿ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "519dabae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from foundry_local import FoundryLocalManager\n",
    "from openai import OpenAI\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f01938",
   "metadata": {},
   "source": [
    "### ವಿವರಣೆ: ಟಾಯ್ ಕಾರ್ಪಸ್ ಮತ್ತು QA ಗ್ರೌಂಡ್ ಟ್ರೂತ್\n",
    "ಸಣ್ಣ ಇನ್-ಮೆಮೊರಿ ಕಾರ್ಪಸ್ (`DOCS`), ಬಳಕೆದಾರರ ಪ್ರಶ್ನೆಗಳ ಸೆಟ್ ಮತ್ತು ನಿರೀಕ್ಷಿತ ಗ್ರೌಂಡ್ ಟ್ರೂತ್ ಉತ್ತರಗಳನ್ನು ವ್ಯಾಖ್ಯಾನಿಸುತ್ತದೆ. ಇವು ಹೊರಗಿನ ಡೇಟಾ ಪಡೆಯುವಿಕೆ ಇಲ್ಲದೆ ವೇಗವಾಗಿ, ನಿರ್ಧಾರಾತ್ಮಕ ಮೌಲ್ಯಮಾಪನವನ್ನು ಅನುಮತಿಸುತ್ತವೆ. ನಿಜವಾದ ಸಂದರ್ಭಗಳಲ್ಲಿ ನೀವು ಉತ್ಪಾದನಾ ಪ್ರಶ್ನೆಗಳು + ಸಂಗ್ರಹಿತ ಉತ್ತರಗಳನ್ನು ಮಾದರಿಯಾಗಿಸಿಕೊಳ್ಳುತ್ತೀರಿ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27307d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS = [\n",
    " 'Foundry Local exposes a local OpenAI-compatible endpoint.',\n",
    " 'RAG retrieves relevant context snippets before generation.',\n",
    " 'Local inference improves privacy and reduces latency.',\n",
    "]\n",
    "QUESTIONS = [\n",
    " 'What advantage does local inference offer?',\n",
    " 'How does RAG improve grounding?',\n",
    "]\n",
    "GROUND_TRUTH = [\n",
    " 'It reduces latency and preserves privacy.',\n",
    " 'It adds retrieved context snippets for factual grounding.',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf3b2ec",
   "metadata": {},
   "source": [
    "### ವಿವರಣೆ: ಸೇವೆ ಪ್ರಾರಂಭ, ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳು ಮತ್ತು ಸುರಕ್ಷತಾ ಪ್ಯಾಚ್  \n",
    "Foundry Local ನಿರ್ವಾಹಕನನ್ನು ಪ್ರಾರಂಭಿಸುತ್ತದೆ, `promptTemplate` ಗೆ ಸ್ಕೀಮಾ-ಡ್ರಿಫ್ಟ್ ಸುರಕ್ಷತಾ ಪ್ಯಾಚ್ ಅನ್ನು ಅನ್ವಯಿಸುತ್ತದೆ, ಮಾದರಿ ಐಡಿಯನ್ನು ಪರಿಹರಿಸುತ್ತದೆ, OpenAI-ಸಮ್ಮತ ಗ್ರಾಹಕನನ್ನು ರಚಿಸುತ್ತದೆ, ಮತ್ತು ಡಾಕ್ಯುಮೆಂಟ್ ಕಾರ್ಪಸ್‌ಗಾಗಿ ಸಾಂದ್ರ ಎम्बೆಡ್ಡಿಂಗ್‌ಗಳನ್ನು ಪೂರ್ವಗಣನೆ ಮಾಡುತ್ತದೆ. ಇದು ಪುನಃಬಳಕೆ ಮಾಡಬಹುದಾದ ಸ್ಥಿತಿಯನ್ನು ರಿಟ್ರೀವಲ್ + ಜನರೇಶನ್‌ಗಾಗಿ ಸಿದ್ಧಪಡಿಸುತ್ತದೆ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156a7bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service running: True | Endpoint: http://127.0.0.1:57127/v1\n",
      "Cached models: [FoundryModelInfo(alias=gpt-oss-20b, id=gpt-oss-20b-cuda-gpu:1, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=9882 MB, license=apache-2.0), FoundryModelInfo(alias=phi-3.5-mini, id=Phi-3.5-mini-instruct-cuda-gpu:1, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=2181 MB, license=MIT), FoundryModelInfo(alias=phi-4-mini, id=Phi-4-mini-instruct-cuda-gpu:4, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=3686 MB, license=MIT), FoundryModelInfo(alias=qwen2.5-0.5b, id=qwen2.5-0.5b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=528 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-7b, id=qwen2.5-7b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=4843 MB, license=apache-2.0), FoundryModelInfo(alias=qwen2.5-coder-7b, id=qwen2.5-coder-7b-instruct-cuda-gpu:3, execution_provider=CUDAExecutionProvider, device_type=GPU, file_size=4843 MB, license=apache-2.0)]\n",
      "Using model id: Phi-4-mini-instruct-cuda-gpu:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leestott\\AppData\\Local\\miniforge\\envs\\demo\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from foundry_local import FoundryLocalManager\n",
    "from foundry_local.models import FoundryModelInfo\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Safe monkeypatch for potential null promptTemplate field (schema drift guard) ---\n",
    "_original_from_list_response = FoundryModelInfo.from_list_response\n",
    "\n",
    "def _safe_from_list_response(response):  # type: ignore\n",
    "    try:\n",
    "        if isinstance(response, dict) and response.get(\"promptTemplate\") is None:\n",
    "            response[\"promptTemplate\"] = {}\n",
    "    except Exception as e:  # pragma: no cover\n",
    "        print(f\"Warning normalizing promptTemplate: {e}\")\n",
    "    return _original_from_list_response(response)\n",
    "\n",
    "if getattr(FoundryModelInfo.from_list_response, \"__name__\", \"\") != \"_safe_from_list_response\":\n",
    "    FoundryModelInfo.from_list_response = staticmethod(_safe_from_list_response)  # type: ignore\n",
    "# --- End monkeypatch ---\n",
    "\n",
    "alias = os.getenv('FOUNDRY_LOCAL_ALIAS','phi-3.5-mini')\n",
    "manager = FoundryLocalManager(alias)\n",
    "print(f\"Service running: {manager.is_service_running()} | Endpoint: {manager.endpoint}\")\n",
    "print('Cached models:', manager.list_cached_models())\n",
    "model_info = manager.get_model_info(alias)\n",
    "model_id = model_info.id\n",
    "print(f\"Using model id: {model_id}\")\n",
    "\n",
    "# OpenAI-compatible client\n",
    "client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or 'not-needed')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "import numpy as np\n",
    "doc_emb = embedder.encode(DOCS, convert_to_numpy=True, normalize_embeddings=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d24a8",
   "metadata": {},
   "source": [
    "### ವಿವರಣೆ: ರಿಟ್ರೀವರ್ ಫಂಕ್ಷನ್\n",
    "ನಾರ್ಮಲೈಜ್ ಮಾಡಿದ ಎಂಬೆಡ್ಡಿಂಗ್‌ಗಳ ಮೇಲೆ ಡಾಟ್ ಪ್ರಾಡಕ್ಟ್ ಬಳಸಿ ಸರಳ ವೆಕ್ಟರ್ ಸಾದೃಶ್ಯ ರಿಟ್ರೀವರ್ ಅನ್ನು ವ್ಯಾಖ್ಯಾನಿಸುತ್ತದೆ. ಟಾಪ್-k ಡಾಕ್ಸ್ (k=2 ಡೀಫಾಲ್ಟ್) ಅನ್ನು ಹಿಂತಿರುಗಿಸುತ್ತದೆ. ಉತ್ಪಾದನೆಯಲ್ಲಿ ಪ್ರಮಾಣ ಮತ್ತು ವಿಳಂಬಕ್ಕಾಗಿ ANN ಸೂಚ್ಯಂಕ (FAISS, Chroma, Milvus) ಜೊತೆಗೆ ಬದಲಾಯಿಸಿ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0af32d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, k=2):\n",
    "    q = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "    sims = doc_emb @ q\n",
    "    return [DOCS[i] for i in sims.argsort()[::-1][:k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00f284e",
   "metadata": {},
   "source": [
    "### ವಿವರಣೆ: ಜನರೇಶನ್ ಫಂಕ್ಷನ್  \n",
    "`generate` ಒಂದು ನಿರ್ಬಂಧಿತ ಪ್ರಾಂಪ್ಟ್ ಅನ್ನು ರಚಿಸುತ್ತದೆ (ಸಿಸ್ಟಮ್ ಕೇವಲ ಸಾಂದರ್ಭಿಕವನ್ನು ಬಳಸಲು ಸೂಚಿಸುತ್ತದೆ) ಮತ್ತು ಸ್ಥಳೀಯ ಮಾದರಿಯನ್ನು ಕರೆಸುತ್ತದೆ. ಕಡಿಮೆ ತಾಪಮಾನ (0.1) ಸೃಜನಶೀಲತೆಯ ಮೇಲೆ ನಿಷ್ಠಾವಂತವಾದ ಹೊರತೆಗೆಯುವಿಕೆಯನ್ನು ಪ್ರೋತ್ಸಾಹಿಸುತ್ತದೆ. ಕಡಿತಗೊಳಿಸಿದ ಉತ್ತರ ಪಠ್ಯವನ್ನು ಹಿಂತಿರುಗಿಸುತ್ತದೆ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7798ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(query, contexts):\n",
    "    ctx = \"\\n\".join(contexts)\n",
    "    messages = [\n",
    "        {'role':'system','content':'Answer using ONLY the provided context.'},\n",
    "        {'role':'user','content':f\"Context:\\n{ctx}\\n\\nQuestion: {query}\"}\n",
    "    ]\n",
    "    resp = client.chat.completions.create(model=model_id, messages=messages, max_tokens=120, temperature=0.1)\n",
    "    return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbde788",
   "metadata": {},
   "source": [
    "### ವಿವರಣೆ: ಫಾಲ್ಬ್ಯಾಕ್ ಕ್ಲೈಂಟ್ ಪ್ರಾರಂಭ\n",
    "ಹಿಂದಿನ ಪ್ರಾರಂಭ ಸೆಲ್ ತಪ್ಪಿಸಿಕೊಂಡಿದ್ದರೂ ಅಥವಾ ವಿಫಲವಾಗಿದ್ದರೂ `client` ಅಸ್ತಿತ್ವದಲ್ಲಿರುತ್ತದೆ ಎಂದು ಖಚಿತಪಡಿಸುತ್ತದೆ—ನಂತರದ ಮೌಲ್ಯಮಾಪನ ಹಂತಗಳಲ್ಲಿ NameError ತಪ್ಪಿಸುತ್ತದೆ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e71f8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fallback client initialization (added after patch failure)\n",
    "try:\n",
    "    client  # type: ignore\n",
    "except NameError:\n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(base_url=manager.endpoint, api_key=manager.api_key or 'not-needed')\n",
    "    print('Initialized OpenAI-compatible client (late init).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17386ee",
   "metadata": {},
   "source": [
    "### ವಿವರಣೆ: ಮೌಲ್ಯಮಾಪನ ಲೂಪ್ ಮತ್ತು ಮೆಟ್ರಿಕ್ಸ್  \n",
    "ಮೌಲ್ಯಮಾಪನ ಡೇಟಾಸೆಟ್ ಅನ್ನು ನಿರ್ಮಿಸುತ್ತದೆ (ಅಗತ್ಯ ಕಾಲಮ್‌ಗಳು: ಪ್ರಶ್ನೆ, ಉತ್ತರ, ಸಂದರ್ಭಗಳು, ಭೂಮಿತಿಯ ಸತ್ಯಗಳು, ಉಲ್ಲೇಖ) ನಂತರ ಆಯ್ದ ರಾಗಾ ಮೆಟ್ರಿಕ್ಸ್‌ಗಳನ್ನು ಪುನರಾವರ್ತಿಸುತ್ತದೆ.  \n",
    "\n",
    "ಆಪ್ಟಿಮೈಜೆಷನ್:  \n",
    "- ಫಾಸ್ಟ್_ಮೋಡ್ ತ್ವರಿತ ಸ್ಮೋಕ್ ಪರೀಕ್ಷೆಗಳಿಗೆ ಉತ್ತರ ಸಂಬಂಧಿತತೆಗೆ ಮಾತ್ರ ಮಿತಿಗೊಳಿಸುತ್ತದೆ.  \n",
    "- ಪ್ರತಿ ಮೆಟ್ರಿಕ್ ಲೂಪ್ ಒಂದು ಮೆಟ್ರಿಕ್ ವಿಫಲವಾದಾಗ ಸಂಪೂರ್ಣ ಮರುಗಣನೆ ತಪ್ಪಿಸುತ್ತದೆ.  \n",
    "\n",
    "ಮೆಟ್ರಿಕ್ -> ಅಂಕ (ವಿಫಲವಾದರೆ NaN) ಎಂಬ ಡಿಕ್ಷನರಿ ಅನ್ನು ಔಟ್‌ಪುಟ್ ಮಾಡುತ್ತದೆ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "521a9163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation dataset columns: ['question', 'answer', 'contexts', 'ground_truths', 'reference']\n",
      "Metrics to compute: ['answer_relevancy']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy finished in 78.1s -> 0.6975427764759168\n",
      "RAG evaluation results: {'answer_relevancy': 0.6975427764759168}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.6975427764759168}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build evaluation dataset with required columns (including 'reference' for context_precision)\n",
    "records = []\n",
    "for q, gt in zip(QUESTIONS, GROUND_TRUTH):\n",
    "    ctxs = retrieve(q)\n",
    "    ans = generate(q, ctxs)\n",
    "    records.append({\n",
    "        'question': q,\n",
    "        'answer': ans,\n",
    "        'contexts': ctxs,\n",
    "        'ground_truths': [gt],\n",
    "        'reference': gt\n",
    "    })\n",
    "\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_precision\n",
    "from langchain_openai import ChatOpenAI\n",
    "from ragas.run_config import RunConfig\n",
    "import math, time, os\n",
    "import numpy as np\n",
    "\n",
    "ragas_llm = ChatOpenAI(model=model_id, base_url=manager.endpoint, api_key=manager.api_key or 'not-needed', temperature=0.0, timeout=60)\n",
    "\n",
    "class LocalEmbeddings:\n",
    "    def embed_documents(self, texts):\n",
    "        return embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=True).tolist()\n",
    "    def embed_query(self, text):\n",
    "        return embedder.encode([text], convert_to_numpy=True, normalize_embeddings=True)[0].tolist()\n",
    "\n",
    "# Fast mode: only answer_relevancy unless RAG_FAST=0\n",
    "FAST_MODE = os.getenv('RAG_FAST','1') == '1'\n",
    "metrics = [answer_relevancy] if FAST_MODE else [answer_relevancy, faithfulness, context_precision]\n",
    "\n",
    "base_timeout = 45 if FAST_MODE else 120\n",
    "\n",
    "ds = Dataset.from_list(records)\n",
    "print('Evaluation dataset columns:', ds.column_names)\n",
    "print('Metrics to compute:', [m.name for m in metrics])\n",
    "\n",
    "results_dict = {}\n",
    "for metric in metrics:\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        cfg = RunConfig(timeout=base_timeout, max_workers=1)\n",
    "        partial = evaluate(ds, metrics=[metric], llm=ragas_llm, embeddings=LocalEmbeddings(), run_config=cfg, show_progress=False)\n",
    "        raw_val = partial[metric.name]\n",
    "        if isinstance(raw_val, list):\n",
    "            numeric = [v for v in raw_val if isinstance(v, (int, float))]\n",
    "            score = float(np.nanmean(numeric)) if numeric else math.nan\n",
    "        else:\n",
    "            score = float(raw_val)\n",
    "        results_dict[metric.name] = score\n",
    "    except Exception as e:\n",
    "        results_dict[metric.name] = math.nan\n",
    "        print(f\"Metric {metric.name} failed: {e}\")\n",
    "    finally:\n",
    "        print(f\"{metric.name} finished in {time.time()-t0:.1f}s -> {results_dict[metric.name]}\")\n",
    "\n",
    "print('RAG evaluation results:', results_dict)\n",
    "results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<!-- CO-OP TRANSLATOR DISCLAIMER START -->\n**ಅಸ್ವೀಕಾರ**:  \nಈ ದಸ್ತಾವೇಜು AI ಅನುವಾದ ಸೇವೆ [Co-op Translator](https://github.com/Azure/co-op-translator) ಬಳಸಿ ಅನುವಾದಿಸಲಾಗಿದೆ. ನಾವು ನಿಖರತೆಯಿಗಾಗಿ ಪ್ರಯತ್ನಿಸುತ್ತಿದ್ದರೂ, ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ತಪ್ಪುಗಳು ಅಥವಾ ಅಸತ್ಯತೆಗಳು ಇರಬಹುದು ಎಂದು ದಯವಿಟ್ಟು ಗಮನಿಸಿ. ಮೂಲ ಭಾಷೆಯಲ್ಲಿರುವ ಮೂಲ ದಸ್ತಾವೇಜನ್ನು ಅಧಿಕೃತ ಮೂಲವಾಗಿ ಪರಿಗಣಿಸಬೇಕು. ಮಹತ್ವದ ಮಾಹಿತಿಗಾಗಿ, ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗುತ್ತದೆ. ಈ ಅನುವಾದ ಬಳಕೆಯಿಂದ ಉಂಟಾಗುವ ಯಾವುದೇ ತಪ್ಪು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವಿಕೆ ಅಥವಾ ತಪ್ಪು ವಿವರಣೆಗಳಿಗೆ ನಾವು ಹೊಣೆಗಾರರಾಗುವುದಿಲ್ಲ.\n<!-- CO-OP TRANSLATOR DISCLAIMER END -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "coopTranslator": {
   "original_hash": "4895a2e01d85b98643a177c89ff7757f",
   "translation_date": "2025-12-16T01:20:49+00:00",
   "source_file": "Workshop/notebooks/session02_rag_eval_ragas.ipynb",
   "language_code": "kn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}