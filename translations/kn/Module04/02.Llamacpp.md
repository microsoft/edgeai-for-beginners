# ವಿಭಾಗ 2 : Llama.cpp ಅನುಷ್ಠಾನ ಮಾರ್ಗದರ್ಶಿ

## ವಿಷಯಗಳ ಪಟ್ಟಿಕೆ
1. [ಪರಿಚಯ](../../../Module04)
2. [Llama.cpp ಎಂದರೆ ಏನು?](../../../Module04)
3. [ಸ್ಥಾಪನೆ](../../../Module04)
4. [ಮೂಲದಿಂದ ನಿರ್ಮಾಣ](../../../Module04)
5. [ಮಾದರಿ ಪ್ರಮಾಣೀಕರಣ](../../../Module04)
6. [ಮೂಲಭೂತ ಬಳಕೆ](../../../Module04)
7. [ಅಧಿಕೃತ ವೈಶಿಷ್ಟ್ಯಗಳು](../../../Module04)
8. [ಪೈಥಾನ್ ಸಂಯೋಜನೆ](../../../Module04)
9. [ಸಮಸ್ಯೆ ಪರಿಹಾರ](../../../Module04)
10. [ಉತ್ತಮ ಅಭ್ಯಾಸಗಳು](../../../Module04)

## ಪರಿಚಯ

ಈ ಸಮಗ್ರ ಪಾಠವು Llama.cpp ಬಗ್ಗೆ ನಿಮಗೆ ತಿಳಿಯಬೇಕಾದ ಎಲ್ಲವನ್ನೂ ಮಾರ್ಗದರ್ಶನ ಮಾಡುತ್ತದೆ, ಮೂಲ ಸ್ಥಾಪನೆಯಿಂದ ಹಿಡಿದು ಅಧಿಕೃತ ಬಳಕೆ ಸಂದರ್ಭಗಳವರೆಗೆ. Llama.cpp ಒಂದು ಶಕ್ತಿಶಾಲಿ C++ ಅನುಷ್ಠಾನವಾಗಿದ್ದು, ಕಡಿಮೆ ಸೆಟಪ್ ಮತ್ತು ವಿವಿಧ ಹಾರ್ಡ್‌ವೇರ್ ಸಂರಚನೆಗಳಲ್ಲಿ ಅತ್ಯುತ್ತಮ ಕಾರ್ಯಕ್ಷಮತೆಯೊಂದಿಗೆ ದೊಡ್ಡ ಭಾಷಾ ಮಾದರಿಗಳ (LLMs) ಪರಿಣಾಮಕಾರಿಯಾದ ನಿರ್ಣಯವನ್ನು ಸಾಧ್ಯಮಾಡುತ್ತದೆ.

## Llama.cpp ಎಂದರೆ ಏನು?

Llama.cpp ಒಂದು LLM ನಿರ್ಣಯ ಫ್ರೇಮ್ವರ್ಕ್ ಆಗಿದ್ದು C/C++ ನಲ್ಲಿ ಬರೆಯಲ್ಪಟ್ಟಿದ್ದು, ಕಡಿಮೆ ಸೆಟಪ್ ಮತ್ತು ವಿವಿಧ ಹಾರ್ಡ್‌ವೇರ್‌ಗಳಲ್ಲಿ ಅತ್ಯಾಧುನಿಕ ಕಾರ್ಯಕ್ಷಮತೆಯೊಂದಿಗೆ ಸ್ಥಳೀಯವಾಗಿ ದೊಡ್ಡ ಭಾಷಾ ಮಾದರಿಗಳನ್ನು ಚಾಲನೆ ಮಾಡಲು ಸಾಧ್ಯವಾಗಿಸುತ್ತದೆ. ಪ್ರಮುಖ ವೈಶಿಷ್ಟ್ಯಗಳು:

### ಮೂಲ ವೈಶಿಷ್ಟ್ಯಗಳು
- **ಆಧಾರರಹಿತ ಸರಳ C/C++ ಅನುಷ್ಠಾನ**
- **ಕ್ರಾಸ್-ಪ್ಲಾಟ್‌ಫಾರ್ಮ್ ಹೊಂದಾಣಿಕೆ** (Windows, macOS, Linux)
- **ವಿವಿಧ ವಾಸ್ತುಶಿಲ್ಪಗಳಿಗೆ ಹಾರ್ಡ್‌ವೇರ್ ಆಪ್ಟಿಮೈಜೆಷನ್**
- **ಪ್ರಮಾಣೀಕರಣ ಬೆಂಬಲ** (1.5-ಬಿಟ್ ರಿಂದ 8-ಬಿಟ್ ಪೂರ್ಣಾಂಕ ಪ್ರಮಾಣೀಕರಣ)
- **CPU ಮತ್ತು GPU ವೇಗವರ್ಧನೆ ಬೆಂಬಲ**
- **ಸ್ಮೃತಿ ದಕ್ಷತೆ** ನಿರ್ಬಂಧಿತ ಪರಿಸರಗಳಿಗೆ

### ಲಾಭಗಳು
- ವಿಶೇಷ ಹಾರ್ಡ್‌ವೇರ್ ಅಗತ್ಯವಿಲ್ಲದೆ CPU ಮೇಲೆ ಪರಿಣಾಮಕಾರಿಯಾಗಿ ಕಾರ್ಯನಿರ್ವಹಿಸುತ್ತದೆ
- ಹಲವಾರು GPU ಬ್ಯಾಕೆಂಡ್ಗಳನ್ನು ಬೆಂಬಲಿಸುತ್ತದೆ (CUDA, Metal, OpenCL, Vulkan)
- ತೂಕದಲ್ಲಿ ತಗ್ಗು ಮತ್ತು ಪೋರ್ಟಬಲ್
- ಆಪಲ್ ಸಿಲಿಕಾನ್ ಪ್ರಥಮ ದರ್ಜೆಯ ನಾಗರಿಕ - ARM NEON, Accelerate ಮತ್ತು Metal ಫ್ರೇಮ್ವರ್ಕ್‌ಗಳ ಮೂಲಕ ಆಪ್ಟಿಮೈಸ್ ಮಾಡಲಾಗಿದೆ
- ಕಡಿಮೆ ಸ್ಮೃತಿ ಬಳಕೆಗೆ ವಿವಿಧ ಪ್ರಮಾಣೀಕರಣ ಮಟ್ಟಗಳನ್ನು ಬೆಂಬಲಿಸುತ್ತದೆ

## ಸ್ಥಾಪನೆ

### ವಿಧಾನ 1: ಪೂರ್ವ-ನಿರ್ಮಿತ ಬೈನರಿಗಳು (ಆರಂಭಿಕರಿಗೆ ಶಿಫಾರಸು)

#### GitHub ಬಿಡುಗಡೆಗಳಿಂದ ಡೌನ್‌ಲೋಡ್ ಮಾಡಿ
1. [Llama.cpp GitHub Releases](https://github.com/ggml-org/llama.cpp/releases) ಗೆ ಭೇಟಿ ನೀಡಿ
2. ನಿಮ್ಮ ವ್ಯವಸ್ಥೆಗೆ ಸೂಕ್ತ ಬೈನರಿ ಡೌನ್‌ಲೋಡ್ ಮಾಡಿ:
   - Windows ಗಾಗಿ `llama-<version>-bin-win-<feature>-<arch>.zip`
   - macOS ಗಾಗಿ `llama-<version>-bin-macos-<feature>-<arch>.zip`
   - Linux ಗಾಗಿ `llama-<version>-bin-linux-<feature>-<arch>.zip`

3. ಆರ್ಕೈವ್ ಅನ್ನು ಎಕ್ಸ್ಟ್ರ್ಯಾಕ್ಟ್ ಮಾಡಿ ಮತ್ತು ಡೈರೆಕ್ಟರಿಯನ್ನು ನಿಮ್ಮ ವ್ಯವಸ್ಥೆಯ PATH ಗೆ ಸೇರಿಸಿ

#### ಪ್ಯಾಕೇಜ್ ಮ್ಯಾನೇಜರ್‌ಗಳ ಬಳಕೆ

**macOS (Homebrew):**
```bash
brew install llama.cpp
```

**Linux (ವಿವಿಧ ವಿತರಣೆಗಳು):**
```bash
# ಉಬುಂಟು/ಡೆಬಿಯನ್
sudo apt install llama.cpp

# ಆರ್ಚ್ ಲಿನಕ್ಸ್ನ್
sudo pacman -S llama.cpp
```

### ವಿಧಾನ 2: ಪೈಥಾನ್ ಪ್ಯಾಕೇಜ್ (llama-cpp-python)

#### ಮೂಲ ಸ್ಥಾಪನೆ
```bash
pip install llama-cpp-python
```

#### ಹಾರ್ಡ್‌ವೇರ್ ವೇಗವರ್ಧನೆಯೊಂದಿಗೆ
```bash
# CUDA (NVIDIA GPU ಗಳು)ಗಾಗಿ
CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python

# ಮೆಟಲ್ (ಆಪಲ್ ಸಿಲಿಕಾನ್)ಗಾಗಿ
CMAKE_ARGS="-DGGML_METAL=on" pip install llama-cpp-python

# OpenBLAS (CPU ಆಪ್ಟಿಮೈಜೆಷನ್)ಗಾಗಿ
CMAKE_ARGS="-DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS" pip install llama-cpp-python
```

## ಮೂಲದಿಂದ ನಿರ್ಮಾಣ

### ಪೂರ್ವಾಪೇಕ್ಷೆಗಳು

**ವ್ಯವಸ್ಥೆ ಅಗತ್ಯಗಳು:**
- C++ ಸಂಯೋಜಕ (GCC, Clang, ಅಥವಾ MSVC)
- CMake (ಆವೃತ್ತಿ 3.14 ಅಥವಾ ಹೆಚ್ಚಿನದು)
- Git
- ನಿಮ್ಮ ವೇದಿಕೆಯ ನಿರ್ಮಾಣ ಸಾಧನಗಳು

**ಪೂರ್ವಾಪೇಕ್ಷೆಗಳ ಸ್ಥಾಪನೆ:**

**macOS:**
```bash
xcode-select --install
```

**Ubuntu/Debian:**
```bash
sudo apt update
sudo apt install build-essential cmake git
```

**Windows:**
- C++ ಅಭಿವೃದ್ಧಿ ಸಾಧನಗಳೊಂದಿಗೆ Visual Studio 2022 ಅನ್ನು ಸ್ಥಾಪಿಸಿ
- ಅಧಿಕೃತ ವೆಬ್‌ಸೈಟ್‌ನಿಂದ CMake ಅನ್ನು ಸ್ಥಾಪಿಸಿ
- Git ಅನ್ನು ಸ್ಥಾಪಿಸಿ

### ಮೂಲ ನಿರ್ಮಾಣ ಪ್ರಕ್ರಿಯೆ

1. **ರಿಪೊಸಿಟರಿಯನ್ನು ಕ್ಲೋನ್ ಮಾಡಿ:**
```bash
git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp
```

2. **ನಿರ್ಮಾಣವನ್ನು ಸಂರಚಿಸಿ:**
```bash
cmake -B build
```

3. **ಪ್ರಾಜೆಕ್ಟ್ ಅನ್ನು ನಿರ್ಮಿಸಿ:**
```bash
cmake --build build --config Release
```

ವೇಗವಾಗಿ ಸಂಯೋಜಿಸಲು, ಸಮಾಂತರ ಕೆಲಸಗಳನ್ನು ಬಳಸಿ:
```bash
cmake --build build --config Release -j 8
```

### ಹಾರ್ಡ್‌ವೇರ್-ನಿರ್ದಿಷ್ಟ ನಿರ್ಮಾಣಗಳು

#### CUDA ಬೆಂಬಲ (NVIDIA GPU ಗಳು)
```bash
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

#### Metal ಬೆಂಬಲ (ಆಪಲ್ ಸಿಲಿಕಾನ್)
```bash
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

#### OpenBLAS ಬೆಂಬಲ (CPU ಆಪ್ಟಿಮೈಜೆಷನ್)
```bash
cmake -B build -DGGML_BLAS=ON -DGGML_BLAS_VENDOR=OpenBLAS
cmake --build build --config Release
```

#### Vulkan ಬೆಂಬಲ
```bash
cmake -B build -DGGML_VULKAN=1
cmake --build build --config Release
```

### ಅಧಿಕೃತ ನಿರ್ಮಾಣ ಆಯ್ಕೆಗಳು

#### ಡಿಬಗ್ ನಿರ್ಮಾಣ
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build
```

#### ಹೆಚ್ಚುವರಿ ವೈಶಿಷ್ಟ್ಯಗಳೊಂದಿಗೆ
```bash
cmake -B build \
    -DGGML_CUDA=ON \
    -DGGML_BLAS=ON \
    -DGGML_BLAS_VENDOR=OpenBLAS \
    -DBUILD_SHARED_LIBS=ON
```

## ಮಾದರಿ ಪ್ರಮಾಣೀಕರಣ

### GGUF ಫಾರ್ಮ್ಯಾಟ್ ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವುದು

GGUF (Generalized GGML Unified Format) ಒಂದು ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿದ ಫೈಲ್ ಫಾರ್ಮ್ಯಾಟ್ ಆಗಿದ್ದು, Llama.cpp ಮತ್ತು ಇತರ ಫ್ರೇಮ್ವರ್ಕ್‌ಗಳೊಂದಿಗೆ ದೊಡ್ಡ ಭಾಷಾ ಮಾದರಿಗಳನ್ನು ಪರಿಣಾಮಕಾರಿಯಾಗಿ ಚಾಲನೆ ಮಾಡಲು ವಿನ್ಯಾಸಗೊಳಿಸಲಾಗಿದೆ. ಇದು ನೀಡುತ್ತದೆ:

- ಮಾನಕೃತ ಮಾದರಿ ತೂಕ ಸಂಗ್ರಹಣೆ
- ವೇದಿಕೆಗಳ ನಡುವೆ ಸುಧಾರಿತ ಹೊಂದಾಣಿಕೆ
- ಉತ್ತಮ ಕಾರ್ಯಕ್ಷಮತೆ
- ಪರಿಣಾಮಕಾರಿ ಮೆಟಾಡೇಟಾ ನಿರ್ವಹಣೆ

### ಪ್ರಮಾಣೀಕರಣ ಪ್ರಕಾರಗಳು

Llama.cpp ವಿವಿಧ ಪ್ರಮಾಣೀಕರಣ ಮಟ್ಟಗಳನ್ನು ಬೆಂಬಲಿಸುತ್ತದೆ:

| ಪ್ರಕಾರ | ಬಿಟ್‌ಗಳು | ವಿವರಣೆ | ಬಳಕೆ ಪ್ರಕರಣ |
|-------|---------|---------|--------------|
| F16   | 16      | ಅರ್ಧ ನಿಖರತೆ | ಉನ್ನತ ಗುಣಮಟ್ಟ, ದೊಡ್ಡ ಸ್ಮೃತಿ |
| Q8_0  | 8       | 8-ಬಿಟ್ ಪ್ರಮಾಣೀಕರಣ | ಉತ್ತಮ ಸಮತೋಲನ |
| Q4_0  | 4       | 4-ಬಿಟ್ ಪ್ರಮಾಣೀಕರಣ | ಮಧ್ಯಮ ಗುಣಮಟ್ಟ, ಚಿಕ್ಕ ಗಾತ್ರ |
| Q2_K  | 2       | 2-ಬಿಟ್ ಪ್ರಮಾಣೀಕರಣ | ಅತ್ಯಂತ ಚಿಕ್ಕ ಗಾತ್ರ, ಕಡಿಮೆ ಗುಣಮಟ್ಟ |

### ಮಾದರಿಗಳನ್ನು ಪರಿವರ್ತಿಸುವುದು

#### PyTorch ನಿಂದ GGUF ಗೆ
```bash
# ಹಗ್ಗಿಂಗ್ ಫೇಸ್ ಮಾದರಿಯನ್ನು ಪರಿವರ್ತಿಸಿ
python convert_hf_to_gguf.py path/to/model --outdir ./models

# ಮಾದರಿಯನ್ನು ಪ್ರಮಾಣೀಕರಿಸಿ
./llama-quantize ./models/model.gguf ./models/model-q4_0.gguf q4_0
```

#### Hugging Face ನಿಂದ ನೇರ ಡೌನ್‌ಲೋಡ್
ಹೆಚ್ಚು ಮಾದರಿಗಳು Hugging Face ನಲ್ಲಿ GGUF ಫಾರ್ಮ್ಯಾಟ್‌ನಲ್ಲಿ ಲಭ್ಯವಿವೆ:
- "GGUF" ಎಂಬ ಹೆಸರಿನೊಂದಿಗೆ ಮಾದರಿಗಳನ್ನು ಹುಡುಕಿ
- ಸೂಕ್ತ ಪ್ರಮಾಣೀಕರಣ ಮಟ್ಟವನ್ನು ಡೌನ್‌ಲೋಡ್ ಮಾಡಿ
- ನೇರವಾಗಿ llama.cpp ಜೊತೆ ಬಳಸಿ

## ಮೂಲಭೂತ ಬಳಕೆ

### ಕಮಾಂಡ್ ಲೈನ್ ಇಂಟರ್ಫೇಸ್

#### ಸರಳ ಪಠ್ಯ ರಚನೆ
```bash
# ಮೂಲ ಪಠ್ಯ ಪೂರ್ಣಗೊಳಿಸುವಿಕೆ
./llama-cli -m model.gguf -p "Hello, my name is" -n 50

# ಸಂವಹನಾತ್ಮಕ ಚಾಟ್ ಮೋಡ್
./llama-cli -m model.gguf -cnv
```

#### Hugging Face ನಿಂದ ಮಾದರಿಗಳನ್ನು ಬಳಸಿ
```bash
# ನೇರವಾಗಿ ಡೌನ್‌ಲೋಡ್ ಮಾಡಿ ಮತ್ತು ಚಾಲನೆ ಮಾಡಿ
./llama-cli -hf microsoft/DialoGPT-medium
```

#### ಸರ್ವರ್ ಮೋಡ್
```bash
# ಸರ್ವರ್ ಪ್ರಾರಂಭಿಸಿ
./llama-server -m model.gguf --host 0.0.0.0 --port 8080

# GPU ವೇಗವರ್ಧನೆಯೊಂದಿಗೆ
./llama-server -m model.gguf --n-gpu-layers 32
```

### ಸಾಮಾನ್ಯ ಪರಿಮಾಣಗಳು

| ಪರಿಮಾಣ | ವಿವರಣೆ | ಉದಾಹರಣೆ |
|---------|---------|-----------|
| `-ಮ`   | ಮಾದರಿ ಫೈಲ್ ಪಥ | `-m model.gguf` |
| `-ಪ`   | ಪ್ರಾಂಪ್ಟ್ ಪಠ್ಯ | `-p "Hello world"` |
| `-ಎನ್` | ರಚಿಸುವ ಟೋಕನ್‌ಗಳ ಸಂಖ್ಯೆ | `-n 100` |
| `-ಸಿ`  | ಸಾಂದರ್ಭಿಕ ಗಾತ್ರ | `-c 4096` |
| `-ಟಿ`  | ಥ್ರೆಡ್‌ಗಳ ಸಂಖ್ಯೆ | `-t 8` |
| `-ಎನ್‌ಜಿಎಲ್` | GPU ಲೇಯರ್‌ಗಳು | `-ngl 32` |
| `-ತಾಪಮಾನ` | ತಾಪಮಾನ | `-temp 0.7` |

### ಸಂವಹನ ಮೋಡ್

```bash
# ಸಂವಾದಾತ್ಮಕ ಸೆಷನ್ ಪ್ರಾರಂಭಿಸಿ
./llama-cli -m model.gguf -cnv

# ಉದಾಹರಣೆಯ ಸಂಭಾಷಣೆ:
# > ನಮಸ್ಕಾರ, ನೀವು ಹೇಗಿದ್ದೀರಾ?
# ನಮಸ್ಕಾರ! ನಾನು ಚೆನ್ನಾಗಿದ್ದೇನೆ, ಕೇಳಿದಕ್ಕೆ ಧನ್ಯವಾದಗಳು...
# > ನೀವು ನನಗೆ ಏನು ಸಹಾಯ ಮಾಡಬಹುದು?
# ನಾನು ವಿವಿಧ ಕಾರ್ಯಗಳಲ್ಲಿ ಸಹಾಯ ಮಾಡಬಹುದು ಉದಾಹರಣೆಗೆ...
```

## ಅಧಿಕೃತ ವೈಶಿಷ್ಟ್ಯಗಳು

### ಸರ್ವರ್ API

#### ಸರ್ವರ್ ಪ್ರಾರಂಭಿಸುವುದು
```bash
./llama-server -m model.gguf \
    --host 0.0.0.0 \
    --port 8080 \
    --ctx-size 4096 \
    --n-gpu-layers 32
```

#### API ಬಳಕೆ
```bash
# ಚಾಟ್ ಪೂರ್ಣಗೊಳಿಸುವಿಕೆ
curl -X POST http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7,
    "max_tokens": 100
  }'

# ಪಠ್ಯ ಪೂರ್ಣಗೊಳಿಸುವಿಕೆ
curl -X POST http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "The capital of France is",
    "n_predict": 50
  }'
```

### ಕಾರ್ಯಕ್ಷಮತೆ ಆಪ್ಟಿಮೈಜೆಷನ್

#### ಸ್ಮೃತಿ ನಿರ್ವಹಣೆ
```bash
# ಸಂದರ್ಭದ ಗಾತ್ರವನ್ನು ಹೊಂದಿಸಿ
./llama-cli -m model.gguf -c 2048

# ಮೆಮೊರಿ ಮ್ಯಾಪಿಂಗ್ ಅನ್ನು ಸಕ್ರಿಯಗೊಳಿಸಿ
./llama-cli -m model.gguf --mlock
```

#### ಬಹು-ಥ್ರೆಡಿಂಗ್
```bash
# ಎಲ್ಲಾ CPU ಕೋರ್‌ಗಳನ್ನು ಬಳಸಿ
./llama-cli -m model.gguf -t $(nproc)

# ನಿರ್ದಿಷ್ಟ ಥ್ರೆಡ್ ಸಂಖ್ಯೆ
./llama-cli -m model.gguf -t 8
```

#### GPU ವೇಗವರ್ಧನೆ
```bash
# ಲೇಯರ್‌ಗಳನ್ನು GPU ಗೆ ವರ್ಗಾಯಿಸಿ
./llama-cli -m model.gguf -ngl 32

# ನಿರ್ದಿಷ್ಟ GPU ಅನ್ನು ಬಳಸಿ
CUDA_VISIBLE_DEVICES=0 ./llama-cli -m model.gguf -ngl 32
```

## ಪೈಥಾನ್ ಸಂಯೋಜನೆ

### llama-cpp-python ನೊಂದಿಗೆ ಮೂಲಭೂತ ಬಳಕೆ

```python
from llama_cpp import Llama

# ಮಾದರಿಯನ್ನು ಪ್ರಾರಂಭಿಸಿ
llm = Llama(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=32
)

# ಪಠ್ಯವನ್ನು ರಚಿಸಿ
output = llm("Hello, my name is", max_tokens=50)
print(output['choices'][0]['text'])
```

### ಚಾಟ್ ಇಂಟರ್ಫೇಸ್

```python
from llama_cpp import Llama

llm = Llama(model_path="./models/chat-model.gguf")

# ಚಾಟ್ ಪೂರ್ಣಗೊಳಿಸುವಿಕೆ
response = llm.create_chat_completion(
    messages=[
        {"role": "user", "content": "Hello!"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response['choices'][0]['message']['content'])
```

### ಸ್ಟ್ರೀಮಿಂಗ್ ಪ್ರತಿಕ್ರಿಯೆಗಳು

```python
# ಸ್ಟ್ರೀಮಿಂಗ್ ಪಠ್ಯ ಉತ್ಪಾದನೆ
stream = llm("Tell me a story", max_tokens=200, stream=True)

for output in stream:
    print(output['choices'][0]['text'], end='', flush=True)
```

### LangChain ಜೊತೆಗೆ ಸಂಯೋಜನೆ

```python
from langchain.llms import LlamaCpp
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# LLM ಅನ್ನು ಪ್ರಾರಂಭಿಸಿ
llm = LlamaCpp(
    model_path="./models/model.gguf",
    n_ctx=2048,
    n_threads=8
)

# ಪ್ರಾಂಪ್ಟ್ ಟೆಂಪ್ಲೇಟನ್ನು ರಚಿಸಿ
template = "Question: {question}\nAnswer:"
prompt = PromptTemplate(template=template, input_variables=["question"])

# ಸರಪಳಿಯನ್ನು ರಚಿಸಿ
chain = LLMChain(llm=llm, prompt=prompt)

# ಸರಪಳಿಯನ್ನು ಬಳಸಿ
result = chain.run(question="What is artificial intelligence?")
print(result)
```

## ಸಮಸ್ಯೆ ಪರಿಹಾರ

### ಸಾಮಾನ್ಯ ಸಮಸ್ಯೆಗಳು ಮತ್ತು ಪರಿಹಾರಗಳು

#### ನಿರ್ಮಾಣ ದೋಷಗಳು

**ಸಮಸ್ಯೆ: CMake ಕಂಡುಬಂದಿಲ್ಲ**
```bash
# ಪರಿಹಾರ: CMake ಅನ್ನು ಸ್ಥಾಪಿಸಿ
# ಉಬುಂಟು/ಡೆಬಿಯನ್
sudo apt install cmake

# ಮ್ಯಾಕ್‌ಒಎಸ್
brew install cmake
```

**ಸಮಸ್ಯೆ: ಸಂಯೋಜಕ ಕಂಡುಬಂದಿಲ್ಲ**
```bash
# ಪರಿಹಾರ: ನಿರ್ಮಾಣ ಸಾಧನಗಳನ್ನು ಸ್ಥಾಪಿಸಿ
# ಉಬುಂಟು/ಡೆಬಿಯನ್
sudo apt install build-essential

# ಮ್ಯಾಕ್‌ಒಎಸ್
xcode-select --install
```

#### ರನ್‌ಟೈಮ್ ಸಮಸ್ಯೆಗಳು

**ಸಮಸ್ಯೆ: ಮಾದರಿ ಲೋಡ್ ಆಗುತ್ತಿಲ್ಲ**
- ಮಾದರಿ ಫೈಲ್ ಪಥವನ್ನು ಪರಿಶೀಲಿಸಿ
- ಫೈಲ್ ಅನುಮತಿಗಳನ್ನು ಪರಿಶೀಲಿಸಿ
- ಸಾಕಷ್ಟು RAM ಇದೆ ಎಂದು ಖಚಿತಪಡಿಸಿಕೊಳ್ಳಿ
- ವಿಭಿನ್ನ ಪ್ರಮಾಣೀಕರಣ ಮಟ್ಟಗಳನ್ನು ಪ್ರಯತ್ನಿಸಿ

**ಸಮಸ್ಯೆ: ದೌರ್ಬಲ್ಯ ಕಾರ್ಯಕ್ಷಮತೆ**
- ಹಾರ್ಡ್‌ವೇರ್ ವೇಗವರ್ಧನೆ ಸಕ್ರಿಯಗೊಳಿಸಿ
- ಥ್ರೆಡ್ ಸಂಖ್ಯೆಯನ್ನು ಹೆಚ್ಚಿಸಿ
- ಸೂಕ್ತ ಪ್ರಮಾಣೀಕರಣವನ್ನು ಬಳಸಿ
- GPU ಸ್ಮೃತಿ ಬಳಕೆಯನ್ನು ಪರಿಶೀಲಿಸಿ

#### ಸ್ಮೃತಿ ಸಮಸ್ಯೆಗಳು

**ಸಮಸ್ಯೆ: ಸ್ಮೃತಿ ಮುಗಿದಿದೆ**
```bash
# ಪರಿಹಾರಗಳು:
# 1. ಚಿಕ್ಕ ಪ್ರಮಾಣೀಕರಣವನ್ನು ಬಳಸಿ
./llama-cli -m model-q4_0.gguf

# 2. ಸಂದರ್ಭದ ಗಾತ್ರವನ್ನು ಕಡಿಮೆ ಮಾಡಿ
./llama-cli -m model.gguf -c 1024

# 3. GPU ಗೆ ಆಫ್‌ಲೋಡ್ ಮಾಡಿ
./llama-cli -m model.gguf -ngl 32
```

### ವೇದಿಕೆ-ನಿರ್ದಿಷ್ಟ ಸಮಸ್ಯೆಗಳು

#### Windows
- MinGW ಅಥವಾ Visual Studio ಸಂಯೋಜಕ ಬಳಸಿ
- ಸರಿಯಾದ PATH ಸಂರಚನೆ ಖಚಿತಪಡಿಸಿಕೊಳ್ಳಿ
- ಆಂಟಿವೈರಸ್ ಹಸ್ತಕ್ಷೇಪವನ್ನು ಪರಿಶೀಲಿಸಿ

#### macOS
- ಆಪಲ್ ಸಿಲಿಕಾನ್‌ಗಾಗಿ Metal ಸಕ್ರಿಯಗೊಳಿಸಿ
- ಅಗತ್ಯವಿದ್ದರೆ ಹೊಂದಾಣಿಕೆಗೆ Rosetta 2 ಬಳಸಿ
- Xcode ಕಮಾಂಡ್ ಲೈನ್ ಸಾಧನಗಳನ್ನು ಪರಿಶೀಲಿಸಿ

#### Linux
- ಅಭಿವೃದ್ಧಿ ಪ್ಯಾಕೇಜ್‌ಗಳನ್ನು ಸ್ಥಾಪಿಸಿ
- GPU ಡ್ರೈವರ್ ಆವೃತ್ತಿಗಳನ್ನು ಪರಿಶೀಲಿಸಿ
- CUDA ಟೂಲ್‌ಕಿಟ್ ಸ್ಥಾಪನೆಯನ್ನು ಖಚಿತಪಡಿಸಿಕೊಳ್ಳಿ

## ಉತ್ತಮ ಅಭ್ಯಾಸಗಳು

### ಮಾದರಿ ಆಯ್ಕೆ
1. ನಿಮ್ಮ ಹಾರ್ಡ್‌ವೇರ್ ಆಧಾರಿತ ಸೂಕ್ತ ಪ್ರಮಾಣೀಕರಣವನ್ನು ಆಯ್ಕೆಮಾಡಿ
2. ಮಾದರಿ ಗಾತ್ರ ಮತ್ತು ಗುಣಮಟ್ಟದ ವ್ಯತ್ಯಾಸಗಳನ್ನು ಪರಿಗಣಿಸಿ
3. ನಿಮ್ಮ ನಿರ್ದಿಷ್ಟ ಬಳಕೆಗಾಗಿ ವಿಭಿನ್ನ ಮಾದರಿಗಳನ್ನು ಪರೀಕ್ಷಿಸಿ

### ಕಾರ್ಯಕ್ಷಮತೆ ಆಪ್ಟಿಮೈಜೆಷನ್
1. ಲಭ್ಯವಿದ್ದರೆ GPU ವೇಗವರ್ಧನೆ ಬಳಸಿ
2. ನಿಮ್ಮ CPU ಗೆ ಸೂಕ್ತ ಥ್ರೆಡ್ ಸಂಖ್ಯೆಯನ್ನು ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿ
3. ನಿಮ್ಮ ಬಳಕೆಗೆ ಸೂಕ್ತ ಸಾಂದರ್ಭಿಕ ಗಾತ್ರವನ್ನು ಹೊಂದಿಸಿ
4. ದೊಡ್ಡ ಮಾದರಿಗಳಿಗಾಗಿ ಸ್ಮೃತಿ ಮ್ಯಾಪಿಂಗ್ ಸಕ್ರಿಯಗೊಳಿಸಿ

### ಉತ್ಪಾದನಾ ನಿಯೋಜನೆ
1. API ಪ್ರವೇಶಕ್ಕಾಗಿ ಸರ್ವರ್ ಮೋಡ್ ಬಳಸಿ
2. ಸರಿಯಾದ ದೋಷ ನಿರ್ವಹಣೆಯನ್ನು ಜಾರಿಗೆ ತರುವಿರಿ
3. ಸಂಪನ್ಮೂಲ ಬಳಕೆಯನ್ನು ಮೇಲ್ವಿಚಾರಣೆ ಮಾಡಿ
4. ಲಾಗಿಂಗ್ ಮತ್ತು ಮೇಲ್ವಿಚಾರಣೆಯನ್ನು ಸ್ಥಾಪಿಸಿ

### ಅಭಿವೃದ್ಧಿ ಕಾರ್ಯಪ್ರವಾಹ
1. ಪರೀಕ್ಷೆಗಾಗಿ ಚಿಕ್ಕ ಮಾದರಿಗಳಿಂದ ಪ್ರಾರಂಭಿಸಿ
2. ಮಾದರಿ ಸಂರಚನೆಗಳಿಗೆ ಆವೃತ್ತಿ ನಿಯಂತ್ರಣ ಬಳಸಿ
3. ನಿಮ್ಮ ಸಂರಚನೆಗಳನ್ನು ದಾಖಲೆ ಮಾಡಿ
4. ವಿಭಿನ್ನ ವೇದಿಕೆಗಳಲ್ಲಿ ಪರೀಕ್ಷಿಸಿ

### ಭದ್ರತಾ ಪರಿಗಣನೆಗಳು
1. ಇನ್‌ಪುಟ್ ಪ್ರಾಂಪ್ಟ್‌ಗಳನ್ನು ಪರಿಶೀಲಿಸಿ
2. ದರ ಮಿತಿ ಜಾರಿಗೆ ತರುವಿರಿ
3. API ಎಂಡ್‌ಪಾಯಿಂಟ್‌ಗಳನ್ನು ಸುರಕ್ಷಿತಗೊಳಿಸಿ
4. ದುರುಪಯೋಗ ಮಾದರಿಗಳನ್ನು ಮೇಲ್ವಿಚಾರಣೆ ಮಾಡಿ

## ಸಮಾರೋಪ

Llama.cpp ವಿವಿಧ ಹಾರ್ಡ್‌ವೇರ್ ಸಂರಚನೆಗಳಲ್ಲಿ ಸ್ಥಳೀಯವಾಗಿ ದೊಡ್ಡ ಭಾಷಾ ಮಾದರಿಗಳನ್ನು ಚಾಲನೆ ಮಾಡಲು ಶಕ್ತಿಶಾಲಿ ಮತ್ತು ಪರಿಣಾಮಕಾರಿ ಮಾರ್ಗವನ್ನು ಒದಗಿಸುತ್ತದೆ. ನೀವು AI ಅಪ್ಲಿಕೇಶನ್‌ಗಳನ್ನು ಅಭಿವೃದ್ಧಿಪಡಿಸುತ್ತಿದ್ದೀರಾ, ಸಂಶೋಧನೆ ನಡೆಸುತ್ತಿದ್ದೀರಾ ಅಥವಾ LLM ಗಳೊಂದಿಗೆ ಪ್ರಯೋಗ ಮಾಡುತ್ತಿದ್ದೀರಾ, ಈ ಫ್ರೇಮ್ವರ್ಕ್ ಅಗತ್ಯವಿರುವ ಲವಚಿಕತೆ ಮತ್ತು ಕಾರ್ಯಕ್ಷಮತೆಯನ್ನು ಒದಗಿಸುತ್ತದೆ.

ಪ್ರಮುಖ ಅಂಶಗಳು:
- ನಿಮ್ಮ ಅಗತ್ಯಗಳಿಗೆ ಸೂಕ್ತ ಸ್ಥಾಪನಾ ವಿಧಾನವನ್ನು ಆಯ್ಕೆಮಾಡಿ
- ನಿಮ್ಮ ನಿರ್ದಿಷ್ಟ ಹಾರ್ಡ್‌ವೇರ್ ಸಂರಚನೆಗಾಗಿ ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿ
- ಮೂಲಭೂತ ಬಳಕೆಯಿಂದ ಪ್ರಾರಂಭಿಸಿ ಮತ್ತು ಕ್ರಮೇಣ ಅಧಿಕೃತ ವೈಶಿಷ್ಟ್ಯಗಳನ್ನು ಅನ್ವೇಷಿಸಿ
- ಸುಲಭ ಸಂಯೋಜನೆಗಾಗಿ ಪೈಥಾನ್ ಬಿಂಡಿಂಗ್‌ಗಳನ್ನು ಪರಿಗಣಿಸಿ
- ಉತ್ಪಾದನಾ ನಿಯೋಜನೆಗಳಿಗೆ ಉತ್ತಮ ಅಭ್ಯಾಸಗಳನ್ನು ಅನುಸರಿಸಿ

ಹೆಚ್ಚಿನ ಮಾಹಿತಿಗಾಗಿ ಮತ್ತು ನವೀಕರಣಗಳಿಗೆ, [ಅಧಿಕೃತ Llama.cpp ರಿಪೊಸಿಟರಿ](https://github.com/ggml-org/llama.cpp) ಗೆ ಭೇಟಿ ನೀಡಿ ಮತ್ತು ಸಮಗ್ರ ಡಾಕ್ಯುಮೆಂಟೇಶನ್ ಮತ್ತು ಸಮುದಾಯ ಸಂಪನ್ಮೂಲಗಳನ್ನು ನೋಡಿ.


## ➡️ ಮುಂದೇನು

- [03: Microsoft Olive Optimization Suite](./03.MicrosoftOlive.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ಅಸ್ವೀಕರಣ**:  
ಈ ದಸ್ತಾವೇಜು AI ಅನುವಾದ ಸೇವೆ [Co-op Translator](https://github.com/Azure/co-op-translator) ಬಳಸಿ ಅನುವಾದಿಸಲಾಗಿದೆ. ನಾವು ನಿಖರತೆಯಿಗಾಗಿ ಪ್ರಯತ್ನಿಸುತ್ತಿದ್ದರೂ, ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ದೋಷಗಳು ಅಥವಾ ಅಸತ್ಯತೆಗಳು ಇರಬಹುದು ಎಂದು ದಯವಿಟ್ಟು ಗಮನಿಸಿ. ಮೂಲ ಭಾಷೆಯಲ್ಲಿರುವ ಮೂಲ ದಸ್ತಾವೇಜನ್ನು ಅಧಿಕೃತ ಮೂಲವೆಂದು ಪರಿಗಣಿಸಬೇಕು. ಮಹತ್ವದ ಮಾಹಿತಿಗಾಗಿ, ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗುತ್ತದೆ. ಈ ಅನುವಾದ ಬಳಕೆಯಿಂದ ಉಂಟಾಗುವ ಯಾವುದೇ ತಪ್ಪು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವಿಕೆ ಅಥವಾ ತಪ್ಪು ವಿವರಣೆಗಳಿಗೆ ನಾವು ಹೊಣೆಗಾರರಾಗುವುದಿಲ್ಲ.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->