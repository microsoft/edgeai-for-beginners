# ವಿಭಾಗ 4 : OpenVINO ಟೂಲ್‌ಕಿಟ್ ಆಪ್ಟಿಮೈಜೆಷನ್ ಸೂಟ್

## ವಿಷಯಗಳ ಪಟ್ಟಿಕೆ
1. [ಪರಿಚಯ](../../../Module04)
2. [OpenVINO ಎಂದರೆ ಏನು?](../../../Module04)
3. [ಸ್ಥಾಪನೆ](../../../Module04)
4. [ತ್ವರಿತ ಪ್ರಾರಂಭ ಮಾರ್ಗದರ್ಶಿ](../../../Module04)
5. [ಉದಾಹರಣೆ: OpenVINO ಬಳಸಿ ಮಾದರಿಗಳನ್ನು ಪರಿವರ್ತನೆ ಮತ್ತು ಆಪ್ಟಿಮೈಸ್ ಮಾಡುವುದು](../../../Module04)
6. [ಅಧಿಕೃತ ಬಳಕೆ](../../../Module04)
7. [ಉತ್ತಮ ಅಭ್ಯಾಸಗಳು](../../../Module04)
8. [ಸಮಸ್ಯೆ ಪರಿಹಾರ](../../../Module04)
9. [ಹೆಚ್ಚಿನ ಸಂಪನ್ಮೂಲಗಳು](../../../Module04)

## ಪರಿಚಯ

OpenVINO (Open Visual Inference and Neural Network Optimization) ಇಂಟೆಲ್‌ನ ಮುಕ್ತ ಮೂಲ ಟೂಲ್‌ಕಿಟ್ ಆಗಿದ್ದು, ಕ್ಲೌಡ್, ಆನ್-ಪ್ರೆಮೈಸಸ್ ಮತ್ತು ಎಡ್ಜ್ ಪರಿಸರಗಳಲ್ಲಿ ಕಾರ್ಯಕ್ಷಮ AI ಪರಿಹಾರಗಳನ್ನು ನಿಯೋಜಿಸಲು ಬಳಸಲಾಗುತ್ತದೆ. ನೀವು CPUಗಳು, GPUಗಳು, VPUಗಳು ಅಥವಾ ವಿಶೇಷ AI ತ್ವರಕಗಳನ್ನು ಗುರಿಯಾಗಿಸಿಕೊಂಡಿದ್ದರೂ, OpenVINO ಮಾದರಿ ನಿಖರತೆಯನ್ನು ಕಾಪಾಡಿಕೊಂಡು ವ್ಯಾಪಕ ಆಪ್ಟಿಮೈಜೆಷನ್ ಸಾಮರ್ಥ್ಯಗಳನ್ನು ಒದಗಿಸುತ್ತದೆ ಮತ್ತು ಕ್ರಾಸ್-ಪ್ಲಾಟ್‌ಫಾರ್ಮ್ ನಿಯೋಜನೆಯನ್ನು ಸಕ್ರಿಯಗೊಳಿಸುತ್ತದೆ.

## OpenVINO ಎಂದರೆ ಏನು?

OpenVINO ಒಂದು ಮುಕ್ತ ಮೂಲ ಟೂಲ್‌ಕಿಟ್ ಆಗಿದ್ದು, ಅಭಿವೃದ್ಧಿಪಡಿಸುವವರಿಗೆ ವಿಭಿನ್ನ ಹಾರ್ಡ್‌ವೇರ್ ವೇದಿಕೆಗಳ ಮೇಲೆ AI ಮಾದರಿಗಳನ್ನು ಪರಿಣಾಮಕಾರಿಯಾಗಿ ಆಪ್ಟಿಮೈಸ್, ಪರಿವರ್ತನೆ ಮತ್ತು ನಿಯೋಜಿಸಲು ಸಹಾಯ ಮಾಡುತ್ತದೆ. ಇದರಲ್ಲಿ ಮೂರು ಪ್ರಮುಖ ಘಟಕಗಳಿವೆ: OpenVINO ರನ್‌ಟೈಮ್ ಇನ್ಫರೆನ್ಸ್‌ಗಾಗಿ, ನ್ಯೂರಲ್ ನೆಟ್‌ವರ್ಕ್ ಕಂಪ್ರೆಷನ್ ಫ್ರೇಮ್ವರ್ಕ್ (NNCF) ಮಾದರಿ ಆಪ್ಟಿಮೈಜೆಷನ್‌ಗಾಗಿ, ಮತ್ತು OpenVINO ಮಾದರಿ ಸರ್ವರ್ ವ್ಯಾಪಕ ನಿಯೋಜನೆಗಾಗಿ.

### ಪ್ರಮುಖ ವೈಶಿಷ್ಟ್ಯಗಳು

- **ಕ್ರಾಸ್-ಪ್ಲಾಟ್‌ಫಾರ್ಮ್ ನಿಯೋಜನೆ**: ಲಿನಕ್ಸ, ವಿಂಡೋಸ್ ಮತ್ತು ಮ್ಯಾಕೋಎಸ್‌ಗಾಗಿ ಪೈಥಾನ್, C++, ಮತ್ತು C APIಗಳನ್ನು ಬೆಂಬಲಿಸುತ್ತದೆ
- **ಹಾರ್ಡ್‌ವೇರ್ ತ್ವರಕ**: CPU, GPU, VPU ಮತ್ತು AI ತ್ವರಕಗಳಿಗಾಗಿ ಸ್ವಯಂಚಾಲಿತ ಸಾಧನ ಪತ್ತೆ ಮತ್ತು ಆಪ್ಟಿಮೈಜೆಷನ್
- **ಮಾದರಿ ಸಂಕುಚಿತ ಫ್ರೇಮ್ವರ್ಕ್**: NNCF ಮೂಲಕ ಸುಧಾರಿತ ಕ್ವಾಂಟೈಜೆಷನ್, ಪ್ರೂನಿಂಗ್ ಮತ್ತು ಆಪ್ಟಿಮೈಜೆಷನ್ ತಂತ್ರಗಳು
- **ಫ್ರೇಮ್ವರ್ಕ್ ಹೊಂದಾಣಿಕೆ**: TensorFlow, ONNX, PaddlePaddle, ಮತ್ತು PyTorch ಮಾದರಿಗಳಿಗೆ ನೇರ ಬೆಂಬಲ
- **ಜನರೇಟಿವ್ AI ಬೆಂಬಲ**: ದೊಡ್ಡ ಭಾಷಾ ಮಾದರಿಗಳು ಮತ್ತು ಜನರೇಟಿವ್ AI ಅಪ್ಲಿಕೇಶನ್‌ಗಳ ನಿಯೋಜನೆಗಾಗಿ ವಿಶೇಷ OpenVINO GenAI

### ಲಾಭಗಳು

- **ಕಾರ್ಯಕ್ಷಮತೆ ಆಪ್ಟಿಮೈಜೆಷನ್**: ಕನಿಷ್ಠ ನಿಖರತೆ ನಷ್ಟದೊಂದಿಗೆ ಪ್ರಮುಖ ವೇಗ ಸುಧಾರಣೆಗಳು
- **ನಿಯೋಜನೆ ಪಾದಚಿಹ್ನೆ ಕಡಿತ**: ಕನಿಷ್ಠ ಬಾಹ್ಯ ಅವಲಂಬನೆಗಳು ಸ್ಥಾಪನೆ ಮತ್ತು ನಿಯೋಜನೆಯನ್ನು ಸರಳಗೊಳಿಸುತ್ತವೆ
- **ತ್ವರಿತ ಪ್ರಾರಂಭ ಸಮಯ**: ವೇಗವಾದ ಅಪ್ಲಿಕೇಶನ್ ಪ್ರಾರಂಭಕ್ಕಾಗಿ ಆಪ್ಟಿಮೈಸ್ ಮಾಡಲಾದ ಮಾದರಿ ಲೋಡಿಂಗ್ ಮತ್ತು ಕ್ಯಾಶಿಂಗ್
- **ವ್ಯಾಪಕ ನಿಯೋಜನೆ**: ಎಡ್ಜ್ ಸಾಧನಗಳಿಂದ ಕ್ಲೌಡ್ ಮೂಲಸೌಕರ್ಯವರೆಗೆ ಸತತ APIಗಳೊಂದಿಗೆ
- **ಉತ್ಪಾದನಾ ಸಿದ್ಧತೆ**: ವ್ಯಾಪಾರ ಮಟ್ಟದ ವಿಶ್ವಾಸಾರ್ಹತೆ ಸಮಗ್ರ ಡಾಕ್ಯುಮೆಂಟೇಶನ್ ಮತ್ತು ಸಮುದಾಯ ಬೆಂಬಲದೊಂದಿಗೆ

## ಸ್ಥಾಪನೆ

### ಪೂರ್ವಾಪೇಕ್ಷಿತಗಳು

- ಪೈಥಾನ್ 3.8 ಅಥವಾ ಹೆಚ್ಚಿನ ಆವೃತ್ತಿ
- ಪಿಪ್ ಪ್ಯಾಕೇಜ್ ಮ್ಯಾನೇಜರ್
- ವರ್ಚುವಲ್ ಪರಿಸರ (ಶಿಫಾರಸು ಮಾಡಲಾಗಿದೆ)
- ಹೊಂದಾಣಿಕೆಯ ಹಾರ್ಡ್‌ವೇರ್ (ಇಂಟೆಲ್ CPUಗಳು ಶಿಫಾರಸು, ಆದರೆ ವಿವಿಧ ವಾಸ್ತುಶಿಲ್ಪಗಳನ್ನು ಬೆಂಬಲಿಸುತ್ತದೆ)

### ಮೂಲ ಸ್ಥಾಪನೆ

ವರ್ಚುವಲ್ ಪರಿಸರವನ್ನು ರಚಿಸಿ ಮತ್ತು ಸಕ್ರಿಯಗೊಳಿಸಿ:

```bash
# ವರ್ಚುವಲ್ ಪರಿಸರವನ್ನು ರಚಿಸಿ
python -m venv openvino-env

# ವರ್ಚುವಲ್ ಪರಿಸರವನ್ನು ಸಕ್ರಿಯಗೊಳಿಸಿ
# ವಿಂಡೋಸ್‌ನಲ್ಲಿ:
openvino-env\Scripts\activate
# ಮ್ಯಾಕ್‌ಒಎಸ್/ಲಿನಕ್ಸ್ನಲ್ಲಿ:
source openvino-env/bin/activate
```

OpenVINO ರನ್‌ಟೈಮ್ ಅನ್ನು ಸ್ಥಾಪಿಸಿ:

```bash
pip install openvino
```

ಮಾದರಿ ಆಪ್ಟಿಮೈಜೆಷನ್‌ಗಾಗಿ NNCF ಅನ್ನು ಸ್ಥಾಪಿಸಿ:

```bash
pip install nncf
```

### OpenVINO GenAI ಸ್ಥಾಪನೆ

ಜನರೇಟಿವ್ AI ಅಪ್ಲಿಕೇಶನ್‌ಗಳಿಗೆ:

```bash
pip install openvino-genai
```

### ಐಚ್ಛಿಕ ಅವಲಂಬನೆಗಳು

ನಿರ್ದಿಷ್ಟ ಬಳಕೆ ಪ್ರಕರಣಗಳಿಗೆ ಹೆಚ್ಚುವರಿ ಪ್ಯಾಕೇಜುಗಳು:

```bash
# ಜುಪೈಟರ್ ನೋಟ್ಬುಕ್‌ಗಳು ಮತ್ತು ಅಭಿವೃದ್ಧಿ ಸಾಧನಗಳಿಗಾಗಿ
pip install openvino[dev]

# ಟೆನ್ಸರ್‌ಫ್ಲೋ ಮಾದರಿ ಬೆಂಬಲಕ್ಕಾಗಿ
pip install openvino[tensorflow]

# ಪೈಟಾರ್ಚ್ ಮಾದರಿ ಬೆಂಬಲಕ್ಕಾಗಿ
pip install openvino[pytorch]

# ONNX ಮಾದರಿ ಬೆಂಬಲಕ್ಕಾಗಿ
pip install openvino[onnx]
```

### ಸ್ಥಾಪನೆ ಪರಿಶೀಲನೆ

```bash
python -c "from openvino import Core; print('OpenVINO version:', Core().get_versions())"
```

ಯಶಸ್ವಿಯಾದರೆ, ನೀವು OpenVINO ಆವೃತ್ತಿ ಮಾಹಿತಿಯನ್ನು ನೋಡಬಹುದು.

## ತ್ವರಿತ ಪ್ರಾರಂಭ ಮಾರ್ಗದರ್ಶಿ

### ನಿಮ್ಮ ಮೊದಲ ಮಾದರಿ ಆಪ್ಟಿಮೈಜೆಷನ್

OpenVINO ಬಳಸಿ Hugging Face ಮಾದರಿಯನ್ನು ಪರಿವರ್ತಿಸಿ ಮತ್ತು ಆಪ್ಟಿಮೈಸ್ ಮಾಡೋಣ:

```python
from optimum.intel import OVModelForCausalLM
from transformers import AutoTokenizer, pipeline

# ಮಾದರಿಯನ್ನು ಲೋಡ್ ಮಾಡಿ ಮತ್ತು OpenVINO IR ಸ್ವರೂಪಕ್ಕೆ ಪರಿವರ್ತಿಸಿ
model_id = "microsoft/DialoGPT-small"
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# ಟೋಕನೈಜರ್ ಅನ್ನು ಲೋಡ್ ಮಾಡಿ
tokenizer = AutoTokenizer.from_pretrained(model_id)

# ಪರಿವರ್ತಿತ ಮಾದರಿಯನ್ನು ಉಳಿಸಿ
save_directory = "models/dialogpt-openvino"
ov_model.save_pretrained(save_directory)
tokenizer.save_pretrained(save_directory)

# ಇನ್ಫರೆನ್ಸ್‌ಗಾಗಿ ಲೋಡ್ ಮಾಡಿ ಮತ್ತು ಸಂಯೋಜಿಸಿ
ov_model = OVModelForCausalLM.from_pretrained(
    save_directory,
    device="CPU"  # ಅಥವಾ "GPU", "AUTO"
)

# ಇನ್ಫರೆನ್ಸ್ ಪೈಪ್‌ಲೈನ್ ರಚಿಸಿ
pipe = pipeline("text-generation", model=ov_model, tokenizer=tokenizer)
result = pipe("Hello, how are you?", max_length=50)
print(result)
```

### ಈ ಪ್ರಕ್ರಿಯೆ ಏನು ಮಾಡುತ್ತದೆ

ಆಪ್ಟಿಮೈಜೆಷನ್ ಕಾರ್ಯಪ್ರವಾಹವು: ಮೂಲ Hugging Face ಮಾದರಿಯನ್ನು ಲೋಡ್ ಮಾಡುವುದು, OpenVINO ಮಧ್ಯಂತರ ಪ್ರತಿನಿಧಿ (IR) ಸ್ವರೂಪಕ್ಕೆ ಪರಿವರ್ತಿಸುವುದು, ಡೀಫಾಲ್ಟ್ ಆಪ್ಟಿಮೈಜೆಷನ್‌ಗಳನ್ನು ಅನ್ವಯಿಸುವುದು, ಮತ್ತು ಗುರಿ ಹಾರ್ಡ್‌ವೇರ್‌ಗೆ ಸಂಯೋಜಿಸುವುದು.

### ಪ್ರಮುಖ ಪರಿಮಾಣಗಳ ವಿವರಣೆ

- `export=True`: ಮಾದರಿಯನ್ನು OpenVINO IR ಸ್ವರೂಪಕ್ಕೆ ಪರಿವರ್ತಿಸುತ್ತದೆ
- `compile=False`: ಸಂಯೋಜನೆಯನ್ನು ರನ್‌ಟೈಮ್‌ಗೆ ತಡಮಾಡುತ್ತದೆ, ಹೆಚ್ಚು ಲವಚಿಕತೆಗಾಗಿ
- `device`: ಗುರಿ ಹಾರ್ಡ್‌ವೇರ್ ("CPU", "GPU", "AUTO" ಸ್ವಯಂಚಾಲಿತ ಆಯ್ಕೆಗಾಗಿ)
- `save_pretrained()`: ಪುನಃಬಳಕೆಗಾಗಿ ಆಪ್ಟಿಮೈಸ್ ಮಾಡಲಾದ ಮಾದರಿಯನ್ನು ಉಳಿಸುತ್ತದೆ

## ಉದಾಹರಣೆ: OpenVINO ಬಳಸಿ ಮಾದರಿಗಳನ್ನು ಪರಿವರ್ತನೆ ಮತ್ತು ಆಪ್ಟಿಮೈಸ್ ಮಾಡುವುದು

### ಹಂತ 1: NNCF ಕ್ವಾಂಟೈಜೆಷನ್ ಮೂಲಕ ಮಾದರಿ ಪರಿವರ್ತನೆ

ಪೋಸ್ಟ್-ಟ್ರೈನಿಂಗ್ ಕ್ವಾಂಟೈಜೆಷನ್ ಅನ್ವಯಿಸುವ ವಿಧಾನ ಇಲ್ಲಿದೆ:

```python
import nncf
from openvino import Core
from optimum.intel import OVModelForCausalLM
import torch
from transformers import AutoTokenizer

# ಪ್ರಮಾಣೀಕರಣಕ್ಕಾಗಿ NNCF ಅನ್ನು ಪ್ರಾರಂಭಿಸಿ
model_id = "microsoft/DialoGPT-small"

# OpenVINO ಸ್ವರೂಪದಲ್ಲಿ ಮಾದರಿಯನ್ನು ಲೋಡ್ ಮಾಡಿ
ov_model = OVModelForCausalLM.from_pretrained(
    model_id, 
    export=True,
    compile=False
)

# ಪ್ರಮಾಣೀಕರಣಕ್ಕಾಗಿ ಕ್ಯಾಲಿಬ್ರೇಶನ್ ಡೇಟಾಸೆಟ್ ರಚಿಸಿ
tokenizer = AutoTokenizer.from_pretrained(model_id)
calibration_data = [
    "Hello, how are you today?",
    "What is artificial intelligence?",
    "Tell me about machine learning.",
    "How does deep learning work?",
    "Explain neural networks."
]

def create_calibration_dataset():
    for text in calibration_data:
        tokens = tokenizer.encode(text, return_tensors="pt")
        yield {"input_ids": tokens}

# ತರಬೇತಿ ನಂತರದ ಪ್ರಮಾಣೀಕರಣವನ್ನು ಅನ್ವಯಿಸಿ
core = Core()
model = core.read_model(ov_model.model_path)

# ಪ್ರಮಾಣೀಕರಣವನ್ನು ಸಂರಚಿಸಿ
quantization_config = nncf.QuantizationConfig(
    input_info=nncf.InputInfo(
        sample_size=(1, 10),  # ಬ್ಯಾಚ್_ಗಾತ್ರ, ಕ್ರಮದ_ನೀളം
        type="long"
    )
)

# ಪ್ರಮಾಣೀಕೃತ ಮಾದರಿಯನ್ನು ರಚಿಸಿ
quantized_model = nncf.quantize_with_tune_runner(
    model,
    create_calibration_dataset(),
    quantization_config
)

# ಪ್ರಮಾಣೀಕೃತ ಮಾದರಿಯನ್ನು ಉಳಿಸಿ
import openvino as ov
ov.save_model(quantized_model, "models/dialogpt-quantized.xml")
```

### ಹಂತ 2: ತೂಕ ಸಂಕುಚಿತದೊಂದಿಗೆ ಸುಧಾರಿತ ಆಪ್ಟಿಮೈಜೆಷನ್

ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಆಧಾರಿತ ಮಾದರಿಗಳಿಗೆ ತೂಕ ಸಂಕುಚಿತ ಅನ್ವಯಿಸಿ:

```python
import nncf
from openvino import Core

# ಮಾದರಿಯನ್ನು ಲೋಡ್ ಮಾಡಿ
core = Core()
model = core.read_model("models/dialogpt-openvino")

# LLM ಗಾಗಿ ತೂಕ ಸಂಕುಚಿತಗೊಳಿಸುವಿಕೆ ಅನ್ವಯಿಸಿ
compressed_model = nncf.compress_weights(
    model,
    mode=nncf.CompressWeightsMode.INT4_SYM,  # ಅಥವಾ INT4_ASYM, INT8
    ratio=0.8,  # ಸಂಕುಚಿತಗೊಳಿಸುವಿಕೆ ಅನುಪಾತ
    group_size=128  # ಪ್ರಮಾಣೀಕರಣಕ್ಕಾಗಿ ಗುಂಪಿನ ಗಾತ್ರ
)

# ಸಂಕುಚಿತಗೊಳಿಸಿದ ಮಾದರಿಯನ್ನು ಉಳಿಸಿ
import openvino as ov
ov.save_model(compressed_model, "models/dialogpt-compressed.xml")
```

### ಹಂತ 3: ಆಪ್ಟಿಮೈಸ್ ಮಾಡಲಾದ ಮಾದರಿಯಿಂದ ಇನ್ಫರೆನ್ಸ್

```python
from openvino import Core
import numpy as np

# OpenVINO ಕೋರ್ ಅನ್ನು ಪ್ರಾರಂಭಿಸಿ
core = Core()

# ಆಪ್ಟಿಮೈಸ್ ಮಾಡಲಾದ ಮಾದರಿಯನ್ನು ಲೋಡ್ ಮಾಡಿ
model = core.read_model("models/dialogpt-compressed.xml")

# ಗುರಿ ಸಾಧನಕ್ಕಾಗಿ ಮಾದರಿಯನ್ನು ಸಂಯೋಜಿಸಿ
compiled_model = core.compile_model(model, "CPU")

# ಇನ್‌ಪುಟ್/ಔಟ್‌ಪುಟ್ ಮಾಹಿತಿಯನ್ನು ಪಡೆಯಿರಿ
input_layer = compiled_model.input(0)
output_layer = compiled_model.output(0)

# ಇನ್‌ಪುಟ್ ಡೇಟಾವನ್ನು ಸಿದ್ಧಪಡಿಸಿ
input_text = "Hello, how are you?"
tokens = tokenizer.encode(input_text, return_tensors="np")

# ಇನ್ಫರೆನ್ಸ್ ಅನ್ನು ನಡೆಸಿ
result = compiled_model([tokens])[output_layer]

# ಔಟ್‌ಪುಟ್ ಅನ್ನು ಡಿಕೋಡ್ ಮಾಡಿ
output_tokens = np.argmax(result, axis=-1)
generated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print(f"Generated: {generated_text}")
```

### ಔಟ್‌ಪುಟ್ ರಚನೆ

ಆಪ್ಟಿಮೈಜೆಷನ್ ನಂತರ, ನಿಮ್ಮ ಮಾದರಿ ಡೈರೆಕ್ಟರಿ ಒಳಗೊಂಡಿರುತ್ತದೆ:

```
models/dialogpt-compressed/
├── dialogpt-compressed.xml    # Model architecture
├── dialogpt-compressed.bin    # Model weights
├── config.json               # Model configuration
├── tokenizer.json            # Tokenizer files
└── tokenizer_config.json     # Tokenizer configuration
```

## ಅಧಿಕೃತ ಬಳಕೆ

### NNCF YAML ಮೂಲಕ ಸಂರಚನೆ

ಸಂಕೀರ್ಣ ಆಪ್ಟಿಮೈಜೆಷನ್ ಕಾರ್ಯಪ್ರವಾಹಗಳಿಗೆ NNCF ಸಂರಚನಾ ಕಡತಗಳನ್ನು ಬಳಸಿ:

```yaml
# nncf_config.yaml
input_info:
  sample_size: [1, 512]
  type: "long"

compression:
  algorithm: quantization
  initializer:
    precision:
      bitwidth_per_scope: [[8, 'default']]
    range:
      num_init_samples: 300
    batchnorm_adaptation:
      num_bn_adaptation_samples: 2000

target_device: CPU
```

ಸಂರಚನೆಯನ್ನು ಅನ್ವಯಿಸಿ:

```python
import nncf
from openvino import Core

# ಮಾದರಿ ಮತ್ತು ಸಂರಚನೆಯನ್ನು ಲೋಡ್ ಮಾಡಿ
core = Core()
model = core.read_model("model.xml")
nncf_config = nncf.NNCFConfig.from_json("nncf_config.yaml")

# ಸಂಕುಚಿತಗೊಳಿಸುವಿಕೆ ಅನ್ವಯಿಸಿ
compressed_model = nncf.create_compressed_model(model, nncf_config)
```

### GPU ಆಪ್ಟಿಮೈಜೆಷನ್

GPU ತ್ವರಕಕ್ಕಾಗಿ:

```python
from optimum.intel import OVModelForCausalLM

# GPU ಸಾಧನದೊಂದಿಗೆ ಮಾದರಿಯನ್ನು ಲೋಡ್ ಮಾಡಿ
ov_model = OVModelForCausalLM.from_pretrained(
    "models/dialogpt-openvino",
    device="GPU",
    ov_config={"PERFORMANCE_HINT": "THROUGHPUT"}
)

# ಹೆಚ್ಚಿನ ಥ್ರೂಪುಟ್‌ಗಾಗಿ ಸಂರಚಿಸಿ
ov_model.ov_config.update({
    "NUM_STREAMS": "AUTO",
    "INFERENCE_NUM_THREADS": "AUTO"
})
```

### ಬ್ಯಾಚ್ ಪ್ರೊಸೆಸಿಂಗ್ ಆಪ್ಟಿಮೈಜೆಷನ್

```python
from openvino import Core

core = Core()
model = core.read_model("model.xml")

# ಬ್ಯಾಚ್ ಪ್ರಕ್ರಿಯೆಗೆ ಸಂರಚಿಸಿ
config = {
    "PERFORMANCE_HINT": "THROUGHPUT",
    "INFERENCE_NUM_THREADS": "AUTO",
    "NUM_STREAMS": "AUTO"
}

compiled_model = core.compile_model(model, "CPU", config)

# ಬಹು ಇನ್‌ಪುಟ್‌ಗಳನ್ನು ಪ್ರಕ್ರಿಯೆ ಮಾಡಿ
batch_inputs = [tokens1, tokens2, tokens3]
results = compiled_model(batch_inputs)
```

### ಮಾದರಿ ಸರ್ವರ್ ನಿಯೋಜನೆ

OpenVINO ಮಾದರಿ ಸರ್ವರ್ ಬಳಸಿ ಆಪ್ಟಿಮೈಸ್ ಮಾಡಲಾದ ಮಾದರಿಗಳನ್ನು ನಿಯೋಜಿಸಿ:

```bash
# OpenVINO ಮಾದರಿ ಸರ್ವರ್ ಅನ್ನು ಸ್ಥಾಪಿಸಿ
pip install ovms

# ಮಾದರಿ ಸರ್ವರ್ ಅನ್ನು ಪ್ರಾರಂಭಿಸಿ
ovms --model_name dialogpt --model_path models/dialogpt-compressed --port 9000
```

ಮಾದರಿ ಸರ್ವರ್‌ಗಾಗಿ ಕ್ಲೈಂಟ್ ಕೋಡ್:

```python
import requests
import json

# ವಿನಂತಿಯನ್ನು ತಯಾರಿಸಿ
data = {
    "inputs": {
        "input_ids": [[1, 2, 3, 4, 5]]  # ಟೋಕನ್ ಐಡಿಗಳು
    }
}

# ಮಾದರಿ ಸರ್ವರ್‌ಗೆ ವಿನಂತಿಯನ್ನು ಕಳುಹಿಸಿ
response = requests.post(
    "http://localhost:9000/v1/models/dialogpt:predict",
    json=data
)

result = response.json()
print(result["outputs"])
```

## ಉತ್ತಮ ಅಭ್ಯಾಸಗಳು

### 1. ಮಾದರಿ ಆಯ್ಕೆ ಮತ್ತು ತಯಾರಿ
- ಬೆಂಬಲಿತ ಫ್ರೇಮ್ವರ್ಕ್‌ಗಳಿಂದ (PyTorch, TensorFlow, ONNX) ಮಾದರಿಗಳನ್ನು ಬಳಸಿ
- ಮಾದರಿ ಇನ್‌ಪುಟ್‌ಗಳಿಗೆ ಸ್ಥಿರ ಅಥವಾ ತಿಳಿದಿರುವ ಡೈನಾಮಿಕ್ ಆಕಾರಗಳನ್ನು ಖಚಿತಪಡಿಸಿಕೊಳ್ಳಿ
- ಕ್ಯಾಲಿಬ್ರೇಷನ್‌ಗಾಗಿ ಪ್ರತಿನಿಧಿ ಡೇಟಾಸೆಟ್‌ಗಳೊಂದಿಗೆ ಪರೀಕ್ಷಿಸಿ

### 2. ಆಪ್ಟಿಮೈಜೆಷನ್ ತಂತ್ರ ಆಯ್ಕೆ
- **ಪೋಸ್ಟ್-ಟ್ರೈನಿಂಗ್ ಕ್ವಾಂಟೈಜೆಷನ್**: ತ್ವರಿತ ಆಪ್ಟಿಮೈಜೆಷನ್‌ಗೆ ಇಲ್ಲಿ ಪ್ರಾರಂಭಿಸಿ
- **ತೂಕ ಸಂಕುಚಿತ**: ದೊಡ್ಡ ಭಾಷಾ ಮಾದರಿಗಳು ಮತ್ತು ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್‌ಗಳಿಗೆ ಸೂಕ್ತ
- **ಕ್ವಾಂಟೈಜೆಷನ್-ಅವೇರ್ ತರಬೇತಿ**: ನಿಖರತೆ ಮುಖ್ಯವಾದಾಗ ಬಳಸಿ

### 3. ಹಾರ್ಡ್‌ವೇರ್-ನಿರ್ದಿಷ್ಟ ಆಪ್ಟಿಮೈಜೆಷನ್
- **CPU**: ಸಮತೋಲನ ಕಾರ್ಯಕ್ಷಮತೆಗಾಗಿ INT8 ಕ್ವಾಂಟೈಜೆಷನ್ ಬಳಸಿ
- **GPU**: FP16 ನಿಖರತೆ ಮತ್ತು ಬ್ಯಾಚ್ ಪ್ರೊಸೆಸಿಂಗ್ ಉಪಯೋಗಿಸಿ
- **VPU**: ಮಾದರಿ ಸರಳೀಕರಣ ಮತ್ತು ಲೇಯರ್ ಫ್ಯೂಷನ್ ಮೇಲೆ ಗಮನಹರಿಸಿ

### 4. ಕಾರ್ಯಕ್ಷಮತೆ ಟ್ಯೂನಿಂಗ್
- **ಥ್ರೂಪುಟ್ ಮೋಡ್**: ಹೆಚ್ಚಿನ ಪ್ರಮಾಣದ ಬ್ಯಾಚ್ ಪ್ರೊಸೆಸಿಂಗ್‌ಗೆ
- **ಲೆಟೆನ್ಸಿ ಮೋಡ್**: ರಿಯಲ್-ಟೈಮ್ ಇಂಟರಾಕ್ಟಿವ್ ಅಪ್ಲಿಕೇಶನ್‌ಗಳಿಗೆ
- **AUTO ಸಾಧನ**: OpenVINO ಗುರಿ ಹಾರ್ಡ್‌ವೇರ್ ಆಯ್ಕೆ ಮಾಡಲು ಬಿಡಿ

### 5. ಮೆಮೊರಿ ನಿರ್ವಹಣೆ
- ಮೆಮೊರಿ ಓವರ್‌ಹೆಡ್ ತಪ್ಪಿಸಲು ಡೈನಾಮಿಕ್ ಆಕಾರಗಳನ್ನು ಜಾಗರೂಕತೆಯಿಂದ ಬಳಸಿ
- ವೇಗವಾದ ನಂತರದ ಲೋಡ್ಗಾಗಿ ಮಾದರಿ ಕ್ಯಾಶಿಂಗ್ ಅನುಷ್ಠಾನಗೊಳಿಸಿ
- ಆಪ್ಟಿಮೈಜೆಷನ್ ಸಮಯದಲ್ಲಿ ಮೆಮೊರಿ ಬಳಕೆಯನ್ನು ಗಮನಿಸಿ

### 6. ನಿಖರತೆ ಪರಿಶೀಲನೆ
- ಆಪ್ಟಿಮೈಸ್ ಮಾಡಲಾದ ಮಾದರಿಗಳನ್ನು ಮೂಲ ಕಾರ್ಯಕ್ಷಮತೆಯೊಂದಿಗೆ ಸದಾ ಪರಿಶೀಲಿಸಿ
- ಮೌಲ್ಯಮಾಪನಕ್ಕಾಗಿ ಪ್ರತಿನಿಧಿ ಪರೀಕ್ಷಾ ಡೇಟಾಸೆಟ್‌ಗಳನ್ನು ಬಳಸಿ
- ಕ್ರಮೇಣ ಆಪ್ಟಿಮೈಜೆಷನ್ ಪರಿಗಣಿಸಿ (ರಕ್ಷಕ ಸೆಟ್ಟಿಂಗ್‌ಗಳಿಂದ ಪ್ರಾರಂಭಿಸಿ)

## ಸಮಸ್ಯೆ ಪರಿಹಾರ

### ಸಾಮಾನ್ಯ ಸಮಸ್ಯೆಗಳು

#### 1. ಸ್ಥಾಪನೆ ಸಮಸ್ಯೆಗಳು
```bash
# ಪಿಪ್ ಕ್ಯಾಶೆ ತೆರವುಗೊಳಿಸಿ ಮತ್ತು ಮರುಸ್ಥಾಪಿಸಿ
pip cache purge
pip uninstall openvino nncf
pip install openvino nncf --no-cache-dir
```

#### 2. ಮಾದರಿ ಪರಿವರ್ತನೆ ದೋಷಗಳು
```python
# ಮಾದರಿ ಹೊಂದಾಣಿಕೆಯನ್ನು ಪರಿಶೀಲಿಸಿ
from openvino.tools.mo import convert_model

try:
    ov_model = convert_model("model.onnx")
    print("Conversion successful")
except Exception as e:
    print(f"Conversion failed: {e}")
```

#### 3. ಕಾರ್ಯಕ್ಷಮತೆ ಸಮಸ್ಯೆಗಳು
```python
# ಕಾರ್ಯಕ್ಷಮತೆ ಸೂಚನೆಗಳನ್ನು ಸಕ್ರಿಯಗೊಳಿಸಿ
config = {
    "PERFORMANCE_HINT": "LATENCY",  # ಅಥವಾ "ಥ್ರೂಪುಟ್"
    "INFERENCE_PRECISION_HINT": "f32"  # ಅಥವಾ "f16"
}
compiled_model = core.compile_model(model, "CPU", config)
```

#### 4. ಮೆಮೊರಿ ಸಮಸ್ಯೆಗಳು
- ಆಪ್ಟಿಮೈಜೆಷನ್ ಸಮಯದಲ್ಲಿ ಮಾದರಿ ಬ್ಯಾಚ್ ಗಾತ್ರವನ್ನು ಕಡಿಮೆ ಮಾಡಿ
- ದೊಡ್ಡ ಡೇಟಾಸೆಟ್‌ಗಳಿಗೆ ಸ್ಟ್ರೀಮಿಂಗ್ ಬಳಸಿ
- ಮಾದರಿ ಕ್ಯಾಶಿಂಗ್ ಸಕ್ರಿಯಗೊಳಿಸಿ: `core.set_property("CPU", {"CACHE_DIR": "./cache"})`

#### 5. ನಿಖರತೆ ಕುಸಿತ
- ಹೆಚ್ಚಿನ ನಿಖರತೆ (INT4 ಬದಲು INT8) ಬಳಸಿ
- ಕ್ಯಾಲಿಬ್ರೇಷನ್ ಡೇಟಾಸೆಟ್ ಗಾತ್ರವನ್ನು ಹೆಚ್ಚಿಸಿ
- ಮಿಶ್ರ ನಿಖರತೆ ಆಪ್ಟಿಮೈಜೆಷನ್ ಅನ್ವಯಿಸಿ

### ಕಾರ್ಯಕ್ಷಮತೆ ಮೇಲ್ವಿಚಾರಣೆ

```python
# ನಿರ್ಣಯ ಕಾರ್ಯಕ್ಷಮತೆಯನ್ನು ಮೇಲ್ವಿಚಾರಣೆ ಮಾಡಿ
import time

start_time = time.time()
result = compiled_model([input_data])
inference_time = time.time() - start_time

print(f"Inference time: {inference_time:.4f} seconds")
```

### ಸಹಾಯ ಪಡೆಯುವುದು

- **ಡಾಕ್ಯುಮೆಂಟೇಶನ್**: [docs.openvino.ai](https://docs.openvino.ai/2025/index.html)
- **GitHub ಸಮಸ್ಯೆಗಳು**: [github.com/openvinotoolkit/openvino/issues](https://github.com/openvinotoolkit/openvino/issues)
- **ಸಮುದಾಯ ಫೋರಂ**: [community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit](https://community.intel.com/t5/Intel-Distribution-of-OpenVINO/bd-p/distribution-openvino-toolkit)

## ಹೆಚ್ಚುವರಿ ಸಂಪನ್ಮೂಲಗಳು

### ಅಧಿಕೃತ ಲಿಂಕ್‌ಗಳು
- **OpenVINO ಹೋಮ್‌ಪೇಜ್**: [openvino.ai](https://openvino.ai/)
- **GitHub ರೆಪೊ**: [github.com/openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino)
- **NNCF ರೆಪೊ**: [github.com/openvinotoolkit/nncf](https://github.com/openvinotoolkit/nncf)
- **ಮಾದರಿ ಜೂ**: [github.com/openvinotoolkit/open_model_zoo](https://github.com/openvinotoolkit/open_model_zoo)

### ಕಲಿಕೆ ಸಂಪನ್ಮೂಲಗಳು
- **OpenVINO ನೋಟ್ಬುಕ್ಸ್**: [github.com/openvinotoolkit/openvino_notebooks](https://github.com/openvinotoolkit/openvino_notebooks)
- **ತ್ವರಿತ ಪ್ರಾರಂಭ ಮಾರ್ಗದರ್ಶಿ**: [docs.openvino.ai/2025/get-started](https://docs.openvino.ai/2025/get-started/install-openvino.html)
- **ಆಪ್ಟಿಮೈಜೆಷನ್ ಮಾರ್ಗದರ್ಶಿ**: [docs.openvino.ai/2025/openvino-workflow/model-optimization](https://docs.openvino.ai/2025/openvino-workflow/model-optimization.html)

### ಏಕೀಕರಣ ಸಾಧನಗಳು
- **Hugging Face Optimum Intel**: [huggingface.co/docs/optimum/intel](https://huggingface.co/docs/optimum/intel/optimization_ov)
- **OpenVINO ಮಾದರಿ ಸರ್ವರ್**: [docs.openvino.ai/2025/model-server](https://docs.openvino.ai/2025/model-server/ovms_what_is_openvino_model_server.html)
- **OpenVINO GenAI**: [docs.openvino.ai/2025/openvino-workflow-generative](https://docs.openvino.ai/2025/openvino-workflow-generative.html)

### ಕಾರ್ಯಕ್ಷಮತೆ ಬೆಂಚ್ಮಾರ್ಕ್‌ಗಳು
- **ಅಧಿಕೃತ ಬೆಂಚ್ಮಾರ್ಕ್‌ಗಳು**: [docs.openvino.ai/2025/about-openvino/performance-benchmarks](https://docs.openvino.ai/2025/about-openvino/performance-benchmarks.html)
- **NNCF ಮಾದರಿ ಜೂ**: [github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md](https://github.com/openvinotoolkit/nncf/blob/develop/docs/ModelZoo.md)

### ಸಮುದಾಯ ಉದಾಹರಣೆಗಳು
- **ಜುಪಿಟರ್ ನೋಟ್ಬುಕ್ಸ್**: [OpenVINO Notebooks Repository](https://github.com/openvinotoolkit/openvino_notebooks) - OpenVINO ನೋಟ್ಬುಕ್ಸ್ ರೆಪೊದಲ್ಲಿ ಸಮಗ್ರ ಟ್ಯುಟೋರಿಯಲ್ಗಳು ಲಭ್ಯವಿವೆ
- **ನಮೂನಾ ಅಪ್ಲಿಕೇಶನ್‌ಗಳು**: [OpenVINO Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo) - ವಿವಿಧ ಕ್ಷೇತ್ರಗಳ (ಕಂಪ್ಯೂಟರ್ ವೀಷನ್, NLP, ಆಡಿಯೋ) ನೈಜ ಜಗತ್ತಿನ ಉದಾಹರಣೆಗಳು
- **ಬ್ಲಾಗ್ ಪೋಸ್ಟ್‌ಗಳು**: [Intel AI Blog](https://www.intel.com/content/www/us/en/artificial-intelligence/blog.html) - ಇಂಟೆಲ್ AI ಮತ್ತು ಸಮುದಾಯ ಬ್ಲಾಗ್ ಪೋಸ್ಟ್‌ಗಳು ವಿವರವಾದ ಬಳಕೆ ಪ್ರಕರಣಗಳೊಂದಿಗೆ

### ಸಂಬಂಧಿತ ಸಾಧನಗಳು
- **Intel Neural Compressor**: [github.com/intel/neural-compressor](https://github.com/intel/neural-compressor) - ಇಂಟೆಲ್ ಹಾರ್ಡ್‌ವೇರ್‌ಗಾಗಿ ಹೆಚ್ಚುವರಿ ಆಪ್ಟಿಮೈಜೆಷನ್ ತಂತ್ರಗಳು
- **TensorFlow Lite**: [tensorflow.org/lite](https://www.tensorflow.org/lite) - ಮೊಬೈಲ್ ಮತ್ತು ಎಡ್ಜ್ ನಿಯೋಜನೆ ಹೋಲಿಕೆಗಾಗಿ
- **ONNX Runtime**: [onnxruntime.ai](https://onnxruntime.ai/) - ಕ್ರಾಸ್-ಪ್ಲಾಟ್‌ಫಾರ್ಮ್ ಇನ್ಫರೆನ್ಸ್ ಎಂಜಿನ್ ಪರ್ಯಾಯಗಳು

## ➡️ ಮುಂದೇನು

- [05: ಆಪಲ್ MLX ಫ್ರೇಮ್ವರ್ಕ್ ಡೀಪ್ ಡೈವ್](./05.AppleMLX.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ಅಸ್ವೀಕರಣ**:  
ಈ ದಸ್ತಾವೇಜು AI ಅನುವಾದ ಸೇವೆ [Co-op Translator](https://github.com/Azure/co-op-translator) ಬಳಸಿ ಅನುವಾದಿಸಲಾಗಿದೆ. ನಾವು ನಿಖರತೆಯಿಗಾಗಿ ಪ್ರಯತ್ನಿಸುತ್ತಿದ್ದರೂ, ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ದೋಷಗಳು ಅಥವಾ ಅಸತ್ಯತೆಗಳು ಇರಬಹುದು ಎಂದು ದಯವಿಟ್ಟು ಗಮನಿಸಿ. ಮೂಲ ಭಾಷೆಯಲ್ಲಿರುವ ಮೂಲ ದಸ್ತಾವೇಜನ್ನು ಅಧಿಕೃತ ಮೂಲವಾಗಿ ಪರಿಗಣಿಸಬೇಕು. ಮಹತ್ವದ ಮಾಹಿತಿಗಾಗಿ, ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗುತ್ತದೆ. ಈ ಅನುವಾದ ಬಳಕೆಯಿಂದ ಉಂಟಾಗುವ ಯಾವುದೇ ತಪ್ಪು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವಿಕೆ ಅಥವಾ ತಪ್ಪು ವಿವರಣೆಗಳಿಗೆ ನಾವು ಹೊಣೆಗಾರರಾಗುವುದಿಲ್ಲ.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->