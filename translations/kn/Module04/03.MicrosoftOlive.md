<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1d2f37246d3818c5a66b951850225082",
  "translation_date": "2025-12-15T23:10:53+00:00",
  "source_file": "Module04/03.MicrosoftOlive.md",
  "language_code": "kn"
}
-->
# ವಿಭಾಗ 3 : ಮೈಕ್ರೋಸಾಫ್ಟ್ ಒಲಿವ್ ಆಪ್ಟಿಮೈಜೆಷನ್ ಸೂಟ್

## ವಿಷಯಗಳ ಪಟ್ಟಿಕೆ
1. [ಪರಿಚಯ](../../../Module04)
2. [ಮೈಕ್ರೋಸಾಫ್ಟ್ ಒಲಿವ್ ಎಂದರೆ ಏನು?](../../../Module04)
3. [ಸ್ಥಾಪನೆ](../../../Module04)
4. [ತ್ವರಿತ ಪ್ರಾರಂಭ ಮಾರ್ಗದರ್ಶಿ](../../../Module04)
5. [ಉದಾಹರಣೆ: Qwen3 ಅನ್ನು ONNX INT4 ಗೆ ಪರಿವರ್ತಿಸುವುದು](../../../Module04)
6. [ಅಧಿಕೃತ ಬಳಕೆ](../../../Module04)
7. [ಒಲಿವ್ ರೆಸಿಪೀಸ್ ರೆಪೊಸಿಟರಿ](../../../Module04)
8. [ಉತ್ತಮ ಅಭ್ಯಾಸಗಳು](../../../Module04)
9. [ಸಮಸ್ಯೆ ಪರಿಹಾರ](../../../Module04)
10. [ಹೆಚ್ಚಿನ ಸಂಪನ್ಮೂಲಗಳು](../../../Module04)

## ಪರಿಚಯ

ಮೈಕ್ರೋಸಾಫ್ಟ್ ಒಲಿವ್ ಒಂದು ಶಕ್ತಿಶಾಲಿ, ಸುಲಭವಾಗಿ ಬಳಸಬಹುದಾದ ಹಾರ್ಡ್‌ವೇರ್-ಅವೇರ್ ಮಾದರಿ ಆಪ್ಟಿಮೈಜೆಷನ್ ಟೂಲ್‌ಕಿಟ್ ಆಗಿದ್ದು, ವಿಭಿನ್ನ ಹಾರ್ಡ್‌ವೇರ್ ವೇದಿಕೆಗಳ ಮೇಲೆ ನಿಯೋಜನೆಗಾಗಿ ಯಂತ್ರ ಅಧ್ಯಯನ ಮಾದರಿಗಳನ್ನು ಆಪ್ಟಿಮೈಸ್ ಮಾಡುವ ಪ್ರಕ್ರಿಯೆಯನ್ನು ಸರಳಗೊಳಿಸುತ್ತದೆ. ನೀವು CPU, GPU ಅಥವಾ ವಿಶೇಷ AI ಅಕ್ಸಿಲೆರೇಟರ್‌ಗಳನ್ನು ಗುರಿಯಾಗಿಸಿಕೊಂಡಿದ್ದರೂ, ಒಲಿವ್ ನಿಮ್ಮ ಮಾದರಿಯ ನಿಖರತೆಯನ್ನು ಕಾಪಾಡಿಕೊಂಡು ಅತ್ಯುತ್ತಮ ಕಾರ್ಯಕ್ಷಮತೆಯನ್ನು ಸಾಧಿಸಲು ಸಹಾಯ ಮಾಡುತ್ತದೆ.

## ಮೈಕ್ರೋಸಾಫ್ಟ್ ಒಲಿವ್ ಎಂದರೆ ಏನು?

ಒಲಿವ್ ಒಂದು ಸುಲಭವಾಗಿ ಬಳಸಬಹುದಾದ ಹಾರ್ಡ್‌ವೇರ್-ಅವೇರ್ ಮಾದರಿ ಆಪ್ಟಿಮೈಜೆಷನ್ ಸಾಧನವಾಗಿದ್ದು, ಮಾದರಿ ಸಂಕುಚಿತಗೊಳಿಸುವಿಕೆ, ಆಪ್ಟಿಮೈಜೆಷನ್ ಮತ್ತು ಸಂಯೋಜನೆ ಕ್ಷೇತ್ರಗಳಲ್ಲಿ ಕೈಗಾರಿಕಾ ಮುಂಚೂಣಿಯ ತಂತ್ರಗಳನ್ನು ಸಂಯೋಜಿಸುತ್ತದೆ. ಇದು ONNX ರನ್‌ಟೈಮ್‌ನೊಂದಿಗೆ E2E ಇನ್ಫರೆನ್ಸ್ ಆಪ್ಟಿಮೈಜೆಷನ್ ಪರಿಹಾರವಾಗಿ ಕಾರ್ಯನಿರ್ವಹಿಸುತ್ತದೆ.

### ಪ್ರಮುಖ ವೈಶಿಷ್ಟ್ಯಗಳು

- **ಹಾರ್ಡ್‌ವೇರ್-ಅವೇರ್ ಆಪ್ಟಿಮೈಜೆಷನ್**: ನಿಮ್ಮ ಗುರಿ ಹಾರ್ಡ್‌ವೇರ್‌ಗೆ ಅತ್ಯುತ್ತಮ ಆಪ್ಟಿಮೈಜೆಷನ್ ತಂತ್ರಗಳನ್ನು ಸ್ವಯಂಚಾಲಿತವಾಗಿ ಆಯ್ಕೆ ಮಾಡುತ್ತದೆ
- **40+ ಒಳಗೊಂಡಿರುವ ಆಪ್ಟಿಮೈಜೆಷನ್ ಘಟಕಗಳು**: ಮಾದರಿ ಸಂಕುಚಿತಗೊಳಿಸುವಿಕೆ, ಪ್ರಮಾಣೀಕರಣ, ಗ್ರಾಫ್ ಆಪ್ಟಿಮೈಜೆಷನ್ ಮತ್ತು ಇನ್ನಷ್ಟು ಒಳಗೊಂಡಿದೆ
- **ಸರಳ CLI ಇಂಟರ್ಫೇಸ್**: ಸಾಮಾನ್ಯ ಆಪ್ಟಿಮೈಜೆಷನ್ ಕಾರ್ಯಗಳಿಗೆ ಸರಳ ಆಜ್ಞೆಗಳು
- **ಬಹು-ಫ್ರೇಮ್ವರ್ಕ್ ಬೆಂಬಲ**: PyTorch, Hugging Face ಮಾದರಿಗಳು ಮತ್ತು ONNX ಜೊತೆಗೆ ಕಾರ್ಯನಿರ್ವಹಿಸುತ್ತದೆ
- **ಜನಪ್ರಿಯ ಮಾದರಿ ಬೆಂಬಲ**: Llama, Phi, Qwen, Gemma ಮುಂತಾದ ಜನಪ್ರಿಯ ಮಾದರಿ ವಾಸ್ತುಶಿಲ್ಪಗಳನ್ನು ಸ್ವಯಂಚಾಲಿತವಾಗಿ ಆಪ್ಟಿಮೈಸ್ ಮಾಡಬಹುದು

### ಲಾಭಗಳು

- **ಅಭಿವೃದ್ಧಿ ಸಮಯ ಕಡಿತ**: ವಿಭಿನ್ನ ಆಪ್ಟಿಮೈಜೆಷನ್ ತಂತ್ರಗಳನ್ನು ಕೈಯಿಂದ ಪ್ರಯೋಗಿಸುವ ಅಗತ್ಯವಿಲ್ಲ
- **ಕಾರ್ಯಕ್ಷಮತೆ ಸುಧಾರಣೆಗಳು**: ಕೆಲವು ಸಂದರ್ಭಗಳಲ್ಲಿ 6x ವರೆಗೆ ವೇಗದ ಸುಧಾರಣೆಗಳು
- **ಕ್ರಾಸ್-ಪ್ಲಾಟ್‌ಫಾರ್ಮ್ ನಿಯೋಜನೆ**: ವಿಭಿನ್ನ ಹಾರ್ಡ್‌ವೇರ್ ಮತ್ತು ಕಾರ್ಯಾಚರಣೆ ವ್ಯವಸ್ಥೆಗಳ ಮೇಲೆ ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿದ ಮಾದರಿಗಳು ಕಾರ್ಯನಿರ್ವಹಿಸುತ್ತವೆ
- **ನಿಖರತೆ ಕಾಪಾಡಿಕೊಳ್ಳುವುದು**: ಕಾರ್ಯಕ್ಷಮತೆಯನ್ನು ಸುಧಾರಿಸುವಾಗ ಮಾದರಿ ಗುಣಮಟ್ಟವನ್ನು ಕಾಪಾಡುತ್ತದೆ

## ಸ್ಥಾಪನೆ

### ಪೂರ್ವಾಪೇಕ್ಷಿತಗಳು

- Python 3.8 ಅಥವಾ ಹೆಚ್ಚಿನ ಆವೃತ್ತಿ
- pip ಪ್ಯಾಕೇಜ್ ಮ್ಯಾನೇಜರ್
- ವರ್ಚುವಲ್ ಪರಿಸರ (ಶಿಫಾರಸು ಮಾಡಲಾಗಿದೆ)

### ಮೂಲ ಸ್ಥಾಪನೆ

ವರ್ಚುವಲ್ ಪರಿಸರವನ್ನು ರಚಿಸಿ ಮತ್ತು ಸಕ್ರಿಯಗೊಳಿಸಿ:

```bash
# ವರ್ಚುವಲ್ ಪರಿಸರವನ್ನು ರಚಿಸಿ
python -m venv olive-env

# ವರ್ಚುವಲ್ ಪರಿಸರವನ್ನು ಸಕ್ರಿಯಗೊಳಿಸಿ
# ವಿಂಡೋಸ್‌ನಲ್ಲಿ:
olive-env\Scripts\activate
# ಮ್ಯಾಕ್‌ಒಎಸ್/ಲಿನಕ್ಸ್ನಲ್ಲಿ:
source olive-env/bin/activate
```

ಸ್ವಯಂಚಾಲಿತ ಆಪ್ಟಿಮೈಜೆಷನ್ ವೈಶಿಷ್ಟ್ಯಗಳೊಂದಿಗೆ ಒಲಿವ್ ಅನ್ನು ಸ್ಥಾಪಿಸಿ:

```bash
pip install olive-ai[auto-opt]
pip install transformers onnxruntime-genai
```

### ಐಚ್ಛಿಕ ಅವಲಂಬನೆಗಳು

ಹೆಚ್ಚಿನ ವೈಶಿಷ್ಟ್ಯಗಳಿಗಾಗಿ ಒಲಿವ್ ವಿವಿಧ ಐಚ್ಛಿಕ ಅವಲಂಬನೆಗಳನ್ನು ನೀಡುತ್ತದೆ:

```bash
# ಅಜೂರ್ ಎಂಎಲ್ ಸಂಯೋಜನೆಗಾಗಿ
pip install olive-ai[azureml]

# ಡೈರೆಕ್ಟ್‌ಎಂಎಲ್ (ವಿಂಡೋಸ್ ಜಿಪಿಯು ವೇಗವರ್ಧನೆ)ಗಾಗಿ
pip install olive-ai[directml]

# ಸಿಪಿಯು ಆಪ್ಟಿಮೈಜೆಷನ್‌ಗಾಗಿ
pip install olive-ai[cpu]

# ಎಲ್ಲಾ ವೈಶಿಷ್ಟ್ಯಗಳಿಗಾಗಿ
pip install olive-ai[all]
```

### ಸ್ಥಾಪನೆ ಪರಿಶೀಲನೆ

```bash
olive --help
```

ಯಶಸ್ವಿಯಾದರೆ, ನೀವು ಒಲಿವ್ CLI ಸಹಾಯ ಸಂದೇಶವನ್ನು ನೋಡಬಹುದು.

## ತ್ವರಿತ ಪ್ರಾರಂಭ ಮಾರ್ಗದರ್ಶಿ

### ನಿಮ್ಮ ಮೊದಲ ಆಪ್ಟಿಮೈಜೆಷನ್

ಒಲಿವ್‌ನ ಸ್ವಯಂಚಾಲಿತ ಆಪ್ಟಿಮೈಜೆಷನ್ ವೈಶಿಷ್ಟ್ಯವನ್ನು ಬಳಸಿ ಸಣ್ಣ ಭಾಷಾ ಮಾದರಿಯನ್ನು ಆಪ್ಟಿಮೈಸ್ ಮಾಡೋಣ:

```bash
olive auto-opt \
  --model_name_or_path HuggingFaceTB/SmolLM2-135M-Instruct \
  --output_path models/smolm2-optimized \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ಈ ಆಜ್ಞೆ ಏನು ಮಾಡುತ್ತದೆ

ಆಪ್ಟಿಮೈಜೆಷನ್ ಪ್ರಕ್ರಿಯೆ: ಸ್ಥಳೀಯ ಕ್ಯಾಶೆನಿಂದ ಮಾದರಿಯನ್ನು ಪಡೆಯುವುದು, ONNX ಗ್ರಾಫ್ ಅನ್ನು ಹಿಡಿದು ONNX ಡೇಟಾ ಫೈಲ್‌ನಲ್ಲಿ ತೂಕಗಳನ್ನು ಸಂಗ್ರಹಿಸುವುದು, ONNX ಗ್ರಾಫ್ ಅನ್ನು ಆಪ್ಟಿಮೈಸ್ ಮಾಡುವುದು ಮತ್ತು RTN ವಿಧಾನವನ್ನು ಬಳಸಿ ಮಾದರಿಯನ್ನು int4 ಗೆ ಪ್ರಮಾಣೀಕರಿಸುವುದು.

### ಆಜ್ಞೆ ಪರಿಮಾಣಗಳ ವಿವರಣೆ

- `--model_name_or_path`: Hugging Face ಮಾದರಿ ಗುರುತಿಸುವಿಕೆ ಅಥವಾ ಸ್ಥಳೀಯ ಮಾರ್ಗ
- `--output_path`: ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿದ ಮಾದರಿ ಉಳಿಸುವ ಡೈರೆಕ್ಟರಿ
- `--device`: ಗುರಿ ಸಾಧನ (cpu, gpu)
- `--provider`: ಕಾರ್ಯಾಚರಣೆ ಒದಗಿಸುವವರು (CPUExecutionProvider, CUDAExecutionProvider, DmlExecutionProvider)
- `--use_ort_genai`: ಇನ್ಫರೆನ್ಸ್‌ಗೆ ONNX ರನ್‌ಟೈಮ್ ಜನರೇಟ್ AI ಬಳಕೆ
- `--precision`: ಪ್ರಮಾಣೀಕರಣ ನಿಖರತೆ (int4, int8, fp16)
- `--log_level`: ಲಾಗಿಂಗ್ verbosity (0=ಕನಿಷ್ಠ, 1=ವಿಸ್ತೃತ)

## ಉದಾಹರಣೆ: Qwen3 ಅನ್ನು ONNX INT4 ಗೆ ಪರಿವರ್ತಿಸುವುದು

[lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU) ನಲ್ಲಿ ನೀಡಲಾದ Hugging Face ಉದಾಹರಣೆಯ ಆಧಾರದ ಮೇಲೆ, Qwen3 ಮಾದರಿಯನ್ನು ಹೇಗೆ ಆಪ್ಟಿಮೈಸ್ ಮಾಡುವುದು:

### ಹಂತ 1: ಮಾದರಿ ಡೌನ್‌ಲೋಡ್ (ಐಚ್ಛಿಕ)

ಡೌನ್‌ಲೋಡ್ ಸಮಯವನ್ನು ಕಡಿಮೆ ಮಾಡಲು, ಅಗತ್ಯವಿರುವ ಫೈಲ್‌ಗಳನ್ನು ಮಾತ್ರ ಕ್ಯಾಶೆ ಮಾಡಿ:

```bash
huggingface-cli download Qwen/Qwen2.5-0.5B-Instruct *.json *.safetensors *.txt
```

### ಹಂತ 2: Qwen3 ಮಾದರಿಯನ್ನು ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿ

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-onnx-int4 \
  --device cpu \
  --provider CPUExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ಹಂತ 3: ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿದ ಮಾದರಿಯನ್ನು ಪರೀಕ್ಷಿಸಿ

ನಿಮ್ಮ ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿದ ಮಾದರಿಯನ್ನು ಪರೀಕ್ಷಿಸಲು ಸರಳ Python ಸ್ಕ್ರಿಪ್ಟ್ ರಚಿಸಿ:

```python
import onnxruntime_genai as og

# ಸುಧಾರಿತ ಮಾದರಿಯನ್ನು ಲೋಡ್ ಮಾಡಿ
model = og.Model('models/qwen3-onnx-int4')
tokenizer = og.Tokenizer(model)

# ಚಾಟ್ ಟೆಂಪ್ಲೇಟನ್ನು ರಚಿಸಿ
chat_template = '<|im_start|>user\n{input}<|im_end|>\n<|im_start|>assistant\n'

# ಪಠ್ಯವನ್ನು ರಚಿಸಿ
prompt = "What is machine learning?"
input_tokens = tokenizer.encode(chat_template.format(input=prompt))

params = og.GeneratorParams(model)
params.set_search_options(max_length=200)
params.input_ids = input_tokens

generator = og.Generator(model, params)

print("Generated response:")
while not generator.is_done():
    generator.compute_logits()
    generator.generate_next_token()
    
    new_token = generator.get_next_tokens()[0]
    print(tokenizer.decode([new_token]), end='', flush=True)

print()
```

### ಔಟ್‌ಪುಟ್ ರಚನೆ

ಆಪ್ಟಿಮೈಜೆಷನ್ ನಂತರ, ನಿಮ್ಮ ಔಟ್‌ಪುಟ್ ಡೈರೆಕ್ಟರಿಯಲ್ಲಿ ಇವು ಇರುತ್ತವೆ:

```
models/qwen3-onnx-int4/
├── model.onnx              # Optimized ONNX model
├── model.onnx.data         # Model weights
├── genai_config.json       # Generation configuration
├── tokenizer.json          # Tokenizer files
├── tokenizer_config.json
└── special_tokens_map.json
```

## ಅಧಿಕೃತ ಬಳಕೆ

### ಸಂರಚನಾ ಫೈಲ್‌ಗಳು

ಹೆಚ್ಚು ಸಂಕೀರ್ಣ ಆಪ್ಟಿಮೈಜೆಷನ್ ಕಾರ್ಯಪ್ರವಾಹಗಳಿಗೆ, ನೀವು JSON ಸಂರಚನಾ ಫೈಲ್‌ಗಳನ್ನು ಬಳಸಬಹುದು:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "Qwen/Qwen2.5-0.5B-Instruct",
        "task": "text-generation"
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "evaluators": {
    "common_evaluator": {
      "metrics": [
        {
          "name": "latency",
          "type": "latency",
          "sub_types": [{"name": "avg"}]
        }
      ]
    }
  },
  "passes": {
    "conversion": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    },
    "optimization": {
      "type": "OrtTransformersOptimization",
      "config": {
        "model_type": "gpt2"
      }
    }
  },
  "engine": {
    "search_strategy": {
      "execution_order": "joint",
      "search_algorithm": "tpe"
    },
    "evaluator": "common_evaluator",
    "host": "local_system",
    "target": "local_system",
    "cache_dir": "cache",
    "output_dir": "models/optimized"
  }
}
```

ಸಂರಚನೆಯೊಂದಿಗೆ ಚಾಲನೆ ಮಾಡಿ:

```bash
olive run --config config.json
```

### GPU ಆಪ್ಟಿಮೈಜೆಷನ್

CUDA GPU ಆಪ್ಟಿಮೈಜೆಷನ್‌ಗೆ:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-gpu-int4 \
  --device gpu \
  --provider CUDAExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

DirectML (ವಿಂಡೋಸ್) ಗೆ:

```bash
olive auto-opt \
  --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct \
  --output_path models/qwen3-directml-int4 \
  --device gpu \
  --provider DmlExecutionProvider \
  --use_ort_genai \
  --precision int4 \
  --log_level 1
```

### ಒಲಿವ್‌ನೊಂದಿಗೆ ಫೈನ್-ಟ್ಯೂನಿಂಗ್

ಒಲಿವ್ ಮಾದರಿಗಳನ್ನು ಫೈನ್-ಟ್ಯೂನ್ ಮಾಡಲೂ ಬೆಂಬಲಿಸುತ್ತದೆ:

```bash
olive finetune \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name microsoft/dolly-15k \
  --text_template "### Question: {instruction}\n### Answer: {response}" \
  --max_steps 100 \
  --output_path models/llama-finetuned
```

## ಉತ್ತಮ ಅಭ್ಯಾಸಗಳು

### 1. ಮಾದರಿ ಆಯ್ಕೆ
- ಪರೀಕ್ಷೆಗಾಗಿ ಸಣ್ಣ ಮಾದರಿಗಳಿಂದ ಪ್ರಾರಂಭಿಸಿ (ಉದಾ: 0.5B-7B ಪ್ಯಾರಾಮೀಟರ್‌ಗಳು)
- ನಿಮ್ಮ ಗುರಿ ಮಾದರಿ ವಾಸ್ತುಶಿಲ್ಪವನ್ನು ಒಲಿವ್ ಬೆಂಬಲಿಸುತ್ತಿದೆಯೇ ಎಂದು ಖಚಿತಪಡಿಸಿಕೊಳ್ಳಿ

### 2. ಹಾರ್ಡ್‌ವೇರ್ ಪರಿಗಣನೆಗಳು
- ನಿಮ್ಮ ಆಪ್ಟಿಮೈಜೆಷನ್ ಗುರಿಯನ್ನು ನಿಯೋಜನೆ ಹಾರ್ಡ್‌ವೇರ್‌ಗೆ ಹೊಂದಿಸಿ
- CUDA-ಸಮ್ಮತ ಹಾರ್ಡ್‌ವೇರ್ ಇದ್ದರೆ GPU ಆಪ್ಟಿಮೈಜೆಷನ್ ಬಳಸಿ
- ವಿಂಡೋಸ್ ಯಂತ್ರಗಳಿಗೆ ಇಂಟಿಗ್ರೇಟೆಡ್ ಗ್ರಾಫಿಕ್ಸ್ ಇದ್ದರೆ DirectML ಪರಿಗಣಿಸಿ

### 3. ನಿಖರತೆ ಆಯ್ಕೆ
- **INT4**: ಗರಿಷ್ಠ ಸಂಕುಚಿತಗೊಳಿಸುವಿಕೆ, ಸ್ವಲ್ಪ ನಿಖರತೆ ನಷ್ಟ
- **INT8**: ಗಾತ್ರ ಮತ್ತು ನಿಖರತೆಯ ಉತ್ತಮ ಸಮತೋಲನ
- **FP16**: ಕನಿಷ್ಠ ನಿಖರತೆ ನಷ್ಟ, ಮಧ್ಯಮ ಗಾತ್ರ ಕಡಿತ

### 4. ಪರೀಕ್ಷೆ ಮತ್ತು ಮಾನ್ಯತೆ
- ನಿಮ್ಮ ನಿರ್ದಿಷ್ಟ ಬಳಕೆ ಪ್ರಕರಣಗಳೊಂದಿಗೆ ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿದ ಮಾದರಿಗಳನ್ನು ಯಾವಾಗಲೂ ಪರೀಕ್ಷಿಸಿ
- ಕಾರ್ಯಕ್ಷಮತೆ ಮೆಟ್ರಿಕ್‌ಗಳನ್ನು ಹೋಲಿಸಿ (ವಿಲಂಬ, ಥ್ರೂಪುಟ್, ನಿಖರತೆ)
- ಮೌಲ್ಯಮಾಪನಕ್ಕಾಗಿ ಪ್ರತಿನಿಧಿ ಇನ್‌ಪುಟ್ ಡೇಟಾವನ್ನು ಬಳಸಿ

### 5. ಪುನರಾವರ್ತಿತ ಆಪ್ಟಿಮೈಜೆಷನ್
- ತ್ವರಿತ ಫಲಿತಾಂಶಗಳಿಗಾಗಿ ಸ್ವಯಂಚಾಲಿತ ಆಪ್ಟಿಮೈಜೆಷನ್‌ನಿಂದ ಪ್ರಾರಂಭಿಸಿ
- ಸೂಕ್ಷ್ಮ ನಿಯಂತ್ರಣಕ್ಕಾಗಿ ಸಂರಚನಾ ಫೈಲ್‌ಗಳನ್ನು ಬಳಸಿ
- ವಿಭಿನ್ನ ಆಪ್ಟಿಮೈಜೆಷನ್ ಪಾಸುಗಳನ್ನು ಪ್ರಯೋಗಿಸಿ

## ಸಮಸ್ಯೆ ಪರಿಹಾರ

### ಸಾಮಾನ್ಯ ಸಮಸ್ಯೆಗಳು

#### 1. ಸ್ಥಾಪನೆ ಸಮಸ್ಯೆಗಳು
```bash
# ನೀವು ಅವಲಂಬನೆ ಸಂಘರ್ಷಗಳನ್ನು ಎದುರಿಸಿದರೆ:
pip install --upgrade pip
pip install olive-ai[auto-opt] --force-reinstall
```

#### 2. CUDA/GPU ಸಮಸ್ಯೆಗಳು
```bash
# CUDA ಸ್ಥಾಪನೆಯನ್ನು ಪರಿಶೀಲಿಸಿ:
nvidia-smi

# ಸರಿಯಾದ ONNX Runtime GPU ಪ್ಯಾಕೇಜ್ ಅನ್ನು ಸ್ಥಾಪಿಸಿ:
pip install onnxruntime-gpu
```

#### 3. ಮೆಮೊರಿ ಸಮಸ್ಯೆಗಳು
- ಆಪ್ಟಿಮೈಜೆಷನ್ ಸಮಯದಲ್ಲಿ ಸಣ್ಣ ಬ್ಯಾಚ್ ಗಾತ್ರಗಳನ್ನು ಬಳಸಿ
- ಮೊದಲು ಹೆಚ್ಚಿನ ನಿಖರತೆಯೊಂದಿಗೆ ಪ್ರಮಾಣೀಕರಣ ಪ್ರಯತ್ನಿಸಿ (int8 ಅನ್ನು int4 ಬದಲು)
- ಮಾದರಿ ಕ್ಯಾಶಿಂಗ್‌ಗೆ ಸಾಕಷ್ಟು ಡಿಸ್ಕ್ ಜಾಗವನ್ನು ಖಚಿತಪಡಿಸಿಕೊಳ್ಳಿ

#### 4. ಮಾದರಿ ಲೋಡಿಂಗ್ ದೋಷಗಳು
- ಮಾದರಿ ಮಾರ್ಗ ಮತ್ತು ಪ್ರವೇಶ ಅನುಮತಿಗಳನ್ನು ಪರಿಶೀಲಿಸಿ
- ಮಾದರಿಗೆ `trust_remote_code=True` ಅಗತ್ಯವಿದೆಯೇ ಎಂದು ಪರಿಶೀಲಿಸಿ
- ಎಲ್ಲಾ ಅಗತ್ಯ ಮಾದರಿ ಫೈಲ್‌ಗಳು ಡೌನ್‌ಲೋಡ್ ಆಗಿವೆ ಎಂದು ಖಚಿತಪಡಿಸಿಕೊಳ್ಳಿ

### ಸಹಾಯ ಪಡೆಯುವುದು

- **ದಾಖಲೆಗಳು**: [microsoft.github.io/Olive](https://microsoft.github.io/Olive/)
- **GitHub ಸಮಸ್ಯೆಗಳು**: [github.com/microsoft/Olive/issues](https://github.com/microsoft/Olive/issues)
- **ಉದಾಹರಣೆಗಳು**: [microsoft.github.io/Olive/examples.html](https://microsoft.github.io/Olive/examples.html)

## ಒಲಿವ್ ರೆಸಿಪೀಸ್ ರೆಪೊಸಿಟರಿ

### ಒಲಿವ್ ರೆಸಿಪೀಸ್ ಗೆ ಪರಿಚಯ

[microsoft/olive-recipes](https://github.com/microsoft/olive-recipes) ರೆಪೊಸಿಟರಿ ಮುಖ್ಯ ಒಲಿವ್ ಟೂಲ್‌ಕಿಟ್‌ಗೆ ಪೂರಕವಾಗಿದ್ದು, ಜನಪ್ರಿಯ AI ಮಾದರಿಗಳಿಗಾಗಿ ಸಿದ್ಧ-ಬಳಕೆ ಆಪ್ಟಿಮೈಜೆಷನ್ ರೆಸಿಪೀಸ್‌ಗಳ ಸಮಗ್ರ ಸಂಗ್ರಹವನ್ನು ಒದಗಿಸುತ್ತದೆ. ಈ ರೆಪೊಸಿಟರಿ ಸಾರ್ವಜನಿಕವಾಗಿ ಲಭ್ಯವಿರುವ ಮಾದರಿಗಳನ್ನು ಆಪ್ಟಿಮೈಸ್ ಮಾಡುವುದಕ್ಕೆ ಮತ್ತು ಸ್ವಂತ ಮಾದರಿಗಳಿಗಾಗಿ ಆಪ್ಟಿಮೈಜೆಷನ್ ಕಾರ್ಯಪ್ರವಾಹಗಳನ್ನು ರಚಿಸುವುದಕ್ಕೆ ಪ್ರಾಯೋಗಿಕ ಉಲ್ಲೇಖವಾಗಿ ಕಾರ್ಯನಿರ್ವಹಿಸುತ್ತದೆ.

### ಪ್ರಮುಖ ವೈಶಿಷ್ಟ್ಯಗಳು

- **100+ ಪೂರ್ವ-ನಿರ್ಮಿತ ರೆಸಿಪೀಸ್**: ಜನಪ್ರಿಯ ಮಾದರಿಗಳಿಗಾಗಿ ಸಿದ್ಧ-ಬಳಕೆ ಆಪ್ಟಿಮೈಜೆಷನ್ ಸಂರಚನೆಗಳು
- **ಬಹು-ವಾಸ್ತುಶಿಲ್ಪ ಬೆಂಬಲ**: ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್ ಮಾದರಿಗಳು, ದೃಶ್ಯ ಮಾದರಿಗಳು ಮತ್ತು ಬಹುಮಾದರಿ ವಾಸ್ತುಶಿಲ್ಪಗಳನ್ನು ಒಳಗೊಂಡಿದೆ
- **ಹಾರ್ಡ್‌ವೇರ್-ನಿರ್ದಿಷ್ಟ ಆಪ್ಟಿಮೈಜೆಷನ್‌ಗಳು**: CPU, GPU ಮತ್ತು ವಿಶೇಷ ಅಕ್ಸಿಲೆರೇಟರ್‌ಗಳಿಗೆ ಹೊಂದಿಕೆಯಾಗುವ ರೆಸಿಪೀಸ್‌ಗಳು
- **ಜನಪ್ರಿಯ ಮಾದರಿ ಕುಟುಂಬಗಳು**: Phi, Llama, Qwen, Gemma, Mistral ಮತ್ತು ಇನ್ನಷ್ಟು

### ಬೆಂಬಲಿತ ಮಾದರಿ ಕುಟುಂಬಗಳು

ರೆಪೊಸಿಟರಿ ಈ ಕೆಳಗಿನ ಆಪ್ಟಿಮೈಜೆಷನ್ ರೆಸಿಪೀಸ್‌ಗಳನ್ನು ಒಳಗೊಂಡಿದೆ:

#### ಭಾಷಾ ಮಾದರಿಗಳು
- **Microsoft Phi**: Phi-3-mini, Phi-3.5-mini, Phi-4-mini, Phi-4-reasoning
- **Meta Llama**: Llama-2-7b, Llama-3.1-8B, Llama-3.2-1B/3B
- **Alibaba Qwen**: Qwen1.5-7B, Qwen2-7B, Qwen2.5 ಸರಣಿ (0.5B ರಿಂದ 14B)
- **Google Gemma**: ವಿವಿಧ Gemma ಮಾದರಿ ಸಂರಚನೆಗಳು
- **Mistral AI**: Mistral-7B ಸರಣಿ
- **DeepSeek**: R1-Distill ಸರಣಿ ಮಾದರಿಗಳು

#### ದೃಶ್ಯ ಮತ್ತು ಬಹುಮಾದರಿ ಮಾದರಿಗಳು
- **Stable Diffusion**: v1.4, XL-base-1.0
- **CLIP ಮಾದರಿಗಳು**: ವಿವಿಧ CLIP-ViT ಸಂರಚನೆಗಳು
- **ResNet**: ResNet-50 ಆಪ್ಟಿಮೈಜೆಷನ್‌ಗಳು
- **Vision Transformers**: ViT-base-patch16-224

#### ವಿಶೇಷ ಮಾದರಿಗಳು
- **Whisper**: OpenAI Whisper-large-v3
- **BERT**: ಮೂಲ ಮತ್ತು ಬಹುಭಾಷಾ ರೂಪಾಂತರಗಳು
- **Sentence Transformers**: all-MiniLM-L6-v2

### ಒಲಿವ್ ರೆಸಿಪೀಸ್ ಬಳಕೆ

#### ವಿಧಾನ 1: ನಿರ್ದಿಷ್ಟ ರೆಸಿಪಿ ಕ್ಲೋನ್ ಮಾಡುವುದು

```bash
# ರೆಸಿಪಿಗಳು ರೆಪೊಸಿಟರಿಯನ್ನು ಕ್ಲೋನ್ ಮಾಡಿ
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes

# ನಿರ್ದಿಷ್ಟ ಮಾದರಿ ರೆಸಿಪಿಗೆ ನಾವಿಗೇಟ್ ಮಾಡಿ
cd microsoft-Phi-4-mini-instruct

# ಆಪ್ಟಿಮೈಜೆಶನ್ ಅನ್ನು ಚಾಲನೆ ಮಾಡಿ
olive run --config olive_config.json
```

#### ವಿಧಾನ 2: ರೆಸಿಪಿಯನ್ನು ಟೆಂಪ್ಲೇಟ್ ಆಗಿ ಬಳಸುವುದು

```bash
# ನಿಮ್ಮ ಮಾದರಿಗಾಗಿ ರೆಸಿಪಿ ಸಂರಚನೆಯನ್ನು ನಕಲಿಸಿ
cp olive-recipes/microsoft-Phi-3-mini-4k-instruct/olive_config.json ./my_config.json

# ನಿಮ್ಮ ಅಗತ್ಯಗಳಿಗೆ ಸಂರಚನೆಯನ್ನು ಬದಲಾಯಿಸಿ
# ಮಾದರಿ ಮಾರ್ಗಗಳು, ಆಪ್ಟಿಮೈಜೆಷನ್ ಪರಿಮಾಣಗಳು ಇತ್ಯಾದಿಗಳನ್ನು ನವೀಕರಿಸಿ

# ನಿಮ್ಮ ಕಸ್ಟಮ್ ಸಂರಚನೆಯೊಂದಿಗೆ ಚಾಲನೆ ಮಾಡಿ
olive run --config my_config.json
```

### ರೆಸಿಪಿ ರಚನೆ

ಪ್ರತಿ ರೆಸಿಪಿ ಡೈರೆಕ್ಟರಿ ಸಾಮಾನ್ಯವಾಗಿ ಒಳಗೊಂಡಿರುತ್ತದೆ:

```
model-name/
├── olive_config.json       # Main optimization configuration
├── requirements.txt        # Python dependencies
├── README.md              # Model-specific instructions
├── user_script.py         # Custom preprocessing/evaluation scripts
└── sample_data/           # Sample input data for testing
```

### ಉದಾಹರಣೆ: Phi-4-mini ರೆಸಿಪಿ ಬಳಕೆ

Phi-4-mini ರೆಸಿಪಿಯನ್ನು ಉದಾಹರಣೆಯಾಗಿ ಬಳಸಿ:

```bash
# ರೆಪೊಸಿಟರಿಯನ್ನು ಕ್ಲೋನ್ ಮಾಡಿ
git clone https://github.com/microsoft/olive-recipes.git
cd olive-recipes/microsoft-Phi-4-mini-instruct

# ಅವಲಂಬನೆಗಳನ್ನು ಸ್ಥಾಪಿಸಿ
pip install -r requirements.txt

# ಆಪ್ಟಿಮೈಜೆಶನ್ ಅನ್ನು ಚಾಲನೆ ಮಾಡಿ
olive run --config olive_config.json
```

ಸಂರಚನಾ ಫೈಲ್ ಸಾಮಾನ್ಯವಾಗಿ ಒಳಗೊಂಡಿರುತ್ತದೆ:

```json
{
  "input_model": {
    "type": "PyTorchModel",
    "config": {
      "hf_config": {
        "model_name": "microsoft/Phi-4-mini-instruct",
        "task": "text-generation",
        "trust_remote_code": true
      }
    }
  },
  "systems": {
    "local_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "cpu",
            "execution_providers": ["CPUExecutionProvider"]
          }
        ]
      }
    }
  },
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int4"
      }
    }
  }
}
```

### ರೆಸಿಪಿಗಳನ್ನು ಕಸ್ಟಮೈಸ್ ಮಾಡುವುದು

#### ಗುರಿ ಹಾರ್ಡ್‌ವೇರ್ ಬದಲಾವಣೆ

`systems` ವಿಭಾಗವನ್ನು ನವೀಕರಿಸಿ:

```json
{
  "systems": {
    "gpu_system": {
      "type": "LocalSystem",
      "config": {
        "accelerators": [
          {
            "device": "gpu",
            "execution_providers": ["CUDAExecutionProvider"]
          }
        ]
      }
    }
  }
}
```

#### ಆಪ್ಟಿಮೈಜೆಷನ್ ಪರಿಮಾಣಗಳನ್ನು ಹೊಂದಿಸುವುದು

ವಿಭಿನ್ನ ಆಪ್ಟಿಮೈಜೆಷನ್ ಮಟ್ಟಗಳಿಗೆ `passes` ವಿಭಾಗವನ್ನು ಬದಲಾಯಿಸಿ:

```json
{
  "passes": {
    "convert": {
      "type": "ModelBuilder",
      "config": {
        "precision": "int8",           // Change from int4 to int8
        "use_ort_genai": true,
        "use_dynamo_exporter": true
      }
    },
    "optimize": {
      "type": "OrtTransformersOptimization",
      "config": {
        "optimization_level": "all"
      }
    }
  }
}
```

### ನಿಮ್ಮದೇ ರೆಸಿಪಿ ರಚಿಸುವುದು

1. **ಸಮಾನ ಮಾದರಿಯಿಂದ ಪ್ರಾರಂಭಿಸಿ**: ಸಮಾನ ವಾಸ್ತುಶಿಲ್ಪದ ಮಾದರಿಗಾಗಿ ರೆಸಿಪಿ ಹುಡುಕಿ
2. **ಮಾದರಿ ಸಂರಚನೆಯನ್ನು ನವೀಕರಿಸಿ**: ಸಂರಚನೆಯಲ್ಲಿ ಮಾದರಿ ಹೆಸರು/ಮಾರ್ಗ ಬದಲಾಯಿಸಿ
3. **ಪರಿಮಾಣಗಳನ್ನು ಹೊಂದಿಸಿ**: ಅಗತ್ಯವಿರುವಂತೆ ಆಪ್ಟಿಮೈಜೆಷನ್ ಪರಿಮಾಣಗಳನ್ನು ಬದಲಾಯಿಸಿ
4. **ಪರೀಕ್ಷಿಸಿ ಮತ್ತು ಮಾನ್ಯತೆ ಮಾಡಿ**: ಆಪ್ಟಿಮೈಜೆಷನ್ ನಡೆಸಿ ಫಲಿತಾಂಶಗಳನ್ನು ಪರಿಶೀಲಿಸಿ
5. **ಹಿಂತಿರುಗಿ ಕೊಡುಗೆ ನೀಡಿ**: ನಿಮ್ಮ ರೆಸಿಪಿಯನ್ನು ರೆಪೊಸಿಟರಿಗೆ ಕೊಡುಗೆ ನೀಡಲು ಪರಿಗಣಿಸಿ

### ರೆಸಿಪಿ ಬಳಕೆಯ ಲಾಭಗಳು

#### 1. **ಪರೀಕ್ಷಿತ ಸಂರಚನೆಗಳು**
- ನಿರ್ದಿಷ್ಟ ಮಾದರಿಗಳಿಗಾಗಿ ಪರೀಕ್ಷಿಸಲಾದ ಆಪ್ಟಿಮೈಜೆಷನ್ ಸೆಟ್ಟಿಂಗ್‌ಗಳು
- ಅತ್ಯುತ್ತಮ ಪರಿಮಾಣಗಳನ್ನು ಹುಡುಕುವ ಪ್ರಯೋಗ-ದೋಷ ತಪ್ಪಿಸುತ್ತದೆ

#### 2. **ಹಾರ್ಡ್‌ವೇರ್-ನಿರ್ದಿಷ್ಟ ಟ್ಯೂನಿಂಗ್**
- ವಿಭಿನ್ನ ಕಾರ್ಯಾಚರಣೆ ಒದಗಿಸುವವರಿಗಾಗಿ ಪೂರ್ವ-ಆಪ್ಟಿಮೈಸ್ ಮಾಡಲಾಗಿದೆ
- CPU, GPU ಮತ್ತು NPU ಗುರಿಗಳಿಗಾಗಿ ಸಿದ್ಧ-ಬಳಕೆ ಸಂರಚನೆಗಳು

#### 3. **ವಿಸ್ತೃತ ವ್ಯಾಪ್ತಿ**
- ಅತ್ಯಂತ ಜನಪ್ರಿಯ ಓಪನ್-ಸೋರ್ಸ್ ಮಾದರಿಗಳನ್ನು ಬೆಂಬಲಿಸುತ್ತದೆ
- ಹೊಸ ಮಾದರಿ ಬಿಡುಗಡೆಗಳೊಂದಿಗೆ ನಿಯಮಿತ ನವೀಕರಣಗಳು

#### 4. **ಸಮುದಾಯ ಕೊಡುಗೆಗಳು**
- AI ಸಮುದಾಯದೊಂದಿಗೆ ಸಹಯೋಗಿ ಅಭಿವೃದ್ಧಿ
- ಹಂಚಿಕೊಂಡ ಜ್ಞಾನ ಮತ್ತು ಉತ್ತಮ ಅಭ್ಯಾಸಗಳು

### ಒಲಿವ್ ರೆಸಿಪೀಸ್‌ಗೆ ಕೊಡುಗೆ ನೀಡುವುದು

ನೀವು ರೆಪೊಸಿಟರಿಯಲ್ಲಿ ಒಳಗೊಂಡಿಲ್ಲದ ಮಾದರಿಯನ್ನು ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿದ್ದರೆ:

1. **ರೆಪೊಸಿಟರಿಯನ್ನು ಫೋರ್ಕ್ ಮಾಡಿ**: olive-recipes ನ ನಿಮ್ಮದೇ ಫೋರ್ಕ್ ರಚಿಸಿ
2. **ರೆಸಿಪಿ ಡೈರೆಕ್ಟರಿ ರಚಿಸಿ**: ನಿಮ್ಮ ಮಾದರಿಗಾಗಿ ಹೊಸ ಡೈರೆಕ್ಟರಿ ಸೇರಿಸಿ
3. **ಸಂರಚನೆಯನ್ನು ಸೇರಿಸಿ**: olive_config.json ಮತ್ತು ಬೆಂಬಲಿಸುವ ಫೈಲ್‌ಗಳನ್ನು ಸೇರಿಸಿ
4. **ಬಳಕೆ ದಾಖಲಿಸಿ**: ಸ್ಪಷ್ಟ README ಮತ್ತು ಸೂಚನೆಗಳನ್ನು ಒದಗಿಸಿ
5. **ಪುಲ್ ರಿಕ್ವೆಸ್ಟ್ ಸಲ್ಲಿಸಿ**: ಸಮುದಾಯಕ್ಕೆ ಹಿಂತಿರುಗಿ ಕೊಡುಗೆ ನೀಡಿ

### ಕಾರ್ಯಕ್ಷಮತೆ ಬೆಂಚ್‌ಮಾರ್ಕ್‌ಗಳು

ಬಹುತೆಕ ರೆಸಿಪೀಸ್ ಕಾರ್ಯಕ್ಷಮತೆ ಬೆಂಚ್‌ಮಾರ್ಕ್‌ಗಳನ್ನು ಒಳಗೊಂಡಿವೆ:
- **ವಿಲಂಬ ಸುಧಾರಣೆಗಳು**: ಸಾಮಾನ್ಯವಾಗಿ 2-6x ವೇಗದ ಸುಧಾರಣೆ
- **ಮೆಮೊರಿ ಕಡಿತ**: ಪ್ರಮಾಣೀಕರಣದೊಂದಿಗೆ 50-75% ಮೆಮೊರಿ ಬಳಕೆ ಕಡಿತ
- **ನಿಖರತೆ ಉಳಿವು**: 95-99% ನಿಖರತೆ ಕಾಪಾಡಿಕೊಳ್ಳುವುದು

### AI ಟೂಲ್‌ಕಿಟ್ ಜೊತೆಗೆ ಏಕೀಕರಣ

ರೆಸಿಪೀಸ್ ಸುಗಮವಾಗಿ ಕಾರ್ಯನಿರ್ವಹಿಸುತ್ತವೆ:
- **VS ಕೋಡ್ AI ಟೂಲ್‌ಕಿಟ್**: ಮಾದರಿ ಆಪ್ಟಿಮೈಜೆಷನ್‌ಗೆ ನೇರ ಏಕೀಕರಣ
- **ಅಜೂರ್ ಮೆಷಿನ್ ಲರ್ನಿಂಗ್**: ಕ್ಲೌಡ್ ಆಧಾರಿತ ಆಪ್ಟಿಮೈಜೆಷನ್ ಕಾರ್ಯಪ್ರವಾಹಗಳು
- **ONNX ರನ್‌ಟೈಮ್**: ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿದ ಇನ್ಫರೆನ್ಸ್ ನಿಯೋಜನೆ

## ಹೆಚ್ಚುವರಿ ಸಂಪನ್ಮೂಲಗಳು

### ಅಧಿಕೃತ ಲಿಂಕ್‌ಗಳು
- **GitHub ರೆಪೊಸಿಟರಿ**: [github.com/microsoft/Olive](https://github.com/microsoft/Olive)
- **ಒಲಿವ್ ರೆಸಿಪೀಸ್ ರೆಪೊಸಿಟರಿ**: [github.com/microsoft/olive-recipes](https://github.com/microsoft/olive-recipes)
- **ONNX ರನ್‌ಟೈಮ್ ಡಾಕ್ಯುಮೆಂಟೇಶನ್**: [onnxruntime.ai/docs/performance/olive.html](https://onnxruntime.ai/docs/performance/olive.html)
- **Hugging Face ಉದಾಹರಣೆ**: [huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU](https://huggingface.co/lokinfey/Qwen3-8B-ONNX-INT4-CPU)

### ಸಮುದಾಯ ಉದಾಹರಣೆಗಳು
- **ಜುಪಿಟರ್ ನೋಟ್ಬುಕ್‌ಗಳು**: ಒಲಿವ್ GitHub ರೆಪೊಸಿಟರಿಯಲ್ಲಿ ಲಭ್ಯವಿದೆ — https://github.com/microsoft/Olive/tree/main/examples
- **VS ಕೋಡ್ ವಿಸ್ತರಣೆ**: VS ಕೋಡ್‌ಗೆ AI ಟೂಲ್‌ಕಿಟ್ ಅವಲೋಕನ — https://learn.microsoft.com/azure/ai-toolkit/overview
- **ಬ್ಲಾಗ್ ಪೋಸ್ಟ್‌ಗಳು**: ಮೈಕ್ರೋಸಾಫ್ಟ್ ಓಪನ್ ಸೋರ್ಸ್ ಬ್ಲಾಗ್ — https://opensource.microsoft.com/blog/

### ಸಂಬಂಧಿತ ಸಾಧನಗಳು
- **ONNX ರನ್‌ಟೈಮ್**: ಉನ್ನತ ಕಾರ್ಯಕ್ಷಮತೆ ಇನ್ಫರೆನ್ಸ್ ಎಂಜಿನ್ — https://onnxruntime.ai/
- **Hugging Face Transformers**: ಅನೇಕ ಹೊಂದಾಣಿಕೆಯ ಮಾದರಿಗಳ ಮೂಲ — https://huggingface.co/docs/transformers/index
- **ಅಜೂರ್ ಮೆಷಿನ್ ಲರ್ನಿಂಗ್**: ಕ್ಲೌಡ್ ಆಧಾರಿತ ಆಪ್ಟಿಮೈಜೆಷನ್ ಕಾರ್ಯಪ್ರವಾಹಗಳು — https://learn.microsoft.com/azure/machine-learning/


## ➡️ ಮುಂದೇನು

- [04: OpenVINO ಟೂಲ್‌ಕಿಟ್ ಆಪ್ಟಿಮೈಜೆಷನ್ ಸೂಟ್](./04.openvino.md)

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ಅಸ್ವೀಕರಣ**:  
ಈ ದಸ್ತಾವೇಜು AI ಅನುವಾದ ಸೇವೆ [Co-op Translator](https://github.com/Azure/co-op-translator) ಬಳಸಿ ಅನುವಾದಿಸಲಾಗಿದೆ. ನಾವು ನಿಖರತೆಯಿಗಾಗಿ ಪ್ರಯತ್ನಿಸುತ್ತಿದ್ದರೂ, ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ದೋಷಗಳು ಅಥವಾ ಅಸತ್ಯತೆಗಳು ಇರಬಹುದು ಎಂದು ದಯವಿಟ್ಟು ಗಮನಿಸಿ. ಮೂಲ ಭಾಷೆಯಲ್ಲಿರುವ ಮೂಲ ದಸ್ತಾವೇಜನ್ನು ಅಧಿಕೃತ ಮೂಲವೆಂದು ಪರಿಗಣಿಸಬೇಕು. ಮಹತ್ವದ ಮಾಹಿತಿಗಾಗಿ, ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗುತ್ತದೆ. ಈ ಅನುವಾದ ಬಳಕೆಯಿಂದ ಉಂಟಾಗುವ ಯಾವುದೇ ತಪ್ಪು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವಿಕೆ ಅಥವಾ ತಪ್ಪು ವಿವರಣೆಗಳಿಗೆ ನಾವು ಹೊಣೆಗಾರರಾಗುವುದಿಲ್ಲ.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->