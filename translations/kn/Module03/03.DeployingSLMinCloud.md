<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "5b46b107cab1e90ad43db8e792794fe8",
  "translation_date": "2025-12-15T22:44:45+00:00",
  "source_file": "Module03/03.DeployingSLMinCloud.md",
  "language_code": "kn"
}
-->
# ಕಂಟೈನರೈಜ್ಡ್ ಕ್ಲೌಡ್ ಡಿಪ್ಲಾಯ್‌ಮೆಂಟ್ - ಉತ್ಪಾದನಾ-ಮಟ್ಟದ ಪರಿಹಾರಗಳು

ಈ ಸಮಗ್ರ ಟ್ಯುಟೋರಿಯಲ್ ಮೈಕ್ರೋಸಾಫ್ಟ್‌ನ Phi-4-mini-instruct ಮಾದರಿಯನ್ನು ಕಂಟೈನರೈಜ್ಡ್ ಪರಿಸರಗಳಲ್ಲಿ ನಿಯೋಜಿಸುವ ಮೂರು ಪ್ರಮುಖ ವಿಧಾನಗಳನ್ನು ಒಳಗೊಂಡಿದೆ: vLLM, Ollama, ಮತ್ತು SLM ಎಂಜಿನ್ ಜೊತೆಗೆ ONNX ರನ್‌ಟೈಮ್. ಈ 3.8B ಪ್ಯಾರಾಮೀಟರ್ ಮಾದರಿ ತರ್ಕಾತ್ಮಕ ಕಾರ್ಯಗಳಿಗೆ ಅತ್ಯುತ್ತಮ ಆಯ್ಕೆಯನ್ನು ಪ್ರತಿನಿಧಿಸುತ್ತದೆ ಮತ್ತು ಎಡ್ಜ್ ನಿಯೋಜನೆಗಾಗಿ ಕಾರ್ಯಕ್ಷಮತೆಯನ್ನು ಕಾಪಾಡುತ್ತದೆ.

## ವಿಷಯಗಳ ಪಟ್ಟಿಕೆ

1. [Phi-4-mini ಕಂಟೈನರ್ ನಿಯೋಜನೆಗೆ ಪರಿಚಯ](../../../Module03)
2. [ಕಲಿಕೆಯ ಉದ್ದೇಶಗಳು](../../../Module03)
3. [Phi-4-mini ವರ್ಗೀಕರಣವನ್ನು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವುದು](../../../Module03)
4. [vLLM ಕಂಟೈನರ್ ನಿಯೋಜನೆ](../../../Module03)
5. [Ollama ಕಂಟೈನರ್ ನಿಯೋಜನೆ](../../../Module03)
6. [SLM ಎಂಜಿನ್ ಜೊತೆಗೆ ONNX ರನ್‌ಟೈಮ್](../../../Module03)
7. [ಹೋಲಿಕೆ ಚಟುವಟಿಕೆ](../../../Module03)
8. [ಉತ್ತಮ ಅಭ್ಯಾಸಗಳು](../../../Module03)

## Phi-4-mini ಕಂಟೈನರ್ ನಿಯೋಜನೆಗೆ ಪರಿಚಯ

ಸಣ್ಣ ಭಾಷಾ ಮಾದರಿಗಳು (SLMs) ಎಡ್ಜ್‌ಎಐನಲ್ಲಿ ಪ್ರಮುಖ ಪ್ರಗತಿಯನ್ನು ಪ್ರತಿನಿಧಿಸುತ್ತವೆ, ಸಂಪನ್ಮೂಲ-ಮಿತಿಯ ಸಾಧನಗಳಲ್ಲಿ ಸುಧಾರಿತ ನೈಸರ್ಗಿಕ ಭಾಷಾ ಪ್ರಕ್ರಿಯೆ ಸಾಮರ್ಥ್ಯಗಳನ್ನು ಸಕ್ರಿಯಗೊಳಿಸುತ್ತವೆ. ಈ ಟ್ಯುಟೋರಿಯಲ್ ಮೈಕ್ರೋಸಾಫ್ಟ್‌ನ Phi-4-mini-instruct ಕಂಟೈನರೈಜ್ಡ್ ನಿಯೋಜನೆ ತಂತ್ರಗಳನ್ನು ಗಮನಿಸುತ್ತದೆ, ಇದು ಸಾಮರ್ಥ್ಯ ಮತ್ತು ಕಾರ್ಯಕ್ಷಮತೆಯನ್ನು ಸಮತೋಲನಗೊಳಿಸುವ ಅತ್ಯಾಧುನಿಕ ತರ್ಕಾತ್ಮಕ ಮಾದರಿ.

### ವೈಶಿಷ್ಟ್ಯಗೊಳಿಸಿದ ಮಾದರಿ: Phi-4-mini-instruct

**Phi-4-mini-instruct (3.8B ಪ್ಯಾರಾಮೀಟರ್‌ಗಳು)**: ಮೈಕ್ರೋಸಾಫ್ಟ್‌ನ ಇತ್ತೀಚಿನ ಲೈಟ್‌ವೈಟ್ ಸೂಚನೆ-ಟ್ಯೂನಿಂಗ್ ಮಾಡಲಾದ ಮಾದರಿ, ಮೆಮೊರಿ/ಕಂಪ್ಯೂಟ್-ಮಿತಿಯ ಪರಿಸರಗಳಿಗೆ ವಿನ್ಯಾಸಗೊಳಿಸಲಾಗಿದೆ ಮತ್ತು ವಿಶೇಷ ಸಾಮರ್ಥ್ಯಗಳನ್ನು ಹೊಂದಿದೆ:
- **ಗಣಿತ ತರ್ಕ ಮತ್ತು ಸಂಕೀರ್ಣ ಲೆಕ್ಕಾಚಾರಗಳು**
- **ಕೋಡ್ ರಚನೆ, ಡಿಬಗ್ ಮತ್ತು ವಿಶ್ಲೇಷಣೆ**
- **ತರ್ಕಾತ್ಮಕ ಸಮಸ್ಯೆ ಪರಿಹಾರ ಮತ್ತು ಹಂತ ಹಂತದ ತರ್ಕ**
- **ವಿವರವಾದ ವಿವರಣೆಗಳನ್ನು ಅಗತ್ಯವಿರುವ ಶೈಕ್ಷಣಿಕ ಅನ್ವಯಿಕೆಗಳು**
- **ಫಂಕ್ಷನ್ ಕರೆ ಮತ್ತು ಉಪಕರಣ ಸಂಯೋಜನೆ**

"ಸಣ್ಣ SLMಗಳು" ವರ್ಗದ ಭಾಗ (1.5B - 13.9B ಪ್ಯಾರಾಮೀಟರ್‌ಗಳು), Phi-4-mini ತರ್ಕ ಸಾಮರ್ಥ್ಯ ಮತ್ತು ಸಂಪನ್ಮೂಲ ಕಾರ್ಯಕ್ಷಮತೆಯ ನಡುವೆ ಉತ್ತಮ ಸಮತೋಲನವನ್ನು ಸಾಧಿಸುತ್ತದೆ.

### ಕಂಟೈನರೈಜ್ಡ್ Phi-4-mini ನಿಯೋಜನೆಯ ಲಾಭಗಳು

- **ಚಾಲನಾ ಕಾರ್ಯಕ್ಷಮತೆ**: ಕಡಿಮೆ ಗಣನೆ ಅಗತ್ಯಗಳೊಂದಿಗೆ ತ್ವರಿತ ತರ್ಕಾತ್ಮಕ ನಿರ್ಣಯ
- **ನಿಯೋಜನೆ ಲವಚಿಕತೆ**: ಸ್ಥಳೀಯ ಪ್ರಕ್ರಿಯೆಯಿಂದ ಸುಧಾರಿತ ಗೌಪ್ಯತೆ ಹೊಂದಿರುವ ಸಾಧನ ಆಧಾರಿತ AI ಸಾಮರ್ಥ್ಯಗಳು
- **ಖರ್ಚು ಪರಿಣಾಮಕಾರಿತ್ವ**: ದೊಡ್ಡ ಮಾದರಿಗಳಿಗಿಂತ ಕಡಿಮೆ ಕಾರ್ಯಾಚರಣೆ ವೆಚ್ಚಗಳು ಮತ್ತು ಗುಣಮಟ್ಟವನ್ನು ಕಾಪಾಡುವುದು
- **ವಿಭಜನೆ**: ಮಾದರಿ ಉದಾಹರಣೆಗಳ ನಡುವೆ ಸ್ವಚ್ಛ ವಿಭಜನೆ ಮತ್ತು ಸುರಕ್ಷಿತ ಕಾರ್ಯಾಚರಣೆ ಪರಿಸರಗಳು
- **ವಿಸ್ತರಣೆ ಸಾಮರ್ಥ್ಯ**: ಹೆಚ್ಚಿದ ತರ್ಕ throughput ಗಾಗಿ ಸುಲಭ ಹೋರಿಜಾಂಟಲ್ ವಿಸ್ತರಣೆ

## ಕಲಿಕೆಯ ಉದ್ದೇಶಗಳು

ಈ ಟ್ಯುಟೋರಿಯಲ್ ಮುಗಿದ ನಂತರ, ನೀವು ಸಾಧ್ಯವಾಗುತ್ತದೆ:

- ವಿವಿಧ ಕಂಟೈನರೈಜ್ಡ್ ಪರಿಸರಗಳಲ್ಲಿ Phi-4-mini-instruct ನಿಯೋಜಿಸಿ ಮತ್ತು ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿ
- ವಿಭಿನ್ನ ನಿಯೋಜನೆ ಸಂದರ್ಭಗಳಿಗೆ ಉನ್ನತ ಮಟ್ಟದ ಕ್ವಾಂಟೈಜೆಷನ್ ಮತ್ತು ಸಂಕುಚಿತ ತಂತ್ರಗಳನ್ನು ಜಾರಿಗೆ ತರುವುದು
- ತರ್ಕ ಕಾರ್ಯಭಾರಗಳಿಗೆ ಉತ್ಪಾದನಾ-ಸಿದ್ಧ ಕಂಟೈನರ್ ಸಂಚಲನವನ್ನು ಸಂರಚಿಸುವುದು
- ನಿರ್ದಿಷ್ಟ ಬಳಕೆ ಪ್ರಕರಣ ಅವಶ್ಯಕತೆಗಳ ಆಧಾರದ ಮೇಲೆ ಸೂಕ್ತ ನಿಯೋಜನೆ ಚಟುವಟಿಕೆಗಳನ್ನು ಮೌಲ್ಯಮಾಪನ ಮತ್ತು ಆಯ್ಕೆ ಮಾಡುವುದು
- ಕಂಟೈನರೈಜ್ಡ್ SLM ನಿಯೋಜನೆಗಳಿಗೆ ಭದ್ರತೆ, ಮೇಲ್ವಿಚಾರಣೆ ಮತ್ತು ವಿಸ್ತರಣೆ ಉತ್ತಮ ಅಭ್ಯಾಸಗಳನ್ನು ಅನ್ವಯಿಸುವುದು

## Phi-4-mini ವರ್ಗೀಕರಣವನ್ನು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವುದು

### ಮಾದರಿ ವಿಶೇಷಣಗಳು

**ತಾಂತ್ರಿಕ ವಿವರಗಳು:**
- **ಪ್ಯಾರಾಮೀಟರ್‌ಗಳು**: 3.8 ಬಿಲಿಯನ್ (ಸಣ್ಣ SLM ವರ್ಗ)
- **ವಾಸ್ತುಶಿಲ್ಪ**: ಗುಂಪು-ಪ್ರಶ್ನೆ ಗಮನದೊಂದಿಗೆ ಡೆನ್ಸ್ ಡಿಕೋಡರ್-ಮಾತ್ರ ಟ್ರಾನ್ಸ್‌ಫಾರ್ಮರ್
- **ಸಂದರ್ಭ ಉದ್ದ**: 128K ಟೋಕನ್ಸ್ (ಉತ್ತಮ ಕಾರ್ಯಕ್ಷಮತಿಗೆ 32K ಶಿಫಾರಸು)
- **ಶಬ್ದಕೋಶ**: ಬಹುಭಾಷಾ ಬೆಂಬಲದ 200K ಟೋಕನ್ಸ್
- **ತರಬೇತಿ ಡೇಟಾ**: 5T ಟೋಕನ್ಸ್ ಉನ್ನತ-ಗುಣಮಟ್ಟದ ತರ್ಕ-ಸಂಯುಕ್ತ ವಿಷಯ

### ಸಂಪನ್ಮೂಲ ಅಗತ್ಯಗಳು

| ನಿಯೋಜನೆ ಪ್ರಕಾರ | ಕನಿಷ್ಠ RAM | ಶಿಫಾರಸು ಮಾಡಿದ RAM | VRAM (GPU) | ಸಂಗ್ರಹಣೆ | ಸಾಮಾನ್ಯ ಬಳಕೆ ಪ್ರಕರಣಗಳು |
|-----------------|-------------|---------------------|------------|----------|--------------------------|
| **ವಿಕಸನ** | 6GB | 8GB | - | 8GB | ಸ್ಥಳೀಯ ಪರೀಕ್ಷೆ, ಪ್ರೋಟೋಟೈಪಿಂಗ್ |
| **ಉತ್ಪಾದನಾ CPU** | 8GB | 12GB | - | 10GB | ಎಡ್ಜ್ ಸರ್ವರ್‌ಗಳು, ವೆಚ್ಚ-ಆಪ್ಟಿಮೈಸ್ ನಿಯೋಜನೆ |
| **ಉತ್ಪಾದನಾ GPU** | 6GB | 8GB | 4-6GB | 8GB | ಹೆಚ್ಚಿನ throughput ತರ್ಕ ಸೇವೆಗಳು |
| **ಎಡ್ಜ್ ಆಪ್ಟಿಮೈಸ್** | 4GB | 6GB | - | 6GB | ಕ್ವಾಂಟೈಜ್ಡ್ ನಿಯೋಜನೆ, IoT ಗೇಟ್ವೇಗಳು |

### Phi-4-mini ಸಾಮರ್ಥ್ಯಗಳು

- **ಗಣಿತ ಶ್ರೇಷ್ಠತೆ**: ಉನ್ನತ ಮಟ್ಟದ ಅಂಕಗಣಿತ, ಬಾಹ್ಯರೇಖಾ ಮತ್ತು ಕ್ಯಾಲ್ಕ್ಯುಲಸ್ ಸಮಸ್ಯೆ ಪರಿಹಾರ
- **ಕೋಡ್ ಬುದ್ಧಿವಂತಿಕೆ**: ಪೈಥಾನ್, ಜಾವಾಸ್ಕ್ರಿಪ್ಟ್ ಮತ್ತು ಬಹುಭಾಷಾ ಕೋಡ್ ರಚನೆ ಮತ್ತು ಡಿಬಗ್
- **ತರ್ಕಾತ್ಮಕ ನಿರ್ಣಯ**: ಹಂತ ಹಂತದ ಸಮಸ್ಯೆ ವಿಭಜನೆ ಮತ್ತು ಪರಿಹಾರ ನಿರ್ಮಾಣ
- **ಶೈಕ್ಷಣಿಕ ಬೆಂಬಲ**: ಕಲಿಕೆ ಮತ್ತು ಬೋಧನೆಗೆ ಸೂಕ್ತವಾದ ವಿವರವಾದ ವಿವರಣೆಗಳು
- **ಫಂಕ್ಷನ್ ಕರೆ**: ಉಪಕರಣ ಸಂಯೋಜನೆ ಮತ್ತು API ಸಂವಹನಕ್ಕೆ ಸ್ಥಳೀಯ ಬೆಂಬಲ

## vLLM ಕಂಟೈನರ್ ನಿಯೋಜನೆ

vLLM Phi-4-mini-instruct ಗೆ ಅತ್ಯುತ್ತಮ ಬೆಂಬಲವನ್ನು ಒದಗಿಸುತ್ತದೆ, ಉನ್ನತ ನಿರ್ಣಯ ಕಾರ್ಯಕ್ಷಮತೆ ಮತ್ತು OpenAI-ಸಮ್ಮತ APIಗಳೊಂದಿಗೆ, ಇದನ್ನು ಉತ್ಪಾದನಾ ತರ್ಕ ಸೇವೆಗಳಿಗೆ ಸೂಕ್ತವಾಗಿಸುತ್ತದೆ.

### ತ್ವರಿತ ಪ್ರಾರಂಭ ಉದಾಹರಣೆಗಳು

#### ಮೂಲ CPU ನಿಯೋಜನೆ (ವಿಕಸನ)
```bash
# ಅಭಿವೃದ್ಧಿ ಮತ್ತು ಪರೀಕ್ಷೆಗಾಗಿ CPU-ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿದ ನಿಯೋಜನೆ
docker run --name phi4-mini-dev \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  --memory="8g" --cpus="4" \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 4096 \
  --max-num-seqs 4 \
  --trust-remote-code
```

#### GPU-ವೇಗವರ್ಧಿತ ಉತ್ಪಾದನಾ ನಿಯೋಜನೆ
```bash
# ಉನ್ನತ ಕಾರ್ಯಕ್ಷಮತೆಯ ತರ್ಕಕ್ಕಾಗಿ GPU ನಿಯೋಜನೆ
docker run --runtime nvidia --gpus all \
  --name phi4-mini-prod \
  -e HUGGING_FACE_HUB_TOKEN=${HF_TOKEN} \
  -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model microsoft/Phi-4-mini-instruct \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.8 \
  --enable-auto-tool-choice \
  --trust-remote-code
```

### ಉತ್ಪಾದನಾ ಸಂರಚನೆ

```yaml
version: '3.8'
services:
  phi4-mini-reasoning:
    image: vllm/vllm-openai:latest
    container_name: phi4-mini-production
    ports:
      - "8000:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./logs:/app/logs
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model microsoft/Phi-4-mini-instruct
      --host 0.0.0.0
      --port 8000
      --max-model-len 4096
      --max-num-seqs 8
      --gpu-memory-utilization 0.8
      --trust-remote-code
      --enable-auto-tool-choice
      --quantization awq
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Phi-4-mini ತರ್ಕ ಸಾಮರ್ಥ್ಯಗಳ ಪರೀಕ್ಷೆ

```bash
# ಗಣಿತೀಯ ತರ್ಕವನ್ನು ಪರೀಕ್ಷಿಸಿ
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "system", "content": "You are a mathematical reasoning assistant. Show your work step by step."},
      {"role": "user", "content": "A train travels 240 km in 3 hours. If it increases its speed by 20 km/h, how long would the same journey take?"}
    ],
    "max_tokens": 200,
    "temperature": 0.3
  }'

# ಕೋಡ್ ರಚನೆಯನ್ನು ಪರೀಕ್ಷಿಸಿ
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Write a Python function to calculate the Fibonacci sequence using dynamic programming. Include comments explaining the approach."}
    ],
    "max_tokens": 300,
    "temperature": 0.5
  }'

# ಫಂಕ್ಷನ್ ಕರೆ ಮಾಡುವ ಸಾಮರ್ಥ್ಯವನ್ನು ಪರೀಕ್ಷಿಸಿ
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "microsoft/Phi-4-mini-instruct",
    "messages": [
      {"role": "user", "content": "Calculate the area of a circle with radius 5 units"}
    ],
    "tools": [
      {
        "type": "function",
        "function": {
          "name": "calculate_circle_area",
          "description": "Calculate the area of a circle given its radius",
          "parameters": {
            "type": "object",
            "properties": {
              "radius": {"type": "number", "description": "The radius of the circle"}
            },
            "required": ["radius"]
          }
        }
      }
    ],
    "tool_choice": "auto"
  }'
```

## Ollama ಕಂಟೈನರ್ ನಿಯೋಜನೆ

Ollama Phi-4-mini-instruct ಗೆ ಸರಳೀಕೃತ ನಿಯೋಜನೆ ಮತ್ತು ನಿರ್ವಹಣೆಯೊಂದಿಗೆ ಉತ್ತಮ ಬೆಂಬಲವನ್ನು ಒದಗಿಸುತ್ತದೆ, ಇದನ್ನು ವಿಕಸನ ಮತ್ತು ಸಮತೋಲನ ಉತ್ಪಾದನಾ ನಿಯೋಜನೆಗಳಿಗೆ ಸೂಕ್ತವಾಗಿಸುತ್ತದೆ.

### ತ್ವರಿತ ಸೆಟ್‌ಅಪ್

```bash
# GPU ಬೆಂಬಲದೊಂದಿಗೆ Ollama ಕಂಟೈನರ್ ಅನ್ನು ನಿಯೋಜಿಸಿ
docker run -d \
  --name ollama-phi4 \
  --gpus all \
  -v ollama-data:/root/.ollama \
  -p 11434:11434 \
  --restart unless-stopped \
  ollama/ollama:latest

# Phi-4-mini-instruct ಮಾದರಿಯನ್ನು ಡೌನ್‌ಲೋಡ್ ಮಾಡಿ
docker exec ollama-phi4 ollama pull phi4-mini

# ಗಣಿತೀಯ ತರ್ಕವನ್ನು ಪರೀಕ್ಷಿಸಿ
docker exec ollama-phi4 ollama run phi4-mini \
  "Solve this step by step: If compound interest on $5000 at 6% annually for 3 years, what is the final amount?"

# ಕೋಡ್ ಉತ್ಪಾದನೆಯನ್ನು ಪರೀಕ್ಷಿಸಿ
docker exec ollama-phi4 ollama run phi4-mini \
  "Write a Python function to implement binary search with detailed comments"
```

### ಉತ್ಪಾದನಾ ಸಂರಚನೆ

```yaml
version: '3.8'
services:
  ollama-phi4:
    image: ollama/ollama:latest
    container_name: ollama-phi4-production
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./modelfiles:/modelfiles
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_FLASH_ATTENTION=1
    deploy:
      resources:
        limits:
          memory: 12G
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web UI for interactive reasoning tasks
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: phi4-webui
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama-phi4:11434
      - DEFAULT_MODELS=phi4-mini
    depends_on:
      - ollama-phi4
    volumes:
      - open-webui-data:/app/backend/data

volumes:
  ollama-data:
  open-webui-data:
```

### ಮಾದರಿ ಆಪ್ಟಿಮೈಜೆಷನ್ ಮತ್ತು ಬದಲಾವಣೆಗಳು

```bash
# ತರ್ಕ-ಆಧಾರಿತ ಪರ್ಯಾಯವನ್ನು ರಚಿಸಿ
cat > /tmp/phi4-reasoning << EOF
FROM phi4-mini
PARAMETER temperature 0.3
PARAMETER top_p 0.8
SYSTEM """You are an expert reasoning assistant specialized in mathematics, logic, and code analysis. 
Always think step by step and show your work clearly. 
For mathematical problems, break down each calculation.
For coding problems, explain your approach and include comments."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-reasoning -f /tmp/phi4-reasoning

# ಕೋಡ್-ಕೇಂದ್ರೀಕೃತ ಪರ್ಯಾಯವನ್ನು ರಚಿಸಿ
cat > /tmp/phi4-coder << EOF
FROM phi4-mini
PARAMETER temperature 0.5
PARAMETER top_p 0.9
SYSTEM """You are a coding assistant specialized in writing clean, efficient, and well-documented code.
Always include detailed comments explaining your approach.
Follow best practices for the target programming language.
Provide examples and test cases when helpful."""
EOF

docker exec ollama-phi4 ollama create phi4-mini-coder -f /tmp/phi4-coder
```

### API ಬಳಕೆ ಉದಾಹರಣೆಗಳು

```bash
# API ಮೂಲಕ ಗಣಿತೀಯ ತರ್ಕ
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-reasoning",
  "prompt": "A rectangle has length 15cm and width 8cm. If we increase both dimensions by 20%, what is the percentage increase in area?",
  "stream": false,
  "options": {
    "temperature": 0.3,
    "top_p": 0.8,
    "num_ctx": 4096
  }
}'

# API ಮೂಲಕ ಕೋಡ್ ಉತ್ಪಾದನೆ
curl http://localhost:11434/api/generate -d '{
  "model": "phi4-mini-coder",
  "prompt": "Create a Python class for a binary tree with methods for insertion, deletion, and in-order traversal. Include comprehensive docstrings.",
  "stream": false,
  "options": {
    "temperature": 0.5,
    "top_p": 0.9,
    "num_ctx": 4096
  }
}'
```

## SLM ಎಂಜಿನ್ ಜೊತೆಗೆ ONNX ರನ್‌ಟೈಮ್

ONNX ರನ್‌ಟೈಮ್ Phi-4-mini-instruct ನ ಎಡ್ಜ್ ನಿಯೋಜನೆಗೆ ಅತ್ಯುತ್ತಮ ಕಾರ್ಯಕ್ಷಮತೆಯನ್ನು ಒದಗಿಸುತ್ತದೆ, ಉನ್ನತ ಆಪ್ಟಿಮೈಜೆಷನ್ ಮತ್ತು ಕ್ರಾಸ್-ಪ್ಲಾಟ್‌ಫಾರ್ಮ್ ಹೊಂದಾಣಿಕೆಯಿಂದ.

### ಮೂಲ ಸೆಟ್‌ಅಪ್

```dockerfile
# Dockerfile for ONNX-optimized Phi-4-mini
FROM python:3.11-slim

RUN pip install --no-cache-dir \
    onnxruntime-gpu \
    optimum[onnxruntime] \
    transformers \
    fastapi \
    uvicorn

COPY app/ /app/
WORKDIR /app
EXPOSE 8080
CMD ["python", "server.py"]
```

### ಸರಳೀಕೃತ ಸರ್ವರ್ ಜಾರಿಗೆ

```python
# app/server.py - ಫೈ-4-ಮಿನಿ ತರ್ಕ ಕಾರ್ಯಗಳಿಗೆ ಆಪ್ಟಿಮೈಸ್ ಮಾಡಲಾಗಿದೆ
import os
import time
import onnxruntime as ort
from transformers import AutoTokenizer
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI(title="Phi-4-mini ONNX Engine")

class ReasoningRequest(BaseModel):
    prompt: str
    task_type: str = "reasoning"  # ತರ್ಕ, ಕೋಡಿಂಗ್, ಗಣಿತ
    max_length: int = 200
    temperature: float = 0.3

class Phi4MiniEngine:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.load_model()
    
    def load_model(self):
        model_path = "/app/models/phi4-mini-instruct.onnx"
        if os.path.exists(model_path):
            # ತರ್ಕ ಕಾರ್ಯಗಳಿಗೆ ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿದ ಪ್ರೊವೈಡರ್‌ಗಳು
            providers = [
                ('CUDAExecutionProvider', {
                    'arena_extend_strategy': 'kSameAsRequested',
                    'cudnn_conv_algo_search': 'HEURISTIC',
                }),
                ('CPUExecutionProvider', {
                    'intra_op_num_threads': 4,
                    'inter_op_num_threads': 2,
                })
            ]
            
            self.model = ort.InferenceSession(model_path, providers=providers)
            self.tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-4-mini-instruct")
            print("✓ Phi-4-mini model loaded successfully")
        else:
            print("✗ Model file not found. Please convert the model first.")
    
    def generate_reasoning(self, request: ReasoningRequest):
        if not self.model:
            raise ValueError("Model not loaded")
        
        # ಉತ್ತಮ ತರ್ಕಕ್ಕಾಗಿ ಕಾರ್ಯ-ನಿರ್ದಿಷ್ಟ ಪ್ರಾಂಪ್ಟಿಂಗ್
        task_prompts = {
            "reasoning": "Think step by step and show your reasoning clearly:",
            "math": "Solve this mathematical problem step by step:",
            "coding": "Write clean, well-commented code for this task:"
        }
        
        system_prompt = task_prompts.get(request.task_type, "")
        full_prompt = f"{system_prompt}\n{request.prompt}"
        
        # ಟೋಕನೈಸ್ ಮಾಡಿ ಮತ್ತು ನಿರ್ಣಯವನ್ನು ನಡೆಸಿ
        inputs = self.tokenizer.encode(full_prompt, return_tensors="np", max_length=2048, truncation=True)
        
        start_time = time.time()
        outputs = self.model.run(None, {"input_ids": inputs})
        inference_time = time.time() - start_time
        
        # ಪ್ರತಿಕ್ರಿಯೆಯನ್ನು ಡಿಕೋಡ್ ಮಾಡಿ
        generated_text = self.tokenizer.decode(outputs[0][0], skip_special_tokens=True)
        
        return {
            "generated_text": generated_text,
            "task_type": request.task_type,
            "inference_time": inference_time,
            "model": "phi4-mini-instruct-onnx"
        }

# ಎಂಜಿನ್ ಅನ್ನು ಪ್ರಾರಂಭಿಸಿ
engine = Phi4MiniEngine()

@app.post("/reasoning")
async def generate_reasoning(request: ReasoningRequest):
    try:
        return engine.generate_reasoning(request)
    except Exception as e:
        return {"error": str(e)}

@app.get("/health")
async def health():
    return {
        "status": "healthy" if engine.model else "model_not_loaded",
        "model": "phi4-mini-instruct",
        "capabilities": ["reasoning", "math", "coding"]
    }

@app.get("/")
async def root():
    return {
        "name": "Phi-4-mini ONNX Engine",
        "model": "microsoft/Phi-4-mini-instruct",
        "endpoints": ["/reasoning", "/health"],
        "capabilities": ["mathematical reasoning", "code generation", "logical problem solving"]
    }
```

### ಮಾದರಿ ಪರಿವರ್ತನೆ ಸ್ಕ್ರಿಪ್ಟ್

```python
# convert_phi4_mini.py - ಫೈ-4-ಮಿನಿ ಅನ್ನು ಆಪ್ಟಿಮೈಸ್ ಮಾಡಿದ ONNX ಗೆ ಪರಿವರ್ತಿಸಿ
import os
from pathlib import Path
from optimum.onnxruntime import ORTModelForCausalLM, ORTOptimizer, ORTQuantizer
from optimum.onnxruntime.configuration import AutoQuantizationConfig, AutoOptimizationConfig
from transformers import AutoTokenizer

def convert_phi4_mini():
    print("Converting Phi-4-mini-instruct to optimized ONNX...")
    
    model_name = "microsoft/Phi-4-mini-instruct"
    output_dir = Path("./models/phi4-mini-onnx")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ಹಂತ 1: ONNX ಗೆ ಪರಿವರ್ತಿಸಿ
    print("Step 1: Converting to ONNX format...")
    model = ORTModelForCausalLM.from_pretrained(
        model_name,
        export=True,
        provider="CPUExecutionProvider",
        use_cache=True
    )
    
    # ಹಂತ 2: ತರ್ಕ ಕಾರ್ಯಗಳಿಗೆ ಆಪ್ಟಿಮೈಜೇಶನ್ ಅನ್ವಯಿಸಿ
    print("Step 2: Applying reasoning-specific optimizations...")
    optimization_config = AutoOptimizationConfig.with_optimization_level(
        optimization_level="O3",
        optimize_for_gpu=True,
        fp16=True
    )
    
    optimizer = ORTOptimizer.from_pretrained(model)
    optimizer.optimize(save_dir=output_dir, optimization_config=optimization_config)
    
    # ಹಂತ 3: ಎಡ್ಜ್ ನಿಯೋಜನೆಗಾಗಿ ಕ್ವಾಂಟೈಜೆಶನ್ ಅನ್ವಯಿಸಿ
    print("Step 3: Applying quantization...")
    quantization_config = AutoQuantizationConfig.avx512_vnni(
        is_static=False,
        per_channel=True
    )
    
    quantizer = ORTQuantizer.from_pretrained(output_dir)
    quantizer.quantize(
        save_dir=output_dir / "quantized",
        quantization_config=quantization_config
    )
    
    # ಹಂತ 4: ಟೋಕನೈಜರ್ ಉಳಿಸಿ
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.save_pretrained(output_dir)
    
    # ಹಂತ 5: ಅಂತಿಮ ಆಪ್ಟಿಮೈಸ್ ಮಾಡಲಾದ ಮಾದರಿಯನ್ನು ರಚಿಸಿ
    final_model_path = Path("./models/phi4-mini-instruct.onnx")
    quantized_files = list((output_dir / "quantized").glob("*.onnx"))
    if quantized_files:
        import shutil
        shutil.copy2(quantized_files[0], final_model_path)
        print(f"✓ Phi-4-mini converted and optimized: {final_model_path}")
    
    return final_model_path

if __name__ == "__main__":
    convert_phi4_mini()
```

### ಉತ್ಪಾದನಾ ಸಂರಚನೆ

```yaml
version: '3.8'
services:
  # Model conversion service (run once)
  phi4-converter:
    build: .
    container_name: phi4-converter
    volumes:
      - ./models:/app/models
      - ~/.cache/huggingface:/root/.cache/huggingface
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_TOKEN}
    command: python convert_phi4_mini.py
    profiles: ["convert"]

  # Main reasoning engine
  phi4-onnx:
    build: .
    container_name: phi4-onnx-engine
    ports:
      - "8080:8080"
    volumes:
      - ./models:/app/models:ro
    environment:
      - LOG_LEVEL=INFO
    deploy:
      resources:
        limits:
          memory: 8G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### ONNX ನಿಯೋಜನೆ ಪರೀಕ್ಷೆ

```bash
# ಗಣಿತೀಯ ತರ್ಕವನ್ನು ಪರೀಕ್ಷಿಸಿ
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "If a car travels at 60 mph for 2.5 hours, then at 80 mph for 1.5 hours, what is the total distance traveled?",
    "task_type": "math",
    "max_length": 150,
    "temperature": 0.3
  }'

# ಕೋಡ್ ಉತ್ಪಾದನೆಯನ್ನು ಪರೀಕ್ಷಿಸಿ
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Python function to find the greatest common divisor of two numbers using the Euclidean algorithm",
    "task_type": "coding",
    "max_length": 250,
    "temperature": 0.5
  }'

# ತಾರ್ಕಿಕ ತರ್ಕವನ್ನು ಪರೀಕ್ಷಿಸಿ
curl -X POST http://localhost:8080/reasoning \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "All cats are mammals. Some mammals are carnivores. Can we conclude that some cats are carnivores?",
    "task_type": "reasoning",
    "max_length": 200,
    "temperature": 0.3
  }'
```

## ಹೋಲಿಕೆ ಚಟುವಟಿಕೆ

### Phi-4-mini ಗೆ ಚಟುವಟಿಕೆ ಹೋಲಿಕೆ

| ವೈಶಿಷ್ಟ್ಯ | vLLM | Ollama | ONNX ರನ್‌ಟೈಮ್ |
|-----------|------|--------|----------------|
| **ಸೆಟ್‌ಅಪ್ ಸಂಕೀರ್ಣತೆ** | ಮಧ್ಯಮ | ಸುಲಭ | ಸಂಕೀರ್ಣ |
| **ಕಾರ್ಯಕ್ಷಮತೆ (GPU)** | ಅತ್ಯುತ್ತಮ (~25 ಟೋಕನ್/ಸೆಕೆ) | ಬಹಳ ಉತ್ತಮ (~20 ಟೋಕನ್/ಸೆಕೆ) | ಉತ್ತಮ (~15 ಟೋಕನ್/ಸೆಕೆ) |
| **ಕಾರ್ಯಕ್ಷಮತೆ (CPU)** | ಉತ್ತಮ (~8 ಟೋಕನ್/ಸೆಕೆ) | ಬಹಳ ಉತ್ತಮ (~12 ಟೋಕನ್/ಸೆಕೆ) | ಅತ್ಯುತ್ತಮ (~15 ಟೋಕನ್/ಸೆಕೆ) |
| **ಮೆಮೊರಿ ಬಳಕೆ** | 8-12GB | 6-10GB | 4-8GB |
| **API ಹೊಂದಾಣಿಕೆ** | OpenAI ಹೊಂದಾಣಿಕೆ | ಕಸ್ಟಮ್ REST | ಕಸ್ಟಮ್ FastAPI |
| **ಫಂಕ್ಷನ್ ಕರೆ** | ✅ ಸ್ಥಳೀಯ | ✅ ಬೆಂಬಲಿತ | ⚠️ ಕಸ್ಟಮ್ ಜಾರಿಗೆ |
| **ಕ್ವಾಂಟೈಜೆಷನ್ ಬೆಂಬಲ** | AWQ, GPTQ | Q4_0, Q5_1, Q8_0 | ONNX ಕ್ವಾಂಟೈಜೆಷನ್ |
| **ಉತ್ಪಾದನಾ ಸಿದ್ಧತೆ** | ✅ ಅತ್ಯುತ್ತಮ | ✅ ಬಹಳ ಉತ್ತಮ | ✅ ಉತ್ತಮ |
| **ಎಡ್ಜ್ ನಿಯೋಜನೆ** | ಉತ್ತಮ | ಅತ್ಯುತ್ತಮ | ಅತ್ಯುತ್ತಮ |

## ಹೆಚ್ಚುವರಿ ಸಂಪನ್ಮೂಲಗಳು

### ಅಧಿಕೃತ ಡಾಕ್ಯುಮೆಂಟೇಶನ್
- **Microsoft Phi-4 ಮಾದರಿ ಕಾರ್ಡ್**: ವಿವರವಾದ ವಿಶೇಷಣಗಳು ಮತ್ತು ಬಳಕೆ ಮಾರ್ಗದರ್ಶಿಗಳು
- **vLLM ಡಾಕ್ಯುಮೆಂಟೇಶನ್**: ಉನ್ನತ ಸಂರಚನೆ ಮತ್ತು ಆಪ್ಟಿಮೈಜೆಷನ್ ಆಯ್ಕೆಗಳು
- **Ollama ಮಾದರಿ ಗ್ರಂಥಾಲಯ**: ಸಮುದಾಯ ಮಾದರಿಗಳು ಮತ್ತು ಕಸ್ಟಮೈಜೆಷನ್ ಉದಾಹರಣೆಗಳು
- **ONNX ರನ್‌ಟೈಮ್ ಮಾರ್ಗದರ್ಶಿಗಳು**: ಕಾರ್ಯಕ್ಷಮತೆ ಆಪ್ಟಿಮೈಜೆಷನ್ ಮತ್ತು ನಿಯೋಜನೆ ತಂತ್ರಗಳು

### ಅಭಿವೃದ್ಧಿ ಸಾಧನಗಳು
- **Hugging Face Transformers**: ಮಾದರಿ ಸಂವಹನ ಮತ್ತು ಕಸ್ಟಮೈಜೆಷನ್‌ಗಾಗಿ
- **OpenAI API ಸ್ಪೆಸಿಫಿಕೇಶನ್**: vLLM ಹೊಂದಾಣಿಕೆ ಪರೀಕ್ಷೆಗೆ
- **ಡೋಕರ್ ಉತ್ತಮ ಅಭ್ಯಾಸಗಳು**: ಕಂಟೈನರ್ ಭದ್ರತೆ ಮತ್ತು ಆಪ್ಟಿಮೈಜೆಷನ್ ಮಾರ್ಗದರ್ಶಿಗಳು
- **ಕ್ಯೂಬರ್ನೇಟಿಸ್ ನಿಯೋಜನೆ**: ಉತ್ಪಾದನಾ ವಿಸ್ತರಣೆಗೆ ಸಂಚಲನ ಮಾದರಿಗಳು

### ಕಲಿಕೆಯ ಸಂಪನ್ಮೂಲಗಳು
- **SLM ಕಾರ್ಯಕ್ಷಮತೆ ಬೆಂಚ್ಮಾರ್ಕಿಂಗ್**: ಹೋಲಿಕೆ ವಿಶ್ಲೇಷಣಾ ವಿಧಾನಗಳು
- **ಎಡ್ಜ್ AI ನಿಯೋಜನೆ**: ಸಂಪನ್ಮೂಲ-ಮಿತಿಯ ಪರಿಸರಗಳಿಗೆ ಉತ್ತಮ ಅಭ್ಯಾಸಗಳು
- **ತರ್ಕಾತ್ಮಕ ಕಾರ್ಯ ಆಪ್ಟಿಮೈಜೆಷನ್**: ಗಣಿತ ಮತ್ತು ತರ್ಕಾತ್ಮಕ ಸಮಸ್ಯೆಗಳಿಗೆ ಪ್ರಾಂಪ್ಟ್ ತಂತ್ರಗಳು
- **ಕಂಟೈನರ್ ಭದ್ರತೆ**: AI ಮಾದರಿ ನಿಯೋಜನೆಗಳಿಗಾಗಿ ಹಾರ್ಡನಿಂಗ್ ಅಭ್ಯಾಸಗಳು

## ಕಲಿಕೆಯ ಫಲಿತಾಂಶಗಳು

ಈ ಘಟಕವನ್ನು ಪೂರ್ಣಗೊಳಿಸಿದ ನಂತರ, ನೀವು ಸಾಧ್ಯವಾಗುತ್ತದೆ:

1. ವಿವಿಧ ಚಟುವಟಿಕೆಗಳನ್ನು ಬಳಸಿಕೊಂಡು Phi-4-mini-instruct ಮಾದರಿಯನ್ನು ಕಂಟೈನರೈಜ್ಡ್ ಪರಿಸರಗಳಲ್ಲಿ ನಿಯೋಜಿಸುವುದು
2. ವಿಭಿನ್ನ ಹಾರ್ಡ್‌ವೇರ್ ಪರಿಸರಗಳಿಗೆ SLM ನಿಯೋಜನೆಗಳನ್ನು ಸಂರಚಿಸಿ ಮತ್ತು ಆಪ್ಟಿಮೈಸ್ ಮಾಡುವುದು
3. ಕಂಟೈನರೈಜ್ಡ್ AI ನಿಯೋಜನೆಗಳಿಗೆ ಭದ್ರತೆ ಉತ್ತಮ ಅಭ್ಯಾಸಗಳನ್ನು ಜಾರಿಗೆ ತರುವುದು
4. ನಿರ್ದಿಷ್ಟ ಬಳಕೆ ಪ್ರಕರಣ ಅವಶ್ಯಕತೆಗಳ ಆಧಾರದ ಮೇಲೆ ಸೂಕ್ತ ನಿಯೋಜನೆ ಚಟುವಟಿಕೆಗಳನ್ನು ಹೋಲಿಸಿ ಆಯ್ಕೆ ಮಾಡುವುದು
5. ಉತ್ಪಾದನಾ-ಮಟ್ಟದ SLM ಸೇವೆಗಳಿಗೆ ಮೇಲ್ವಿಚಾರಣೆ ಮತ್ತು ವಿಸ್ತರಣೆ ತಂತ್ರಗಳನ್ನು ಅನ್ವಯಿಸುವುದು

## ಮುಂದೇನು

- [ಮಾಡ್ಯೂಲ್ 1](../Module01/README.md) ಗೆ ಹಿಂತಿರುಗಿ
- [ಮಾಡ್ಯೂಲ್ 2](../Module02/README.md) ಗೆ ಹಿಂತಿರುಗಿ

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**ಅಸ್ವೀಕರಣ**:  
ಈ ದಸ್ತಾವೇಜು AI ಅನುವಾದ ಸೇವೆ [Co-op Translator](https://github.com/Azure/co-op-translator) ಬಳಸಿ ಅನುವಾದಿಸಲಾಗಿದೆ. ನಾವು ನಿಖರತೆಯಿಗಾಗಿ ಪ್ರಯತ್ನಿಸುತ್ತಿದ್ದರೂ, ಸ್ವಯಂಚಾಲಿತ ಅನುವಾದಗಳಲ್ಲಿ ದೋಷಗಳು ಅಥವಾ ತಪ್ಪುಗಳು ಇರಬಹುದು ಎಂದು ದಯವಿಟ್ಟು ಗಮನಿಸಿ. ಮೂಲ ಭಾಷೆಯಲ್ಲಿರುವ ಮೂಲ ದಸ್ತಾವೇಜನ್ನು ಅಧಿಕೃತ ಮೂಲವೆಂದು ಪರಿಗಣಿಸಬೇಕು. ಮಹತ್ವದ ಮಾಹಿತಿಗಾಗಿ, ವೃತ್ತಿಪರ ಮಾನವ ಅನುವಾದವನ್ನು ಶಿಫಾರಸು ಮಾಡಲಾಗುತ್ತದೆ. ಈ ಅನುವಾದ ಬಳಕೆಯಿಂದ ಉಂಟಾಗುವ ಯಾವುದೇ ತಪ್ಪು ಅರ್ಥಮಾಡಿಕೊಳ್ಳುವಿಕೆ ಅಥವಾ ತಪ್ಪು ವಿವರಣೆಗಳಿಗೆ ನಾವು ಹೊಣೆಗಾರರಾಗುವುದಿಲ್ಲ.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->