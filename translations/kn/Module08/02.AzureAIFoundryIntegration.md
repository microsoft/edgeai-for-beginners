# ‡≤∏‡≥Ü‡≤∑‡≤®‡≥ç 2: OpenAI SDK ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å Azure AI Foundry ‡≤ú‡≥ä‡≤§‡≥Ü‡≤ó‡≥Ü ‡≤è‡≤ï‡≥Ä‡≤ï‡≤∞‡≤£

## ‡≤Ö‡≤µ‡≤≤‡≥ã‡≤ï‡≤®

‡≤®‡≤ø‡≤Æ‡≥ç‡≤Æ Foundry Local ‡≤Ü‡≤ß‡≤æ‡≤∞‡≤¶ ‡≤Æ‡≥á‡≤≤‡≥Ü ‡≤®‡≤ø‡≤∞‡≥ç‡≤Æ‡≤ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≤æ, ‡≤à ‡≤∏‡≥Ü‡≤∑‡≤®‡≥ç ‡≤â‡≤®‡≥ç‡≤®‡≤§ OpenAI SDK ‡≤è‡≤ï‡≥Ä‡≤ï‡≤∞‡≤£ ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥ ‡≤Æ‡≥á‡≤≤‡≥Ü ‡≤ï‡≥á‡≤Ç‡≤¶‡≥ç‡≤∞‡≥Ä‡≤ï‡≤∞‡≤ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü, ‡≤á‡≤¶‡≥Å Microsoft Foundry Local ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å Azure OpenAI ‡≤é‡≤∞‡≤°‡≤®‡≥ç‡≤®‡≥Ç ‡≤∏‡≥å‡≤ï‡≤∞‡≥ç‡≤Ø‡≤µ‡≤æ‡≤ó‡≤ø ‡≤¨‡≥Ü‡≤Ç‡≤¨‡≤≤‡≤ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü. ‡≤®‡≥Ä‡≤µ‡≥Å ‡≤ó‡≥å‡≤™‡≥ç‡≤Ø‡≤§‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤Ö‡≤≠‡≤ø‡≤µ‡≥É‡≤¶‡≥ç‡≤ß‡≤ø‡≤ó‡≤æ‡≤ó‡≤ø ‡≤∏‡≥ç‡≤•‡≤≥‡≥Ä‡≤Ø‡≤µ‡≤æ‡≤ó‡≤ø ‡≤ö‡≤æ‡≤≤‡≤®‡≥Ü ‡≤Æ‡≤æ‡≤°‡≤¨‡≤π‡≥Å‡≤¶‡≤æ‡≤¶, ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤Ö‡≤ó‡≤§‡≥ç‡≤Ø‡≤µ‡≤ø‡≤¶‡≥ç‡≤¶‡≤æ‡≤ó Azure OpenAI ‡≤Æ‡≥Ç‡≤≤‡≤ï ‡≤ï‡≥ç‡≤≤‡≥å‡≤°‡≥ç ‡≤µ‡≤ø‡≤∏‡≥ç‡≤§‡≤∞‡≤£‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤í‡≤¶‡≤ó‡≤ø‡≤∏‡≥Å‡≤µ ‡≤®‡≤Ø‡≤µ‡≤æ‡≤¶ AI ‡≤Ö‡≤™‡≥ç‡≤≤‡≤ø‡≤ï‡≥á‡≤∂‡≤®‡≥ç‚Äå‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤®‡≤ø‡≤∞‡≥ç‡≤Æ‡≤ø‡≤∏‡≥Å‡≤µ ‡≤ï‡≤≤‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤Ü‡≤≥‡≤µ‡≤æ‡≤ó‡≤ø ‡≤ï‡≤≤‡≤ø‡≤Ø‡≥Å‡≤§‡≥ç‡≤§‡≥Ä‡≤∞‡≤ø.

## ‡≤ï‡≤≤‡≤ø‡≤ï‡≥Ü‡≤Ø ‡≤â‡≤¶‡≥ç‡≤¶‡≥á‡≤∂‡≤ó‡≤≥‡≥Å

‡≤à ‡≤∏‡≥Ü‡≤∑‡≤®‡≥ç ‡≤ï‡≥ä‡≤®‡≥Ü‡≤Ø‡≤≤‡≥ç‡≤≤‡≤ø, ‡≤®‡≥Ä‡≤µ‡≥Å:
- Foundry Local ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å Azure OpenAI ‡≤é‡≤∞‡≤°‡≤∞‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤â‡≤®‡≥ç‡≤®‡≤§ OpenAI SDK ‡≤è‡≤ï‡≥Ä‡≤ï‡≤∞‡≤£‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤®‡≤ø‡≤™‡≥Å‡≤£‡≤∞‡≤æ‡≤ó‡≤ø ‡≤Æ‡≤æ‡≤°‡≤ø‡≤ï‡≥ä‡≤≥‡≥ç‡≤≥‡≥Å‡≤µ‡≤ø‡≤∞‡≤ø
- ‡≤∏‡≥Å‡≤ß‡≤æ‡≤∞‡≤ø‡≤§ ‡≤¨‡≤≥‡≤ï‡≥Ü‡≤¶‡≤æ‡≤∞ ‡≤Ö‡≤®‡≥Å‡≤≠‡≤µ‡≤ï‡≥ç‡≤ï‡≤æ‡≤ó‡≤ø ‡≤∏‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Æ‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤™‡≥ç‡≤∞‡≤§‡≤ø‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤ú‡≤æ‡≤∞‡≤ø‡≤ó‡≥Ü ‡≤§‡≤∞‡≥Å‡≤µ‡≤ø‡≤∞‡≤ø
- ‡≤¨‡≤π‡≥Å-‡≤™‡≥ç‡≤∞‡≤¶‡≤æ‡≤§ ‡≤¨‡≥Ü‡≤Ç‡≤¨‡≤≤‡≤ï‡≥ç‡≤ï‡≤æ‡≤ó‡≤ø ‡≤¨‡≤≤‡≤µ‡≤æ‡≤¶ ‡≤ï‡≥ç‡≤≤‡≥à‡≤Ç‡≤ü‡≥ç ‡≤´‡≥ç‡≤Ø‡≤æ‡≤ï‡≥ç‡≤ü‡≤∞‡≤ø ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≥Å‡≤µ‡≤ø‡≤∞‡≤ø
- ‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≤æ ‡≤µ‡≥ç‡≤Ø‡≤µ‡≤∏‡≥ç‡≤•‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≥É‡≤∑‡≥ç‡≤ü‡≤ø‡≤∏‡≤ø, ‡≤∏‡≤Ç‡≤¶‡≤∞‡≥ç‡≤≠ ‡≤∏‡≤Ç‡≤∞‡≤ï‡≥ç‡≤∑‡≤£‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≤æ‡≤ß‡≤ø‡≤∏‡≥Å‡≤µ‡≤ø‡≤∞‡≤ø
- ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü ‡≤Æ‡≥å‡≤≤‡≥ç‡≤Ø‡≤Æ‡≤æ‡≤™‡≤® ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤Ü‡≤∞‡≥ã‡≤ó‡≥ç‡≤Ø ‡≤Æ‡≥á‡≤≤‡≥ç‡≤µ‡≤ø‡≤ö‡≤æ‡≤∞‡≤£‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≥ç‡≤•‡≤æ‡≤™‡≤ø‡≤∏‡≥Å‡≤µ‡≤ø‡≤∞‡≤ø
- ‡≤∏‡≤∞‡≤ø‡≤Ø‡≤æ‡≤¶ ‡≤¶‡≥ã‡≤∑ ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≥Ü‡≤Ø‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≤æ-‡≤∏‡≤ø‡≤¶‡≥ç‡≤ß ‡≤Ö‡≤™‡≥ç‡≤≤‡≤ø‡≤ï‡≥á‡≤∂‡≤®‡≥ç‚Äå‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤®‡≤ø‡≤Ø‡≥ã‡≤ú‡≤ø‡≤∏‡≥Å‡≤µ‡≤ø‡≤∞‡≤ø

## ‡≤™‡≥Ç‡≤∞‡≥ç‡≤µ‡≤æ‡≤™‡≥á‡≤ï‡≥ç‡≤∑‡≤ø‡≤§‡≤ó‡≤≥‡≥Å

- ‡≤∏‡≥Ü‡≤∑‡≤®‡≥ç 1: Foundry Local ‡≤™‡≥ç‡≤∞‡≤æ‡≤∞‡≤Ç‡≤≠‡≤ø‡≤∏‡≥Å‡≤µ‡≤ø‡≤ï‡≥Ü ‡≤™‡≥Ç‡≤∞‡≥ç‡≤£‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü
- ‡≤ö‡≤æ‡≤≤‡≤®‡≥Ü‡≤Ø‡≤≤‡≥ç‡≤≤‡≤ø‡≤∞‡≥Å‡≤µ ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤∏‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø Foundry Local ‡≤∏‡≥ç‡≤•‡≤æ‡≤™‡≤®‡≥Ü
- Python 3.8 ‡≤Ö‡≤•‡≤µ‡≤æ ‡≤®‡≤Ç‡≤§‡≤∞‡≤¶ ‡≤Ü‡≤µ‡≥É‡≤§‡≥ç‡≤§‡≤ø ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤µ‡≤∞‡≥ç‡≤ö‡≥Å‡≤µ‡≤≤‡≥ç ‡≤™‡≤∞‡≤ø‡≤∏‡≤∞ ‡≤∏‡≤æ‡≤Æ‡≤∞‡≥ç‡≤•‡≥ç‡≤Ø
- OpenAI Python SDK ‡≤∏‡≥ç‡≤•‡≤æ‡≤™‡≤ø‡≤∏‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü (`pip install openai foundry-local-sdk`)
- OpenAI ‡≤∏‡≥á‡≤µ‡≥Ü‡≤Ø‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü Azure ‡≤ñ‡≤æ‡≤§‡≥Ü (‡≤ê‡≤ö‡≥ç‡≤õ‡≤ø‡≤ï, ‡≤ï‡≥ç‡≤≤‡≥å‡≤°‡≥ç ‡≤∏‡≤Ç‡≤¶‡≤∞‡≥ç‡≤≠‡≤ó‡≤≥‡≤ø‡≤ó‡≥Ü)
- Python async/await ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥ ‡≤Æ‡≥Ç‡≤≤‡≤≠‡≥Ç‡≤§ ‡≤Ö‡≤∞‡≥ç‡≤•

## ‡≤≠‡≤æ‡≤ó 1: OpenAI SDK ‡≤ï‡≥ç‡≤≤‡≥à‡≤Ç‡≤ü‡≥ç ‡≤´‡≥ç‡≤Ø‡≤æ‡≤ï‡≥ç‡≤ü‡≤∞‡≤ø ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø

### ‡≤¨‡≤π‡≥Å-‡≤™‡≥ç‡≤∞‡≤¶‡≤æ‡≤§ ‡≤µ‡≤æ‡≤∏‡≥ç‡≤§‡≥Å‡≤∂‡≤ø‡≤≤‡≥ç‡≤™‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤Ö‡≤∞‡≥ç‡≤•‡≤Æ‡≤æ‡≤°‡≤ø‡≤ï‡≥ä‡≤≥‡≥ç‡≤≥‡≥Å‡≤µ‡≥Å‡≤¶‡≥Å

Foundry Local ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å Azure OpenAI ‡≤é‡≤∞‡≤°‡≤∞‡≥ä‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ü ‡≤ï‡≥Ü‡≤≤‡≤∏ ‡≤Æ‡≤æ‡≤°‡≥Å‡≤µ ‡≤Ö‡≤™‡≥ç‡≤≤‡≤ø‡≤ï‡≥á‡≤∂‡≤®‡≥ç‚Äå‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤®‡≤ø‡≤∞‡≥ç‡≤Æ‡≤ø‡≤∏‡≤≤‡≥Å ‡≤®‡≤Ø‡≤µ‡≤æ‡≤¶ ‡≤ï‡≥ç‡≤≤‡≥à‡≤Ç‡≤ü‡≥ç ‡≤∏‡≥É‡≤∑‡≥ç‡≤ü‡≤ø ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤Ø ‡≤Ö‡≤ó‡≤§‡≥ç‡≤Ø‡≤µ‡≤ø‡≤¶‡≥Ü:

```python
# sdk_integration.py - ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø 02 ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø
import os
from openai import OpenAI
from typing import Tuple

try:
    from foundry_local import FoundryLocalManager
    FOUNDRY_SDK_AVAILABLE = True
except ImportError:
    FOUNDRY_SDK_AVAILABLE = False


def create_azure_client() -> Tuple[OpenAI, str]:
    """Create Azure OpenAI client."""
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    azure_api_version = os.environ.get("AZURE_OPENAI_API_VERSION", "2024-08-01-preview")
    
    if not azure_endpoint or not azure_api_key:
        raise ValueError("Azure OpenAI endpoint and API key are required")
    
    model = os.environ.get("MODEL", "your-deployment-name")
    client = OpenAI(
        base_url=f"{azure_endpoint}/openai",
        api_key=azure_api_key,
        default_query={"api-version": azure_api_version},
    )
    
    print(f"üåê Azure OpenAI client created with model: {model}")
    return client, model


def create_foundry_client() -> Tuple[OpenAI, str]:
    """Create Foundry Local client with SDK management."""
    alias = os.environ.get("MODEL", "phi-4-mini")
    
    if FOUNDRY_SDK_AVAILABLE:
        try:
            # ‡≤∏‡≤∞‡≤ø‡≤Ø‡≤æ‡≤¶ ‡≤∏‡≥á‡≤µ‡≤æ ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≥Ü‡≤ó‡≥Ü FoundryLocalManager ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å ‡≤¨‡≤≥‡≤∏‡≤ø
            manager = FoundryLocalManager(alias)
            model_info = manager.get_model_info(alias)
            
            # ‡≤∏‡≥ç‡≤•‡≤≥‡≥Ä‡≤Ø Foundry ‡≤∏‡≥á‡≤µ‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤¨‡≤≥‡≤∏‡≤≤‡≥Å OpenAI ‡≤ï‡≥ç‡≤≤‡≥à‡≤Ç‡≤ü‡≥ç ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
            client = OpenAI(
                base_url=manager.endpoint,
                api_key=manager.api_key
            )
            
            print(f"üè† Foundry Local SDK initialized with model: {model_info.id}")
            return client, model_info.id
        except Exception as e:
            print(f"‚ö†Ô∏è Could not use Foundry SDK ({e}), falling back to manual configuration")
    
    # ‡≤ï‡≥à‡≤Ø‡≤ø‡≤Ç‡≤¶ ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≥Ü‡≤ó‡≥ÜFallback ‡≤Ü‡≤ó‡≤ø
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    api_key = os.environ.get("API_KEY", "")
    
    client = OpenAI(
        base_url=f"{base_url}/v1",
        api_key=api_key
    )
    
    print(f"üîß Manual configuration with model: {alias}")
    return client, alias


def initialize_client() -> Tuple[OpenAI, str, str]:
    """Initialize the appropriate OpenAI client."""
    # Azure OpenAI ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≥Ü‡≤ó‡≤æ‡≤ó‡≤ø ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤ø‡≤∏‡≤ø
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    
    if azure_endpoint and azure_api_key:
        try:
            client, model = create_azure_client()
            return client, model, "azure"
        except Exception as e:
            print(f"‚ùå Azure OpenAI initialization failed: {e}")
            print("üîÑ Falling back to Foundry Local...")
    
    # Foundry Local ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å ‡≤¨‡≤≥‡≤∏‡≤ø
    client, model = create_foundry_client()
    return client, model, "foundry"
```

## ‡≤≠‡≤æ‡≤ó 2: ‡≤∏‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Æ‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤™‡≥ç‡≤∞‡≤§‡≤ø‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≥Ü‡≤ó‡≤≥‡≥Å ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤∞‡≤ø‡≤Ø‡≤≤‡≥ç-‡≤ü‡≥à‡≤Æ‡≥ç ‡≤∏‡≤Ç‡≤µ‡≤π‡≤®

### ‡≤∏‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Æ‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤ö‡≤æ‡≤ü‡≥ç ‡≤™‡≥Ç‡≤∞‡≥ç‡≤£‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≥Å‡≤µ‡≤ø‡≤ï‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤ú‡≤æ‡≤∞‡≤ø‡≤ó‡≥Ü ‡≤§‡≤∞‡≥Å‡≤µ‡≤ø‡≤ï‡≥Ü

‡≤∏‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Æ‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤™‡≥ç‡≤∞‡≤§‡≤ø‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤§‡≤Ø‡≤æ‡≤∞‡≤æ‡≤ó‡≥Å‡≤§‡≥ç‡≤§‡≤≤‡≥á ‡≤§‡≥ã‡≤∞‡≤ø‡≤∏‡≥Å‡≤µ ‡≤Æ‡≥Ç‡≤≤‡≤ï ‡≤â‡≤§‡≥ç‡≤§‡≤Æ ‡≤¨‡≤≥‡≤ï‡≥Ü‡≤¶‡≤æ‡≤∞ ‡≤Ö‡≤®‡≥Å‡≤≠‡≤µ‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤í‡≤¶‡≤ó‡≤ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü:

```python
# streaming_chat.py - ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø 02 ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤Ö‡≤®‡≥Å‡≤∏‡≤∞‡≤ø‡≤∏‡≤ø
def streaming_chat_completion(client: OpenAI, model: str, prompt: str, max_tokens: int = 300):
    """Demonstrate streaming responses for better UX."""
    try:
        print("ü§ñ Assistant (streaming):")
        
        # ‡≤∏‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Æ‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤™‡≥Ç‡≤∞‡≥ç‡≤£‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≥Å‡≤µ‡≤ø‡≤ï‡≥Ü ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
        stream = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            stream=True
        )
        
        full_response = ""
        for chunk in stream:
            if chunk.choices[0].delta.content is not None:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_response += content
        
        print("\n")  # ‡≤∏‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Æ‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤®‡≤Ç‡≤§‡≤∞ ‡≤π‡≥ä‡≤∏ ‡≤∏‡≤æ‡≤≤‡≥Å
        return full_response
    except Exception as e:
        error_msg = f"Error: {e}"
        print(error_msg)
        return error_msg


# ‡≤¨‡≤≥‡≤ï‡≥Ü ‡≤â‡≤¶‡≤æ‡≤π‡≤∞‡≤£‡≥Ü
client, model, provider = initialize_client()
prompt = "Explain the key benefits of using Microsoft Foundry Local for AI development."
response = streaming_chat_completion(client, model, prompt)
```

### ‡≤¨‡≤π‡≥Å-‡≤§‡≤ø‡≤∞‡≥Å‡≤µ‡≥Å ‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≥Ü

```python
# conversation_manager.py
class ConversationManager:
    """Manages multi-turn conversations with context preservation."""
    
    def __init__(self, client: OpenAI, model: str, system_prompt: str = None):
        self.client = client
        self.model = model
        self.messages = []
        
        if system_prompt:
            self.messages.append({"role": "system", "content": system_prompt})
    
    def send_message(self, user_message: str, max_tokens: int = 200, stream: bool = False):
        """Send a message and get response while maintaining context."""
        # ‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü‡≤ó‡≥Ü ‡≤¨‡≤≥‡≤ï‡≥Ü‡≤¶‡≤æ‡≤∞ ‡≤∏‡≤Ç‡≤¶‡≥á‡≤∂‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≥á‡≤∞‡≤ø‡≤∏‡≤ø
        self.messages.append({"role": "user", "content": user_message})
        
        try:
            if stream:
                return self._stream_response(max_tokens)
            else:
                return self._regular_response(max_tokens)
        except Exception as e:
            return f"Error: {e}"
    
    def _regular_response(self, max_tokens: int):
        """Get regular (non-streaming) response."""
        response = self.client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            max_tokens=max_tokens
        )
        
        assistant_message = response.choices[0].message.content
        self.messages.append({"role": "assistant", "content": assistant_message})
        return assistant_message
    
    def _stream_response(self, max_tokens: int):
        """Get streaming response."""
        stream = self.client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            max_tokens=max_tokens,
            stream=True
        )
        
        full_response = ""
        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                print(content, end="", flush=True)
                full_response += content
        
        print()  # ‡≤π‡≥ä‡≤∏ ‡≤∏‡≤æ‡≤≤‡≥Å
        self.messages.append({"role": "assistant", "content": full_response})
        return full_response
    
    def get_conversation_length(self) -> int:
        """Get the number of messages in the conversation."""
        return len(self.messages)
    
    def clear_conversation(self, keep_system: bool = True):
        """Clear conversation history."""
        if keep_system and self.messages and self.messages[0]["role"] == "system":
            self.messages = [self.messages[0]]
        else:
            self.messages = []


# ‡≤â‡≤¶‡≤æ‡≤π‡≤∞‡≤£‡≥Ü‡≤Ø ‡≤¨‡≤≥‡≤ï‡≥Ü
client, model, provider = initialize_client()
system_prompt = "You are a helpful AI assistant specialized in explaining AI and machine learning concepts."
conversation = ConversationManager(client, model, system_prompt)

# ‡≤¨‡≤π‡≥Å-‡≤§‡≤ø‡≤∞‡≥Å‡≤µ‡≥Å ‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü
conversation_turns = [
    "What is the difference between AI inference on-device vs in the cloud?",
    "Which approach is better for privacy?",
    "What about performance and latency considerations?"
]

for i, turn in enumerate(conversation_turns, 1):
    print(f"\nTurn {i}: {turn}")
    response = conversation.send_message(turn, stream=True)
```

## ‡≤≠‡≤æ‡≤ó 3: ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü ‡≤Æ‡≥å‡≤≤‡≥ç‡≤Ø‡≤Æ‡≤æ‡≤™‡≤® ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤µ‡≤ø‡≤∂‡≥ç‡≤≤‡≥á‡≤∑‡≤£‡≥Ü

### ‡≤™‡≥ç‡≤∞‡≤§‡≤ø‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≥Ü ‡≤∏‡≤Æ‡≤Ø‡≤¶ ‡≤Ö‡≤≥‡≥Ü‡≤Ø‡≥Å‡≤µ‡≤ø‡≤ï‡≥Ü

‡≤µ‡≤ø‡≤≠‡≤ø‡≤®‡≥ç‡≤® ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≥Ü‡≤ó‡≤≥ ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤Ö‡≤≥‡≥Ü‡≤Ø‡≤ø‡≤∞‡≤ø ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤π‡≥ã‡≤≤‡≤ø‡≤∏‡≤ø:

```python
# performance_benchmark.py - ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø 02 ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≥Å
import time
from typing import Dict, List
from openai import OpenAI

def benchmark_response_time(client: OpenAI, model: str, prompt: str, iterations: int = 3) -> Dict:
    """Benchmark response time for a given prompt."""
    times = []
    responses = []
    
    for i in range(iterations):
        start_time = time.time()
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=50  # ‡≤∏‡≤Æ‡≤Ø‡≤ï‡≥ç‡≤ï‡≤æ‡≤ó‡≤ø ‡≤™‡≥ç‡≤∞‡≤§‡≤ø‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≤Ç‡≤ï‡≥ç‡≤∑‡≤ø‡≤™‡≥ç‡≤§‡≤µ‡≤æ‡≤ó‡≤ø‡≤∞‡≤ø‡≤∏‡≤ø
            )
            
            end_time = time.time()
            response_time = end_time - start_time
            
            times.append(response_time)
            responses.append(response.choices[0].message.content)
            
        except Exception as e:
            print(f"Error in iteration {i+1}: {e}")
    
    if times:
        return {
            "average_time": sum(times) / len(times),
            "min_time": min(times),
            "max_time": max(times),
            "all_times": times,
            "sample_response": responses[0] if responses else None,
            "success_rate": len(times) / iterations * 100
        }
    
    return {"error": "No successful responses"}


def compare_providers(foundry_client: OpenAI, foundry_model: str, 
                     azure_client: OpenAI, azure_model: str, test_prompts: List[str]):
    """Compare performance between Foundry Local and Azure OpenAI."""
    results = {
        "foundry_local": [],
        "azure_openai": []
    }
    
    for prompt in test_prompts:
        print(f"\nTesting prompt: '{prompt}'")
        
        # ‡≤´‡≥å‡≤Ç‡≤°‡≥ç‡≤∞‡≤ø ‡≤∏‡≥ç‡≤•‡≤≥‡≥Ä‡≤Ø ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü
        foundry_result = benchmark_response_time(foundry_client, foundry_model, prompt)
        results["foundry_local"].append({
            "prompt": prompt,
            "benchmark": foundry_result
        })
        
        # ‡≤Ö‡≤ú‡≥Ç‡≤∞‡≥ç ‡≤ì‡≤™‡≤®‡≥ç‚Äå‡≤é‡≤ê ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü
        azure_result = benchmark_response_time(azure_client, azure_model, prompt)
        results["azure_openai"].append({
            "prompt": prompt,
            "benchmark": azure_result
        })
        
        # ‡≤´‡≤≤‡≤ø‡≤§‡≤æ‡≤Ç‡≤∂‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤π‡≥ã‡≤≤‡≤ø‡≤∏‡≤ø
        if "error" not in foundry_result and "error" not in azure_result:
            foundry_time = foundry_result["average_time"]
            azure_time = azure_result["average_time"]
            
            print(f"  Foundry Local: {foundry_time:.2f}s")
            print(f"  Azure OpenAI: {azure_time:.2f}s")
            print(f"  Winner: {'Foundry Local' if foundry_time < azure_time else 'Azure OpenAI'}")
    
    return results


# ‡≤â‡≤¶‡≤æ‡≤π‡≤∞‡≤£‡≥Ü‡≤Ø ‡≤¨‡≤≥‡≤ï‡≥Ü
benchmark_prompts = [
    "What is AI?",
    "Explain machine learning in simple terms.",
    "List 3 benefits of edge computing."
]

# ‡≤ï‡≥ç‡≤≤‡≥à‡≤Ç‡≤ü‡≥ç‚Äå‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≥ç‡≤∞‡≤æ‡≤∞‡≤Ç‡≤≠‡≤ø‡≤∏‡≤ø
foundry_client, foundry_model, _ = initialize_client()
# azure_client, azure_model = create_azure_client()  # ‡≤Ö‡≤ú‡≥Ç‡≤∞‡≥ç ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤ø‡≤∏‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü ‡≤é‡≤Ç‡≤¶‡≤æ‡≤¶‡≤∞‡≥Ü ‡≤ï‡≤æ‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥ç ‡≤§‡≥Ü‡≤ó‡≥Ü‡≤¶‡≥Å‡≤π‡≤æ‡≤ï‡≤ø

for prompt in benchmark_prompts:
    print(f"\nüìù Benchmarking: '{prompt}'")
    result = benchmark_response_time(foundry_client, foundry_model, prompt)
    
    if "error" not in result:
        print(f"   ‚è∞ Average time: {result['average_time']:.2f}s")
        print(f"   ‚ö° Fastest: {result['min_time']:.2f}s")
        print(f"   üêå Slowest: {result['max_time']:.2f}s")
        print(f"   ‚úÖ Success rate: {result['success_rate']:.1f}%")
```

### ‡≤§‡≤æ‡≤™‡≤Æ‡≤æ‡≤® ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤™‡≤∞‡≤ø‡≤Æ‡≤æ‡≤£ ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü

```python
# parameter_testing.py
def test_temperature_effects(client: OpenAI, model: str, prompt: str):
    """Test how different temperature values affect responses."""
    temperatures = [0.1, 0.5, 0.9]
    
    print(f"Testing prompt: '{prompt}'")
    print("=" * 60)
    
    for temp in temperatures:
        print(f"\nüå°Ô∏è Temperature: {temp}")
        print("-" * 30)
        
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=100,
                temperature=temp
            )
            
            print(f"Response: {response.choices[0].message.content[:150]}...")
            
        except Exception as e:
            print(f"Error with temperature {temp}: {e}")


# ‡≤∏‡≥É‡≤ú‡≤®‡≤æ‡≤§‡≥ç‡≤Æ‡≤ï ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤µ‡≤ø‡≤∂‡≥ç‡≤≤‡≥á‡≤∑‡≤£‡≤æ‡≤§‡≥ç‡≤Æ‡≤ï ‡≤™‡≥ç‡≤∞‡≤æ‡≤Ç‡≤™‡≥ç‡≤ü‡≥ç‚Äå‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø
creative_prompt = "Write a creative short story about AI."
analytical_prompt = "Explain the technical differences between GPT and BERT models."

test_temperature_effects(foundry_client, foundry_model, creative_prompt)
test_temperature_effects(foundry_client, foundry_model, analytical_prompt)
```

## ‡≤≠‡≤æ‡≤ó 4: ‡≤∏‡≥á‡≤µ‡≤æ ‡≤Ü‡≤∞‡≥ã‡≤ó‡≥ç‡≤Ø ‡≤Æ‡≥á‡≤≤‡≥ç‡≤µ‡≤ø‡≤ö‡≤æ‡≤∞‡≤£‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤∞‡≥ã‡≤ó‡≤®‡≤ø‡≤∞‡≥ç‡≤£‡≤Ø

### ‡≤∏‡≤Æ‡≤ó‡≥ç‡≤∞ ‡≤Ü‡≤∞‡≥ã‡≤ó‡≥ç‡≤Ø ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤®‡≤æ ‡≤µ‡≥ç‡≤Ø‡≤µ‡≤∏‡≥ç‡≤•‡≥Ü

```python
# health_monitoring.py - ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø 02 ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≥Å
def comprehensive_health_check(client: OpenAI, model: str, provider: str) -> Dict:
    """Perform comprehensive health check of the AI service."""
    print("üè• Comprehensive Health Check")
    print("=" * 50)
    
    health_results = {
        "provider": provider,
        "model": model,
        "timestamp": time.time(),
        "tests": {}
    }
    
    # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü 1: ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø ‡≤™‡≤ü‡≥ç‡≤ü‡≤ø
    try:
        models_response = client.models.list()
        available_models = [m.id for m in models_response.data]
        health_results["tests"]["model_listing"] = {
            "status": "success",
            "available_models": available_models,
            "current_model_available": model in available_models
        }
        print(f"‚úÖ Model listing: SUCCESS ({len(available_models)} models)")
    except Exception as e:
        health_results["tests"]["model_listing"] = {
            "status": "failed",
            "error": str(e)
        }
        print(f"‚ùå Model listing: FAILED - {e}")
    
    # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü 2: ‡≤Æ‡≥Ç‡≤≤ ‡≤™‡≥Ç‡≤∞‡≥ç‡≤£‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≥Å‡≤µ‡≤ø‡≤ï‡≥Ü
    try:
        start_time = time.time()
        test_response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": "Say 'Health check successful'"}],
            max_tokens=10
        )
        response_time = time.time() - start_time
        
        health_results["tests"]["basic_completion"] = {
            "status": "success",
            "response_time": response_time,
            "response": test_response.choices[0].message.content
        }
        print(f"‚úÖ Basic completion: SUCCESS ({response_time:.2f}s)")
    except Exception as e:
        health_results["tests"]["basic_completion"] = {
            "status": "failed",
            "error": str(e)
        }
        print(f"‚ùå Basic completion: FAILED - {e}")
    
    # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü 3: ‡≤∏‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Æ‡≤ø‡≤Ç‡≤ó‡≥ç
    try:
        start_time = time.time()
        stream = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": "Count to 3"}],
            max_tokens=20,
            stream=True
        )
        
        stream_content = ""
        chunk_count = 0
        for chunk in stream:
            if chunk.choices[0].delta.content:
                stream_content += chunk.choices[0].delta.content
                chunk_count += 1
        
        streaming_time = time.time() - start_time
        
        health_results["tests"]["streaming"] = {
            "status": "success",
            "response_time": streaming_time,
            "chunks_received": chunk_count,
            "content": stream_content.strip()
        }
        print(f"‚úÖ Streaming: SUCCESS ({streaming_time:.2f}s, {chunk_count} chunks)")
    except Exception as e:
        health_results["tests"]["streaming"] = {
            "status": "failed",
            "error": str(e)
        }
        print(f"‚ùå Streaming: FAILED - {e}")
    
    # ‡≤í‡≤ü‡≥ç‡≤ü‡≥Å ‡≤Ü‡≤∞‡≥ã‡≤ó‡≥ç‡≤Ø ‡≤Ö‡≤Ç‡≤ï‡≥Ü
    successful_tests = sum(1 for test in health_results["tests"].values() if test["status"] == "success")
    total_tests = len(health_results["tests"])
    health_score = (successful_tests / total_tests) * 100
    
    health_results["overall_health"] = {
        "score": health_score,
        "successful_tests": successful_tests,
        "total_tests": total_tests,
        "status": "healthy" if health_score >= 70 else "degraded" if health_score >= 30 else "unhealthy"
    }
    
    print(f"\nüìä Overall Health: {health_score:.1f}% ({health_results['overall_health']['status'].upper()})")
    
    return health_results


# ‡≤¨‡≤≥‡≤ï‡≥Ü ‡≤â‡≤¶‡≤æ‡≤π‡≤∞‡≤£‡≥Ü
client, model, provider = initialize_client()
health_status = comprehensive_health_check(client, model, provider)
```

### ‡≤™‡≤∞‡≤ø‡≤∏‡≤∞ ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≤æ ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤ï

```python
# config_validator.py
def validate_environment_configuration() -> Dict:
    """Validate environment configuration for both providers."""
    validation_results = {
        "foundry_local": {},
        "azure_openai": {},
        "recommendations": []
    }
    
    # ‡≤´‡≥å‡≤Ç‡≤°‡≥ç‡≤∞‡≤ø ‡≤≤‡≥ã‡≤ï‡≤≤‡≥ç ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤ø‡≤∏‡≤ø
    foundry_sdk_available = FOUNDRY_SDK_AVAILABLE
    base_url = os.environ.get("BASE_URL", "http://localhost:8000")
    
    validation_results["foundry_local"] = {
        "sdk_available": foundry_sdk_available,
        "base_url": base_url,
        "model": os.environ.get("MODEL", "phi-4-mini"),
        "api_key": bool(os.environ.get("API_KEY"))
    }
    
    if not foundry_sdk_available:
        validation_results["recommendations"].append(
            "Install Foundry Local SDK: pip install foundry-local-sdk"
        )
    
    # ‡≤Ö‡≤ú‡≥Ç‡≤∞‡≥ç ‡≤ì‡≤™‡≤®‡≥ç‚Äå‡≤é‡≤ê ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤ø‡≤∏‡≤ø
    azure_endpoint = os.environ.get("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.environ.get("AZURE_OPENAI_API_KEY")
    azure_api_version = os.environ.get("AZURE_OPENAI_API_VERSION")
    
    validation_results["azure_openai"] = {
        "endpoint_configured": bool(azure_endpoint),
        "api_key_configured": bool(azure_api_key),
        "api_version": azure_api_version or "2024-08-01-preview",
        "model": os.environ.get("MODEL", "your-deployment-name")
    }
    
    if azure_endpoint and not azure_api_key:
        validation_results["recommendations"].append(
            "Azure endpoint is set but API key is missing"
        )
    
    # ‡≤í‡≤ü‡≥ç‡≤ü‡≥Å ‡≤Æ‡≥å‡≤≤‡≥ç‡≤Ø‡≤Æ‡≤æ‡≤™‡≤®
    can_use_foundry = foundry_sdk_available or base_url
    can_use_azure = azure_endpoint and azure_api_key
    
    if not can_use_foundry and not can_use_azure:
        validation_results["recommendations"].append(
            "No valid configuration found. Set up either Foundry Local or Azure OpenAI."
        )
    
    validation_results["summary"] = {
        "foundry_ready": can_use_foundry,
        "azure_ready": can_use_azure,
        "total_options": sum([can_use_foundry, can_use_azure])
    }
    
    return validation_results


# ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≤æ ‡≤∏‡≥ç‡≤•‡≤ø‡≤§‡≤ø‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≥ç‡≤∞‡≤¶‡≤∞‡≥ç‡≤∂‡≤ø‡≤∏‡≤ø
config_status = validate_environment_configuration()
print("‚öôÔ∏è Environment Configuration Status")
print("=" * 40)
print(f"üè† Foundry Local Ready: {'‚úÖ' if config_status['summary']['foundry_ready'] else '‚ùå'}")
print(f"üåê Azure OpenAI Ready: {'‚úÖ' if config_status['summary']['azure_ready'] else '‚ùå'}")
print(f"üìã Available Options: {config_status['summary']['total_options']}")

if config_status["recommendations"]:
    print("\nüí° Recommendations:")
    for rec in config_status["recommendations"]:
        print(f"   ‚Ä¢ {rec}")
```

## ‡≤≠‡≤æ‡≤ó 5: ‡≤™‡≤∞‡≤ø‡≤∏‡≤∞ ‡≤ö‡≤∞‡≤ó‡≤≥‡≥Å ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≤æ ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≥Ü

### ‡≤™‡≤∞‡≤ø‡≤∏‡≤∞ ‡≤ö‡≤∞‡≤ó‡≤≥ ‡≤â‡≤≤‡≥ç‡≤≤‡≥á‡≤ñ

‡≤é‡≤∞‡≤°‡≥Å ‡≤™‡≥ç‡≤∞‡≤¶‡≤æ‡≤§‡≤∞‡≤®‡≥ç‡≤®‡≥Ç ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤ø‡≤∏‡≤≤‡≥Å ‡≤∏‡≤Ç‡≤™‡≥Ç‡≤∞‡≥ç‡≤£ ‡≤â‡≤≤‡≥ç‡≤≤‡≥á‡≤ñ:

```python
# config_reference.py - ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø 02 ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≥Å
import os
from typing import Dict, Optional

class ConfigurationManager:
    """Manages environment configuration for multi-provider setup."""
    
    @staticmethod
    def get_foundry_config() -> Dict[str, Optional[str]]:
        """Get Foundry Local configuration from environment."""
        return {
            "MODEL": os.environ.get("MODEL", "phi-4-mini"),
            "BASE_URL": os.environ.get("BASE_URL", "http://localhost:8000"),
            "API_KEY": os.environ.get("API_KEY", ""),
        }
    
    @staticmethod
    def get_azure_config() -> Dict[str, Optional[str]]:
        """Get Azure OpenAI configuration from environment."""
        return {
            "AZURE_OPENAI_ENDPOINT": os.environ.get("AZURE_OPENAI_ENDPOINT"),
            "AZURE_OPENAI_API_KEY": os.environ.get("AZURE_OPENAI_API_KEY"),
            "AZURE_OPENAI_API_VERSION": os.environ.get("AZURE_OPENAI_API_VERSION", "2024-08-01-preview"),
            "MODEL": os.environ.get("MODEL", "your-deployment-name"),
        }
    
    @staticmethod
    def display_current_config():
        """Display current configuration status."""
        print("‚öôÔ∏è Current Configuration")
        print("=" * 40)
        
        foundry_config = ConfigurationManager.get_foundry_config()
        azure_config = ConfigurationManager.get_azure_config()
        
        print("üè† Foundry Local:")
        for key, value in foundry_config.items():
            display_value = value if value else "(not set)"
            if key == "API_KEY" and value:
                display_value = "***" + value[-4:] if len(value) > 4 else "***"
            print(f"   {key}: {display_value}")
        
        print("\nüåê Azure OpenAI:")
        for key, value in azure_config.items():
            display_value = value if value else "(not set)"
            if "KEY" in key and value:
                display_value = "***" + value[-4:] if len(value) > 4 else "***"
            print(f"   {key}: {display_value}")
        
        # ‡≤∏‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø ‡≤™‡≥Ç‡≤∞‡≥à‡≤ï‡≥Ü‡≤¶‡≤æ‡≤∞‡≤®‡≤®‡≥ç‡≤®‡≥Å ‡≤®‡≤ø‡≤∞‡≥ç‡≤ß‡≤∞‡≤ø‡≤∏‡≤ø
        azure_ready = azure_config["AZURE_OPENAI_ENDPOINT"] and azure_config["AZURE_OPENAI_API_KEY"]
        foundry_ready = True  # ‡≤´‡≥å‡≤Ç‡≤°‡≥ç‡≤∞‡≤ø ‡≤Ø‡≤æ‡≤µ‡≤æ‡≤ó‡≤≤‡≥Ç ‡≤°‡≥Ä‡≤´‡≤æ‡≤≤‡≥ç‡≤ü‡≥ç‚Äå‡≤ó‡≤≥‡≤ø‡≤ó‡≥Ü ‡≤π‡≤ø‡≤Ç‡≤§‡≤ø‡≤∞‡≥Å‡≤ó‡≤¨‡≤π‡≥Å‡≤¶‡≥Å
        
        print(f"\nüìä Provider Status:")
        print(f"   Azure OpenAI: {'‚úÖ Ready' if azure_ready else '‚ùå Not configured'}")
        print(f"   Foundry Local: {'‚úÖ Ready' if foundry_ready else '‚ùå Not available'}")
        print(f"   Active: {'Azure OpenAI' if azure_ready else 'Foundry Local'}")


# ‡≤™‡≥ç‡≤∞‡≤∏‡≥ç‡≤§‡≥Å‡≤§ ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≥ç‡≤∞‡≤¶‡≤∞‡≥ç‡≤∂‡≤ø‡≤∏‡≤ø
config_manager = ConfigurationManager()
config_manager.display_current_config()
```

### ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≤æ ‡≤â‡≤¶‡≤æ‡≤π‡≤∞‡≤£‡≥Ü‡≤ó‡≤≥‡≥Å

**Windows ‡≤ï‡≤Æ‡≤æ‡≤Ç‡≤°‡≥ç ‡≤™‡≥ç‡≤∞‡≤æ‡≤Ç‡≤™‡≥ç‡≤ü‡≥ç ‡≤∏‡≥Ü‡≤ü‡≤™‡≥ç:**

```cmd
REM Foundry Local configuration
set MODEL=phi-4-mini
set BASE_URL=http://localhost:8000
set API_KEY=

REM Azure OpenAI configuration (alternative)
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key-here
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name

REM Run the sample
python samples\02\sdk_quickstart.py
```

**PowerShell ‡≤∏‡≥Ü‡≤ü‡≤™‡≥ç:**

```powershell
# ‡≤´‡≥å‡≤Ç‡≤°‡≥ç‡≤∞‡≤ø ‡≤∏‡≥ç‡≤•‡≤≥‡≥Ä‡≤Ø ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≥Ü
$env:MODEL = "phi-4-mini"
$env:BASE_URL = "http://localhost:8000"
$env:API_KEY = ""

# ‡≤Ö‡≤ú‡≥Ç‡≤∞‡≥ç ‡≤ì‡≤™‡≤®‡≥ç‚Äå‡≤é‡≤ê ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≥Ü (‡≤µ‡≥à‡≤ï‡≤≤‡≥ç‡≤™‡≤ø‡≤ï)
$env:AZURE_OPENAI_ENDPOINT = "https://your-resource.openai.azure.com"
$env:AZURE_OPENAI_API_KEY = "your-api-key-here"
$env:AZURE_OPENAI_API_VERSION = "2024-08-01-preview"
$env:MODEL = "your-deployment-name"

# ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤ö‡≤æ‡≤≤‡≤®‡≥Ü ‡≤Æ‡≤æ‡≤°‡≤ø
python samples/02/sdk_quickstart.py
```

## ‡≤≠‡≤æ‡≤ó 6: ‡≤ï‡≥à‡≤Ø‡≤≤‡≥ç‡≤≤‡≤ø ‡≤Ö‡≤≠‡≥ç‡≤Ø‡≤æ‡≤∏‡≤ó‡≤≥‡≥Å

### ‡≤Ö‡≤≠‡≥ç‡≤Ø‡≤æ‡≤∏ 1: ‡≤¨‡≤π‡≥Å-‡≤™‡≥ç‡≤∞‡≤¶‡≤æ‡≤§ SDK ‡≤è‡≤ï‡≥Ä‡≤ï‡≤∞‡≤£

‡≤™‡≥ç‡≤∞‡≤¶‡≤æ‡≤§‡≤∞ ‡≤®‡≤°‡≥Å‡≤µ‡≥Ü ‡≤∏‡≥å‡≤ï‡≤∞‡≥ç‡≤Ø‡≤µ‡≤æ‡≤ó‡≤ø ‡≤¨‡≤¶‡≤≤‡≤æ‡≤Ø‡≤ø‡≤∏‡≥Å‡≤µ ‡≤∏‡≤Ç‡≤™‡≥Ç‡≤∞‡≥ç‡≤£ ‡≤Ö‡≤™‡≥ç‡≤≤‡≤ø‡≤ï‡≥á‡≤∂‡≤®‡≥ç ‡≤®‡≤ø‡≤∞‡≥ç‡≤Æ‡≤ø‡≤∏‡≤ø:

```python
# exercise_1_multi_provider.py
from openai import OpenAI
from typing import Tuple, Dict, Any
import time

class MultiProviderSDKDemo:
    """Demonstrates seamless switching between Foundry Local and Azure OpenAI."""
    
    def __init__(self):
        self.clients = {}
        self.models = {}
        self.setup_clients()
    
    def setup_clients(self):
        """Initialize all available clients."""
        # ‡≤´‡≥å‡≤Ç‡≤°‡≥ç‡≤∞‡≤ø ‡≤≤‡≥ã‡≤ï‡≤≤‡≥ç ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≥ç‡≤∞‡≤æ‡≤∞‡≤Ç‡≤≠‡≤ø‡≤∏‡≤≤‡≥Å ‡≤™‡≥ç‡≤∞‡≤Ø‡≤§‡≥ç‡≤®‡≤ø‡≤∏‡≤ø
        try:
            foundry_client, foundry_model, _ = initialize_client()
            self.clients["foundry"] = foundry_client
            self.models["foundry"] = foundry_model
            print("‚úÖ Foundry Local client ready")
        except Exception as e:
            print(f"‚ùå Foundry Local setup failed: {e}")
        
        # ‡≤Ö‡≤ú‡≥Ç‡≤∞‡≥ç ‡≤ì‡≤™‡≤®‡≥ç‚Äå‡≤é‡≤ê ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≥ç‡≤∞‡≤æ‡≤∞‡≤Ç‡≤≠‡≤ø‡≤∏‡≤≤‡≥Å ‡≤™‡≥ç‡≤∞‡≤Ø‡≤§‡≥ç‡≤®‡≤ø‡≤∏‡≤ø
        try:
            if os.environ.get("AZURE_OPENAI_ENDPOINT") and os.environ.get("AZURE_OPENAI_API_KEY"):
                azure_client, azure_model = create_azure_client()
                self.clients["azure"] = azure_client
                self.models["azure"] = azure_model
                print("‚úÖ Azure OpenAI client ready")
        except Exception as e:
            print(f"‚ùå Azure OpenAI setup failed: {e}")
    
    def compare_providers(self, prompt: str, max_tokens: int = 100) -> Dict[str, Any]:
        """Compare responses from all available providers."""
        results = {}
        
        for provider_name, client in self.clients.items():
            model = self.models[provider_name]
            print(f"\nTesting {provider_name} ({model})...")
            
            start_time = time.time()
            try:
                response = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens
                )
                
                response_time = time.time() - start_time
                
                results[provider_name] = {
                    "model": model,
                    "response": response.choices[0].message.content,
                    "response_time": response_time,
                    "status": "success"
                }
                
                print(f"   ‚úÖ Success ({response_time:.2f}s)")
                
            except Exception as e:
                results[provider_name] = {
                    "model": model,
                    "error": str(e),
                    "status": "failed"
                }
                print(f"   ‚ùå Failed: {e}")
        
        return results
    
    def streaming_comparison(self, prompt: str, max_tokens: int = 150):
        """Compare streaming responses from providers."""
        for provider_name, client in self.clients.items():
            model = self.models[provider_name]
            print(f"\nüåä Streaming from {provider_name} ({model}):")
            print("-" * 50)
            
            try:
                stream = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    stream=True
                )
                
                for chunk in stream:
                    if chunk.choices[0].delta.content:
                        print(chunk.choices[0].delta.content, end="", flush=True)
                
                print("\n")
                
            except Exception as e:
                print(f"Streaming failed: {e}")


# ‡≤µ‡≥ç‡≤Ø‡≤æ‡≤Ø‡≤æ‡≤Æ 1 ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å ‡≤ö‡≤æ‡≤≤‡≤®‡≥Ü ‡≤Æ‡≤æ‡≤°‡≤ø
exercise_1 = MultiProviderSDKDemo()

test_prompt = "Explain the benefits of running AI models locally versus in the cloud."
print(f"üó∫Ô∏è Exercise 1: Multi-Provider Comparison")
print(f"Prompt: {test_prompt}")
print("=" * 60)

comparison_results = exercise_1.compare_providers(test_prompt)
exercise_1.streaming_comparison(test_prompt)
```

### ‡≤Ö‡≤≠‡≥ç‡≤Ø‡≤æ‡≤∏ 2: ‡≤â‡≤®‡≥ç‡≤®‡≤§ ‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≥Ü

```python
# exercise_2_conversation.py
class AdvancedConversationManager:
    """Advanced conversation management with multiple features."""
    
    def __init__(self, client: OpenAI, model: str):
        self.client = client
        self.model = model
        self.conversations = {}  # ‡≤¨‡≤π‡≥Å ‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü ‡≤Ö‡≤ß‡≤ø‡≤µ‡≥á‡≤∂‡≤®‡≤ó‡≤≥‡≥Å
    
    def create_conversation(self, session_id: str, system_prompt: str = None) -> str:
        """Create a new conversation session."""
        self.conversations[session_id] = {
            "messages": [],
            "created_at": time.time(),
            "message_count": 0
        }
        
        if system_prompt:
            self.conversations[session_id]["messages"].append({
                "role": "system", 
                "content": system_prompt
            })
        
        return f"Conversation {session_id} created"
    
    def send_message(self, session_id: str, message: str, 
                    temperature: float = 0.7, max_tokens: int = 200) -> Dict[str, Any]:
        """Send message in a specific conversation session."""
        if session_id not in self.conversations:
            return {"error": f"Conversation {session_id} not found"}
        
        conversation = self.conversations[session_id]
        conversation["messages"].append({"role": "user", "content": message})
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=conversation["messages"],
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            assistant_message = response.choices[0].message.content
            conversation["messages"].append({
                "role": "assistant", 
                "content": assistant_message
            })
            conversation["message_count"] += 2  # ‡≤¨‡≤≥‡≤ï‡≥Ü‡≤¶‡≤æ‡≤∞ + ‡≤∏‡≤π‡≤æ‡≤Ø‡≤ï
            
            return {
                "session_id": session_id,
                "response": assistant_message,
                "message_count": conversation["message_count"],
                "status": "success"
            }
            
        except Exception as e:
            return {"error": str(e), "session_id": session_id}
    
    def get_conversation_summary(self, session_id: str) -> Dict[str, Any]:
        """Get summary of conversation session."""
        if session_id not in self.conversations:
            return {"error": f"Conversation {session_id} not found"}
        
        conversation = self.conversations[session_id]
        
        return {
            "session_id": session_id,
            "message_count": conversation["message_count"],
            "created_at": conversation["created_at"],
            "duration": time.time() - conversation["created_at"],
            "has_system_prompt": len(conversation["messages"]) > 0 and 
                               conversation["messages"][0]["role"] == "system"
        }
    
    def export_conversation(self, session_id: str) -> str:
        """Export conversation as formatted text."""
        if session_id not in self.conversations:
            return f"Conversation {session_id} not found"
        
        conversation = self.conversations[session_id]
        export_text = f"Conversation Export: {session_id}\n"
        export_text += "=" * 50 + "\n\n"
        
        for msg in conversation["messages"]:
            role = msg["role"].title()
            content = msg["content"]
            export_text += f"{role}: {content}\n\n"
        
        return export_text


# ‡≤µ‡≥ç‡≤Ø‡≤æ‡≤Ø‡≤æ‡≤Æ 2 ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å ‡≤ö‡≤æ‡≤≤‡≤®‡≥Ü ‡≤Æ‡≤æ‡≤°‡≤ø
client, model, provider = initialize_client()
conv_manager = AdvancedConversationManager(client, model)

# ‡≤¨‡≤π‡≥Å ‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü ‡≤Ö‡≤ß‡≤ø‡≤µ‡≥á‡≤∂‡≤®‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤∞‡≤ö‡≤ø‡≤∏‡≤ø
print("üí¨ Exercise 2: Advanced Conversation Management")
print("=" * 60)

# ‡≤§‡≤æ‡≤Ç‡≤§‡≥ç‡≤∞‡≤ø‡≤ï ‡≤ö‡≤∞‡≥ç‡≤ö‡≥Ü
conv_manager.create_conversation("tech_discussion", 
    "You are a technical expert explaining AI concepts clearly.")

# ‡≤∏‡≥É‡≤ú‡≤®‡≤æ‡≤§‡≥ç‡≤Æ‡≤ï ‡≤Ö‡≤ß‡≤ø‡≤µ‡≥á‡≤∂‡≤®
conv_manager.create_conversation("creative_session", 
    "You are a creative writing assistant helping with storytelling.")

# ‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≤ø‡≤∏‡≤ø
tech_questions = [
    "What is the difference between inference and training?",
    "How does quantization improve model performance?"
]

creative_prompts = [
    "Start a story about an AI that lives on an edge device.",
    "Continue the story with a plot twist."
]

# ‡≤§‡≤æ‡≤Ç‡≤§‡≥ç‡≤∞‡≤ø‡≤ï ‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü
print("\nüîß Technical Discussion:")
for question in tech_questions:
    result = conv_manager.send_message("tech_discussion", question)
    print(f"Q: {question}")
    print(f"A: {result['response'][:100]}...\n")

# ‡≤∏‡≥É‡≤ú‡≤®‡≤æ‡≤§‡≥ç‡≤Æ‡≤ï ‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü
print("üé® Creative Session:")
for prompt in creative_prompts:
    result = conv_manager.send_message("creative_session", prompt, temperature=0.9)
    print(f"Prompt: {prompt}")
    print(f"Response: {result['response'][:100]}...\n")

# ‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü ‡≤∏‡≤æ‡≤∞‡≤æ‡≤Ç‡≤∂‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤§‡≥ã‡≤∞‡≤ø‡≤∏‡≤ø
print("üìä Conversation Summaries:")
for session_id in conv_manager.conversations.keys():
    summary = conv_manager.get_conversation_summary(session_id)
    print(f"   {session_id}: {summary['message_count']} messages, {summary['duration']:.1f}s")
```

### ‡≤Ö‡≤≠‡≥ç‡≤Ø‡≤æ‡≤∏ 3: ‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≤æ-‡≤∏‡≤ø‡≤¶‡≥ç‡≤ß ‡≤Ü‡≤∞‡≥ã‡≤ó‡≥ç‡≤Ø ‡≤Æ‡≥á‡≤≤‡≥ç‡≤µ‡≤ø‡≤ö‡≤æ‡≤∞‡≤£‡≥Ü

```python
# exercise_3_monitoring.py
class ProductionHealthMonitor:
    """Production-ready health monitoring for AI services."""
    
    def __init__(self):
        self.health_history = []
        self.alert_thresholds = {
            "response_time": 5.0,
            "error_rate": 10.0,
            "availability": 95.0
        }
    
    def run_comprehensive_check(self, client: OpenAI, model: str, provider: str) -> Dict[str, Any]:
        """Run comprehensive health check with detailed reporting."""
        check_results = {
            "timestamp": time.time(),
            "provider": provider,
            "model": model,
            "tests": {},
            "overall_health": "unknown"
        }
        
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü 1: ‡≤Æ‡≥Ç‡≤≤ ‡≤∏‡≤Ç‡≤™‡≤∞‡≥ç‡≤ï
        connectivity_result = self._test_connectivity(client)
        check_results["tests"]["connectivity"] = connectivity_result
        
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü 2: ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø ‡≤≤‡≤≠‡≥ç‡≤Ø‡≤§‡≥Ü
        model_result = self._test_model_availability(client, model)
        check_results["tests"]["model_availability"] = model_result
        
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü 3: ‡≤™‡≥ç‡≤∞‡≤§‡≤ø‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≥Ü ‡≤∏‡≤Æ‡≤Ø ‡≤Æ‡≤æ‡≤®‡≤¶‡≤Ç‡≤°
        performance_result = self._test_performance(client, model)
        check_results["tests"]["performance"] = performance_result
        
        # ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü 4: ‡≤í‡≤§‡≥ç‡≤§‡≤° ‡≤™‡≤∞‡≥Ä‡≤ï‡≥ç‡≤∑‡≥Ü
        stress_result = self._test_stress(client, model)
        check_results["tests"]["stress_test"] = stress_result
        
        # ‡≤í‡≤ü‡≥ç‡≤ü‡≥Å ‡≤Ü‡≤∞‡≥ã‡≤ó‡≥ç‡≤Ø‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤≤‡≥Ü‡≤ï‡≥ç‡≤ï‡≤π‡≤æ‡≤ï‡≤ø
        check_results["overall_health"] = self._calculate_health_score(check_results["tests"])
        
        # ‡≤™‡≥ç‡≤∞‡≤µ‡≥É‡≤§‡≥ç‡≤§‡≤ø‡≤ó‡≤æ‡≤ó‡≤ø ‡≤∏‡≤Ç‡≤ó‡≥ç‡≤∞‡≤π‡≤ø‡≤∏‡≤ø
        self.health_history.append(check_results)
        
        return check_results
    
    def _test_connectivity(self, client: OpenAI) -> Dict[str, Any]:
        """Test basic service connectivity."""
        try:
            start_time = time.time()
            models = client.models.list()
            response_time = time.time() - start_time
            
            return {
                "status": "success",
                "response_time": response_time,
                "models_count": len(models.data)
            }
        except Exception as e:
            return {"status": "failed", "error": str(e)}
    
    def _test_model_availability(self, client: OpenAI, model: str) -> Dict[str, Any]:
        """Test specific model availability."""
        try:
            response = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": "Health check"}],
                max_tokens=5
            )
            
            return {
                "status": "success",
                "model": model,
                "response_received": bool(response.choices[0].message.content)
            }
        except Exception as e:
            return {"status": "failed", "error": str(e)}
    
    def _test_performance(self, client: OpenAI, model: str) -> Dict[str, Any]:
        """Test response time performance."""
        response_times = []
        
        for i in range(3):
            try:
                start_time = time.time()
                client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": f"Test {i+1}"}],
                    max_tokens=10
                )
                response_time = time.time() - start_time
                response_times.append(response_time)
            except Exception:
                pass
        
        if response_times:
            avg_time = sum(response_times) / len(response_times)
            return {
                "status": "success",
                "average_response_time": avg_time,
                "min_time": min(response_times),
                "max_time": max(response_times),
                "within_threshold": avg_time < self.alert_thresholds["response_time"]
            }
        else:
            return {"status": "failed", "error": "No successful responses"}
    
    def _test_stress(self, client: OpenAI, model: str) -> Dict[str, Any]:
        """Test service under concurrent requests."""
        import concurrent.futures
        
        def single_request():
            try:
                client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": "Stress test"}],
                    max_tokens=5
                )
                return True
            except Exception:
                return False
        
        # 5 ‡≤∏‡≤Æ‡≤ï‡≤æ‡≤≤‡≥Ä‡≤® ‡≤µ‡≤ø‡≤®‡≤Ç‡≤§‡≤ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤ö‡≤æ‡≤≤‡≤®‡≥Ü ‡≤Æ‡≤æ‡≤°‡≤ø
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(single_request) for _ in range(5)]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        success_rate = (sum(results) / len(results)) * 100
        
        return {
            "status": "success" if success_rate > 80 else "degraded",
            "concurrent_requests": len(results),
            "success_rate": success_rate,
            "within_threshold": success_rate >= self.alert_thresholds["availability"]
        }
    
    def _calculate_health_score(self, tests: Dict[str, Any]) -> str:
        """Calculate overall health score."""
        successful_tests = sum(1 for test in tests.values() if test["status"] == "success")
        total_tests = len(tests)
        health_percentage = (successful_tests / total_tests) * 100
        
        if health_percentage >= 90:
            return "healthy"
        elif health_percentage >= 70:
            return "degraded"
        else:
            return "unhealthy"
    
    def generate_health_report(self) -> str:
        """Generate formatted health report."""
        if not self.health_history:
            return "No health data available"
        
        latest = self.health_history[-1]
        report = f"Health Report - {time.ctime(latest['timestamp'])}\n"
        report += "=" * 60 + "\n"
        report += f"Provider: {latest['provider']}\n"
        report += f"Model: {latest['model']}\n"
        report += f"Overall Health: {latest['overall_health'].upper()}\n\n"
        
        for test_name, test_result in latest["tests"].items():
            status_icon = "‚úÖ" if test_result["status"] == "success" else "‚ùå"
            report += f"{status_icon} {test_name.replace('_', ' ').title()}: {test_result['status']}\n"
        
        return report


# ‡≤µ‡≥ç‡≤Ø‡≤æ‡≤Ø‡≤æ‡≤Æ 3 ‡≤Ö‡≤®‡≥ç‡≤®‡≥Å ‡≤ö‡≤æ‡≤≤‡≤®‡≥Ü ‡≤Æ‡≤æ‡≤°‡≤ø
client, model, provider = initialize_client()
health_monitor = ProductionHealthMonitor()

print("üè• Exercise 3: Production Health Monitoring")
print("=" * 60)

health_results = health_monitor.run_comprehensive_check(client, model, provider)
print(health_monitor.generate_health_report())
```

## ‡≤≠‡≤æ‡≤ó 7: ‡≤∏‡≤æ‡≤∞‡≤æ‡≤Ç‡≤∂ ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤Æ‡≥Å‡≤Ç‡≤¶‡≤ø‡≤® ‡≤π‡≤Ç‡≤§‡≤ó‡≤≥‡≥Å

### ‡≤∏‡≥Ü‡≤∑‡≤®‡≥ç ‡≤∏‡≤æ‡≤ß‡≤®‡≥Ü‡≤ó‡≤≥‡≥Å

‡≤à ‡≤∏‡≥Ü‡≤∑‡≤®‡≥ç‚Äå‡≤®‡≤≤‡≥ç‡≤≤‡≤ø, ‡≤®‡≥Ä‡≤µ‡≥Å ‡≤®‡≤ø‡≤™‡≥Å‡≤£‡≤∞‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Ä‡≤∞‡≤ø:
- ‚úÖ **OpenAI SDK ‡≤è‡≤ï‡≥Ä‡≤ï‡≤∞‡≤£**: Foundry Local ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å Azure OpenAI ‡≤é‡≤∞‡≤°‡≤∞‡≤ø‡≤ó‡≥Ç ‡≤â‡≤®‡≥ç‡≤®‡≤§ ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≥Å
- ‚úÖ **‡≤∏‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Æ‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤™‡≥ç‡≤∞‡≤§‡≤ø‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≥Ü‡≤ó‡≤≥‡≥Å**: ‡≤∏‡≥Å‡≤ß‡≤æ‡≤∞‡≤ø‡≤§ ‡≤¨‡≤≥‡≤ï‡≥Ü‡≤¶‡≤æ‡≤∞ ‡≤Ö‡≤®‡≥Å‡≤≠‡≤µ‡≤ï‡≥ç‡≤ï‡≤æ‡≤ó‡≤ø ‡≤∞‡≤ø‡≤Ø‡≤≤‡≥ç-‡≤ü‡≥à‡≤Æ‡≥ç ‡≤ö‡≤æ‡≤ü‡≥ç ‡≤™‡≥Ç‡≤∞‡≥ç‡≤£‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≥Å‡≤µ‡≤ø‡≤ï‡≥Ü‡≤ó‡≤≥‡≥Å
- ‚úÖ **‡≤¨‡≤π‡≥Å-‡≤™‡≥ç‡≤∞‡≤¶‡≤æ‡≤§ ‡≤¨‡≥Ü‡≤Ç‡≤¨‡≤≤**: ‡≤∏‡≥ç‡≤•‡≤≥‡≥Ä‡≤Ø ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤ï‡≥ç‡≤≤‡≥å‡≤°‡≥ç AI ‡≤∏‡≥á‡≤µ‡≥Ü‡≤ó‡≤≥ ‡≤®‡≤°‡≥Å‡≤µ‡≥Ü ‡≤∏‡≥å‡≤ï‡≤∞‡≥ç‡≤Ø‡≤™‡≥Ç‡≤∞‡≥ç‡≤£ ‡≤¨‡≤¶‡≤≤‡≤æ‡≤µ‡≤£‡≥Ü
- ‚úÖ **‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≥Ü**: ‡≤∏‡≤Ç‡≤¶‡≤∞‡≥ç‡≤≠-‡≤ú‡≤æ‡≤ó‡≤∞‡≥Ç‡≤ï ‡≤¨‡≤π‡≥Å-‡≤§‡≤ø‡≤∞‡≥Å‡≤µ‡≥Å ‡≤∏‡≤Ç‡≤≠‡≤æ‡≤∑‡≤£‡≥Ü‡≤ó‡≤≥‡≥Å
- ‚úÖ **‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü ‡≤Æ‡≥á‡≤≤‡≥ç‡≤µ‡≤ø‡≤ö‡≤æ‡≤∞‡≤£‡≥Ü**: ‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≤æ ‡≤®‡≤ø‡≤Ø‡≥ã‡≤ú‡≤®‡≥Ü‡≤ó‡≤≥‡≤ø‡≤ó‡≥Ü ‡≤Æ‡≥å‡≤≤‡≥ç‡≤Ø‡≤Æ‡≤æ‡≤™‡≤® ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤Ü‡≤∞‡≥ã‡≤ó‡≥ç‡≤Ø ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤®‡≥Ü
- ‚úÖ **‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≤æ ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≥Å**: ‡≤¨‡≤≤‡≤µ‡≤æ‡≤¶ ‡≤¶‡≥ã‡≤∑ ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≥Ü ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≤æ ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≥Ü

### ‡≤™‡≥ç‡≤∞‡≤Æ‡≥Å‡≤ñ ‡≤µ‡≤æ‡≤∏‡≥ç‡≤§‡≥Å‡≤∂‡≤ø‡≤≤‡≥ç‡≤™ ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≥Å

**‡≤ï‡≥ç‡≤≤‡≥à‡≤Ç‡≤ü‡≥ç ‡≤´‡≥ç‡≤Ø‡≤æ‡≤ï‡≥ç‡≤ü‡≤∞‡≤ø ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø:**
```
Environment Detection ‚Üí Provider Selection ‚Üí Client Creation ‚Üí Model Configuration
        ‚Üì                    ‚Üì                  ‚Üì                 ‚Üì
    Azure/Local       Azure OpenAI/        OpenAI Client    Model Selection
    Credentials       Foundry Local        Initialization   and Validation
```

**‡≤∏‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Æ‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤™‡≥ç‡≤∞‡≤§‡≤ø‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≥Ü ‡≤™‡≥ç‡≤∞‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≥Ü:**
```
User Input ‚Üí Chat Completion ‚Üí Stream Processing ‚Üí Real-time Display
     ‚Üì             ‚Üì                ‚Üì                 ‚Üì
   Prompt         Stream=True         Token Chunks      Progressive UI
```

### ‡≤â‡≤§‡≥ç‡≤§‡≤Æ ‡≤Ö‡≤≠‡≥ç‡≤Ø‡≤æ‡≤∏‡≤ó‡≤≥ ‡≤∏‡≤æ‡≤∞‡≤æ‡≤Ç‡≤∂

1. **üîÑ ‡≤∏‡≤¶‡≤æ ‡≤´‡≤æ‡≤≤‡≥ç‡≤¨‡≥ç‡≤Ø‡≤æ‡≤ï‡≥ç‚Äå‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤ú‡≤æ‡≤∞‡≤ø‡≤ó‡≥Ü ‡≤§‡≤∞‡≥Å‡≤µ‡≤ø‡≤∞‡≤ø**: Azure ‚Üí Foundry Local ‚Üí ‡≤¶‡≥ã‡≤∑ ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≥Ü
2. **üåä ‡≤¶‡≥Ä‡≤∞‡≥ç‡≤ò ‡≤™‡≥ç‡≤∞‡≤§‡≤ø‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≥Ü‡≤ó‡≤≥‡≤ø‡≤ó‡≥Ü ‡≤∏‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Æ‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤¨‡≤≥‡≤∏‡≤ø**: ‡≤â‡≤§‡≥ç‡≤§‡≤Æ ‡≤ó‡≥ç‡≤∞‡≤π‡≤£‡≥Ä‡≤Ø ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü
3. **üõ°Ô∏è ‡≤∏‡≤Æ‡≤ó‡≥ç‡≤∞ ‡≤¶‡≥ã‡≤∑ ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤ú‡≤æ‡≤∞‡≤ø‡≤ó‡≥Ü ‡≤§‡≤∞‡≥Å‡≤µ‡≤ø‡≤∞‡≤ø**: ‡≤¨‡≤≥‡≤ï‡≥Ü‡≤¶‡≤æ‡≤∞ ‡≤∏‡≥ç‡≤®‡≥á‡≤π‡≤ø ‡≤¶‡≥ã‡≤∑ ‡≤∏‡≤Ç‡≤¶‡≥á‡≤∂‡≤ó‡≤≥‡≥Å
4. **üìà ‡≤ï‡≤æ‡≤∞‡≥ç‡≤Ø‡≤ï‡≥ç‡≤∑‡≤Æ‡≤§‡≥Ü‡≤Ø‡≤®‡≥ç‡≤®‡≥Å ‡≤Æ‡≥á‡≤≤‡≥ç‡≤µ‡≤ø‡≤ö‡≤æ‡≤∞‡≤£‡≥Ü ‡≤Æ‡≤æ‡≤°‡≤ø‡≤∞‡≤ø**: ‡≤™‡≥ç‡≤∞‡≤§‡≤ø‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≥Ü ‡≤∏‡≤Æ‡≤Ø ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤Ø‡≤∂‡≤∏‡≥ç‡≤∏‡≤ø‡≤® ‡≤¶‡≤∞‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤ü‡≥ç‡≤∞‡≥ç‡≤Ø‡≤æ‡≤ï‡≥ç ‡≤Æ‡≤æ‡≤°‡≤ø‡≤∞‡≤ø
5. **‚öôÔ∏è ‡≤™‡≤∞‡≤ø‡≤∏‡≤∞ ‡≤Ü‡≤ß‡≤æ‡≤∞‡≤ø‡≤§ ‡≤∏‡≤Ç‡≤∞‡≤ö‡≤®‡≥Ü**: ‡≤Ö‡≤≠‡≤ø‡≤µ‡≥É‡≤¶‡≥ç‡≤ß‡≤ø/‡≤∏‡≥ç‡≤ü‡≥á‡≤ú‡≤ø‡≤Ç‡≤ó‡≥ç/‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≥Ü ‡≤®‡≤°‡≥Å‡≤µ‡≥Ü ‡≤∏‡≥Å‡≤≤‡≤≠ ‡≤¨‡≤¶‡≤≤‡≤æ‡≤µ‡≤£‡≥Ü
6. **üîí ‡≤≠‡≤¶‡≥ç‡≤∞‡≤§‡≤æ ‡≤™‡≥ç‡≤∞‡≤Æ‡≤æ‡≤£‡≤™‡≤§‡≥ç‡≤∞ ‡≤®‡≤ø‡≤∞‡≥ç‡≤µ‡≤π‡≤£‡≥Ü**: API ‡≤ï‡≥Ä‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤é‡≤Ç‡≤¶‡≤ø‡≤ó‡≥Ç ‡≤π‡≤æ‡≤∞‡≥ç‡≤°‡≥ç‚Äå‡≤ï‡≥ã‡≤°‡≥ç ‡≤Æ‡≤æ‡≤°‡≤¨‡≥á‡≤°‡≤ø

### ‡≤™‡≥ç‡≤∞‡≤¶‡≤æ‡≤§‡≤∞ ‡≤Ü‡≤Ø‡≥ç‡≤ï‡≥Ü ‡≤Æ‡≤æ‡≤∞‡≥ç‡≤ó‡≤¶‡≤∞‡≥ç‡≤∂‡≤ø‡≤ó‡≤≥‡≥Å

| ‡≤∏‡≤Ç‡≤¶‡≤∞‡≥ç‡≤≠ | ‡≤∂‡≤ø‡≤´‡≤æ‡≤∞‡≤∏‡≥Å ‡≤Æ‡≤æ‡≤°‡≤ø‡≤¶ ‡≤™‡≥ç‡≤∞‡≤¶‡≤æ‡≤§ | ‡≤ï‡≤æ‡≤∞‡≤£ |
|----------|---------------------|----------|
| **‡≤Ö‡≤≠‡≤ø‡≤µ‡≥É‡≤¶‡≥ç‡≤ß‡≤ø** | Foundry Local | ‡≤µ‡≥á‡≤ó‡≤¶ ‡≤™‡≥Å‡≤®‡≤∞‡≤æ‡≤µ‡≥É‡≤§‡≥ç‡≤§‡≤ø, ‡≤Ø‡≤æ‡≤µ‡≥Å‡≤¶‡≥á API ‡≤µ‡≥Ü‡≤ö‡≥ç‡≤ö‡≤µ‡≤ø‡≤≤‡≥ç‡≤≤ |
| **‡≤ó‡≥å‡≤™‡≥ç‡≤Ø‡≤§‡≥Ü-‡≤ó‡≤Ç‡≤≠‡≥Ä‡≤∞** | Foundry Local | ‡≤°‡≥á‡≤ü‡≤æ ‡≤∏‡≤æ‡≤ß‡≤®‡≤¶‡≤ø‡≤Ç‡≤¶ ‡≤π‡≥ä‡≤∞‡≤ó‡≥Ü ‡≤π‡≥ã‡≤ó‡≥Å‡≤µ‡≥Å‡≤¶‡≤ø‡≤≤‡≥ç‡≤≤ |
| **‡≤π‡≥Ü‡≤ö‡≥ç‡≤ö‡≥Å ‡≤™‡≥ç‡≤∞‡≤Æ‡≤æ‡≤£‡≤¶ ‡≤â‡≤§‡≥ç‡≤™‡≤æ‡≤¶‡≤®‡≥Ü** | Azure OpenAI | ‡≤â‡≤§‡≥ç‡≤§‡≤Æ ‡≤µ‡≤ø‡≤∏‡≥ç‡≤§‡≤∞‡≤£‡≥Ü, ‡≤é‡≤Ç‡≤ü‡≤∞‡≥ç‚Äå‡≤™‡≥ç‡≤∞‡≥à‡≤∏‡≥ç SLA |
| **‡≤á‡≤§‡≥ç‡≤§‡≥Ä‡≤ö‡≤ø‡≤® ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≥Å** | Azure OpenAI | ‡≤π‡≥ä‡≤∏ ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø ‡≤¨‡≤ø‡≤°‡≥Å‡≤ó‡≤°‡≥Ü‡≤ó‡≤≥‡≤ø‡≤ó‡≥Ü ‡≤™‡≥ç‡≤∞‡≤µ‡≥á‡≤∂ |
| **‡≤Ü‡≤´‡≥ç‚Äå‡≤≤‡≥à‡≤®‡≥ç ‡≤Ö‡≤ó‡≤§‡≥ç‡≤Ø‡≤ó‡≤≥‡≥Å** | Foundry Local | ‡≤á‡≤Ç‡≤ü‡≤∞‡≥ç‡≤®‡≥Ü‡≤ü‡≥ç ‡≤Ö‡≤µ‡≤≤‡≤Ç‡≤¨‡≤®‡≥Ü ‡≤á‡≤≤‡≥ç‡≤≤ |
| **‡≤µ‡≥Ü‡≤ö‡≥ç‡≤ö-‡≤∏‡≤Ç‡≤µ‡≥á‡≤¶‡≤ø** | Foundry Local | ‡≤™‡≥ç‡≤∞‡≤§‡≤ø ‡≤ü‡≥ã‡≤ï‡≤®‡≥ç ‡≤∂‡≥Å‡≤≤‡≥ç‡≤ï‡≤µ‡≤ø‡≤≤‡≥ç‡≤≤ |

### ‡≤™‡≤∞‡≤ø‡≤∏‡≤∞ ‡≤ö‡≤∞‡≤ó‡≤≥ ‡≤§‡≥ç‡≤µ‡≤∞‡≤ø‡≤§ ‡≤â‡≤≤‡≥ç‡≤≤‡≥á‡≤ñ

```cmd
REM Foundry Local (default)
set MODEL=phi-4-mini
set BASE_URL=http://localhost:8000
set API_KEY=

REM Azure OpenAI (cloud)
set AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com
set AZURE_OPENAI_API_KEY=your-api-key
set AZURE_OPENAI_API_VERSION=2024-08-01-preview
set MODEL=your-deployment-name
```

### ‡≤∏‡≥Ü‡≤∑‡≤®‡≥ç 3: ‡≤ì‡≤™‡≤®‡≥ç-‡≤∏‡≥ã‡≤∞‡≥ç‡≤∏‡≥ç ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≤ø‡≤ó‡≥Ü ‡≤∏‡≤ø‡≤¶‡≥ç‡≤ß‡≤§‡≥Ü

1. **‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø ‡≤ï‡≥ç‡≤Ø‡≤æ‡≤ü‡≤≤‡≤æ‡≤ó‡≥ç ‡≤Ö‡≤®‡≥ç‡≤µ‡≥á‡≤∑‡≤£‡≥Ü**: Foundry Local ‡≤®‡≤≤‡≥ç‡≤≤‡≤ø ‡≤≤‡≤≠‡≥ç‡≤Ø‡≤µ‡≤ø‡≤∞‡≥Å‡≤µ ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≤ø‡≤∂‡≥Ä‡≤≤‡≤ø‡≤∏‡≤ø
2. **‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø ‡≤´‡≤æ‡≤∞‡≥ç‡≤Æ‡≥ç‡≤Ø‡≤æ‡≤ü‡≥ç‚Äå‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤Ö‡≤∞‡≥ç‡≤•‡≤Æ‡≤æ‡≤°‡≤ø‡≤ï‡≥ä‡≤≥‡≥ç‡≤≥‡≤ø**: ONNX, ‡≤ï‡≥ç‡≤µ‡≤æ‡≤Ç‡≤ü‡≥à‡≤ú‡≥Ü‡≤∑‡≤®‡≥ç ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤Ü‡≤™‡≥ç‡≤ü‡≤ø‡≤Æ‡≥à‡≤ú‡≥Ü‡≤∑‡≤®‡≥ç ‡≤¨‡≤ó‡≥ç‡≤ó‡≥Ü ‡≤§‡≤ø‡≤≥‡≤ø‡≤¶‡≥Å‡≤ï‡≥ä‡≤≥‡≥ç‡≤≥‡≤ø
3. **‡≤ï‡≤∏‡≥ç‡≤ü‡≤Æ‡≥ç ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤™‡≤∞‡≤ø‡≤ó‡≤£‡≤ø‡≤∏‡≤ø**: ‡≤°‡≥ä‡≤Æ‡≥á‡≤®‡≥ç-‡≤®‡≤ø‡≤∞‡≥ç‡≤¶‡≤ø‡≤∑‡≥ç‡≤ü ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø ‡≤Ö‡≤ó‡≤§‡≥ç‡≤Ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤Ø‡≥ã‡≤ö‡≤ø‡≤∏‡≤ø

### ‡≤∏‡≤Æ‡≤∏‡≥ç‡≤Ø‡≥Ü ‡≤™‡≤∞‡≤ø‡≤π‡≤æ‡≤∞ ‡≤§‡≥ç‡≤µ‡≤∞‡≤ø‡≤§ ‡≤Æ‡≤æ‡≤∞‡≥ç‡≤ó‡≤¶‡≤∞‡≥ç‡≤∂‡≤ø

**‡≤∏‡≤æ‡≤Æ‡≤æ‡≤®‡≥ç‡≤Ø ‡≤∏‡≤Æ‡≤∏‡≥ç‡≤Ø‡≥Ü‡≤ó‡≤≥‡≥Å:**
```cmd
REM Issue: Could not use Foundry SDK
pip install foundry-local-sdk

REM Issue: Connection refused
foundry service status
foundry model run phi-4-mini

REM Issue: Azure authentication failed
echo %AZURE_OPENAI_ENDPOINT%
echo %AZURE_OPENAI_API_KEY%

REM Issue: Model not found
foundry model list
curl http://localhost:8000/v1/models
```

## ‡≤â‡≤≤‡≥ç‡≤≤‡≥á‡≤ñ‡≤ó‡≤≥‡≥Å

- **[OpenAI Python SDK ‡≤°‡≤æ‡≤ï‡≥ç‡≤Ø‡≥Å‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥á‡≤∂‡≤®‡≥ç](https://github.com/openai/openai-python)**: ‡≤Ö‡≤ß‡≤ø‡≤ï‡≥É‡≤§ SDK ‡≤â‡≤≤‡≥ç‡≤≤‡≥á‡≤ñ
- **[Azure OpenAI ‡≤°‡≤æ‡≤ï‡≥ç‡≤Ø‡≥Å‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥á‡≤∂‡≤®‡≥ç](https://learn.microsoft.com/azure/ai-services/openai/)**: Azure OpenAI ‡≤∏‡≥á‡≤µ‡≤æ ‡≤Æ‡≤æ‡≤∞‡≥ç‡≤ó‡≤¶‡≤∞‡≥ç‡≤∂‡≤ø
- **[Foundry Local SDK ‡≤â‡≤≤‡≥ç‡≤≤‡≥á‡≤ñ](https://learn.microsoft.com/azure/ai-foundry/foundry-local/)**: ‡≤∏‡≥ç‡≤•‡≤≥‡≥Ä‡≤Ø ‡≤®‡≤ø‡≤∞‡≥ç‡≤£‡≤Ø ‡≤°‡≤æ‡≤ï‡≥ç‡≤Ø‡≥Å‡≤Æ‡≥Ü‡≤Ç‡≤ü‡≥á‡≤∂‡≤®‡≥ç
- **[‡≤∏‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Æ‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤™‡≥Ç‡≤∞‡≥ç‡≤£‡≤ó‡≥ä‡≤≥‡≤ø‡≤∏‡≥Å‡≤µ‡≤ø‡≤ï‡≥Ü ‡≤Æ‡≤æ‡≤∞‡≥ç‡≤ó‡≤¶‡≤∞‡≥ç‡≤∂‡≤ø](https://learn.microsoft.com/azure/ai-foundry/foundry-local/how-to/integrate-with-inference-sdks)**: ‡≤â‡≤®‡≥ç‡≤®‡≤§ ‡≤∏‡≥ç‡≤ü‡≥ç‡≤∞‡≥Ä‡≤Æ‡≤ø‡≤Ç‡≤ó‡≥ç ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≥Å
- **[‡≤®‡≤Æ‡≥Ç‡≤®‡≥Ü 01: OpenAI SDK ‡≤Æ‡≥Ç‡≤≤‡≤ï ‡≤§‡≥ç‡≤µ‡≤∞‡≤ø‡≤§ ‡≤ö‡≤æ‡≤ü‡≥ç](samples/01/README.md)**: ‡≤Æ‡≥Ç‡≤≤ ‡≤è‡≤ï‡≥Ä‡≤ï‡≤∞‡≤£ ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≥Å
- **[‡≤®‡≤Æ‡≥Ç‡≤®‡≥Ü 02: ‡≤â‡≤®‡≥ç‡≤®‡≤§ SDK ‡≤è‡≤ï‡≥Ä‡≤ï‡≤∞‡≤£](samples/02/README.md)**: ‡≤à ‡≤∏‡≥Ü‡≤∑‡≤®‡≥ç‚Äå‡≤® ‡≤ï‡≥à‡≤Ø‡≤≤‡≥ç‡≤≤‡≤ø ‡≤â‡≤¶‡≤æ‡≤π‡≤∞‡≤£‡≥Ü‡≤ó‡≤≥‡≥Å
- **[‡≤®‡≤Æ‡≥Ç‡≤®‡≥Ü 04: Chainlit ‡≤Ö‡≤™‡≥ç‡≤≤‡≤ø‡≤ï‡≥á‡≤∂‡≤®‡≥ç](samples/04/README.md)**: ‡≤µ‡≥Ü‡≤¨‡≥ç UI ‡≤Ö‡≤≠‡≤ø‡≤µ‡≥É‡≤¶‡≥ç‡≤ß‡≤ø
- **[‡≤®‡≤Æ‡≥Ç‡≤®‡≥Ü 05: ‡≤¨‡≤π‡≥Å-‡≤è‡≤ú‡≥Ü‡≤Ç‡≤ü‡≥ç ‡≤µ‡≥ç‡≤Ø‡≤µ‡≤∏‡≥ç‡≤•‡≥Ü‡≤ó‡≤≥‡≥Å](samples/05/README.md)**: ‡≤â‡≤®‡≥ç‡≤®‡≤§ ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ú‡≤®‡≤æ ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≥Å

‡≤®‡≥Ä‡≤µ‡≥Å ‡≤à‡≤ó ‡≤∏‡≥ç‡≤•‡≤≥‡≥Ä‡≤Ø ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤ï‡≥ç‡≤≤‡≥å‡≤°‡≥ç AI ‡≤∏‡≤æ‡≤Æ‡≤∞‡≥ç‡≤•‡≥ç‡≤Ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤∏‡≥å‡≤ï‡≤∞‡≥ç‡≤Ø‡≤µ‡≤æ‡≤ó‡≤ø ‡≤è‡≤ï‡≥Ä‡≤ï‡≤∞‡≤ø‡≤∏‡≥Å‡≤µ ‡≤®‡≤Ø‡≤µ‡≤æ‡≤¶ AI ‡≤Ö‡≤™‡≥ç‡≤≤‡≤ø‡≤ï‡≥á‡≤∂‡≤®‡≥ç‚Äå‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤®‡≤ø‡≤∞‡≥ç‡≤Æ‡≤ø‡≤∏‡≤≤‡≥Å ‡≤∏‡≤ú‡≥ç‡≤ú‡≤æ‡≤ó‡≤ø‡≤¶‡≥ç‡≤¶‡≥Ä‡≤∞‡≤ø, ‡≤™‡≥ç‡≤∞‡≤§‡≤ø ‡≤®‡≤ø‡≤∞‡≥ç‡≤¶‡≤ø‡≤∑‡≥ç‡≤ü ‡≤¨‡≤≥‡≤ï‡≥Ü ‡≤™‡≥ç‡≤∞‡≤ï‡≤∞‡≤£‡≤ï‡≥ç‡≤ï‡≥Ü ‡≤∏‡≤∞‡≤ø‡≤Ø‡≤æ‡≤¶ ‡≤™‡≥ç‡≤∞‡≤¶‡≤æ‡≤§‡≤∞‡≤®‡≥ç‡≤®‡≥Å ‡≤Ü‡≤Ø‡≥ç‡≤ï‡≥Ü‡≤Æ‡≤æ‡≤°‡≥Å‡≤µ ‡≤∏‡≥ç‡≤µ‡≤æ‡≤§‡≤Ç‡≤§‡≥ç‡≤∞‡≥ç‡≤Ø‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤í‡≤¶‡≤ó‡≤ø‡≤∏‡≥Å‡≤µ‡≤æ‡≤ó, ‡≤∏‡≤§‡≤§ ‡≤Ö‡≤≠‡≤ø‡≤µ‡≥É‡≤¶‡≥ç‡≤ß‡≤ø ‡≤Æ‡≤æ‡≤¶‡≤∞‡≤ø‡≤ó‡≤≥‡≤®‡≥ç‡≤®‡≥Å ‡≤ï‡≤æ‡≤™‡≤æ‡≤°‡≤ø‡≤ï‡≥ä‡≤≥‡≥ç‡≤≥‡≥Å‡≤§‡≥ç‡≤§‡≥Ä‡≤∞‡≤ø.

---

<!-- CO-OP TRANSLATOR DISCLAIMER START -->
**‡≤Ö‡≤∏‡≥ç‡≤µ‡≥Ä‡≤ï‡≤∞‡≤£**:  
‡≤à ‡≤¶‡≤∏‡≥ç‡≤§‡≤æ‡≤µ‡≥á‡≤ú‡≥Å AI ‡≤Ö‡≤®‡≥Å‡≤µ‡≤æ‡≤¶ ‡≤∏‡≥á‡≤µ‡≥Ü [Co-op Translator](https://github.com/Azure/co-op-translator) ‡≤¨‡≤≥‡≤∏‡≤ø ‡≤Ö‡≤®‡≥Å‡≤µ‡≤æ‡≤¶‡≤ø‡≤∏‡≤≤‡≤æ‡≤ó‡≤ø‡≤¶‡≥Ü. ‡≤®‡≤æ‡≤µ‡≥Å ‡≤®‡≤ø‡≤ñ‡≤∞‡≤§‡≥Ü‡≤Ø‡≤ø‡≤ó‡≤æ‡≤ó‡≤ø ‡≤™‡≥ç‡≤∞‡≤Ø‡≤§‡≥ç‡≤®‡≤ø‡≤∏‡≥Å‡≤§‡≥ç‡≤§‡≤ø‡≤¶‡≥ç‡≤¶‡≤∞‡≥Ç, ‡≤∏‡≥ç‡≤µ‡≤Ø‡≤Ç‡≤ö‡≤æ‡≤≤‡≤ø‡≤§ ‡≤Ö‡≤®‡≥Å‡≤µ‡≤æ‡≤¶‡≤ó‡≤≥‡≤≤‡≥ç‡≤≤‡≤ø ‡≤¶‡≥ã‡≤∑‡≤ó‡≤≥‡≥Å ‡≤Ö‡≤•‡≤µ‡≤æ ‡≤Ö‡≤∏‡≤§‡≥ç‡≤Ø‡≤§‡≥Ü‡≤ó‡≤≥‡≥Å ‡≤á‡≤∞‡≤¨‡≤π‡≥Å‡≤¶‡≥Å ‡≤é‡≤Ç‡≤¶‡≥Å ‡≤¶‡≤Ø‡≤µ‡≤ø‡≤ü‡≥ç‡≤ü‡≥Å ‡≤ó‡≤Æ‡≤®‡≤ø‡≤∏‡≤ø. ‡≤Æ‡≥Ç‡≤≤ ‡≤≠‡≤æ‡≤∑‡≥Ü‡≤Ø‡≤≤‡≥ç‡≤≤‡≤ø‡≤∞‡≥Å‡≤µ ‡≤Æ‡≥Ç‡≤≤ ‡≤¶‡≤∏‡≥ç‡≤§‡≤æ‡≤µ‡≥á‡≤ú‡≤®‡≥ç‡≤®‡≥Å ‡≤Ö‡≤ß‡≤ø‡≤ï‡≥É‡≤§ ‡≤Æ‡≥Ç‡≤≤‡≤µ‡≤æ‡≤ó‡≤ø ‡≤™‡≤∞‡≤ø‡≤ó‡≤£‡≤ø‡≤∏‡≤¨‡≥á‡≤ï‡≥Å. ‡≤Æ‡≤π‡≤§‡≥ç‡≤µ‡≤¶ ‡≤Æ‡≤æ‡≤π‡≤ø‡≤§‡≤ø‡≤ó‡≤æ‡≤ó‡≤ø, ‡≤µ‡≥É‡≤§‡≥ç‡≤§‡≤ø‡≤™‡≤∞ ‡≤Æ‡≤æ‡≤®‡≤µ ‡≤Ö‡≤®‡≥Å‡≤µ‡≤æ‡≤¶‡≤µ‡≤®‡≥ç‡≤®‡≥Å ‡≤∂‡≤ø‡≤´‡≤æ‡≤∞‡≤∏‡≥Å ‡≤Æ‡≤æ‡≤°‡≤≤‡≤æ‡≤ó‡≥Å‡≤§‡≥ç‡≤§‡≤¶‡≥Ü. ‡≤à ‡≤Ö‡≤®‡≥Å‡≤µ‡≤æ‡≤¶ ‡≤¨‡≤≥‡≤ï‡≥Ü‡≤Ø‡≤ø‡≤Ç‡≤¶ ‡≤â‡≤Ç‡≤ü‡≤æ‡≤ó‡≥Å‡≤µ ‡≤Ø‡≤æ‡≤µ‡≥Å‡≤¶‡≥á ‡≤§‡≤™‡≥ç‡≤™‡≥Å ‡≤Ö‡≤∞‡≥ç‡≤•‡≤Æ‡≤æ‡≤°‡≤ø‡≤ï‡≥Ü‡≥Ç‡≤≥‡≥ç‡≤≥‡≥Å‡≤µ‡≤ø‡≤ï‡≥Ü ‡≤Ö‡≤•‡≤µ‡≤æ ‡≤§‡≤™‡≥ç‡≤™‡≥Å ‡≤µ‡≤ø‡≤µ‡≤∞‡≤£‡≥Ü‡≤ó‡≤≥‡≤ø‡≤ó‡≥Ü ‡≤®‡≤æ‡≤µ‡≥Å ‡≤π‡≥ä‡≤£‡≥Ü‡≤ó‡≤æ‡≤∞‡≤∞‡≤æ‡≤ó‡≥Å‡≤µ‡≥Å‡≤¶‡≤ø‡≤≤‡≥ç‡≤≤.
<!-- CO-OP TRANSLATOR DISCLAIMER END -->